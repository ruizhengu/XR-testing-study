@inproceedings{10.1145/2669592.2669651,
author = {Reisman, Ronald J. and Feiner, Steven K. and Brown, David M.},
title = {Augmented reality tower technology flight test},
year = {2014},
isbn = {9781450325608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669592.2669651},
doi = {10.1145/2669592.2669651},
abstract = {Augmented reality technology adapted for air traffic control tower applications was used to track an OH-58C helicopter in proximity to an airport. A camera and 'see-through' display system was used to measure the registration error of static airport features and dynamic test aircraft. The observed registration errors of the test aircraft were largely attributable to two terms of error: 1) aircraft surveillance transport latency, and 2) registration error (from all sources) of the static environment. Compensating for registration errors of static objects and modeling aircraft movement reduces registration errors for dynamic (aircraft) objects to ≤2° of error for aircraft-surveillance transport latency ≤ 5 seconds, and to ≤1° of error for transport latency ≤ 2 seconds.},
booktitle = {Proceedings of the International Conference on Human-Computer Interaction in Aerospace},
articleno = {8},
numpages = {8},
keywords = {ADS-B, air traffic control towers, augmented reality, dynamic registration, flight test, head mounted display, static registration},
location = {Santa Clara, California},
series = {HCI-Aero '14}
}

@inproceedings{10.1145/3552327.3552337,
author = {Liinasuo, Marja and Kuula, Timo and Goriachev, Vladimir and Helin, Kaj},
title = {Remote Testing of an Augmented Reality System},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552327.3552337},
doi = {10.1145/3552327.3552337},
abstract = {Remote test settings have become more common due to COVID-19. Our paper presents two user tests focusing on the usability and user experience of an augmented reality-based solution, i.e., augmented reality system. We describe the proceeding of the tests from the perspective of what party has participated in the test in the same location as the test participant, i.e., locally, and what party remotely. The importance - or unimportance - of physical presence is contemplated from the perspective of the successfulness of the test. The physical presence of a person providing technical support to the test participant during the testing proved vital for the augmented reality related testing; the location of other test organisers appears more indifferent in this context.},
booktitle = {Proceedings of the 33rd European Conference on Cognitive Ergonomics},
articleno = {3},
numpages = {4},
keywords = {Augmented reality, Testing methodology, Usability, User experience},
location = {<conf-loc>, <city>Kaiserslautern</city>, <country>Germany</country>, </conf-loc>},
series = {ECCE '22}
}

@inproceedings{10.1145/3473856.3474034,
author = {Peintner, Jakob and Funk Drechsler, Maikol and Reway, Fabio and Seifert, Georg and Huber, Werner and Riener, Andreas},
title = {Mixed Reality Environment for Complex Scenario Testing},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3474034},
doi = {10.1145/3473856.3474034},
abstract = {Driver assistance systems are currently evaluated using standardized test procedures as defined, for example, by Euro NCAP, but which only represent a narrow, idealized spectrum of real-life situations. Test conditions hardly take into account the variability that occur in everyday traffic – adverse weather conditions such as fog, rain, snow or darkness are typically not considered, nor are the resulting changes in, e.g., the behavior of pedestrians. In an emergency braking situation, the behavior of the VRU can also be directly affected by the interaction with the vehicle. For example, a pedestrian might be alarmed by an approaching car. This could elicit a very different response in the human that cannot be simulated in the dummy motion as defined in the standard test procedure. In order to create a more versatile and interactive simulation and test environment, we have developed the Mixed Reality Test Environment MiRE. The goal of Mixed Reality Test Environment MiRE is to enable testing of a broader range of scenarios, including interaction between VRUs and vehicles. During the demo, we will demonstrate MiRE live to interested visitors and show its full potential for future driving safety tests.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {605–608},
numpages = {4},
keywords = {Automated Driving Systems, Mixed Reality, Test Procedures, Vehicle in the Loop},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1145/3603555.3608543,
author = {T\"{o}lgyesi, Borb\'{a}la and Bakk, \'{A}gnes Karolina and Bark\'{o}czi, M\'{a}t\'{e} and Buri, Bal\'{a}zs and Szab\'{o}, Andr\'{a}s and Tobai, Botond and Sadie, Bonita and Cserj\'{e}si, Ren\'{a}ta and Georgieva, Iva and Roth, Christian},
title = {"Virtual reality nature as our next retreat?": User experience testing of a simulated natural environment in virtual reality},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3608543},
doi = {10.1145/3603555.3608543},
abstract = {In this article, we present the initial user experience test results of an experiment conducted in an interactive natural environment created in virtual reality (VR). The computer-generated (CG) VR application, entitled Zenctuary VR, was designed with the intention of giving the users a pleasant experience with the benefits of easing anxiety symptoms, boosting positive feelings and lowering negative ones, thus having a restorative effect. For this, we designed a graphically high-resolution, natural environment with playful interactions. The testing was conducted with adults diagnosed with attentional deficit and hyperactivity disorder (ADHD) and with a neurotypical control group in order to investigate whether the ADHD group experiences the app differently from the latter group and to enhance its effectiveness specifically for individuals with ADHD. While the assessed scales for presence, immersion, feeling of flow, gained skills, usability, and judgment did not differ between the two groups, overall their evaluations of the experience and the software application were favorable. We also gathered insightful comments on the application's shortcomings, which will enable us to improve it and make it more enjoyable and ultimately more beneficial for the users.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {448–453},
numpages = {6},
keywords = {interactive virtual reality, nature environment, restorative effect, user experience},
location = {<conf-loc>, <city>Rapperswil</city>, <country>Switzerland</country>, </conf-loc>},
series = {MuC '23}
}

@inproceedings{10.5555/850976.854950,
author = {Geiger, C. and Paelke, V. and Reimann, C. and Rosenbach, W. and Stoecklein, J.},
title = {Testable Design Representations for Mobile Augmented Reality Authoring},
year = {2002},
isbn = {0769517811},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper applies the idea of a continuously testable design representation to authoring of augmented realities for mobile devices.},
booktitle = {Proceedings of the 1st International Symposium on Mixed and Augmented Reality},
pages = {281},
keywords = {Authoring Augmented Realities, Prototyping},
series = {ISMAR '02}
}

@inproceedings{10.1145/3286978.3287028,
author = {Amano, Tatsuya and Kajita, Shugo and Yamaguchi, Hirozumi and Higashino, Teruo and Takai, Mineo},
title = {Smartphone Applications Testbed Using Virtual Reality},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287028},
doi = {10.1145/3286978.3287028},
abstract = {Due to the nature of smartphones' portability and mobility, many mobile apps are usually utilized in the real field environment using GPS, Wi-Fi and embedded sensors. For example, any navigation app uses GPS and Wi-Fi to locate the user in the map, and streaming apps may be used in cafeteria or even outside to satisfy the users' demand to watch soccer games anywhere and anytime. To test the usability and performance of such mobile apps in in-situ environment, we need to bring the apps to such physical world and run (a number of) test scenarios, which is often cost-inefficient depending on the size, apps and situations assumed in those scenarios. In this paper, we design and develop a testbed to test mobile apps in VR space. The system allows developers to use a real smartphone in VR and to test and evaluate their apps at the interested locations, with various network environment. The system builds and reproduces the real world environment of 3D space and real networks in the VR environment, using the existing 3D city models and our original Wi-Fi database. Then it enables to real-timely integrate the screen of the VR user's smartphone in the VR space. The user can operate the app via the VR view, and test the usability and performance of the app in such an emulated environment. The experimental result shows our architecture could achieve such cyber-physical integration with 695.5 ms delay, which is negligible in many semi-realtime services such as navigations.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {422–431},
numpages = {10},
keywords = {QoE, Smartphoe Application, VR, Wi-Fi},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.5555/2525493.2525500,
author = {Irlitti, Andrew and Von Itzstein, Stewart},
title = {Validating constraint driven design techniques in spatial augmented reality},
year = {2013},
isbn = {9781921770241},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {We describe new techniques to allow constraint driven design using spatial augmented reality (SAR), using projectors to animate a physical prop. The goal is to bring the designer into the visual working space, interacting directly with a dynamic design, allowing for intuitive interactions, while gaining access to affordance through the use of physical objects. We address the current industrial design process, expressing our intended area of improvement with the use of SAR. To corroborate our hypothesis, we have created a prototype system, which we have called SARventor. Within this paper, we describe the constraint theory we have applied, the interaction techniques devised to help illustrate our ideas and goals, and finally the combination of all input and output tasks provided by SARventor.To validate the new techniques, an evaluation of the prototype system was conducted. The results of this evaluation indicated promises for a system allowing a dynamic design solution within SAR. Design experts see potential in leveraging SAR to assist in the collaborative process during industrial design sessions, offering a high fidelity, transparent application, presenting an enhanced insight into critical design decisions to the projects stakeholders. Through the rich availability of affordance in SAR, designers and stakeholders have the opportunity to see first-hand the effects of the proposed design while considering both the ergonomic and safety requirements.},
booktitle = {Proceedings of the Fourteenth Australasian User Interface Conference - Volume 139},
pages = {63–72},
numpages = {10},
keywords = {industrial design process, spatial augmented reality, tangible user interface},
location = {Melbourne, Australia},
series = {AUIC '13}
}

@inproceedings{10.1145/769953.769966,
author = {Bierbaum, Allen and Hartling, Patrick and Cruz-Neira, Carolina},
title = {Automated testing of virtual reality application interfaces},
year = {2003},
isbn = {1581136862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/769953.769966},
doi = {10.1145/769953.769966},
abstract = {We describe a technique for supporting testing of the interaction aspect of virtual reality (VR) applications. Testing is a fundamental development practice that forms the basis of many software engineering methodologies. It is used to ensure the correct behavior of applications. Currently, there is no common pattern for automated testing of VR application interaction. We review current software engineering practices used in testing and explore how they may be applied to the specific realm of VR applications. We then discuss the ways in which current practices are insuficient to test VR application interaction and propose a testing architecture for addressing the problems. We present an implementation of the design written on top of the VR Juggler platform. This system allows VR developers to employ standard software engineering techniques that require automated testing methods.},
booktitle = {Proceedings of the Workshop on Virtual Environments 2003},
pages = {107–114},
numpages = {8},
keywords = {VR Juggler, extreme programming, unit testing},
location = {Zurich, Switzerland},
series = {EGVE '03}
}

@inproceedings{10.1145/3473682.3481878,
author = {Funk Drechsler, Maikol and Peintner, Jakob Benedikt and Seifert, Georg and Huber, Werner and Riener, Andreas},
title = {Mixed Reality Environment for Testing Automated Vehicle and Pedestrian Interaction},
year = {2021},
isbn = {9781450386418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473682.3481878},
doi = {10.1145/3473682.3481878},
abstract = {The test and development of Automated Driving Systems is usually realized by scenario based testing or virtual testing environments. These methods apply artificial targets to trigger the safety critical functions under specific predefined scenarios, as the NCAP or IIHS test catalogues. Despite having a good reproducibility, these approaches hardly permit the evaluation of new interaction concepts like external Human-Machine Interfaces (eHMIs), since the interaction between real users and the vehicle cannot be realistic reproduced without risks to the participants. The novel Mixed Reality Test Environment (MiRE) overcomes this limitation by the integration of Virtual Reality (VR) technologies, Dynamic Vehicle-in-the-Loop (DynViL) and the Virtual Environment. In MiRE the movement and positioning of the vehicles and the VRUs are tracked in real time and reproduced in the virtual environment. Synthetic data from the virtual environment is generated to stimulate the vehicle and the human participant, enabling a safe interaction between both entities.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {229–232},
numpages = {4},
keywords = {Automated Driving Systems, External Human-Machine Interfaces, Sensor stimulation, Test Procedures, Vehicle-in-the-Loop},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21 Adjunct}
}

@inproceedings{10.1145/3573381.3597215,
author = {Bakk, \'{A}gnes Karolina and T\"{o}lgyesi, Borb\'{a}la and Bark\'{o}czi, M\'{a}t\'{e} and Buri, Bal\'{a}zs and Szab\'{o}, Andr\'{a}s and Tobai, Botond and Georgieva, Iva and Roth, Christian},
title = {Zenctuary VR: Simulating Nature in an Interactive Virtual Reality Application: Description of the design process of creating a garden in Virtual Reality with the aim of testing its restorative effects.},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597215},
doi = {10.1145/3573381.3597215},
abstract = {In this paper we present the design process of a virtual reality experience the aim of which is to have a restorative effect on users. In the simulated natural site, the user can interact with some elements of the environment and can also explore the view. We describe how we tried to create a more realistic sense of nature by relying on high quality graphics, the use of free-roaming space, and naturalistic interactions. During the design process we avoided gameful interactions and instead created playful interactions, while also relying on the multimodal aspect of the virtual reality technology.&nbsp;},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {165–169},
numpages = {5},
keywords = {immersive virtual reality, nature simulation&nbsp;, restorative effect of nature, virtual restorative environments},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/2970930.2970946,
author = {Langlois, Sabine and That, Thomas NGuyen and Mermillod, Pierre},
title = {Virtual Head-up Displays for Augmented Reality in Cars: a User Testing to Validate the Congruence},
year = {2016},
isbn = {9781450342445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970930.2970946},
doi = {10.1145/2970930.2970946},
abstract = {Head-up displays (HUD) permit augmented reality (AR) information in cars. Simulation is a convenient way to design and evaluate the benefit of such innovation for the driver. For this purpose, we have developed a virtual HUD that we compare to real AR HUDs from depth perception features. User testing was conducted with 24 participants in a stereoscopic driving simulator. It showed the ability of the virtual HUD to reproduce the perception of the distance between real objects and their augmentation. Three AR overlay designs to highlight the car ahead were compared: the trapezoid shape was perceived as more congruent that the U shape overlay.},
booktitle = {Proceedings of the European Conference on Cognitive Ergonomics},
articleno = {15},
numpages = {8},
keywords = {Automotive head-up display, augmented reality, depth perception, distance warning, human machine interface, simulation, virtual environment},
location = {Nottingham, United Kingdom},
series = {ECCE '16}
}

@article{10.1145/3626238,
author = {Bartlett, Kristin A. and Palacios-Ib\'{a}\~{n}ez, Almudena and Camba, Jorge Dorribo},
title = {Design and Validation of a Virtual Reality Mental Rotation Test},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1544-3558},
url = {https://doi.org/10.1145/3626238},
doi = {10.1145/3626238},
abstract = {Mental rotation, a common measure of spatial ability, has traditionally been assessed through paper-based instruments like the Mental Rotation Test (MRT) or the Purdue Spatial Visualization Test: Rotations (PSVT:R). The fact that these instruments present 3D shapes in a 2D format devoid of natural cues like shading and perspective likely limits their ability to accurately assess the fundamental skill of mentally rotating 3D shapes. In this paper, we describe the Virtual Reality Mental Rotation Assessment (VRMRA), a virtual reality-based mental rotation assessment derived from the Revised PSVT:R and MRT. The VRMRA reimagines traditional mental rotation assessments in a room-scale virtual environment and uses hand-tracking and elements of gamification in attempts to create an intuitive, engaging experience for test-takers. To validate the instrument, we compared response patterns in the VRMRA with patterns observed on the MRT and Revised PSVT:R. For the PSVT:R-type questions, items requiring a rotation around two axes were significantly harder than items requiring rotations around a single axis in the VRMRA, which is not the case in the Revised PSVT:R. For the MRT-type questions in the VRMRA, a moderate negative correlation was found between the degree of rotation in the X direction and item difficulty. While the problem of occlusion was reduced, features of the shapes and distractors accounted for 50.6\% of the variance in item difficulty. Results suggest that the VRMRA is likely a more accurate tool to assess mental rotation ability in comparison to traditional instruments which present the stimuli through 2D media. Our findings also point to potential problems with the fundamental designs of the Revised PSVT:R and MRT question formats.},
journal = {ACM Trans. Appl. Percept.},
month = {jan},
articleno = {5},
numpages = {22},
keywords = {Spatial ability, mental rotation, virtual reality, visual perception, MRT, PSVT:R}
}

@inproceedings{10.1145/3505284.3532988,
author = {Mevlevio\u{g}lu, Deniz and Tabirca, Sabin and Murphy, David},
title = {Emotional Virtual Reality Stroop Task: an Immersive Cognitive Test},
year = {2022},
isbn = {9781450392129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505284.3532988},
doi = {10.1145/3505284.3532988},
abstract = {Stroop Colour-Word Task has been widely used as a cognitive task. There are computerised and Virtual Reality versions of this task that are commonly used. The emotional version of the task, called the Emotional Stroop Colour-Word task is commonly used to induce certain emotions in a person. We are developing an application that brings the Emotional Stroop Colour-Word task into Virtual Reality. The aim of this application is to elicit different stress levels on the user and to record associated brain, heart and skin activity using wearable sensors. It is an immersive application that includes a tutorial, artificial intelligence generated audio instructions and a logging system for the user activity.},
booktitle = {Proceedings of the 2022 ACM International Conference on Interactive Media Experiences},
pages = {387–390},
numpages = {4},
keywords = {Stroop color-word test, VR, anxiety, biosensors, biosignals, emotional Stroop},
location = {Aveiro, JB, Portugal},
series = {IMX '22}
}

@inproceedings{10.1145/3359996.3364247,
author = {Gradl, Stefan and Wirth, Markus and M\"{a}chtlinger, Nico and Poguntke, Romina and Wonner, Andrea and Rohleder, Nicolas and Eskofier, Bjoern M.},
title = {The Stroop Room: A Virtual Reality-Enhanced Stroop Test},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359996.3364247},
doi = {10.1145/3359996.3364247},
abstract = {The Stroop Test is a well known and regularly employed stressor in laboratory research. In contrast to other methods, it is not based on fear of physical harm or social shame. Consequently, it is more likely accepted by a wide population. In our always-on, technology-driven, social-media centered world, large-scale in-field stress research will need adequate experimental tools to explore the increasing prevalence of stress-related diseases without bringing subjects into laboratories. This is why we designed the Stroop Room: A virtual reality-based adaptation of the Stroop Test using elements of the virtual world to extend the demands of the original test and at the same time make it easily accessible. It is open source and can be used and improved by anyone as an in-the-wild, repeatable, laboratory-quality stressor. In this work, the method is presented and an evaluation study described, to demonstrate its effectiveness in provoking cognitive stress. 16 male and 16 female subjects were tested in the Stroop Room while recording the electrocardiogram, electrodermal activity, saliva based cortisol and alpha-amylase, performance metrics and an array of questionnaire-based assessments regarding psychological confounders, stress state and likability of the simulation. Our results show that the Stroop Room increases heart rate on average by 19\%, other heart rate variability time-domain parameters (RMSSD, pNN50) decrease by 24\%-47\%, and its most stress-correlated frequency-parameter (LF/HF) increases by 107\%. Skin conductance (SC) level increases by 63\% and non-specific SC responses by 135\% on average. Salivary cortisol and alpha-amylase concentrations increase significantly in some specific conditions. Compared to related work using the Stroop Test, this is an improvement for some metrics by around 30\%-40\%. Questionnaire evaluation show a strong engagement of users with the simulation and some aspects of a flow-induction. These findings support the effectiveness of a Stroop Test involving 3-dimensional interactivity and thus the Stroop Room demonstrates how this can be applied in a playful interaction that could be used pervasively.},
booktitle = {Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {12},
keywords = {EDA, HRV, amylase, cortisol, heart rate, psychological stress, stroop test, virtual reality},
location = {Parramatta, NSW, Australia},
series = {VRST '19}
}

@inproceedings{10.1145/3576914.3588016,
author = {Ledgerwood, Scott and Lewis, Jack and Karhoff, Jeffrey and Zhu, Qi and Whitlock, Matthew and Chelen, Julia},
title = {The Technical Development of an Extended Reality Research Testbed for Public Safety},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3588016},
doi = {10.1145/3576914.3588016},
abstract = {The study of public safety technology, interventions, and training involves variable and hazardous conditions, which complicate observation and measurement. Informative evaluation approaches require reasonable representation of these conditions. Nonetheless, representing these conditions is resource intensive and difficult to replicate. For these reasons, public safety research may be limited by low-fidelity approaches that differ from the intended real-world application and by the inaccessibility of more realistic training. Extended Reality (XR) environments offer highly immersive and repeatable training for first responders, as well as controlled methods for technical research. In this paper, we discuss the development of the National Institute of Standards and Technology (NIST) Public Safety Immersive Test Center (PSITC): a testbed for multi-sensory extended reality-based research for public safety scenarios. We describe how the PSITC supports realistic training for public safety with an overview of the center’s design, development, and technical implementation. We discuss methods to address the challenges of building such a testbed for XR-based research, including integration of nascent technologies from various vendors, extensive use of sensor and imaging technologies, and intergovernmental cooperation between the First Responder Network Authority and the NIST Public Safety Communications Research Division. This paper introduces a model for the development of immersive centers built to evaluate prototypes for public safety operations, improve training for emergency response, and support public safety technology research.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {292–296},
numpages = {5},
keywords = {Emergency Response, Haptics, Human-Centered Computing, Mixed / Extended / Augmented Reality, Optics, Public Safety, Training, User Experience, Virtual Reality},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {CPS-IoT Week '23}
}

@inproceedings{10.1145/3551349.3561160,
author = {Rafi, Tahmid and Zhang, Xueling and Wang, Xiaoyin},
title = {PredART: Towards Automatic Oracle Prediction of Object Placements in Augmented Reality Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561160},
doi = {10.1145/3551349.3561160},
abstract = {While the emerging Augmented Reality (AR) technique allows a lot of new application opportunities, from education and communication to gaming, current augmented apps often have complaints about their usability and/or user experience due to placement errors of virtual objects. Therefore, identifying noticeable placement errors is an important goal in the testing of AR apps. However, placement errors can only be perceived by human beings and may need to be confirmed by multiple users, making automatic testing very challenging. In this paper, we propose PredART, a novel approach to predict human ratings of virtual object placements that can be used as test oracles in automated AR testing. PredART is based on automatic screenshot sampling, crowd sourcing, and a hybrid neural network for image regression. The evaluation on a test set of 480 screenshots shows that our approach can achieve an accuracy of 85.0\% and a mean absolute error, mean squared error, and root mean squared error of 0.047, 0.008, and 0.091, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {77},
numpages = {13},
keywords = {Augmented Reality, Placement Error, Virtual Objects},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@article{10.1145/3311748,
author = {Roberto, Pierdicca and Emanuele, Frontoni and Primo, Zingaretti and Adriano, Mancini and Jelena, Loncarski and Marina, Paolanti},
title = {Design, Large-Scale Usage Testing, and Important Metrics for Augmented Reality Gaming Applications},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3311748},
doi = {10.1145/3311748},
abstract = {Augmented Reality (AR) offers the possibility to enrich the real world with digital mediated content, increasing in this way the quality of many everyday experiences. While in some research areas such as cultural heritage, tourism, or medicine there is a strong technological investment, AR for game purposes struggles to become a widespread commercial application. In this article, a novel framework for AR kid games is proposed, already developed by the authors for other AR applications such as Cultural Heritage and Arts. In particular, the framework includes different layers such as the development of a series of AR kid puzzle games in an intermediate structure which can be used as a standard for different applications development, the development of a smart configuration tool, together with general guidelines and long-life usage tests and metrics. The proposed application is designed for augmenting the puzzle experience, but can be easily extended to other AR gaming applications. Once the user has assembled the real puzzle, AR functionality within the mobile application can be unlocked, bringing to life puzzle characters, creating a seamless game that merges AR interactions with the puzzle reality. The main goals and benefits of this framework can be seen in the development of a novel set of AR tests and metrics in the pre-release phase (in order to help the commercial launch and developers), and in the release phase by introducing the measures for long-life app optimization, usage tests and hint on final users together with a measure to design policy, providing a method for automatic testing of quality and popularity improvements. Moreover, smart configuration tools, as part of the general framework, enabling multi-app and eventually also multi-user development, have been proposed, facilitating the serialization of the applications. Results were obtained from a large-scale user test with about 4 million users on a set of eight gaming applications, providing the scientific community a workflow for implicit quantitative analysis in AR gaming. Different data analytics developed on the data collected by the framework prove that the proposed approach is affordable and reliable for long-life testing and optimization.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
articleno = {41},
numpages = {18},
keywords = {Augmented reality, gaming framework, large scale testing, mobile gaming, puzzle}
}

@inproceedings{10.1145/3543407.3543412,
author = {Liu, Qinghe and Kong, Lingming and Zhang, Guanzhe and Zhao, Lijun},
title = {Application of Mixed Reality in Driverless Vehicles Technology Courses Testing and Teaching Activities},
year = {2022},
isbn = {9781450396790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543407.3543412},
doi = {10.1145/3543407.3543412},
abstract = {At present, driverless vehicles technology has become the most concerned hot spot in the automotive field, and college students' learning enthusiasm in this field is unprecedented. However, the existing teaching and testing methods and resources are far from meeting the requirements. The main difficulties are (1) it is difficult to realize the traffic scene of driverless vehicles technology; (2) Using real vehicle experiment is prone to collision, resulting in loss of personnel and property. In this paper, a technical scheme of constructing driverless vehicles technology testing and teaching environment in the laboratory by using mixed reality technology is proposed. The complex traffic scene is realized in the simulation software by using virtual reality technology, and the driverless vehicles adopts the real scaling-down automatic chassis model. Through hardware in the loop simulation, the mixture of virtual traffic environment and the actual reality of driverless vehicle model is realized. This method has the advantages of rich traffic scenes and flexible adjustment, and the collision between real vehicles and virtual vehicles will not cause actual property damage.},
booktitle = {Proceedings of the 4th International Conference on Modern Educational Technology},
pages = {28–32},
numpages = {5},
keywords = {Driverless vehicles Technologies, Mixed Reality, Mixture of Virtual and Reality, Teaching and Testing},
location = {Macau, China},
series = {ICMET '22}
}

@inproceedings{10.1145/3565387.3565448,
author = {Wang, Peng and Hu, Yaoguang and Yang, Xiaonan and Wang, Jingfei},
title = {Augmented Reality-Based Rapid Digital Verification of the Body-in-white for Intelligent Manufacturing},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565448},
doi = {10.1145/3565387.3565448},
abstract = {The body-in-white (BIW) is the structural assembly of the automobile after it is welded, which is the skeleton of the automobile. BIW design verification is an important part of the automobile development, not only to test the structural performance, but also to verify its manufacturability in subsequent production lines. Augmented reality, as an important tool for rapid digital verification, has been widely used in intelligent manufacturing systems. This paper presents an augmented reality-based method and associated system for rapid digital verification of BIW manufacturability during the BIW design phase. The system approach is described: 1) AR based auto BIW verification environment construction, 2) Bare hand interaction assisted AR verification, 3) Distance measurement and collision detection. Finally, an experimental scenario was built to illustrate how this AR system assist in verifying BIW structures on a production line and reveal the huge potential of AR in product design verification.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {61},
numpages = {5},
keywords = {Augmented reality, Body-in-white, Digital verification},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.4108/icst.pervasivehealth.2013.252359,
author = {Pedroli, Elisa and Cipresso, Pietro and Serino, Silvia and Riva, Giuseppe and Albani, Giovanni and Riva, Giuseppe},
title = {A virtual reality test for the assessment of cognitive deficits: usability and perspectives},
year = {2013},
isbn = {9781936968800},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2013.252359},
doi = {10.4108/icst.pervasivehealth.2013.252359},
abstract = {The assessment of cognitive deficits, such as executive functions, can be critical in neuropsychology because is not easy to catch all the important cues of real-life behavior using the classic paper and pensil tests or even sophisticated laboratory tasks. On the other hand, using virtual simulations of prototypical daily situations, by the means of 3D virtual environments, it is possible to create situations which could help the therapeutic assessment, within the safe context of the therapist's laboratory. In this study, we described how we assessing executive functions using a Virtual Reality Test.},
booktitle = {Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {453–458},
numpages = {6},
keywords = {VMET, assessment, cognitive deficits, psychometrics, usability, virtual reality test},
location = {Venice, Italy},
series = {PervasiveHealth '13}
}

@inproceedings{10.1145/3544549.3585860,
author = {Becker, Leonie and Nilsson, Tommy and Demedeiros, Paul and Rometsch, Flavie},
title = {Augmented Reality in Service of Human Operations on the Moon: Insights from a Virtual Testbed},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585860},
doi = {10.1145/3544549.3585860},
abstract = {Future astronauts living and working on the Moon will face extreme environmental conditions impeding their operational safety and performance. While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs) could potentially help mitigate some of these adversities, the applicability of AR in the unique lunar context remains underexplored. To address this limitation, we have produced an accurate representation of the lunar setting in virtual reality (VR) which then formed our testbed for the exploration of prospective operational scenarios with aerospace experts. Herein we present findings based on qualitative reflections made by the first 6 study participants. AR was found instrumental in several use cases, including the support of navigation and risk awareness. Major design challenges were likewise identified, including the importance of redundancy and contextual appropriateness. Drawing on these findings, we conclude by outlining directions for future research aimed at developing AR-based assistive solutions tailored to the lunar setting.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {43},
numpages = {8},
keywords = {astronaut, augmented reality, head-up display, human factors, human space flight, lunar exploration, virtual reality},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI EA '23}
}

@inproceedings{10.1145/3551349.3560510,
author = {Qin, Xue and Hassan, Foyzul},
title = {DyTRec: A Dynamic Testing Recommendation tool for Unity-based Virtual Reality Software},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560510},
doi = {10.1145/3551349.3560510},
abstract = {Virtual Reality (VR) technology has been utilized in other fields besides gaming, such as education, training, arts, shopping, and e-commerce. However, the technical support of VR software is not growing as fast as its market size, especially for testing. Because of the immersive feature that requires VR apps to act and react to all the interactions dynamically, the traditional static testing techniques such as unit test generation cannot fully guarantee the correctness of the tested functions. In this paper, we proposed a Dynamic Testing Recommendation tool (DyTRec) to suggest the potential types of dynamic testing for the target VR projects. Specifically, we categorize the dynamic testing types by analyzing the official APIs from the VR engine documentation and then apply the extracting and searching on all VR script files. We evaluated DyTRec on 20 VR projects and successfully reported 39 suggested results.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {227},
numpages = {5},
keywords = {Software Testing, Unity, Virtual Reality},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3597926.3598134,
author = {Rzig, Dhia Elhaq and Iqbal, Nafees and Attisano, Isabella and Qin, Xue and Hassan, Foyzul},
title = {Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598134},
doi = {10.1145/3597926.3598134},
abstract = {Virtual Reality (VR) is an emerging technique that provides a unique real-time experience for users. VR technologies have provided revolutionary user experiences in various scenarios (e.g., training, education, gaming, etc.). However, testing VR applications is challenging due to their nature which necessitates physical interactivity, and their reliance on specific hardware systems. Despite the recent advancements in VR technology and its usage scenarios, we still know little about VR application testing. To fill up this knowledge gap, we performed an empirical study on 314 open-source VR applications. Our analysis identified that 79\% of the VR projects evaluated did not have any automatic tests, and for the VR projects that did, the median functional-method to test-method ratios were lower than those of other project types. Moreover, we uncovered tool support issues concerning the measurement of VR code coverage, and the assertion density results we were able to generate were relatively low, with an average of 17.63\%. Finally, through a manual analysis of 370 test cases, we identified the different categories of test cases being used to validate VR application quality attributes. Furthermore, we extracted which of these categories are VR-attention, meaning that test writers need to pay special attention to VR characteristics when writing tests of these categories. We believe that our findings constitute a call to action for the VR development community to improve their automatic testing practices and provide directions for software engineering researchers to develop advanced techniques for automatic test case generation and test quality analysis for VR applications. Our replication package containing the dataset we used, software tools we developed, and the results we found, is accessible at ‍https://doi.org/10.6084/m9.figshare.19678938.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1269–1281},
numpages = {13},
keywords = {Empirical Study, Software Testing, Virtual Reality},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3603421.3603432,
author = {Gulrez, Tauseef and Kekoc, Vlado and Gaurvit, Emmanuel and Schuhmacher, Mark and Mills, Tony},
title = {Machine Learning Enabled Mixed Reality Systems - For Evaluation and Validation of Augmented Experience in Aircraft Maintenance},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603432},
doi = {10.1145/3603421.3603432},
abstract = {In this paper, a framework to assess the usability of mixed reality (MR) systems in terms of physical load metric is presented. The participants used a MR system for an aircraft maintenance task, while a computer vision (CV) based machine learning (ML) method extracted the skeletal data of the participants. A bi-manual musculoskeletal computational model of human upper-limb was overlayed onto the skeletal data to derive upper-limb muscle forces/energy spent during the aircraft maintenance tasks. The experiments were conducted with and without the use of MR devices, while CV based ML was applied to detect the energy used of aircraft maintainers in real-time while they performed tasks on an MRH90 helicopter. An analysis of augmented experience has been presented, which will act as a driver for the use of MR based applications in aircraft maintenance as well as battle-ready platforms. Preliminary results are encouraging and we envisage to use the proposed methodology for an objective assessment and detection of physical workload.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {77–83},
numpages = {7},
keywords = {Aircraft maintenance, Machine learning, Mixed reality, Physical load},
location = {<conf-loc>, <city>Sydney</city>, <country>Australia</country>, </conf-loc>},
series = {ICVARS '23}
}

@inproceedings{10.1145/3580585.3606282,
author = {von Sawitzky, Tamara and Himmels, Chantal and L\"{o}cken, Andreas and Grauschopf, Thomas and Riener, Andreas},
title = {Investigating Hazard Notifications for Cyclists in Mixed Reality: A Comparative Analysis with a Test Track Study},
year = {2023},
isbn = {9798400701054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580585.3606282},
doi = {10.1145/3580585.3606282},
abstract = {One way to improve road safety for cyclists is the development of hazard notification systems. Instead of in field experiments, such systems could be tested in safe and more controlled simulated environments; however, their validity needs verification. We evaluated the validity of mixed reality (MR) simulation for bicycle support systems notifying of dooring hazards. In a mixed-design study (N=43) with environment type(MR/test track) as within and hazard notifications (with/without) as between factor, comparing subjective and objective measures across environments. In conclusion, MR simulation is absolutely valid for user experience and perceived safety and relatively valid for workload, standard deviation of lateral position, and speed. However, MR simulation was not valid for lateral distance, as participants cycled more in the center of the street than on the test track, perhaps to avoid simulator sickness. Thus, we conclude that MR simulation is valuable for studying bicycle safety.},
booktitle = {Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {202–212},
numpages = {11},
keywords = {Cyclist Safety, Dooring, Hazard Notifications, Mixed Reality Study, Test Track Study, Validation},
location = {Ingolstadt, Germany},
series = {AutomotiveUI '23}
}

@inproceedings{10.1145/3613904.3642158,
author = {Qian, Xun and Wang, Tianyi and Xu, Xuhai and Jonker, Tanya R. and Todi, Kashyap},
title = {Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642158},
doi = {10.1145/3613904.3642158},
abstract = {Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user’s context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {951},
numpages = {17},
keywords = {Context-Aware Policy, Extended Reality, Unit Test, Validation},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{10.1145/3656650.3656665,
author = {O'Toole, Patrick and Mancini, Maurizio and Pitt, Ian},
title = {Sound and Colour: Evaluating Auditory-Visual Tests in Virtual Reality and Traditional Desktop Settings},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656665},
doi = {10.1145/3656650.3656665},
abstract = {In this pilot study, we used a balanced A/B testing method to evaluate how an environment—a Virtual Reality (VR) head-mounted display (HMD) versus a traditional desktop—affects the performance of 20 participants on a musical pitch-colour association test. We aimed to discern the influence of testing environments on musical pitch-colour associations. Our findings revealed no significant difference in performance between the VR HMD and the traditional desktop conditions. Previous studies highlight VR’s potential, but the results of our investigation—focusing on limited immersive and presence qualities—suggest that working in VR as opposed to a standard desktop environment has no significant influence on musical pitch-colour association test results. This outcome prompts a further investigation into other inherent VR characteristics, such as spatial audio, natural environment simulations, and interactive object manipulation, which could potentially enhance the effectiveness of auditory-visual association tests. This study is the first in a series of studies to explore how VR technologies can be exploited to augment multi-modal testing, with a fully immersive musical pitch-colour association test envisioned.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {21},
numpages = {5},
keywords = {audio, colour, cross-modal associations, synaesthesia, virtual reality, visuals},
location = {<conf-loc>, <city>Arenzano, Genoa</city>, <country>Italy</country>, </conf-loc>},
series = {AVI '24}
}

@article{10.5555/2835562.2835564,
author = {Seffah, Ahmed and Benn, Jonathan and Mammar, Halima Habieb},
title = {A low-cost test environment for usability studies of head-mounted virtual reality systems},
year = {2008},
issue_date = {February 2008},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {3},
number = {2},
abstract = {There is a need to develop new usability testing environments and methodologies for unconventional interactive systems. Pursuant to that need, we developed a low-cost test environment for a Head-Mounted Display (HMD)-based, virtual reality system called Osmose. Osmose was difficult to test for many reasons, one of which was its style of interaction. We began setting up the testing environment about two weeks before the start of the usability testing. We learned many lessons throughout the experience. This paper summarizes the study findings, both methodological -- how to setup and conduct a usability lab for such an environment -- as well as conceptual --the human experiences and behavioral patterns involved in using an immersive environment.},
journal = {J. Usability Studies},
month = {feb},
pages = {60–73},
numpages = {14},
keywords = {breathing interaction, head-mounted display, immersive systems, sensing-based interaction, usability testing, virtual reality}
}

@inproceedings{10.1145/3411764.3445216,
author = {Uriu, Daisuke and Obushi, Noriyasu and Kashino, Zendai and Hiyama, Atsushi and Inami, Masahiko},
title = {Floral Tribute Ritual in Virtual Reality: Design and Validation of SenseVase with Virtual Memorial},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445216},
doi = {10.1145/3411764.3445216},
abstract = {While floral tributes are commonly used for the public commemoration of victims of disasters, war, and other accidents, flowers in vases color everyday life. In this research, these features of flowers are intertwined with the recent phenomenon of online memorials to develop a virtual floral tribute concept that includes physical rituals. We designed SenseVase, a smart vase to detect flowers placed in it, and a 3DCG Virtual Memorial that illustrates floral tributes given by people using SenseVases at home. This paper describes how we developed our design concept by reviewing previous literature and social aspects, and presents a video illustrating the concept. To validate the current concept, we interviewed several experts knowledgeable in public commemorations, virtual and online communities, and the floral business. Through a discussion of our findings from the design process and interviews, we propose a new direction for how HCI technology can contribute to public commemoration in addition to personal memorialization.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {628},
numpages = {15},
keywords = {Commemoration, Death Rituals, Memorialization, Mourning, Online Memorial, Research through Design, Techno-spiritual Practices, Thanatosensitive Design},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI '21}
}

@inproceedings{10.1145/3587889.3588212,
author = {Obermair, Franz and Feichtenschlager, Hans-Peter},
title = {Human-Centered Assembly Process Validation in Virtual Reality using Tool-, Part- and Auxiliary Geometry Tracking},
year = {2023},
isbn = {9781450398527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587889.3588212},
doi = {10.1145/3587889.3588212},
abstract = {Virtual reality has become a common tool in vehicle development for both product and process development. For product improvement, design reviews are carried out in VR, for process improvement, assembly feasibility analyses. The involvement of developers, factory and production planners, as well as those who will be directly affected in the future, such as customers, assembly workers or maintenance staff, allows good products and mature processes to be designed at an early stage of development. In this article, human-centered improvement of product assembly and maintainability is considered in VR. In order to represent assembly conditions in VR as close to reality as possible, the use of tracked real tools, parts and auxiliary geometries is tested in addition to virtual representations. More realistic assemblability analyses could be achieved with the use of these tracked real tools, parts and auxiliary geometries, which provide passive haptic feedback. In the daily VR application for design reviews and assemblability analyses, the use of purely virtual parts and tools has proven itself for reasons of simplicity, for assembly training this passive haptic feedback has potential.},
booktitle = {Proceedings of the 2023 10th International Conference on Industrial Engineering and Applications},
pages = {283–288},
numpages = {6},
keywords = {Assemblability, Assembly, Auxiliary Teometry, Passive Haptic Feedback, Tool Tracking},
location = {Rome, Italy},
series = {ICIEAEU '23}
}

@inproceedings{10.1145/3411764.3445478,
author = {Mathis, Florian and Vaniea, Kami and Khamis, Mohamed},
title = {RepliCueAuth: Validating the Use of a Lab-Based Virtual Reality Setup for Evaluating Authentication Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445478},
doi = {10.1145/3411764.3445478},
abstract = {Evaluating novel authentication systems is often costly and time-consuming. In this work, we assess the suitability of using Virtual Reality (VR) to evaluate the usability and security of real-world authentication systems. To this end, we conducted a replication study and built a virtual replica of CueAuth [52], a recently introduced authentication scheme, and report on results from: (1) a lab-based in-VR usability study (N=20) evaluating user performance; (2) an online security study (N=22) evaluating system’s observation resistance through virtual avatars; and (3) a comparison between our results and those previously reported in the real-world evaluation. Our analysis indicates that VR can serve as a suitable test-bed for human-centred evaluations of real-world authentication schemes, but the used VR technology can have an impact on the evaluation. Our work is a first step towards augmenting the design and evaluation spectrum of authentication systems and offers ground work for more research to follow.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {534},
numpages = {18},
keywords = {Authentication, Research Method, Usable Security, Virtual Reality},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI '21}
}

@inproceedings{10.1145/3340764.3344435,
author = {Volkmann, Torben and Wessel, Daniel and Franke, Thomas and Jochems, Nicole},
title = {Testing the Social Presence Aspect of the Multimodal Presence Scale in a Virtual Reality Game},
year = {2019},
isbn = {9781450371988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340764.3344435},
doi = {10.1145/3340764.3344435},
abstract = {Presence is a key variable in virtual reality (VR), however, it is a complex variable consisting of different aspects. The Multimodal Presence Scale (MPS) by Makransky, Lilleholt, and Aaby was developed to measure physical, social and self-presence. But how well can it actually detect changes in one aspect of presence, and differentiate between different aspects of presence? To answer these questions, we use a German translation of the MPS in an experiment with a VR game with 45 participants. We examine social presence -the sense of being with another - specifically, and compare three conditions (abstraction levels) that should differ in the degree of social presence, but not in other aspects of presence. Results indicate that the MPS is reliable and useful for measuring social presence, while still correlating strongly with presence overall.},
booktitle = {Proceedings of Mensch Und Computer 2019},
pages = {433–437},
numpages = {5},
keywords = {HMD, measurement, presence, scale, virtual characters, virtual reality},
location = {Hamburg, Germany},
series = {MuC '19}
}

@inproceedings{10.1145/3607822.3618004,
author = {Liu, Jen-Shuo and Wang, Chongyang and Tversky, Barbara and Feiner, Steven},
title = {A Testbed for Exploring Virtual Reality User Interfaces for Assigning Tasks to Agents at Multiple Sites},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3618004},
doi = {10.1145/3607822.3618004},
abstract = {In virtual reality (VR) teleoperation and remote task guidance, a remote user may need to assign tasks to local technicians or robots at multiple sites. We are interested in scenarios where the user works with one site at a time, but must maintain awareness of the other sites for future intervention. We present an instrumented VR testbed for exploring how different spatial layouts of site representations impact user performance. In addition, we investigate ways of supporting the remote user in handling errors and interruptions from sites other than the one with which they are currently working, and switching between sites. We conducted a pilot study and explored how these factors affect user performance.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {35},
numpages = {2},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {SUI '23}
}

@inproceedings{10.1145/3489849.3489915,
author = {Jangid, Vishal and Kongsilp, Sirisilp},
title = {Fishtank Sandbox: A Software Framework for Collaborative Usability Testing of Fish Tank Virtual Reality Interaction Techniques},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489915},
doi = {10.1145/3489849.3489915},
abstract = {Human-computer interaction researchers have been studying how we can interact with virtual objects in a virtual environment efficiently. Many usability experiments do not have the same control parameters. The lack of consistency makes comparing different interaction techniques difficult. In this article, we present a software framework for usability study in FTVR interaction techniques. The software framework provides fixed control parameters (e.g., task, graphic settings, and measuring parameters), the ability for other researchers to incorporate their interaction techniques as an add-on, and enabling individuals to participate in the experiment over the internet. The article explores a new way for VR/AR researchers to approach usability experiments using the framework and discuss the challenges that it brings.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {70},
numpages = {3},
keywords = {Fish Tank Virtual Reality, Framework, Usability Testing},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3644116.3644208,
author = {Chen, Yali and Zhu, Ruiyan and Xiong, Wei},
title = {Investigating the Efficacy of Virtual Reality Forest Meditation as an Intervention for Test Anxiety among College Students},
year = {2024},
isbn = {9798400708138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644116.3644208},
doi = {10.1145/3644116.3644208},
abstract = {The primary objective of this study is to verify the effectiveness of virtual reality forest meditation as an intervention for test anxiety among college students and to investigate the impact of three factors: namely forest style, meditation state, and pranayama guidance, on the intervention effect. A total of thirty participants exhibiting moderate or higher levels of test anxiety were subjected to an induction of test anxiety followed by a virtual reality forest meditation intervention comprising various combinations of the aforementioned three factors. Participants' levels of test anxiety were measured by the TAS scores and the quantification of brainwave energy. The results of the paired sample T-test demonstrate that virtual reality forest meditation constitutes an efficacious intervention for test anxiety among college students. The results of a three-factor analysis of variance revealed that both meditation state and pranayama guidance significantly influenced the effectiveness of virtual reality forest meditation intervention, while forest style did not demonstrate a significant impact. Specifically, wandering meditation was found to be more effective than sitting meditation, and interventions with pranayama guidance were more successful than those without pranayama guidance. Therefore, it can be concluded that guided wandering meditation combined with pranayama in a virtual reality forest environment offers superior alleviation of test anxiety among college students.},
booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science},
pages = {558–562},
numpages = {5},
location = {<conf-loc>, <city>Chengdu</city>, <country>China</country>, </conf-loc>},
series = {ISAIMS '23}
}

@inproceedings{10.1145/2838739.2838803,
author = {Ma, Minhua and Coward, Sarah and Walker, Chris},
title = {Interact: A Mixed Reality Virtual Survivor for Holocaust Testimonies},
year = {2015},
isbn = {9781450336734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838739.2838803},
doi = {10.1145/2838739.2838803},
abstract = {In this paper we present Interact---a mixed reality virtual survivor for Holocaust education. It was created to preserve the powerful and engaging experience of listening to, and interacting with, Holocaust survivors, allowing future generations of audience access to their unique stories. Interact demonstrates how advanced filming techniques, 3D graphics and natural language processing can be integrated and applied to specially-recorded testimonies to enable users to ask questions and receive answers from that virtualised individuals. This provides a new and rich interactive narratives of remembrance to engage with primary testimony. We discuss the design and development of Interact, and argue that this new form of mixed reality is promising media to overcome the uncanny valley.},
booktitle = {Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction},
pages = {250–254},
numpages = {5},
keywords = {Mixed reality, holocaust survivor, natural language processing, question-answering, virtual human},
location = {Parkville, VIC, Australia},
series = {OzCHI '15}
}

@inproceedings{10.1145/1358628.1358893,
author = {Yang, Ungyeon and Jo, Dongsik and Son, Wooho},
title = {Uvmode: usability verification mixed reality system for mobile devices},
year = {2008},
isbn = {9781605580128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1358628.1358893},
doi = {10.1145/1358628.1358893},
abstract = {UVMODE is a mixed reality based usability evaluation system for mobile information device development. The system contributes to increasing efficiency of the usability evaluation process by replacing real products with virtual models. With our system, users can change the design of a virtual product easily, and investigate how it affects its usability. While users can review and test the virtual product by manipulating it with MR interfaces, the system also provides evaluation tools for measuring objective usability measures, including estimated design quality and users' hand load. In this paper, we present the system design and implementation details of our system, and discuss how it could improve the current usability evaluation processes held in mobile information device industry.},
booktitle = {CHI '08 Extended Abstracts on Human Factors in Computing Systems},
pages = {3573–3578},
numpages = {6},
keywords = {affective engineering, design reviewing, mixed reality, operation simulation, photo-realistic rendering, product-life-cycle, tangible interface, usability test},
location = {Florence, Italy},
series = {CHI EA '08}
}

@inproceedings{10.1145/3365610.3368471,
author = {Lakhnati, Younes and Springer, Raphael and Gerken, Jens},
title = {Mensch ARgere dich nicht: a board game testbed for mixed reality interactions},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368471},
doi = {10.1145/3365610.3368471},
abstract = {"Mensch ARgere Dich Nicht" is a Mixed Reality (MR) game based on a popular German board game "Mensch \"{A}rgere Dich Nicht", from the early 1900s, similar to Pachisi. Developed to be a test bed for investigating mixed interactions, the application offers real and virtual variations of all core game elements (board, dice, pieces) for a fully modifiable experience. Using the Microsoft HoloLens, players can interact with the virtual game elements and receive additional information about the current game state. The game also recognizes interactions with real game elements and translates them into the virtual world.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {57},
numpages = {5},
keywords = {augmented reality, augmented virtuality, board game, mixed reality, reality-virtuality trade-off},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3301019.3325146,
author = {Morozova, Alyona and Rheinst\"{a}dter, Verena and Wallach, Dieter},
title = {MixedUX: A Mixed Prototyping Framework for Usability Testing in Augmented Reality},
year = {2019},
isbn = {9781450362702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301019.3325146},
doi = {10.1145/3301019.3325146},
abstract = {In this paper, we rethink and challenge conventional ways of prototyping. We present a new perspective on how usability testings of digital products may benefit from emerging augmented reality (AR) technologies. We demonstrate a conceptual prototype of an innovative framework that makes it possible to combine 3D models of complex devices, represented by holograms, with 2D user interfaces (UIs) opened in a web browser on a physical touch display. The framework aims to facilitate the processes of UI design and interactions with complex, costly, or even not-yet-existing systems. For demonstration purposes we use the Microsoft HoloLens Development Edition and a conventional touch display of a smartphone.},
booktitle = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
pages = {41–44},
numpages = {4},
keywords = {augmented reality, design facilitation, hololens, mixed prototyping},
location = {<conf-loc>, <city>San Diego</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {DIS '19 Companion}
}

@inproceedings{10.1145/3301293.3309565,
author = {Ullah, Rehmat and Rehman, Muhammad Atif Ur and Kim, Byung Seo},
title = {Poster: A Testbed Implementation of NDN-based Edge Computing For Mobile Augmented Reality},
year = {2019},
isbn = {9781450362733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301293.3309565},
doi = {10.1145/3301293.3309565},
abstract = {Future Augmented Reality (AR) applications require fast information response time and significant computational power and memory for many of its tasks. To enable future AR applications, in this poster, we combine Named Data Networking (NDN) and Edge Computing (EC) in order to achieve fast information response time. The outcomes are implemented and evaluated through testbed and simulations.},
booktitle = {Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications},
pages = {181},
numpages = {1},
keywords = {augmented reality, edge computing, fast response time, future internet, named data networking},
location = {Santa Cruz, CA, USA},
series = {HotMobile '19}
}

@inproceedings{10.1145/3510454.3516870,
author = {Wang, Xiaoyin},
title = {VRTest: an extensible framework for automatic testing of virtual reality scenes},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516870},
doi = {10.1145/3510454.3516870},
abstract = {Virtual Reality (VR) is an emerging technique that attracts interest from various application domains such as training, education, remote communication, gaming, and navigation. Despite the ever growing number of VR software projects, the quality assurance techniques for VR software has not been well studied. Therefore, the validation of VR software largely rely on pure manual testing. In this paper, we present a novel testing framework called VRTest to automate the testing of scenes in VR software. In particular, VRTest extracts information from a VR scene and controls the user camera to explore the scene and interact with the virtual objects with certain testing strategies. VRTest currently supports two built-in testing strategies: VRMonkey and VRGreed, which use pure random exploration and greedy algorithm to explore interact-able objects in VR scenes. The video of our tool is available on Youtube at https://www.youtube.com/watch?v=TARqTEaa7_Q},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {232–236},
numpages = {5},
keywords = {scene exploration, software testing, virtual reality},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3340764.3344885,
author = {Jelonek, Markus and Herrmann, Thomas},
title = {Atentiveness for Potential Accidents at the Construction Site: Virtual Reality Test Environment with Tactile Warnings for Behavior Tests in Hazardous Situations},
year = {2019},
isbn = {9781450371988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340764.3344885},
doi = {10.1145/3340764.3344885},
abstract = {Construction sites are still considered to be one of the most dangerous working environments for human beings. In 2017, approximately one third of all fatal accidents at a workplace in Germany occurred in the construction industry. To a certain extent, this is because double tasks are performed continuously in this rather dynamic working environment: On the one hand, construction workers have to carry out their work tasks and on the other hand, they have to take into account their own safety. This paper presents a concept for a test environment based on virtual reality, which should make it possible to realistically design several hazard scenarios and examine to what extent the chosen type of vibration-based warnings helps to avoid danger in hazardous situations. Finally, different challenges for such a test environment will be discussed.},
booktitle = {Proceedings of Mensch Und Computer 2019},
pages = {649–653},
numpages = {5},
keywords = {Aufmerksamkeit, Vibration, Virtual Reality, Warnungen},
location = {Hamburg, Germany},
series = {MuC '19}
}

@inproceedings{10.1109/ISMAR.2009.5336464,
author = {Lee, Cha and Bonebrake, Scott and Hollerer, Tobias and Bowman, Doug A.},
title = {A replication study testing the validity of AR simulation in VR for controlled experiments},
year = {2009},
isbn = {9781424453900},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2009.5336464},
doi = {10.1109/ISMAR.2009.5336464},
abstract = {It is extremely challenging to run controlled studies comparing multiple Augmented Reality (AR) systems. We use an “AR simulation” approach, in which a Virtual Reality (VR) system is used to simulate multiple AR systems. In order to validate this approach, we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results.},
booktitle = {Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality},
pages = {203–204},
numpages = {2},
series = {ISMAR '09}
}

@inproceedings{10.1109/ISMAR.2009.5336441,
author = {Tamura, Hideyuki and Kato, Hirokazu},
title = {Proposal of international voluntary activities on establishing benchmark test schemes for AR/MR geometric registration and tracking methods},
year = {2009},
isbn = {9781424453900},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2009.5336441},
doi = {10.1109/ISMAR.2009.5336441},
abstract = {This is a proposal to the ISMAR community from Japanese AR/MR researchers for the future progress of AR/MR technology. We hope to expand our activities over the ISMAR community and call for international participants who would take part in a number of voluntary works. At the same time, this paper presents a current view of the outcomes of these activities may have be in due course.},
booktitle = {Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality},
pages = {233–236},
numpages = {4},
series = {ISMAR '09}
}

@inproceedings{10.1145/3377290.3377315,
author = {Nikolov, Ivan},
title = {Testing VR headset cameras for capturing written content},
year = {2020},
isbn = {9781450377744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377290.3377315},
doi = {10.1145/3377290.3377315},
abstract = {Virtual reality (VR) has become an important tool for providing immersive and collaborative teleprensence experiences. The technology has the possibility of bringing people in different geographical places together in an immersive way and makes sharing knowledge, ideas and experience easier. An important evolution in the immersive experience is the introduction of items from the real world into the virtual environment and the creation of a mixed reality experience. We present a pilot study in the use of the built-in head mounted display (HMD) cameras, for capturing parts of the environment and presenting them in VR. Particular emphasis is put on capturing physical written content on flat surfaces and visualizing it in VR for sharing with other users. We test the HTC Vive and Valve Index cameras and compare them to two external solutions by applying optical character recognition (OCR) on captured and rectified images from them. We demonstrate that the Valve Index camera has the potential to be used for capturing readable text for use in shared VR.},
booktitle = {Proceedings of the 23rd International Conference on Academic Mindtrek},
pages = {153–156},
numpages = {4},
keywords = {VR, benchmarking, cameras, image acquisition, image processing, virtual reality},
location = {Tampere, Finland},
series = {AcademicMindtrek '20}
}

@article{10.1145/3656340,
author = {Xu, Congying and Terragni, Valerio and Zhu, Hengcheng and Wu, Jiarong and Cheung, Shing-Chi},
title = {MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3656340},
doi = {10.1145/3656340},
abstract = {Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs), that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities. In this paper, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97\% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52\% and 9.42\% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76\% to 76.92\% of codified MRs are easily comprehensible for developers.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
keywords = {Software Testing, Metamorphic Testing, Metamorphic Relation, Automated Test Case Generation}
}

@inproceedings{10.1145/3565387.3565450,
author = {Li, Chengshun and Yang, Xiaonan and Hu, Yaoguang and Mao, Wanting and Fang, Haonan},
title = {AR-based Accessibility Verification Method for Smart Manufacturing System with Human Motion Capture},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565450},
doi = {10.1145/3565387.3565450},
abstract = {With the development of industrial production towards personalization and intelligence, process validation of manufacturability has become a complex and time-consuming problem in manufacturing system. Smart manufacturing requires manufacturing system to have the ability of rapid response to various product designs. However, there is an obvious time span between product design and accessibility verification process. Augmented reality (AR) constructs an environment of virtual reality integration, which brings people intuitive visual feeling and natural interaction feedback. It can be easily applied to the early stage of product accessibility verification. This paper presents a digital human assisted accessibility verification system based on AR, which can perform accessibility verification in the product design stage. In this system, augmented reality glasses are used to realize the natural interaction between human and product 3D model, and a motion capture device is used to realize the superposition of digital human and operators. This paper expounds the above methods and related devices in detail, verifies the feasibility of this system through an industrial example, and shows its application potential in smart manufacturing system.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {63},
numpages = {6},
keywords = {Accessibility verification, Augmented Reality, Motion capture, Smart Manufacturing System},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3613904.3642408,
author = {Ring, Patrizia and Tietenberg, Julius and Emmerich, Katharina and Masuch, Maic},
title = {Development and Validation of the Collision Anxiety Questionnaire for VR Applications},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642408},
doi = {10.1145/3613904.3642408},
abstract = {The high degree of sensory immersion is a distinctive feature of head-mounted virtual reality (VR) systems. While the visual detachment from the real world enables unique immersive experiences, users risk collisions due to their inability to perceive physical obstacles in their environment. Even the mere anticipation of a collision can adversely affect the overall experience and erode user confidence in the VR system. However, there are currently no valid tools for assessing collision anxiety. We present the iterative development and validation of the Collision Anxiety Questionnaire (CAQ), involving an exploratory and a confirmatory factor analysis with a total of 159 participants. The results provide evidence for both discriminant and convergent validity and a good model fit for the final CAQ with three subscales: general collision anxiety, orientation, and interpersonal collision anxiety. By utilizing the CAQ, researchers can examine potential confounding effects of collision anxiety and evaluate methods for its mitigation.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {605},
numpages = {13},
keywords = {assessment, collision anxiety, discomfort, fear, user experience, virtual reality},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{10.1145/3489849.3489959,
author = {Mukhopadhyay, Abhishek and Reddy, G S Rajshekar and Ghosh, Subhankar and L R D, Murthy and Biswas, Pradipta},
title = {Validating Social Distancing through Deep Learning and VR-Based Digital Twins},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489959},
doi = {10.1145/3489849.3489959},
abstract = {The Covid-19 pandemic resulted in a catastrophic loss to global economies, and social distancing was consistently found to be an effective means to curb the virus's spread. However, it is only as effective when every individual partakes in it with equal alacrity. Past literature outlined scenarios where computer vision was used to detect people and to enforce social distancing automatically. We have created a Digital Twin (DT) of an existing laboratory space for remote monitoring of room occupancy and automatically detecting violation of social distancing. To evaluate the proposed solution, we have implemented a Convolutional Neural Network (CNN) model for detecting people, both in a limited-sized dataset of real humans, and a synthetic dataset of humanoid figures. Our proposed computer vision models are validated for both real and synthetic data in terms of accurately detecting persons, posture, and intermediate distances among people.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {104},
numpages = {2},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3592834.3592877,
author = {Lee, Kuan-Yu and Fang, Jia-Wei and Sun, Yuan-Chun and Hsu, Cheng-Hsin},
title = {Modeling Gamer Quality-of-Experience Using a Real Cloud VR Gaming Testbed},
year = {2023},
isbn = {9798400701894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592834.3592877},
doi = {10.1145/3592834.3592877},
abstract = {Cloud Virtual Reality (VR) gaming offloads computationally-intensive VR games to resourceful data centers. Ensuring good Quality of Experience (QoE) in cloud VR gaming, however, is inherently challenging as VR gamers demand high visual quality, short response time, and low cybersickness. In this paper, we investigate the QoE of cloud VR gaming in multiple steps. First, we build a cloud VR gaming testbed, which allows us to measure various Quality of Service (QoS) metrics. Second, we carry out a user study to understand the effects of diverse factors, including encoding settings, network conditions, and game genres on gamer QoE, quantified by Mean Opinion Score (MOS). Using our user study results, we construct QoE models for cloud VR gaming, which to the best of our knowledge, has not been done in the literature. Last, we apply our QoE models to develop a bitrate allocation algorithm for multiple cloud VR gamers to achieve better overall QoE compared to the bandwidth-fair bitrate allocation.},
booktitle = {Proceedings of the 15th International Workshop on Immersive Mixed and Virtual Environment Systems},
pages = {12–17},
numpages = {6},
keywords = {VR gaming, cloud gaming, QoE modeling, bitrate allocation},
location = {Vancouver, BC, Canada},
series = {MMVE '23}
}

