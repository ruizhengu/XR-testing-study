@article{CHEN2020101948,
title = {A virtual-physical collision detection interface for AR-based interactive teaching of robot},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {64},
pages = {101948},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101948},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300481},
author = {Chengjun Chen and Yong Pan and Dongnian Li and Shilei Zhang and Zhengxu Zhao and Jun Hong},
keywords = {Robot path planning, Teaching programming, Augmented reality, Virtual-physical collision detection, Depth image},
abstract = {At present, online lead-through and offline programming methods are widely used in programming of industrial robots. However, both methods have some drawbacks for unskilled shopworkers. This paper presents an Augmented Reality (AR)-based interactive robot teaching programming system, which virtually projected the robot onto the physical industrial environment. The unskilled shopworkers can use Handheld Teaching Device (HTD) to move end-effector of virtual robot to follow endpoint of the HTD. In this way, the path of the virtual robot can be planned or tested interactively. In addition, collisions detection between virtual robot and physical environment is key to test the feasibility of robot path. So, a method for detecting virtual-physical collisions is presented in this paper by comparing the depth values of corresponding pixels in depth image acquired by Kinect and computer-generated image in order to get collision-free paths of the virtual robot. The Quadtree model is used to accelerate the collision detection process and get distance between virtual model and physical environment. Using the AR-based interactive robot teaching programming system presented in this paper, all workers even unskilled ones in robot programming, can quickly and effectively get the collision-free robot path.}
}
@article{KHALIL2018218,
title = {Performance evaluation of feature extraction techniques in MR-Brain image classification system},
journal = {Procedia Computer Science},
volume = {127},
pages = {218-225},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301297},
author = {Mohammed Khalil and Habib Ayad and Abdellah Adib},
keywords = {MR-Brain image classification, LBP, GLCM, HOG, k-NN, Fusion},
abstract = {In this paper, we present a MR-Brain image classification system to classify a given MR-brain image as normal or abnormal. This system first employs three feature extraction techniques namely, Gray-Level Co-Occurrence Matrix (GLCM), Local Binary Pattern (LBP) and Histogram of Oriented Gradient (HOG). The obtained feature vector of each technique is passed through a k-Nearest Neighbor (k-NN) classifier. The resulting dissimilarity measure values of the classifiers are combined then by a fusion operator in order to increase the classification accuracy. Two benchmark MR image datasets, Dataset-66 and Dataset-160, have been used to validate the system performance. A cross-validation scheme is adopted to improve the generalization capability of the system. The obtained simulation results are compared with those ones of the existing methods to evaluate the performance of the presented MR-Brain classification system.}
}
@article{SAKUMA2002243,
title = {Virtual reality experiments on a digital servosphere: guiding male silkworm moths to a virtual odour source},
journal = {Computers and Electronics in Agriculture},
volume = {35},
number = {2},
pages = {243-254},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(02)00021-2},
url = {https://www.sciencedirect.com/science/article/pii/S0168169902000212},
author = {Masayuki Sakuma},
keywords = {, Servosphere, Locomotion compensator, Orientation, Pheromone},
abstract = {A digital ‘servosphere’ locomotion-compensation apparatus has been developed for use in experiments on the mechanisms of spatial orientation in ambulatory animals. The sphere is driven by a pair of digital AC-servo motors, which provide a negative feedback response to any displacements of the test animal. A high-speed video tracker continually reports the position of the animal to a computer, which controls the servo-motors via a motor-control board and also logs the position of the motor axes. Movements of the servos define the path of the test animal on a virtual plane. The computer also generates odour cues in real time via a relay-control board and an airflow system. Odour presentation is programmed to occur in response to the animal's movements on the virtual plane, and can result in the animal being led to a virtual source. The servosphere was first used to investigate orientation of a walking male silkworm moth (Bombyx mori) towards a female in still air. When a calling female moth was placed beside the sphere, the male moth beat its wings and walked straight towards the female. Unilaterally dewinged males also walked in a straight line, though slightly to one side of the direction of the female, but bilaterally dewinged moths remained stationary. This indicates that in still air the male moths orientate to pheromone sources by fanning air from in front of them over their antennae with their wings. If a moth continually reverses its turning movements when it encounters a train of pheromone concentration peaks, it will maintain a course towards the source and eventually arrive there. This idea was demonstrated on the servosphere by incorporating a computer-controlled solenoid valve which releases pheromone when a moth points towards a virtual odour source. In this sensory field, both intact and bilaterally dewinged males reached the source irrespective of the wind direction. This result supports the proposed orientation mechanism and also demonstrates the applicability of the servosphere virtual-reality environment to the experimental investigation of insect orientation mechanisms.}
}
@article{AYAZ2024108115,
title = {Brain MR image simulation for deep learning based medical image analysis networks},
journal = {Computer Methods and Programs in Biomedicine},
volume = {248},
pages = {108115},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108115},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001111},
author = {Aymen Ayaz and Yasmina {Al Khalil} and Sina Amirrajab and Cristian Lorenz and Jürgen Weese and Josien Pluim and Marcel Breeuwer},
keywords = {Brain MRI simulation, Large synthetic population, Brain MRI segmentation, WM/GM/CSF segmentation},
abstract = {Background and Objective
As large sets of annotated MRI data are needed for training and validating deep learning based medical image analysis algorithms, the lack of sufficient annotated data is a critical problem. A possible solution is the generation of artificial data by means of physics-based simulations. Existing brain simulation data is limited in terms of anatomical models, tissue classes, fixed tissue characteristics, MR sequences and overall realism.
Methods
We propose a realistic simulation framework by incorporating patient-specific phantoms and Bloch equations-based analytical solutions for fast and accurate MRI simulations. A large number of labels are derived from open-source high-resolution T1w MRI data using a fully automated brain classification tool. The brain labels are taken as ground truth (GT) on which MR images are simulated using our framework. Moreover, we demonstrate that the T1w MR images generated from our framework along with GT annotations can be utilized directly to train a 3D brain segmentation network. To evaluate our model further on larger set of real multi-source MRI data without GT, we compared our model to existing brain segmentation tools, FSL-FAST and SynthSeg.
Results
Our framework generates 3D brain MRI for variable anatomy, sequence, contrast, SNR and resolution. The brain segmentation network for WM/GM/CSF trained only on T1w simulated data shows promising results on real MRI data from MRBrainS18 challenge dataset with a Dice scores of 0.818/0.832/0.828. On OASIS data, our model exhibits a close performance to FSL, both qualitatively and quantitatively with a Dice scores of 0.901/0.939/0.937.
Conclusions
Our proposed simulation framework is the initial step towards achieving truly physics-based MRI image generation, providing flexibility to generate large sets of variable MRI data for desired anatomy, sequence, contrast, SNR, and resolution. Furthermore, the generated images can effectively train 3D brain segmentation networks, mitigating the reliance on real 3D annotated data.}
}
@article{LORENZOVALDES2004255,
title = {Segmentation of 4D cardiac MR images using a probabilistic atlas and the EM algorithm},
journal = {Medical Image Analysis},
volume = {8},
number = {3},
pages = {255-265},
year = {2004},
note = {Medical Image Computing and Computer-Assisted Intervention - MICCAI 2003},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2004.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1361841504000271},
author = {Maria Lorenzo-Valdés and Gerardo I. Sanchez-Ortiz and Andrew G. Elkington and Raad H. Mohiaddin and Daniel Rueckert},
keywords = {Classification, Markov Random Fields, Heart, Cardiac Magnetic Resonance, Largest connected component, Time sequences},
abstract = {In this paper an automatic atlas-based segmentation algorithm for 4D cardiac MR images is proposed. The algorithm is based on the 4D extension of the expectation maximisation (EM) algorithm. The EM algorithm uses a 4D probabilistic cardiac atlas to estimate the initial model parameters and to integrate a priori information into the classification process. The probabilistic cardiac atlas has been constructed from the manual segmentations of 3D cardiac image sequences of 14 healthy volunteers. It provides space and time-varying probability maps for the left and right ventricles, the myocardium, and background structures such as the liver, stomach, lungs and skin. In addition to using the probabilistic cardiac atlas as a priori information, the segmentation algorithm incorporates spatial and temporal contextual information by using 4D Markov Random Fields. After the classification, the largest connected component of each structure is extracted using a global connectivity filter which improves the results significantly, especially for the myocardium. Validation against manual segmentations and computation of the correlation between manual and automatic segmentation on 249 3D volumes were calculated. We used the 'leave one out' test where the image set to be segmented was not used in the construction of its corresponding atlas. Results show that the procedure can successfully segment the left ventricle (LV) (r=0.96), myocardium (r=0.92) and right ventricle (r=0.92). In addition, 4D images from 10 patients with hypertrophic cardiomyopathy were also manually and automatically segmented yielding a good correlation in the volumes of the LV (r=0.93) and myocardium (0.94) when the atlas constructed with volunteers is blurred.}
}
@article{AKDEMIRAKAR201687,
title = {Determination of optimal parameters for bilateral filter in brain MR image denoising},
journal = {Applied Soft Computing},
volume = {43},
pages = {87-96},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300953},
author = {Saime {Akdemir Akar}},
keywords = {Bilateral filter, Genetic algorithm, Rician noise, MR denoising, Optimal parameters},
abstract = {Noise elimination is an important pre-processing step in magnetic resonance (MR) images for clinical purposes. In the present study, as an edge-preserving method, bilateral filter (BF) was used for Rician noise removal in MR images. The choice of BF parameters affects the performance of denoising. Therefore, as a novel approach, the parameters of BF were optimized using genetic algorithm (GA). First, the Rician noise with different variances (σ=10, 20, 30) was added to simulated T1-weighted brain MR images. To find the optimum filter parameters, GA was applied to the noisy images in searching regions of window size [3×3, 5×5, 7×7, 11×11, and 21×21], spatial sigma [0.1–10] and intensity sigma [1–60]. The peak signal-to-noise ratio (PSNR) was adjusted as fitness value for optimization. After determination of optimal parameters, we investigated the results of proposed BF parameters with both the simulated and clinical MR images. In order to understand the importance of parameter selection in BF, we compared the results of denoising with proposed parameters and other previously used BFs using the quality metrics such as mean squared error (MSE), PSNR, signal-to-noise ratio (SNR) and structural similarity index metric (SSIM). The quality of the denoised images with the proposed parameters was validated using both visual inspection and quantitative metrics. The experimental results showed that the BF with parameters proposed by us showed a better performance than BF with other previously proposed parameters in both the preservation of edges and removal of different level of Rician noise from MR images. It can be concluded that the performance of BF for denoising is highly dependent on optimal parameter selection.}
}
@article{GUDIGAR2019359,
title = {Application of multiresolution analysis for automated detection of brain abnormality using MR images: A comparative study},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {359-367},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314560},
author = {Anjan Gudigar and U. Raghavendra and Tan Ru San and Edward J. Ciaccio and U. Rajendra Acharya},
keywords = {Brain MR images, Classification, Particle swarm optimization, Shearlet transform, Support vector machine, Texture features},
abstract = {Neurological disorders are abnormalities related to the human nervous system, and comprise electrical, biochemical, or structural changes in the spinal cord, brain, or central nervous system that induce various symptoms. These symptoms may be in the form of muscle weakness, paralysis, and poor coordination, among other factors. Early diagnosis of these changes is important for treatment, to limit disease progression. Magnetic resonance (MR) imaging is a widely used modality for diagnosing brain abnormality. Expert reading and interpretation of MR images is time-consuming, tedious, and subject to interobserver variability. Hence, various automated computer aided diagnosis (CAD) tools have been developed to detect brain abnormalities from MR imaging. Multiresolution analysis involves the transformation of images to capture obscure signatures. In this paper, we compare the performance of three different multi-resolution analysis techniques – the discrete wavelet transform, curvelet transform and shearlet transform – for detecting brain abnormality. Further, textural features extracted from the transformed image are optimally selected using particle swarm optimization (PSO), and classified using a support vector machine (SVM). The proposed method is applied on 83 control images, as well as 529 abnormal images from patients with cerebrovascular, neoplastic, degenerative and inflammatory diseases. For quantitative analysis, a cross validation scheme is implemented to improve system generality. Among the three techniques, the shearlet transform achieves a highest classification accuracy of 97.38% using only fifteen optimally selected features. The proposed system requires testing on a large data set prior to implementation as a standalone system to assist neurologists and radiologists in the early detection of brain abnormality.}
}
@article{LIANG201663,
title = {Improving the discrimination of hand motor imagery via virtual reality based visual guidance},
journal = {Computer Methods and Programs in Biomedicine},
volume = {132},
pages = {63-74},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301073},
author = {Shuang Liang and Kup-Sze Choi and Jing Qin and Wai-Man Pang and Qiong Wang and Pheng-Ann Heng},
keywords = {Brain–computer interface, Hand motor imagery, Visual guidance, Subject-specific frequency and time bands, Event-related desynchronization, Virtual reality},
abstract = {While research on the brain–computer interface (BCI) has been active in recent years, how to get high-quality electrical brain signals to accurately recognize human intentions for reliable communication and interaction is still a challenging task. The evidence has shown that visually guided motor imagery (MI) can modulate sensorimotor electroencephalographic (EEG) rhythms in humans, but how to design and implement efficient visual guidance during MI in order to produce better event-related desynchronization (ERD) patterns is still unclear. The aim of this paper is to investigate the effect of using object-oriented movements in a virtual environment as visual guidance on the modulation of sensorimotor EEG rhythms generated by hand MI. To improve the classification accuracy on MI, we further propose an algorithm to automatically extract subject-specific optimal frequency and time bands for the discrimination of ERD patterns produced by left and right hand MI. The experimental results show that the average classification accuracy of object-directed scenarios is much better than that of non-object-directed scenarios (76.87% vs. 69.66%). The result of the t-test measuring the difference between them is statistically significant (p = 0.0207). When compared to algorithms based on fixed frequency and time bands, contralateral dominant ERD patterns can be enhanced by using the subject-specific optimal frequency and the time bands obtained by our proposed algorithm. These findings have the potential to improve the efficacy and robustness of MI-based BCI applications.}
}
@article{HUTTINGA2023102843,
title = {Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy},
journal = {Medical Image Analysis},
volume = {88},
pages = {102843},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102843},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001032},
author = {Niek R.F. Huttinga and Tom Bruijnen and Cornelis A.T. {van den Berg} and Alessandro Sbrizzi},
keywords = {Respiratory motion, MR-guided radiotherapy, MR-linac, Real-time, Motion estimation, Uncertainty estimation, Gaussian Processes},
abstract = {Respiratory motion during radiotherapy causes uncertainty in the tumor’s location, which is typically addressed by an increased radiation area and a decreased dose. As a result, the treatments’ efficacy is reduced. The recently proposed hybrid MR-linac scanner holds the promise to efficiently deal with such respiratory motion through real-time adaptive MR-guided radiotherapy (MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the radiotherapy plan should be adapted in real-time according to the estimated motion-fields. All of this should be performed with a total latency of maximally 200 ms, including data acquisition and reconstruction. A measure of confidence in such estimated motion-fields is highly desirable, for instance to ensure the patient’s safety in case of unexpected and undesirable motion. In this work, we propose a framework based on Gaussian Processes to infer 3D motion-fields and uncertainty maps in real-time from only three readouts of MR-data. We demonstrated an inference frame rate up to 69 Hz including data acquisition and reconstruction, thereby exploiting the limited amount of required MR-data. Additionally, we designed a rejection criterion based on the motion-field uncertainty maps to demonstrate the framework’s potential for quality assurance. The framework was validated in silico and in vivo on healthy volunteer data (n=5) acquired using an MR-linac, thereby taking into account different breathing patterns and controlled bulk motion. Results indicate end-point-errors with a 75th percentile below 1 mm in silico, and a correct detection of erroneous motion estimates with the rejection criterion. Altogether, the results show the potential of the framework for application in real-time MR-guided radiotherapy with an MR-linac.}
}
@article{ONOFREY201729,
title = {Learning Non-rigid Deformations for Robust, Constrained Point-based Registration in Image-Guided MR-TRUS Prostate Intervention},
journal = {Medical Image Analysis},
volume = {39},
pages = {29-43},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300452},
author = {John A. Onofrey and Lawrence H. Staib and Saradwata Sarkar and Rajesh Venkataraman and Cayce B. Nawaf and Preston C. Sprenkle and Xenophon Papademetris},
keywords = {Non-rigid registration, Statistical deformation model, Dimensionality reduction, Prostate, Magnetic resonance imaging, Trans-rectal ultrasound, Image-guided biopsy},
abstract = {Accurate and robust non-rigid registration of pre-procedure magnetic resonance (MR) imaging to intra-procedure trans-rectal ultrasound (TRUS) is critical for image-guided biopsies of prostate cancer. Prostate cancer is one of the most prevalent forms of cancer and the second leading cause of cancer-related death in men in the United States. TRUS-guided biopsy is the current clinical standard for prostate cancer diagnosis and assessment. State-of-the-art, clinical MR-TRUS image fusion relies upon semi-automated segmentations of the prostate in both the MR and the TRUS images to perform non-rigid surface-based registration of the gland. Segmentation of the prostate in TRUS imaging is itself a challenging task and prone to high variability. These segmentation errors can lead to poor registration and subsequently poor localization of biopsy targets, which may result in false-negative cancer detection. In this paper, we present a non-rigid surface registration approach to MR-TRUS fusion based on a statistical deformation model (SDM) of intra-procedural deformations derived from clinical training data. Synthetic validation experiments quantifying registration volume of interest overlaps of the PI-RADS parcellation standard and tests using clinical landmark data demonstrate that our use of an SDM for registration, with median target registration error of 2.98 mm, is significantly more accurate than the current clinical method. Furthermore, we show that the low-dimensional SDM registration results are robust to segmentation errors that are not uncommon in clinical TRUS data.}
}
@article{LOEVE201638,
title = {Workflow and intervention times of MR-guided focused ultrasound – Predicting the impact of new techniques},
journal = {Journal of Biomedical Informatics},
volume = {60},
pages = {38-48},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416000022},
author = {Arjo J. Loeve and Jumana Al-Issawi and Fabiola Fernandez-Gutiérrez and Thomas Langø and Jan Strehlow and Sabrina Haase and Matthias Matzko and Alessandro Napoli and Andreas Melzer and Jenny Dankelman},
keywords = {Workflow analysis, Focused ultrasound surgery, Workflow simulation, MRgFUS, High intensity focused ultrasound},
abstract = {Magnetic resonance guided focused ultrasound surgery (MRgFUS) has become an attractive, non-invasive treatment for benign and malignant tumours, and offers specific benefits for poorly accessible locations in the liver. However, the presence of the ribcage and the occurrence of liver motion due to respiration limit the applicability MRgFUS. Several techniques are being developed to address these issues or to decrease treatment times in other ways. However, the potential benefit of such improvements has not been quantified. In this research, the detailed workflow of current MRgFUS procedures was determined qualitatively and quantitatively by using observation studies on uterine MRgFUS interventions, and the bottlenecks in MRgFUS were identified. A validated simulation model based on discrete events simulation was developed to quantitatively predict the effect of new technological developments on the intervention duration of MRgFUS on the liver. During the observation studies, the duration and occurrence frequencies of all actions and decisions in the MRgFUS workflow were registered, as were the occurrence frequencies of motion detections and intervention halts. The observation results show that current MRgFUS uterine interventions take on average 213min. Organ motion was detected on average 2.9 times per intervention, of which on average 1.0 actually caused a need for rework. Nevertheless, these motion occurrences and the actions required to continue after their detection consumed on average 11% and up to 29% of the total intervention duration. The simulation results suggest that, depending on the motion occurrence frequency, the addition of new technology to automate currently manual MRgFUS tasks and motion compensation could potentially reduce the intervention durations by 98.4% (from 256h 5min to 4h 4min) in the case of 90% motion occurrence, and with 24% (from 5h 19min to 4h 2min) in the case of no motion. In conclusion, new tools were developed to predict how intervention durations will be affected by future workflow changes and by the introduction of new technology.}
}
@article{BANERJEE2018167,
title = {Transfer learning on fused multiparametric MR images for classifying histopathological subtypes of rhabdomyosarcoma},
journal = {Computerized Medical Imaging and Graphics},
volume = {65},
pages = {167-175},
year = {2018},
note = {Advances in Biomedical Image Processing},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0895611117300459},
author = {Imon Banerjee and Alexis Crawley and Mythili Bhethanabotla and Heike E Daldrup-Link and Daniel L. Rubin},
keywords = {Rhabdomyosarcoma, Computer aided diagnosis, Image fusion, Transfer learning, Deep neural networks},
abstract = {This paper presents a deep-learning-based CADx for the differential diagnosis of embryonal (ERMS) and alveolar (ARMS) subtypes of rhabdomysarcoma (RMS) solely by analyzing multiparametric MR images. We formulated an automated pipeline that creates a comprehensive representation of tumor by performing a fusion of diffusion-weighted MR scans (DWI) and gadolinium chelate-enhanced T1−weighted MR scans (MRI). Finally, we adapted transfer learning approach where a pre-trained deep convolutional neural network has been fine-tuned based on the fused images for performing classification of the two RMS subtypes. We achieved 85% cross validation prediction accuracy from the fine-tuned deep CNN model. Our system can be exploited to provide a fast, efficient and reproducible diagnosis of RMS subtypes with less human interaction. The framework offers an efficient integration between advanced image processing methods and cutting-edge deep learning techniques which can be extended to deal with other clinical domains that involve multimodal imaging for disease diagnosis.}
}
@article{KIM2017370,
title = {Automatic localization of anatomical landmarks in cardiac MR perfusion using random forests},
journal = {Biomedical Signal Processing and Control},
volume = {38},
pages = {370-378},
year = {2017},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1746809417301349},
author = {Yoon-Chul Kim and Younjoon Chung and Yeon Hyeon Choe},
keywords = {MRI, Dynamic contrast enhanced imaging, Myocardial perfusion, Random forests, Machine learning, Landmark localization},
abstract = {Automatic localization of anatomical landmarks in myocardial perfusion magnetic resonance (MR) data is considered to be a preliminary step toward fully automatic quantification of regional upslope or blood flow in the myocardium. The goal of this work is to develop an automatic method based on supervised learning and detect anatomical landmarks with high accuracy from myocardial perfusion MR data. Dynamic contrast enhanced myocardial MR perfusion data were acquired using a standard perfusion sequence. To effectively extract characteristic features for left ventricle (LV) center point and anterior right ventricle (RV) insertion point, we performed feature extraction of the two landmarks independently from an LV enhancement frame and an RV enhancement frame. Feature extraction of pixel intensity, Sobel gradient, and Haar-like features was performed. Cross-validation from training data was used to build a random forests classifier. We used 38 subjects’ data as training datasets and 21 subjects’ data as test datasets. The proposed method provided high accuracy in localization of the LV center point and anterior RV insertion point. The mean (±SD) localization errors of the proposed method were 3.38 (±2.36) mm for the LV center point and 4.23 (±1.97) mm for the anterior RV insertion point. The proposed method shows the potential to automatically localize anatomical landmarks for the segmental analysis of myocardial perfusion in MRI.}
}
@article{KHANAL201449,
title = {Collaborative virtual reality based advanced cardiac life support training simulator using virtual reality principles},
journal = {Journal of Biomedical Informatics},
volume = {51},
pages = {49-59},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000902},
author = {Prabal Khanal and Akshay Vankipuram and Aaron Ashby and Mithra Vankipuram and Ashish Gupta and Denise Drumm-Gurnee and Karen Josey and Linda Tinker and Marshall Smith},
keywords = {Computer uses in education – , Multimedia information systems – , Serious games, Computer applications in medicine, Advanced cardiac life support, Medical team training},
abstract = {Background
Advanced Cardiac Life Support (ACLS) is a series of team-based, sequential and time constrained interventions, requiring effective communication and coordination of activities that are performed by the care provider team on a patient undergoing cardiac arrest or respiratory failure. The state-of-the-art ACLS training is conducted in a face-to-face environment under expert supervision and suffers from several drawbacks including conflicting care provider schedules and high cost of training equipment.
Objective
The major objective of the study is to describe, including the design, implementation, and evaluation of a novel approach of delivering ACLS training to care providers using the proposed virtual reality simulator that can overcome the challenges and drawbacks imposed by the traditional face-to-face training method.
Methods
We compare the efficacy and performance outcomes associated with traditional ACLS training with the proposed novel approach of using a virtual reality (VR) based ACLS training simulator. One hundred and forty-eight (148) ACLS certified clinicians, translating into 26 care provider teams, were enrolled for this study. Each team was randomly assigned to one of the three treatment groups: control (traditional ACLS training), persuasive (VR ACLS training with comprehensive feedback components), or minimally persuasive (VR ACLS training with limited feedback components). The teams were tested across two different ACLS procedures that vary in the degree of task complexity: ventricular fibrillation or tachycardia (VFib/VTach) and pulseless electric activity (PEA).
Results
The difference in performance between control and persuasive groups was not statistically significant (P=.37 for PEA and P=.1 for VFib/VTach). However, the difference in performance between control and minimally persuasive groups was significant (P=.05 for PEA and P=.02 for VFib/VTach). The pre-post comparison of performances of the groups showed that control (P=.017 for PEA, P=.01 for VFib/VTach) and persuasive (P=.02 for PEA, P=.048 for VFib/VTach) groups improved their performances significantly, whereas minimally persuasive group did not (P=.45 for PEA, P=.46 for VFib/VTach). Results also suggest that the benefit of persuasiveness is constrained by the potentially interruptive nature of these features.
Conclusions
Our results indicate that the VR-based ACLS training with proper feedback components can provide a learning experience similar to face-to-face training, and therefore could serve as a more easily accessed supplementary training tool to the traditional ACLS training. Our findings also suggest that the degree of persuasive features in VR environments have to be designed considering the interruptive nature of the feedback elements.}
}
@article{LI2024108045,
title = {Cooperative-Net: An end-to-end multi-task interaction network for unified reconstruction and segmentation of MR image},
journal = {Computer Methods and Programs in Biomedicine},
volume = {245},
pages = {108045},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108045},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724000415},
author = {Xiaodi Li and Yue Hu},
keywords = {MRI, Multi-task interaction, MR imaging acceleration, Multi-class tissue segmentation, Structural prior},
abstract = {Background and objective
In clinical applications, there is an increasing demand for rapid acquisition and automated analysis of magnetic resonance imaging (MRI) data. However, most existing methods focus on either MR image reconstruction from undersampled data or segmentation using fully sampled data, hardly considering MR image segmentation in fast imaging scenarios. Consequently, it is imperative to investigate a multi-task approach that can simultaneously achieve high scanning acceleration and accurate segmentation results.
Methods
In this paper, we propose a novel end-to-end multi-task interaction network, termed as the Cooperative-Net, which integrates accelerated MR imaging and multi-class tissue segmentation into a unified framework. The Cooperative-Net consists of alternating reconstruction modules and segmentation modules. To facilitate effective interaction between the two tasks, we introduce the spatial-adaptive semantic guidance module, which leverages the semantic map as a structural prior to guide MR image reconstruction. Furthermore, we propose a novel unrolling network with a multi-path shrinkage structure for MR image reconstruction. This network consists of parallel learnable shrinkage paths to handle varying degrees of degradation across different frequency components in the undersampled MR image, effectively improving the quality of the recovered image.
Results
We use two publicly available datasets, including the cardiac and knee MR datasets, to validate the efficacy of our proposed Cooperative-Net. Through qualitative and quantitative analysis, we demonstrate that our method outperforms existing state-of-the-art multi-task approaches for joint MR image reconstruction and segmentation.
Conclusions
The proposed Cooperative-Net is capable of achieving both high accelerated MR imaging and accurate multi-class tissue segmentation.}
}
@article{JEYARAJ2023117013,
title = {MR image restoration and segmentation via denoising deep adversarial network for blood vessels accurate diagnosis},
journal = {Signal Processing: Image Communication},
volume = {117},
pages = {117013},
year = {2023},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2023.117013},
url = {https://www.sciencedirect.com/science/article/pii/S0923596523000954},
author = {Pandia Rajan Jeyaraj and Edward Rajan Samuel Nadar},
keywords = {Deep adversarial learning, Multiphase MRI, Image restoration, Segmentation, Interclass decision making},
abstract = {Segmenting the hepatic and portal veins is a difficult task, since it has multiple distortions. For effective restoration and to minimize distortions, a multi-stage deep adversarial learning network was proposed. The proposed network provides high reliability in segmenting hepatic and portal veins from distorted Magnetic Resonance (MR) images. The Proposed network directly considers the variation in vascular flow for vessel segmentation. By a two-stage deep adversarial network, which consist of geometric distortion by regularization in the first stage and in the second stage image blurriness is adjusted by a sub-pixel mechanism. To detect the hepatic venous flow, shared clustering was used to correlate the hepatic flow and venous flow. To have better image segmentation by restoration, end-to-end training by image adaptive by denoising was employed. The proposed Denoising Deep Adversarial Network (DDAN) generalizability is shown by curating a distorted TCGA dataset. We perform experiments by segmenting the distorted MR image with estimating depth, semantic interclass decision, and classification. The proposed DDAN is evaluated on a multi-phase open MRI public dataset from a local clinic and Kaggle Br35H. The obtained quantitative and qualitative evaluation data shows the practical applicability of the proposed deep learning model. We observed that the image edges, portal, and hepatic vein scale texture are clearly segmented to assist real-time applications like tumour surgery.}
}
@article{RASOOL2014327,
title = {Surface myoelectric signal classification using the AR-GARCH model},
journal = {Biomedical Signal Processing and Control},
volume = {13},
pages = {327-336},
year = {2014},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1746809414000883},
author = {Ghulam Rasool and Nidhal Bouaynaya and Kamran Iqbal and Gannon White},
keywords = {Myoelectric signal, Autoregressive (AR) model, Heteroscedasticity, Autoregressive-autoregressive generalized conditional heteroscedastic (AR-GARCH) model, Myoelectric control},
abstract = {In myoelectric prostheses design, it is normally assumed that the necessary control information can be extracted from the surface myoelectric signals. In the pattern classification paradigm for controlling myoelectric prosthesis, the autoregressive (AR) model coefficients are generally considered an efficient and robust feature set. However, no formal statistical methodologies or tests are reported in the literature to analyze and model the myoelectric signal as an AR process. We analyzed the myoelectric signal as a stochastic time-series and found that the signal is heteroscedastic, i.e., the AR modeling residuals exhibit a time-varying variance. Heteroscedasticity is a major concern in statistical modeling because it can invalidate statistical tests of significance which may assume that the modeling errors are uncorrelated and that the error variances do not vary with the effects being modeled. We subsequently proposed to model the myoelectric signal as an autoregressive-generalized autoregressive conditional heteroscedastic (AR-GARCH) process and used the model parameters as a feature set for signal classification. Multiple statistical tests including the Ljung–Box Q-test, Engle's test for heteroscedasticity, the Kolmogorov–Smirnov test and the goodness of fit test were performed to show the validity of the proposed model. Our experimental results show that the proposed AR-GARCH model coefficients, when used as a feature set in two different classification schemes, significantly outperformed (p<.01) the conventional AR model coefficients.}
}
@article{AWADA2021101227,
title = {An integrated emotional and physiological assessment for VR-based active shooter incident experiments},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101227},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101227},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301968},
author = {Mohamad Awada and Runhe Zhu and Burcin Becerik-Gerber and Gale Lucas and Erroll Southers},
keywords = {Active shooter incidents, Virtual reality, Emotional response, Physiological response, Locomotion technique},
abstract = {Unfortunately, active shooter incidents are on the rise in the United States. With the recent technological advancements, virtual reality (VR) experiments could serve as an effective method to prepare civilians and law enforcement personnel for such scenarios. However, for VR experiments to be effective for active shooter training and research, such experiments must be able to evoke emotional and physiological responses as live active shooter drills and events do. The objective of this study is thus to test the effectiveness of an active shooter VR experiment on emotional and physiological responses. Additionally, we consider different locomotion techniques (i.e., walk-in-place and controller) and explore their impact on users’ sense of presence. The results suggest that the VR active shooter experiment in this study can induce emotional arousal and increase heart rate of the participants immersed in the virtual environment. Furthermore, compared to the controller, the walk-in-place technique resulted in a higher emotional arousal in terms of negative emotions and a stronger sense of presence. The study presents a foundation for future active shooter experiments as it supports the ecological validity using VR for active shooter incident related work for the purposes of training or research.}
}
@article{AUGUSTIN2022103736,
title = {An Improved Deep Persistent Memory Network for Rician Noise Reduction in MR Images},
journal = {Biomedical Signal Processing and Control},
volume = {77},
pages = {103736},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103736},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422002580},
author = {Anate Mary Augustin and Chandrasekharan Kesavadas and P.V. Sudeep},
keywords = {Deep learning, Denoising, MemNet, MRI, Parallel imaging, Rician, Quantum ReLU},
abstract = {Magnetic Resonance Imaging (MRI) is extensively employed in medical, scientific and investigative contexts today. Noise on the other hand, restricts the diagnostic utility of MR images by deteriorating their quality during acquisition. The noise in single coil magnitude MRI has stationary Rician distribution and images reconstructed with parallel MR-imaging techniques have non-stationary noise levels. Recently, deep learning models are finding ubiquitous employment in image restoration tasks, owing to their powerful capabilities in learning and solving inverse-problems. Nonetheless, only a few such techniques have been reported to suppress noise in MRI. In this paper, we propose a robust MR image denoising approach based on the concept of memory persistence. Accordingly, we improvised and optimized the deep model of memory networks by introducing a data sensitive activation function and a robust cost function, resulting in a compact design with improved noise filtering, feature preservation and enhanced performance. Experiments on real and synthetic data reveal that the proposed method outperformed state-of-the-art (SoTA) methods.}
}
@article{LO20121153,
title = {Support vector machine for breast MR image classification},
journal = {Computers & Mathematics with Applications},
volume = {64},
number = {5},
pages = {1153-1162},
year = {2012},
note = {Advanced Technologies in Computer, Consumer and Control},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2012.03.033},
url = {https://www.sciencedirect.com/science/article/pii/S0898122112002386},
author = {Chien-Shun Lo and Chuin-Mu Wang},
keywords = {SVM, C-means, Breast MR image, Classification},
abstract = {MR images have been used extensively in clinical trials in recent years because they are harmless to the human body and can obtain detailed information by scanning the same slice with various frequencies and parameters. In this paper, we want to detect the breast tissues within multi-spectral MR images. In the image classification, we apply a support vector machine (SVM) to breast multi-spectral magnetic resonance images to classify the tissues of the breast. In order to verify the feasibility and efficiency of this method, evaluations using classification rate and likelihood ratios are adopted based on manifold assessment and a series of experiments are conducted and compared with the commonly used C-means (CM) for performance evaluation. The results show that the SVM method is a promising and effective spectral technique for MR image classification.}
}
@article{ELWIN2022103712,
title = {Ar-HGSO: Autoregressive-Henry Gas Sailfish Optimization enabled deep learning model for diabetic retinopathy detection and severity level classification},
journal = {Biomedical Signal Processing and Control},
volume = {77},
pages = {103712},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103712},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422002348},
author = {J. Granty Regina Elwin and Jyothi Mandala and Balajee Maram and R. Ramesh Kumar},
keywords = {Diabetic retinopathy, Entropy Weighted and kernalized Power k-Means Clustering, Deep Convolutional Neural Network, Shepard Convolutional Neural network, Henry Gas Solubility Optimization model},
abstract = {Diabetic Retinopathy (DR) is one the most important problems of diabetics and it directs to the main cause of blindness. When proper treatment is afforded for DR patients, almost 90% of patients are protected from visual damage. DR does not produce any symptoms at the initial phase of the disease, thus various physical assessments, namely pupil dilation, visual acuity test, and so on are needed for DR disease detection. It is more complex to detect the DR during manual testing, because of the variations and complications of DR. The early detection and appropriate treatment assist to prevent vision loss for DR patients. Thus, it is very indispensable to categorize the levels and severity of DR for recommendation of essential treatment. In this paper, Autoregressive-Henry Gas Sailfish Optimization (Ar-HGSO)-based deep learning technique is proposed for DR detection and severity level classification of DR and Macular Edema (ME) based on color fundus images. The segmentation process is more essential for proper detection and classification process, which segments the image into various subgroups. The Deep Learning approach is utilized for effective identification of DR and severity classification of DR and ME. Moreover, the deep learning technique is trained by the designed Ar-HGSO scheme for obtaining better performance. The performance of the devised technique is evaluated using the IDRID dataset and DDR dataset. The introduced Ar-HGSO-based deep learning approach obtained better performance than other existing DR detection and classification techniques with regards to testing accuracy, sensitivity, and specificity of 0.9142, 0.9254, and 0.9142 using the IDRID dataset.}
}
@article{FLETCHER20131045,
title = {The development of an integrated haptic VR machining environment for the automatic generation of process plans},
journal = {Computers in Industry},
volume = {64},
number = {8},
pages = {1045-1060},
year = {2013},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513001395},
author = {Craig Fletcher and James Ritchie and Theo Lim and Raymond Sung},
keywords = {CAPP, Haptics, Virtual reality, Virtual machining, Prototyping, Simulation},
abstract = {In modern manufacturing it is crucial to be able to produce a product right first time, as efficiently as possible. Virtual prototyping can play a key part in fulfilling this requirement, allowing the simulation of production in advance without the need for unnecessary downtime, material waste or increased lead times. This makes virtual reality (VR) potentially an ideal planning tool since, with each simulation, a better understanding of requirements can be reached and a more effective solution found. This application of haptic virtual reality is particularly relevant in the field of machining where both material and resource costs are high. Planning for the machining of a product requires an intricate knowledge of materials, processes and machining methods. Intuitive virtual reality interfaces can provide new, engineer-friendly means of generating such manufacturing sequences. Current research in haptic virtual reality focuses on specific individual aspects of machining such as path planning or simulation for training; however, in order to simulate multi-operation machining with a view to generating practical and usable process plans, in which there are several processes involved in bringing a product to fruition; an approach is required which allows this type of functionality. This paper presents such a haptic virtual process planning system that allows an operator to load and set up a billet for machining, set up, sequence and tear down any combination of milling, drilling or turning operations then produce a time-estimated process plan. All of the human expertise captured during this process is logged and parsed to generate the final instructions including operation times, tool lists, operation lists and route sheets in easy-to-read format. Via three planning examples, the generation of process plans is demonstrated; with these subsequently validated through their use by a shop floor machinist who utilised them to produce the parts, and a comparison to previously existing plans for the same parts.}
}
@article{DEVI2019235,
title = {Hiding medical information in brain MR images without affecting accuracy of classifying pathological brain},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {235-246},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18322647},
author = {Swagatika Devi and Manmath Narayan Sahoo and Khan Muhammad and Weiping Ding and Sambit Bakshi},
keywords = {Medical data hiding, Pathological brain detection, Steganography, Privacy preservation, MR image classification, Internet of Medical Things},
abstract = {This work aims at validating the effect of steganography on a Computer-Aided Diagnosis (CAD) system for segregating pathological brain from normal brain from seeing MR images. Usually MR images of the brain achieves significant accuracy in classifying “normal” and “abnormal” brains. However, the brain images labeled with patient information is a severe threat to privacy preservation of the medical data, especially when the medical data is a part of Internet of Medical Things (IoMT) and travels over the network. Therefore, our article investigates on a two-fold objective i.e. ensuring visual hiding of steganographic data containing identity of the patient and at the same time retaining almost similar classification accuracy towards detecting a diseased MR brain image. In this approach, the identity of the patient is hidden in the image itself by employing a spatial steganographic algorithm on the T2-weighted MR images, thereby ensuring minimal chance of misuse of the private medical data. Two MR image datasets collected from Harvard whole brain Atlas have been used to validate our framework comparing against some well performing classification techniques. The classification techniques are applied on normal as well as stego images. It is concluded that using a steganographic algorithm on the MR images does not much affect the classification rate and yield equally satisfactory performance.}
}
@article{NAYAK2016188,
title = {Brain MR image classification using two-dimensional discrete wavelet transform and AdaBoost with random forests},
journal = {Neurocomputing},
volume = {177},
pages = {188-197},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017531},
author = {Deepak Ranjan Nayak and Ratnakar Dash and Banshidhar Majhi},
keywords = {Magnetic resonance imaging (MRI), Discrete wavelet transform (DWT), Probabilistic principal component analysis (PPCA), AdaBoost with random forests (ADBRF), Computer-aided diagnosis (CAD)},
abstract = {This paper presents an automated and accurate computer-aided diagnosis (CAD) system for brain magnetic resonance (MR) image classification. The system first utilizes two-dimensional discrete wavelet transform (2D DWT) for extracting features from the images. After feature vector normalization, probabilistic principal component analysis (PPCA) is employed to reduce the dimensionality of the feature vector. The reduced features are applied to the classifier to categorize MR images into normal and abnormal. This scheme uses an AdaBoost algorithm with random forests as its base classifier. Three benchmark MR image datasets, Dataset-66, Dataset-160, and Dataset-255, have been used to validate the proposed system. A 5×5-fold stratified cross validation scheme is used to enhance the generalization capability of the proposed scheme. Simulation results are compared with the existing schemes and it is observed that the proposed scheme outperforms others in all the three datasets.}
}
@article{FIORUCCI2012835,
title = {Objective and subjective quality assessment between JPEG XR with overlap and JPEG 2000},
journal = {Journal of Visual Communication and Image Representation},
volume = {23},
number = {6},
pages = {835-844},
year = {2012},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2012.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S104732031200079X},
author = {Federico Fiorucci and Giuseppe Baruffa and Fabrizio Frescura},
keywords = {JPEG XR, JPEG 2000, Overlap operator, Subjective assessment, Stimulus comparison, Digital projection, VIF, Coding complexity},
abstract = {JPEG XR (eXtended Range) is a recently standardized format for still images compression. It adopts a Lapped Biorthogonal Transform (LBT) that helps in reducing visual artifacts, in particular those due to blocking effects. In this paper, we compare JPEG XR with JPEG 2000, in terms of both objective and subjective visual quality. The adopted objective parameters are the computational complexity and the PSNR. In order to improve the analysis, and to evaluate if JPEG XR can be a feasible alternative to JPEG 2000, subjective tests in a projector-based environment have been set up, from which the benefits of the overlap operator of JPEG XR have been assessed, especially at high compression ratios.}
}
@article{SUN201315,
title = {Lung segmentation refinement based on optimal surface finding utilizing a hybrid desktop/virtual reality user interface},
journal = {Computerized Medical Imaging and Graphics},
volume = {37},
number = {1},
pages = {15-27},
year = {2013},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0895611113000050},
author = {Shanhui Sun and Milan Sonka and Reinhard R. Beichel},
keywords = {Lung segmentation, Virtual reality, Segmentation refinement, Optimal surface finding, Computed tomography},
abstract = {Recently, the optimal surface finding (OSF) and layered optimal graph image segmentation of multiple objects and surfaces (LOGISMOS) approaches have been reported with applications to medical image segmentation tasks. While providing high levels of performance, these approaches may locally fail in the presence of pathology or other local challenges. Due to the image data variability, finding a suitable cost function that would be applicable to all image locations may not be feasible. This paper presents a new interactive refinement approach for correcting local segmentation errors in the automated OSF-based segmentation. A hybrid desktop/virtual reality user interface was developed for efficient interaction with the segmentations utilizing state-of-the-art stereoscopic visualization technology and advanced interaction techniques. The user interface allows a natural and interactive manipulation of 3-D surfaces. The approach was evaluated on 30 test cases from 18 CT lung datasets, which showed local segmentation errors after employing an automated OSF-based lung segmentation. The performed experiments exhibited significant increase in performance in terms of mean absolute surface distance errors (2.54±0.75mm prior to refinement vs. 1.11±0.43mm post-refinement, p≪0.001). Speed of the interactions is one of the most important aspects leading to the acceptance or rejection of the approach by users expecting real-time interaction experience. The average algorithm computing time per refinement iteration was 150ms, and the average total user interaction time required for reaching complete operator satisfaction was about 2min per case. This time was mostly spent on human-controlled manipulation of the object to identify whether additional refinement was necessary and to approve the final segmentation result. The reported principle is generally applicable to segmentation problems beyond lung segmentation in CT scans as long as the underlying segmentation utilizes the OSF framework. The two reported segmentation refinement tools were optimized for lung segmentation and might need some adaptation for other application domains.}
}
@article{WAN2020102636,
title = {Fusion identification and estimation of multisensor multichannel AR signals with missing measurements and sensor biases},
journal = {Digital Signal Processing},
volume = {98},
pages = {102636},
year = {2020},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2019.102636},
url = {https://www.sciencedirect.com/science/article/pii/S1051200419301903},
author = {Tao Wan and Shuli Sun},
keywords = {Multisensor multichannel AR signal, Missing measurement, Self-tuning filter, Sensor bias, Fusion identification and estimation},
abstract = {This paper studies the problem of fusion identification and estimation for multisensor multichannel autoregressive (AR) signals with unknown missing measurement rates, model parameters (MPs), sensor biases and measurement noise variances. Optimal local filters, cross-covariance matrices (CCMs), and a distributed fusion filter for AR signal are presented when the system model is accurately known, respectively. A self-tuning (ST) fusion filter is presented when MPs of AR signal, sensor biases, missing measurement rates and measurement noise variances in the system model are unknown. First, MPs of AR signal and sensor biases are identified by using a multi-dimensional recursive extended least squares (MRELS) algorithm. Further, distributed fusion estimates of MPs of AR signal are obtained by using a matrix-weighted optimal fusion estimation algorithm in the sense of linear unbiased minimum variance (LUMV). Then, receiving measurement rates and measurement noise variances are identified by correlation functions. Finally, substituting the identified parameters into the proposed optimal filtering algorithms, a matrix-weighted self-tuning (ST) fusion filtering algorithm is obtained. An example verifies the effectiveness of algorithms.}
}
@article{ARRATIALOPEZ2023102925,
title = {WarpPINN: Cine-MR image registration with physics-informed neural networks},
journal = {Medical Image Analysis},
volume = {89},
pages = {102925},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102925},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001858},
author = {Pablo {Arratia López} and Hernán Mella and Sergio Uribe and Daniel E. Hurtado and Francisco {Sahli Costabal}},
keywords = {Cardiac strain, Cardiac mechanics, Image registration, Physics-informed neural networks},
abstract = {The diagnosis of heart failure usually includes a global functional assessment, such as ejection fraction measured by magnetic resonance imaging. However, these metrics have low discriminate power to distinguish different cardiomyopathies, which may not affect the global function of the heart. Quantifying local deformations in the form of cardiac strain can provide helpful information, but it remains a challenge. In this work, we introduce WarpPINN, a physics-informed neural network to perform image registration to obtain local metrics of heart deformation. We apply this method to cine magnetic resonance images to estimate the motion during the cardiac cycle. We inform our neural network of the near-incompressibility of cardiac tissue by penalizing the Jacobian of the deformation field. The loss function has two components: an intensity-based similarity term between the reference and the warped template images, and a regularizer that represents the hyperelastic behavior of the tissue. The architecture of the neural network allows us to easily compute the strain via automatic differentiation to assess cardiac activity. We use Fourier feature mappings to overcome the spectral bias of neural networks, allowing us to capture discontinuities in the strain field. The algorithm is tested on synthetic examples and on a cine SSFP MRI benchmark of 15 healthy volunteers, where it is trained to learn the deformation mapping of each case. We outperform current methodologies in landmark tracking and provide physiological strain estimations in the radial and circumferential directions. WarpPINN provides precise measurements of local cardiac deformations that can be used for a better diagnosis of heart failure and can be used for general image registration tasks. Source code is available at https://github.com/fsahli/WarpPINN.}
}
@article{AFYOUNI202448,
title = {Usability Evaluation of Handheld and Wearable AR devices: Exploring Collaboration and the Role of Physical Props},
journal = {Procedia Computer Science},
volume = {231},
pages = {48-55},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923021683},
author = {Imad Afyouni and Mohammed Lataifeh and Zulaiha Afrah and Naveed Ahmed},
keywords = {Augmented Reality, Collaboration, Physical Props, User Interaction},
abstract = {This work presents a study to evaluate the usability of handheld and wearable Augmented Reality (AR) devices for object manipulation in an individual and collaborative AR environment. We propose a scenario that incorporates physical props to foster an immersive AR experience, with tasks involving moving a virtual light source, changing the color of a virtual object anchored to the prop, and placing the object on the light. A controlled evaluation was performed that includes both quantitative and qualitative analyses. Two test sessions were conducted - an individual and a collaborative session. In the individual session, the participants completed the experiment using a smartphone and Magic Leap One AR headset, with tasks carried out via a touch screen on the smartphone and controller / hand gestures on the AR headset. Twelve participants were paired up to perform the same tasks in a shared AR space for the collaborative sessions. We performed statistical analysis on the collected data to provide insights into statistically significant results. The results demonstrated that though AR headsets are better suited for interactions that require spatial movements in the three-dimensional space, the difference in time taken to execute these interactions compared to a smartphone was not statistically significant. On the other hand, executing gestures on a smartphone was significantly faster compared to hand-based gestures using the AR headset.}
}
@article{WEBER2014272,
title = {Semi-active vibration absorber based on real-time controlled MR damper},
journal = {Mechanical Systems and Signal Processing},
volume = {46},
number = {2},
pages = {272-288},
year = {2014},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2014.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0888327014000508},
author = {F. Weber},
keywords = {Control, MR damper, Tuned mass damper, Negative stiffness, Semi-active, Vibration absorber},
abstract = {A semi-active vibration absorber with real-time controlled magnetorheological damper (MR-SVA) for the mitigation of harmonic structural vibrations is presented. The MR damper force targets to realize the frequency and damping adaptations to the actual structural frequency according to the principle of the undamped vibration absorber. The relative motion constraint of the MR-SVA is taken into account by an adaptive nonlinear control of the internal damping of the MR-SVA. The MR-SVA is numerically and experimentally validated for harmonic excitation of the primary structure when the natural frequency of the passive mass spring system of the MR-SVA is correctly tuned to the targeted structural resonance frequency and when de-tuning is present. The results demonstrate that the MR-SVA outperforms the passive TMD at structural resonance frequency by at least 12.4% and up to 60.0%.}
}
@article{ZHAO2022105042,
title = {A CNN-based multi-target fast classification method for AR-SSVEP},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105042},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105042},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008362},
author = {Xincan Zhao and Yulin Du and Rui Zhang},
keywords = {Convolutional neural network, Augmented reality, Brain–computer interfaces, Steady-state visual evoked potentials},
abstract = {Because an augmented-reality-based brain-computer interface (AR-BCI) is easily disturbed by external factors, the traditional electroencephalograph (EEG) classification algorithms fail to meet the real-time processing requirements with a large number of stimulus targets or in a real environment. We propose a multi-target fast classification method for augmented-reality-based steady-state visual evoked potential (AR-SSVEP), using a convolutional neural network (CNN). To explore the availability and accuracy of high-efficiency multi-target classification methods in AR-SSVEP with a short stimulation duration, a similar stimulus layout was used for a computer screen (PC) and an optical see-through head-mounted display (OST-HMD) device (HoloLens). The experiment included nine flicker stimuli of different frequencies, and a multi-target fast classification method based on a CNN was constructed to complete nine classification tasks, for which the average accuracy of AR-BCI in our CNN model at 0.5- and 1-s stimulus duration was 67.93% and 80.83%, respectively. These results verified the efficacy of the proposed model for processing multi-target classification in AR-BCI.}
}
@article{SUAREZWARDEN2015306,
title = {Test of Voltage for Electrical Diagnosis Aided by AR in Equipment Adaptation or Predictive Maintenance},
journal = {Procedia Computer Science},
volume = {75},
pages = {306-315},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037138},
author = {Fernando Suárez-Warden and Eduardo González Mendívil},
keywords = {voltage test, Augmented Reality, diagnostic and functional tests, predictive maintenance, equipment adaptation.},
abstract = {Electrical flops, as typical problems of machines which components suffer unsatisfactory maintenance or which parts, subject to wear or diagnostic errors or with design mistakes, occur due to inefficient adaptation. So when a flaw appears or a maintenance program suggests monitoring for an electrical part, a mandatory diagnosis process (to determine components condition) must be executed. Similarly if an investigation tries to undertake an adaptation of machine to the process, various functional tests are commanded among which the voltage test is distinguished. A diagram of electrical test for being aided by Augmented Reality and a related study case are developed.}
}
@article{RUEDA2013113,
title = {Single-image super-resolution of brain MR images using overcomplete dictionaries},
journal = {Medical Image Analysis},
volume = {17},
number = {1},
pages = {113-132},
year = {2013},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2012.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841512001326},
author = {Andrea Rueda and Norberto Malpica and Eduardo Romero},
keywords = {Image super-resolution, Sparse representation, Principal component analysis, Magnetic resonance imaging},
abstract = {Resolution in Magnetic Resonance (MR) is limited by diverse physical, technological and economical considerations. In conventional medical practice, resolution enhancement is usually performed with bicubic or B-spline interpolations, strongly affecting the accuracy of subsequent processing steps such as segmentation or registration. This paper presents a sparse-based super-resolution method, adapted for easily including prior knowledge, which couples up high and low frequency information so that a high-resolution version of a low-resolution brain MR image is generated. The proposed approach includes a whole-image multi-scale edge analysis and a dimensionality reduction scheme, which results in a remarkable improvement of the computational speed and accuracy, taking nearly 26min to generate a complete 3D high-resolution reconstruction. The method was validated by comparing interpolated and reconstructed versions of 29 MR brain volumes with the original images, acquired in a 3T scanner, obtaining a reduction of 70% in the root mean squared error, an increment of 10.3dB in the peak signal-to-noise ratio, and an agreement of 85% in the binary gray matter segmentations. The proposed method is shown to outperform a recent state-of-the-art algorithm, suggesting a substantial impact in voxel-based morphometry studies.}
}
@article{VGRAVES2023102283,
title = {Siamese pyramidal deep learning network for strain estimation in 3D cardiac cine-MR},
journal = {Computerized Medical Imaging and Graphics},
volume = {108},
pages = {102283},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102283},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123001015},
author = {Catharine {V. Graves} and Marina F.S. Rebelo and Ramon A. Moreno and Roberto N. Dantas-Jr and Antonildes N. Assunção-Jr and Cesar H. Nomura and Marco A. Gutierrez},
keywords = {Myocardium strain, Cardiac magnetic resonance, Deep learning},
abstract = {Strain represents the quantification of regional tissue deformation within a given area. Myocardial strain has demonstrated considerable utility as an indicator for the assessment of cardiac function. Notably, it exhibits greater sensitivity in detecting subtle myocardial abnormalities compared to conventional cardiac function indices, like left ventricle ejection fraction (LVEF). Nonetheless, the estimation of strain poses considerable challenges due to the necessity for precise tracking of myocardial motion throughout the complete cardiac cycle. This study introduces a novel deep learning-based pipeline, designed to automatically and accurately estimate myocardial strain from three-dimensional (3D) cine-MR images. Consequently, our investigation presents a comprehensive pipeline for the precise quantification of local and global myocardial strain. This pipeline incorporates a supervised Convolutional Neural Network (CNN) for accurate segmentation of the cardiac muscle and an unsupervised CNN for robust left ventricle motion tracking, enabling the estimation of strain in both artificial phantoms and real cine-MR images. Our investigation involved a comprehensive comparison of our findings with those obtained from two commonly utilized commercial software in this field. This analysis encompassed the examination of both intra- and inter-user variability. The proposed pipeline exhibited demonstrable reliability and reduced divergence levels when compared to alternative systems. Additionally, our approach is entirely independent of previous user data, effectively eliminating any potential user bias that could influence the strain analyses.}
}
@article{SRAN2021102964,
title = {Integrating saliency with fuzzy thresholding for brain tumor extraction in MR images},
journal = {Journal of Visual Communication and Image Representation},
volume = {74},
pages = {102964},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102964},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320301899},
author = {Paramveer Kaur Sran and Savita Gupta and Sukhwinder Singh},
keywords = {Saliency, Fuzzy, Segmentation, ROI, Medical Images},
abstract = {The automatic detection and extraction of tumor area in Magnetic Resonance Imaging (MRI) is an important and challenging task. This paper presents a fully automatic and unsupervised method for fast and accurate extraction of brain tumor area from MR images. The proposed method named as Saliency Based Segmentation (SBS) is based on visual saliency. The saliency model detects the pathologically important area and then fuzzy thresholding is used for extraction of the detected region. The performance of SBS is compared with Adaptively Regularized Kernel-Based Fuzzy C-Means Clustering, Mean Shift and Fuzzy C-Means clustering with Level Set Method. The experimental evaluation validated on BRATS database using Jaccard index (0.84 ± 0.04), Dice Index (0.91 ± 0.02), Execution time (2.99 ± 0.29), Precision (0.82 ± 0.16), Recall (0.97 ± 0.03) and F-measure (0.88 ± 0.10) demonstrates that SBS achieves better segmentation results even in the presence of noise and uneven illumination in images.}
}
@article{BREDIKHIN2022254,
title = {Diagnostics of motion sickness (kinetosis) and training of resistance to it in VR simulators},
journal = {Procedia Computer Science},
volume = {212},
pages = {254-263},
year = {2022},
note = {11th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017008},
author = {Artem Bredikhin and Maxim Liulukin and Ekaterina Nikitina and Dmitry Nikushchenko and Anton Stiopin and Yulia Mikholazhina},
keywords = {virtual reality, marine simulator, motion sickness, kinetosis, symptomatology, feedback},
abstract = {Approximately 5-15% of adults suffer from motion sickness (ICD-10: T75.3, ICD-9: 994.6), also known as kinetosis, in one way or another. Being prone to it may negatively affect not only the daily activities of an average person, but also maritime specialists, whose professional activity is directly related to sailing in sea. The study of motion sickness, as well as the development of simulators that would allow training a resistance and adaptation to it and its symptomatology is an urgent task for the maritime industry. Various approaches for the physiological motion sickness resistance and adaptation training exist, among which the use of virtual reality (hereinafter referred to as VR) technologies is of high importance. Key advantage of using VR is the ability to simulate any visual environment, limited only by the task, hardware and software capabilities of the researcher. Currently, VR is already being used in the study of motion sickness [1]. We investigated not only the possibility of motion sickness, but also methods of reducing manifestations and symptoms by dynamically changing the viewing angles, as well as assessing the influence of peripheral and stereoscopic vision, including their various variations. The primary goal of the study is to investigate the direct effect of a visual channel on the symptoms of motion sickness, including changes of channel parameters (e.g., changes in viewing angles), and to assess the possibility of training a control group of users with vivid symptoms to reduce them. This paper considers 50 participants to be involved in an experiment, which is based on the developed research methodology for physiological motion sickness. Series of tests is carried out to identify symptoms and conduct a training, after which the research methodology has been evaluated.}
}
@article{KATAHIRA20151595,
title = {Development and Evaluation of a System for AR Enabling Realistic Display of Gripping Motions Using Leap Motion Controller},
journal = {Procedia Computer Science},
volume = {60},
pages = {1595-1603},
year = {2015},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.269},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915023960},
author = {Reiji Katahira and Masato Soga},
keywords = {Augmented Reality, depth, Leap Motion Controller, finger, hand, 3Dmodel},
abstract = {Augmented Reality (AR) in the traditional systems have a problem that the drawn objects are always displayed in the foreground because 3D models by AR are superimposed later than the picture of the actual world. This paper proposed a system to produce a realistic picture of AR in accordance with every depth. We developed a prototype system to verify the effect of the method. The prototype system was developed by focusing on a human hand. This paper utilized a Leap Motion Controller as a motion capture device to acquire the depth data of the hand and fingers.}
}
@article{HE2016149,
title = {A mass-redistributed finite element method (MR-FEM) for acoustic problems using triangular mesh},
journal = {Journal of Computational Physics},
volume = {323},
pages = {149-170},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0021999116303205},
author = {Z.C. He and Eric Li and G.R. Liu and G.Y. Li and A.G. Cheng},
keywords = {Acoustic, Numerical method, Mass-redistributed finite element method (MR-FEM), Dispersion error},
abstract = {The accuracy of numerical results using standard finite element method (FEM) in acoustic problems will deteriorate with increasing frequency due to the “dispersion error”. Such dispersion error depends on the balance between the “stiffness” and “mass” of discretization equation systems. This paper reports an improved finite element method (FEM) for solving acoustic problems by re-distributing the mass in the mass matrix to “tune” the balance, aiming to minimize the dispersion errors. This is done by shifting the integration point locations when computing the entries of the mass matrix, while ensuring the mass conservation. The new method is verified through the detailed numerical error analysis, and a strategy is also proposed for the best mass redistribution in terms of minimizing dispersion error. The relative dispersion error of present mass-redistributed finite element method (MR-FEM) is found to be much smaller than the FEM solution, in both theoretical prediction and numerical examination. The present MR-FEM works well by using the linear triangular elements that can be generated automatically, which enables automation in computation and saving computational cost in mesh generation. Numerical examples demonstrate the advantages of MR-FEM, in comparison with the standard FEM using the same triangular meshes and quadrilateral meshes.}
}
@article{CHEN2021102810,
title = {Harmonized neonatal brain MR image segmentation model for cross-site datasets},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102810},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102810},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421004079},
author = {Jian Chen and Yue Sun and Zhenghan Fang and Weili Lin and Gang Li and Li Wang},
keywords = {Neonatal brain, Artifacts, Cross-site datasets, Segmentation, CycleGAN, Cross-time},
abstract = {Accurate segmentation of white matter, gray matter and cerebrospinal fluid from neonatal brain MR images is of great importance in characterizing early brain development. Deep-learning-based methods have been successfully applied to neonatal brain MRIs with superior performance if testing subjects were acquired with the same imaging protocols/scanners as training subjects. However, for the testing subjects acquired with different imaging protocols/scanners, they cannot achieve accurate segmentation results due to large appearance/pattern differences between the testing and training subjects. Besides, imaging artifacts, like head motion, which are inevitable during the imaging acquisition process, also pose a challenge for the segmentation methods. To address these issues, in this paper, we propose a harmonized neonatal brain MR image segmentation model that harmonizes testing images acquired by different protocols/scanners into the domain of training images through a cycle-consistent generative adversarial network (CycleGAN). Meanwhile, the artifacts can be largely alleviated during the harmonization. Then, a densely-connected U-Net based segmentation model trained in the domain of training images can be applied robustly for segmenting the harmonized testing images. Comparisons with existing methods illustrate the better performance of the proposed method on neonatal brain MR images from cross-sites, a grand segmentation challenge, as well as images with artifacts.}
}
@article{ESTAKHRAJI2023102227,
title = {On the effect of training database size for MR-based synthetic CT generation in the head},
journal = {Computerized Medical Imaging and Graphics},
volume = {107},
pages = {102227},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102227},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000459},
author = {Seyed Iman Zare Estakhraji and Ali Pirasteh and Tyler Bradshaw and Alan McMillan},
keywords = {Generative adversarial networks (GAN), Synthetic CT generation, Fine-Tuning, MR-guided radiotherapy},
abstract = {Generation of computed tomography (CT) images from magnetic resonance (MR) images using deep learning methods has recently demonstrated promise in improving MR-guided radiotherapy and PET/MR imaging.
Purpose:
To investigate the performance of unsupervised training using a large number of unpaired data sets as well as the potential gain in performance after fine-tuning with supervised training using spatially registered data sets in generation of synthetic computed tomography (sCT) from magnetic resonance (MR) images.
Materials and methods:
A cycleGAN method consisting of two generators (residual U-Net) and two discriminators (patchGAN) was used for unsupervised training. Unsupervised training utilized unpaired T1-weighted MR and CT images (2061 sets for each modality). Five supervised models were then fine-tuned starting with the generator of the unsupervised model for 1, 10, 25, 50, and 100 pairs of spatially registered MR and CT images. Four supervised training models were also trained from scratch for 10, 25, 50, and 100 pairs of spatially registered MR and CT images using only the residual U-Net generator. All models were evaluated on a holdout test set of spatially registered images from 253 patients, including 30 with significant pathology. sCT images were compared against the acquired CT images using mean absolute error (MAE), Dice coefficient, and structural similarity index (SSIM). sCT images from 60 test subjects generated by the unsupervised, and most accurate of the fine-tuned and supervised models were qualitatively evaluated by a radiologist.
Results:
While unsupervised training produced realistic-appearing sCT images, addition of even one set of registered images improved quantitative metrics. Addition of more paired data sets to the training further improved image quality, with the best results obtained using the highest number of paired data sets (n=100). Supervised training was found to be superior to unsupervised training, while fine-tuned training showed no clear benefit over supervised learning, regardless of the training sample size.
Conclusion:
Supervised learning (using either fine tuning or full supervision) leads to significantly higher quantitative accuracy in the generation of sCT from MR images. However, fine-tuned training using both a large number of unpaired image sets was generally no better than supervised learning using registered image sets alone, suggesting the importance of well registered paired data set for training compared to a large set of unpaired data.}
}
@article{WANG2024102402,
title = {A 3D framework for segmentation of carotid artery vessel wall and identification of plaque compositions in multi-sequence MR images},
journal = {Computerized Medical Imaging and Graphics},
volume = {116},
pages = {102402},
year = {2024},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102402},
url = {https://www.sciencedirect.com/science/article/pii/S089561112400079X},
author = {Jian Wang and Fan Yu and Mengze Zhang and Jie Lu and Zhen Qian},
keywords = {Atherosclerosis, Carotid artery, Multi-sequence MR images, Plaque components, Segmentation},
abstract = {Accurately assessing carotid artery wall thickening and identifying risky plaque components are critical for early diagnosis and risk management of carotid atherosclerosis. In this paper, we present a 3D framework for automated segmentation of the carotid artery vessel wall and identification of the compositions of carotid plaque in multi-sequence magnetic resonance (MR) images under the challenge of imperfect manual labeling. Manual labeling is commonly done in 2D slices of these multi-sequence MR images and often lacks perfect alignment across 2D slices and the multiple MR sequences, leading to labeling inaccuracies. To address such challenges, our framework is split into two parts: a segmentation subnetwork and a plaque component identification subnetwork. Initially, a 2D localization network pinpoints the carotid artery’s position, extracting the region of interest (ROI) from the input images. Following that, a signed-distance-map-enabled 3D U-net (Çiçek etal, 2016)an adaptation of the nnU-net (Ronneberger and Fischer, 2015) segments the carotid artery vessel wall. This method allows for the concurrent segmentation of the vessel wall area using the signed distance map (SDM) loss (Xue et al., 2020) which regularizes the segmentation surfaces in 3D and reduces erroneous segmentation caused by imperfect manual labels. Subsequently, the ROI of the input images and the obtained vessel wall masks are extracted and combined to obtain the identification results of plaque components in the identification subnetwork. Tailored data augmentation operations are introduced into the framework to reduce the false positive rate of calcification and hemorrhage identification. We trained and tested our proposed method on a dataset consisting of 115 patients, and it achieves an accurate segmentation result of carotid artery wall (0.8459 Dice), which is superior to the best result in published studies (0.7885 Dice). Our approach yielded accuracies of 0.82, 0.73 and 0.88 for the identification of calcification, lipid-rich core and hemorrhage components. Our proposed framework can be potentially used in clinical and research settings to help radiologists perform cumbersome reading tasks and evaluate the risk of carotid plaques.}
}
@article{HE2023106839,
title = {An optimized segmentation convolutional neural network with dynamic energy loss function for 3D reconstruction of lumbar spine MR images},
journal = {Computers in Biology and Medicine},
volume = {160},
pages = {106839},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106839},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523003049},
author = {Siyuan He and Qi Li and Xianda Li and Mengchao Zhang},
keywords = {Loss function, Lumbar spine 3D reconstruction, MR image Segmentation, Convolutional neural network},
abstract = {3D reconstruction for lumbar spine based on segmentation of Magnetic Resonance (MR) images is meaningful for diagnosis of degenerative lumbar spine diseases. However, spine MR images with unbalanced pixel distribution often cause the segmentation performance of Convolutional Neural Network (CNN) reduced. Designing a composite loss function for CNN is an effective way to enhance the segmentation capacity, yet composition loss values with fixed weight may still cause underfitting in CNN training. In this study, we designed a composite loss function with a dynamic weight, called Dynamic Energy Loss, for spine MR images segmentation. In our loss function, the weight percentage of different loss values could be dynamically adjusted during training, thus CNN could fast converge in earlier training stage and focus on detail learning in the later stage. Two datasets were used in control experiments, and the U-net CNN model with our proposed loss function achieved superior performance with Dice similarity coefficient values of 0.9484 and 0.8284 respectively, which were also verified by the Pearson correlation, Bland-Altman, and intra-class correlation coefficient analysis. Furthermore, to improve the 3D reconstruction based on the segmentation results, we proposed a filling algorithm to generate contextually related slices by computing the pixel-level difference between adjacent slices of segmented images, which could enhance the structural information of tissues between slices, and improve the performance of 3D lumbar spine model rendering. Our methods could help radiologists to build a 3D lumbar spine graphical model accurately for diagnosis while reducing burden of manual image reading.}
}
@article{HULSMANN201847,
title = {Classification of motor errors to provide real-time feedback for sports coaching in virtual reality — A case study in squats and Tai Chi pushes},
journal = {Computers & Graphics},
volume = {76},
pages = {47-59},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301304},
author = {Felix Hülsmann and Jan Philip Göpfert and Barbara Hammer and Stefan Kopp and Mario Botsch},
keywords = {Sports coaching in virtual reality, Motor learning environments, Motor performance quality, Human motion analysis, Auto-generated augmented feedback},
abstract = {For successful fitness coaching in virtual reality, movements of a trainee must be analyzed in order to provide feedback. To date, most coaching systems only provide coarse information on movement quality. We propose a novel pipeline to detect a trainee’s errors during exercise that is designed to automatically generate feedback for the trainee. Our pipeline consists of an online temporal warp of a trainee’s motion, followed by Random-Forest-based feature selection. The selected features are used for the classification performed by Support Vector Machines. Our feedback to the trainee can consist of predefined verbal information as well as automatically generated visual augmentations. For the latter, we exploit information on feature importance to generate real-time feedback in terms of augmented color highlights on the trainee’s avatar. We show our pipeline’s superiority over two popular approaches from human activity recognition applied to our problem, k-Nearest Neighbor, combined with Dynamic Time Warping (KNN-DTW), as well as a recent combination of Convolutional Neural Networks with a Long Short-term Memory Network. We compare classification quality, time needed for classification, as well as the classifiers’ ability to automatically generate augmented feedback. In an exemplary application, we demonstrate that our pipeline is suitable to deliver verbal as well as automatically generated augmented feedback inside a CAVE-based sports training environment in virtual reality.}
}
@article{CHEN2019416,
title = {ImmerTai: Immersive Motion Learning in VR Environments},
journal = {Journal of Visual Communication and Image Representation},
volume = {58},
pages = {416-427},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.11.039},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318303183},
author = {Xiaoming Chen and Zhibo Chen and Ye Li and Tianyu He and Junhui Hou and Sen Liu and Ying He},
keywords = {Immersive education, Motion training, VR education},
abstract = {Immersive learning in Virtual Reality (VR) environments is the developing trend for future education systems including remote physical training. This paper presents “ImmerTai”, a system that is designed for effective remote motion training, particularly for Chinese Taichi, in an immersive way. With ImmerTai, the Taichi expert’s motion is captured and delivered to remote students in CAVE, HMD and PC environments for learning. The students’ motions are also captured for motion quality assessment and a group of students can form a virtual collaborative learning scenario. We built up a Taichi motion dataset with ground truth of motion quality, and based on this, we developed and evaluated several motion quality assessment methods. Then, user tests were designed and carried out to measure and compare the learning outcomes (learning time, quality and overall efficiency) of students in Cave Automatic Virtual Environment (CAVE), Head Mounted Display (HMD) and Personal Computer (PC) environments. Meanwhile, the connections between students’ learning outcomes and their VR experience were investigated and discussed too. Our results show that ImmerTai can accelerate the learning process of students noticeably (up to 17%) compared to non-immersive learning with the conventional PC setup. However, we observed a substantial difference in the quality of the learnt motion between CAVE (26% gain) and HMD (23% drop) compared to PC (baseline). While strong VR presence can enhance the learning experience of students, their learning outcomes are not fully consistent to their experience. Overall, ImmerTai with CAVE demonstrated a significantly higher learning efficiency than other tested environments.}
}
@article{MASAD2024107842,
title = {CT-based generation of synthetic-pseudo MR images with different weightings for human knee},
journal = {Computers in Biology and Medicine},
volume = {169},
pages = {107842},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107842},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523013070},
author = {Ihssan S. Masad and Isam F. Abu-Qasmieh and Hiam H. Al-Quran and Khaled Z. Alawneh and Khalid M. Abdalla and Ali M. Al-Qudah},
keywords = {Synthetic MRI, Computed tomography, Spin echo, Human knee},
abstract = {Synthetic MR images are generated for their high soft-tissue contrast avoiding the discomfort by the long acquisition time and placing claustrophobic patients in the MR scanner's confined space. The aim of this study is to generate synthetic pseudo-MR images from a real CT image for the knee region in vivo. 19 healthy subjects were scanned for model training, while 13 other healthy subjects were imaged for testing. The approach used in this work is novel such that the registration was performed between the MR and CT images, and the femur bone, patella, and the surrounding soft tissue were segmented on the CT image. The tissue type was mapped to its corresponding mean and standard deviation values of the CT# of a window moving on each pixel in the reconstructed CT images, which enabled the remapping of the tissue to its MRI intrinsic parameters: T1, T2, and proton density (ρ). To generate the synthetic MR image of a knee slice, a classic spin-echo sequence was simulated using proper intrinsic and contrast parameters. Results showed that the synthetic MR images were comparable to the real images acquired with the same TE and TR values, and the average slope between them (for all knee segments) was 0.98, while the average percentage root mean square difference (PRD) was 25.7%. In conclusion, this study has shown the feasibility and validity of accurately generating synthetic MR images of the knee region in vivo with different weightings from a single real CT image.}
}
@article{LIN2018130,
title = {Integrated BIM, game engine and VR technologies for healthcare design: A case study in cancer hospital},
journal = {Advanced Engineering Informatics},
volume = {36},
pages = {130-145},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617303373},
author = {Yu-Cheng Lin and Yen-Pei Chen and Huey-Wen Yien and Chao-Yung Huang and Yu-Chih Su},
keywords = {Building information modeling, BIM, Healthcare design, Virtual reality, Game engine, Design phase, Communication, Semi-immersed VR},
abstract = {The results of healthcare design should meet the requirements of design teams as well as healthcare stakeholders. However, misunderstandings that occur between the design teams and healthcare stakeholders when using 2D illustrations leads to the need for re-design and rework during the design phase. To overcome this problem, this study develops a Database-supported VR/BIM-based Communication and Simulation (DVBCS) system integrated with BIM, game engine and VR technologies for healthcare design special in the Semi-immersed VR environment. The DVBCS system is applied in a case study of a design project of a cancer center in Taiwan to verify the system and demonstrate its effectiveness in practice. The results demonstrate that a DVBCS system is an effective visual communication and simulation platform for healthcare design. The advantage of the DVBCS system lies not only in improving the communication efficiency between the design teams and healthcare stakeholders, but also in facilitating visual interactions and easing the decision-making process while communicating in the 3D VR/BIM environment. The effective use of the proposed DVBCS system will assist design teams and stakeholders significantly in systematically handling healthcare design work in future healthcare design.}
}
@article{WANG2021325,
title = {Denoising auto-encoding priors in undecimated wavelet domain for MR image reconstruction},
journal = {Neurocomputing},
volume = {437},
pages = {325-338},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.09.086},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221000990},
author = {Siyuan Wang and Junjie Lv and Zhuonan He and Dong Liang and Yang Chen and Minghui Zhang and Qiegen Liu},
keywords = {MRI, Image reconstruction, Undecimated wavelet transform, Denoising autoencoding, Proximal gradient descent},
abstract = {Compressive sensing is an impressive approach for fast MRI. It aims at reconstructing MR image using only a few under-sampled data in k-space, enhancing the efficiency of the data acquisition. In this study, we propose to learn priors based on undecimated wavelet transform and an iterative image reconstruction algorithm. At the stage of prior learning, transformed feature images obtained by undecimated wavelet transform are stacked as an input of denoising autoencoder network (DAE). The highly redundant and multi-scale input enables the correlation of feature images at different channels, which allows a robust network-driven prior. At the iterative reconstruction, the transformed DAE prior is incorporated into the classical iterative procedure by means of proximal gradient algorithm. Experimental comparisons on different sampling trajectories and ratios validated the great potential of the presented algorithm.}
}
@article{TAN201778,
title = {Convolutional neural network regression for short-axis left ventricle segmentation in cardiac cine MR sequences},
journal = {Medical Image Analysis},
volume = {39},
pages = {78-86},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300543},
author = {Li Kuo Tan and Yih Miin Liew and Einly Lim and Robert A. McLaughlin},
keywords = {Cardiac MRI, LV segmentation, Deep learning, Convolutional neural networks},
abstract = {Automated left ventricular (LV) segmentation is crucial for efficient quantification of cardiac function and morphology to aid subsequent management of cardiac pathologies. In this paper, we parameterize the complete (all short axis slices and phases) LV segmentation task in terms of the radial distances between the LV centerpoint and the endo- and epicardial contours in polar space. We then utilize convolutional neural network regression to infer these parameters. Utilizing parameter regression, as opposed to conventional pixel classification, allows the network to inherently reflect domain-specific physical constraints. We have benchmarked our approach primarily against the publicly-available left ventricle segmentation challenge (LVSC) dataset, which consists of 100 training and 100 validation cardiac MRI cases representing a heterogeneous mix of cardiac pathologies and imaging parameters across multiple centers. Our approach attained a .77 Jaccard index, which is the highest published overall result in comparison to other automated algorithms. To test general applicability, we also evaluated against the Kaggle Second Annual Data Science Bowl, where the evaluation metric was the indirect clinical measures of LV volume rather than direct myocardial contours. Our approach attained a Continuous Ranked Probability Score (CRPS) of .0124, which would have ranked tenth in the original challenge. With this we demonstrate the effectiveness of convolutional neural network regression paired with domain-specific features in clinical segmentation.}
}
@article{MECHETER2022232,
title = {Brain MR images segmentation using 3D CNN with features recalibration mechanism for segmented CT generation},
journal = {Neurocomputing},
volume = {491},
pages = {232-243},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222003332},
author = {Imene Mecheter and Maysam Abbod and Habib Zaidi and Abbes Amira},
keywords = {Pseudo CT, MR images, Segmentation, CNN, Features recalibration},
abstract = {The segmentation of MR (magnetic resonance) images is a simple approach to create Pseudo CT images which are useful for many medical imaging analysis applications. One of the main challenges of this process is the bone segmentation of brain MR images. Deep convolutional neural networks (CNNs) have been widely and efficiently applied to perform MR images segmentation. The aim of this work is to propose a novel excitation-based CNN by recalibrating the network features adaptively to enhance the bone segmentation by segmenting the brain MR images into three tissue classes: bone, soft tissue, and air. The proposed method combines two types of features excitation mechanisms namely: (1) spatial squeeze and channel excitation block (cSE) and (2) channel squeeze and spatial excitation block (sSE). The two blocks are combined sequentially and integrated seamlessly into a 3D convolutional encoder decoder network. The novelty of this work emerges in the combination of the two excitation blocks sequentially to improve the segmentation performance and reduce the model complexity. The proposed approach is evaluated through a comparison with computed tomography (CT) images as ground truth and validated with other methods in the literature that applied deep CNN approaches to perform MR image segmentation for PET attenuation correction. Brain MR and CT datasets which consist of 50 patients are used to evaluate the proposed method. The segmentation performance of the three brain classes is evaluated using precision, recall, dice similarity coefficient (DSC), and Jaccard index. The presented method improves the bone tissue segmentation compared to the baseline model and other methods in the literature where the DSC is improved from 0.6278 ± 0.0006 to 0.6437  ±  0.0006 with an improvement percentage of 2.53% for bone class. The proposed excitation-based segmentation network architecture demonstrates promising and competitive results compared with other methods in the literature and reduces the model complexity thanks to the sequential combination of the two excitation blocks.}
}
@incollection{BRAND2012930,
title = {Validation of an absorber model of carbon dioxide capture in an aqueous amine solvent developed based on the SAFT-VR framework},
editor = {Iftekhar A. Karimi and Rajagopalan Srinivasan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {31},
pages = {930-934},
year = {2012},
booktitle = {11th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59506-5.50017-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444595065500171},
author = {C.V. Brand and J. Rodríguez and A. Galindo and G. Jackson and C.S. Adjiman},
keywords = {CO capture, reactive separation, SAFT-VR, monoethanolamine},
abstract = {The development of a model of an absorber for the removal of carbon dioxide (CO2) from flue gas using aqueous solutions of monoethanolamine (MEA) is presented. This novel model incorporates state-of-the-art SAFT-VR thermodynamics into a rate-based process model. A characteristic of the proposed approach is that all the reactions are treated within a thermodynamic description, assuming chemical equilibrium throughout. This greatly reduces the amount of experimental data required to model the behaviour of CO2 in the solvent. Furthermore, in contrast with traditional treatments of reactive systems of this type, no enhancement factor is used in the process model. The absorber process model is implemented in the gPROMS software platform and validated using published pilot plant experimental data. The predictive capabilities of the mass transfer correlations used in this model are assessed through a sensitivity analysis and a scaling of the diffusivity in the liquid phase is proposed. The scaling of the diffusivity is transferable to different operating conditions and good predictions are obtained for the composition profiles in the gas and liquid phases.}
}
@article{YU2019206,
title = {An evaluation for VR glasses system user experience: The influence factors of interactive operation and motion sickness},
journal = {Applied Ergonomics},
volume = {74},
pages = {206-213},
year = {2019},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2018.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0003687018302850},
author = {Mengli Yu and Ronggang Zhou and Huiwen Wang and Weihua Zhao},
keywords = {VR glasses system, User experience evaluation, Interactive operation, Motion sickness},
abstract = {As a new type of Virtual Reality (VR) headset, VR glasses rise rapidly in a number of areas. It's essential to understand the importance of user experience (UX) on VR glasses design. This study aimed to develop questionnaires for evaluating VR glasses' UX, as well as to investigate the relationship between various UX variables. With using lab-based usability tests, this study analyzed participants' self-reports and performance based on testing eight VR glasses and seven mobile applications. A nine-item questionnaire and a ten-item questionnaire were successfully developed to measure VR glasses systems' UX quality in terms of hardware and application, respectively. Within a proposed UX evaluation framework, the perceived UX quality relative to VR glasses hardware emerged as a core predictor in predicting interactive operation performance, whereas the application UX perception was a significant predictor of motion sickness.}
}
@article{ZHOU2023102260,
title = {Unsupervised registration for liver CT-MR images based on the multiscale integrated spatial-weight module and dual similarity guidance},
journal = {Computerized Medical Imaging and Graphics},
volume = {108},
pages = {102260},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102260},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000782},
author = {Zhiyong Zhou and Shuaikun Wang and Jisu Hu and Anqi Liu and Xusheng Qian and Chen Geng and Jian Zheng and Guangqiang Chen and Jiansong Ji and Yakang Dai},
keywords = {Medical image registration, Deep learning, Multimodal registration, Spatial-weight module, Multi-resolution},
abstract = {Purpose
Multimodal registration is a key task in medical image analysis. Due to the large differences of multimodal images in intensity scale and texture pattern, it is a great challenge to design distinctive similarity metrics to guide deep learning-based multimodal image registration. Besides, since the limitation of the small receptive field, existing deep learning-based methods are mainly suitable for small deformation, but helpless for large deformation. To address the above issues, we present an unsupervised multimodal image registration method based on the multiscale integrated spatial-weight module and dual similarity guidance.
Methods
In this method, a U-shape network with our multiscale integrated spatial-weight module is embedded into a multi-resolution image registration architecture to achieve end-to-end large deformation registration, where the spatial-weight module can effectively highlight the regions with large deformation and aggregate discriminative features, and the multi-resolution architecture further helps to solve the optimization problem of the network in a coarse-to-fine pattern. Furthermore, we introduce a special loss function based on dual similarity, which represents both global gray-scale similarity and local feature similarity, to optimize the unsupervised multimodal registration network.
Results
We verified the effectiveness of the proposed method on liver CT-MR images. Experimental results indicate that the proposed method achieves the optimal DSC value and TRE value of 92.70 ± 1.75(%) and 6.52 ± 2.94(mm), compared with other state-of-the-art registration algorithms.
Conclusion
The proposed method can accurately estimate the large deformation field by aggregating multiscale features, and achieve higher registration accuracy and fast registration speed. Comparative experiments also demonstrate the effectiveness and generalization ability of the algorithm.}
}
@article{LIN2023112471,
title = {A new type of increasingly higher order finite difference and finite volume MR-WENO schemes with adaptive linear weights for hyperbolic conservation laws},
journal = {Journal of Computational Physics},
volume = {493},
pages = {112471},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112471},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123005661},
author = {Yicheng Lin and Zhenming Wang and Jun Zhu},
keywords = {Multi-resolution WENO scheme, Adaptive linear weights, Hyperbolic conservation laws, Finite difference scheme, Finite volume scheme},
abstract = {In this paper, the new fifth-order, seventh-order, and ninth-order finite difference and finite volume multi-resolution weighted essentially non-oscillatory (MR-WENO) schemes with adaptive linear weights are presented for hyperbolic conservation laws on structured meshes. They are termed as high-order finite difference and finite volume ALW-WENO schemes. These ALW-WENO schemes only apply one small stencil and one large stencil in reconstruction processes, which could achieve the desired accuracy in the region of smoothness and non-oscillatory properties in the region of containing strong shocks. The linear weights that sum to one can be automatically adjusted to any positive numbers. This is the first time that arbitrary high-order finite difference and finite volume WENO schemes are designed by using only two unequal-sized central spatial stencils. The structure of these novel WENO schemes is simple, so it is easier for obtaining high-order accuracy and solving multi-dimensional problems in large scale engineering applications. Compared to traditional MR-WENO schemes with same order, the computational efficiency can be further improved. Some benchmark tests indicate that these new ALW-WENO schemes have good robustness and performance.}
}
@article{POP2012505,
title = {Construction of 3D MR image-based computer models of pathologic hearts, augmented with histology and optical fluorescence imaging to characterize action potential propagation},
journal = {Medical Image Analysis},
volume = {16},
number = {2},
pages = {505-523},
year = {2012},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1361841511001691},
author = {Mihaela Pop and Maxime Sermesant and Garry Liu and Jatin Relan and Tommaso Mansi and Alan Soong and Jean-Marc Peyrat and Michael V. Truong and Paul Fefer and Elliot R. McVeigh and Herve Delingette and Alexander J. Dick and Nicholas Ayache and Graham A. Wright},
keywords = {Cardiac computer models, MRI, Optical imaging, Electrophysiology},
abstract = {Cardiac computer models can help us understand and predict the propagation of excitation waves (i.e., action potential, AP) in healthy and pathologic hearts. Our broad aim is to develop accurate 3D MR image-based computer models of electrophysiology in large hearts (translatable to clinical applications) and to validate them experimentally. The specific goals of this paper were to match models with maps of the propagation of optical AP on the epicardial surface using large porcine hearts with scars, estimating several parameters relevant to macroscopic reaction–diffusion electrophysiological models. We used voltage-sensitive dyes to image AP in large porcine hearts with scars (three specimens had chronic myocardial infarct, and three had radiofrequency RF acute scars). We first analyzed the main AP waves’ characteristics: duration (APD) and propagation under controlled pacing locations and frequencies as recorded from 2D optical images. We further built 3D MR image-based computer models that have information derived from the optical measures, as well as morphologic MRI data (i.e., myocardial anatomy, fiber directions and scar definition). The scar morphology from MR images was validated against corresponding whole-mount histology. We also compared the measured 3D isochronal maps of depolarization to simulated isochrones (the latter replicating precisely the experimental conditions), performing model customization and 3D volumetric adjustments of the local conductivity. Our results demonstrated that mean APD in the border zone (BZ) of the infarct scars was reduced by ∼13% (compared to ∼318ms measured in normal zone, NZ), but APD did not change significantly in the thin BZ of the ablation scars. A generic value for velocity ratio (1:2.7) in healthy myocardial tissue was derived from measured values of transverse and longitudinal conduction velocities relative to fibers direction (22cm/s and 60cm/s, respectively). The model customization and 3D volumetric adjustment reduced the differences between measurements and simulations; for example, from one pacing location, the adjustment reduced the absolute error in local depolarization times by a factor of 5 (i.e., from 58ms to 11ms) in the infarcted heart, and by a factor of 6 (i.e., from 60ms to 9ms) in the heart with the RF scar. Moreover, the sensitivity of adjusted conductivity maps to different pacing locations was tested, and the errors in activation times were found to be of approximately 10–12ms independent of pacing location used to adjust model parameters, suggesting that any location can be used for model predictions.}
}
@article{OUALI2020602,
title = {A New Architecture based AR for Detection and Recognition of Objects and Text to Enhance Navigation of Visually Impaired People},
journal = {Procedia Computer Science},
volume = {176},
pages = {602-611},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031886X},
author = {Imene OUALI and Mohamed Saifeddine {HADJ SASSI} and Mohamed BEN HALIMA and Ali WALI},
keywords = {Augmented reality, Vuforia, Usability, Visually impaired, Objects, Text, Detection, Recognition},
abstract = {Navigation in unfamiliar places is a big challenge for partially sighted and visually impaired people. Improving visual information on the location and content of objects such as drugs can help navigation in unfamiliar environments. There are several existing navigation solutions capable of helping these people. However, navigation solutions are rarely adopted and implemented in reality. In order to optimize the perception of digital information of objects from sensors and to naturally interact with the pervasive computing landscape, Augmented Reality (AR) equipment has to be seamlessly integrated into the user’s environment. For this purpose, we develop an architecture of text and objects recognition based on AR in order to assist partially sighted and visually impaired people. Such architecture makes navigation easier in the environment by helping users to find drugs and to verify the number of pills using speech. The proposed architecture uses context-aware mobile computing and shows great potential to integrate AR for object recognition through AR engines.}
}
@article{MEDELLINCASTILLO201646,
title = {The evaluation of a novel haptic-enabled virtual reality approach for computer-aided cephalometry},
journal = {Computer Methods and Programs in Biomedicine},
volume = {130},
pages = {46-53},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715300055},
author = {H.I. Medellín-Castillo and E.H. Govea-Valladares and C.N. Pérez-Guerrero and J. Gil-Valladares and Theodore Lim and James M. Ritchie},
keywords = {Cephalometry, Haptic technologies, Cephalometric values, Landmark selection, Task completion time (TCT)},
abstract = {Background and objective
In oral and maxillofacial surgery, conventional radiographic cephalometry is one of the standard auxiliary tools for diagnosis and surgical planning. While contemporary computer-assisted cephalometric systems and methodologies support cephalometric analysis, they tend neither to be practical nor intuitive for practitioners. This is particularly the case for 3D methods since the associated landmarking process is difficult and time consuming. In addition to this, there are no 3D cephalometry norms or standards defined; therefore new landmark selection methods are required which will help facilitate their establishment. This paper presents and evaluates a novel haptic-enabled landmarking approach to overcome some of the difficulties and disadvantages of the current landmarking processes used in 2D and 3D cephalometry.
Method
In order to evaluate this new system's feasibility and performance, 21 dental surgeons (comprising 7 Novices, 7 Semi-experts and 7 Experts) performed a range of case studies using a haptic-enabled 2D, 2½D and 3D digital cephalometric analyses.
Results
The results compared the 2D, 2½D and 3D cephalometric values, errors and standard deviations for each case study and associated group of participants and revealed that 3D cephalometry significantly reduced landmarking errors and variability compared to 2D methods.
Conclusions
Through enhancing the process by providing a sense of touch, the haptic-enabled 3D digital cephalometric approach was found to be feasible and more intuitive than its counterparts as well effective at reducing errors, the variability of the measurements taken and associated task completion times.}
}
@article{PANDA201794,
title = {An evolutionary gray gradient algorithm for multilevel thresholding of brain MR images using soft computing techniques},
journal = {Applied Soft Computing},
volume = {50},
pages = {94-108},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616305804},
author = {Rutuparna Panda and Sanjay Agrawal and Leena Samantaray and Ajith Abraham},
keywords = {Multilevel thresholding, MRI brain, Soft computing techniques, Otsu’s method},
abstract = {The conventional two dimensional (2-D) histogram based Otsu’s method gives unreliable results while considering multilevel thresholding of brain magnetic resonance (MR) images, because the edges of the brain regions are not preserved due to the local averaging process involved. Moreover, some of the useful pixels present inside the off-diagonal regions are ignored in the calculation. This article presents an evolutionary gray gradient algorithm (EGGA) for optimal multilevel thresholding of brain MR images. In this paper, more edge information is preserved by computing 2-D histogram based gray gradient. The key to our success is the use of the gray gradient information between the pixel values and the pixel average values to minimize the information loss. In addition, the speed improvement is achieved. Theoretical formulations are derived for computing the maximum between class variance from the 2-D histogram of the brain image. A first-hand fitness function is suggested for the EGGA. A novel adaptive swallow swarm optimization (ASSO) algorithm is introduced to optimize the fitness function. The performance of ASSO is validated using twenty three standard Benchmark test functions. The performance of ASSO is better than swallow swarm optimization (SSO). The optimum threshold value is obtained by maximizing the between class variance using ASSO. Our method is tested using the standard axial T2 − weighted brain MRI database of Harvard medical education using 100 slices. Performance of our method is compared to the Otsu’s method based on the one dimensional (1-D) and the 2-D histogram. The results are also compared among four different soft computing techniques. It is observed that results obtained using our method is better than the other methods, both qualitatively and quantitatively. Benefits of our method are – (i) the EGGA exhibits better objective function values; (ii) the EGGA provides us significantly improved results; and (iii) more computational speed is achieved.}
}
@article{TEJASHWINI2024106312,
title = {EBT Deep Net: Ensemble brain tumor Deep Net for multi-classification of brain tumor in MR images},
journal = {Biomedical Signal Processing and Control},
volume = {95},
pages = {106312},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106312},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424003707},
author = {P.S. Tejashwini and J. Thriveni and K.R. Venugopal},
keywords = {Brain Tumors, Deep learning, ML, CNN, ResNet50V2, DenseNet201, MobileNetV3, VGG19},
abstract = {The classification of brain tumors (BTs) is an expensive intricate challenge in the section of clinical image analysis. Radiologists were able to reliably detect tumors using machine learning (ML) algorithms without the need for extensive surgery. Hand-crafted features are necessary for the conventional ML classifiers, which take a lot of effort. Deep Learning (DL), on the other hand, has gained popularity recently for identification and categorization-based functionalities because of its effective feature extraction capabilities. Creating the most precise DL architecture for classifying BTs is a huge task. These challenges inspired us to develop a cutting-edge and highly accurate framework based on DL and computational methods that evolve to effectively create the hybrid framework automatically for categorizing three distinct types of brain cancers on a vast library of MR images. We present a unique EBT Deep Net DL model that uses a substantial convolutional neural network (CNN) technique for the classification of three distinct forms of brain cancers: gliomas, tumors in brain meningeal tissue, and tumors in pituitary glands. Pretrained deep CNN models like ResNet50V2, ResNet152, DenseNet201, MobileNetV3, and VGG19 were used to extract deep features. The efficacy of this paper is examined using classification accuracy. In comparison to the most recent classification findings from ResNet50V2, ResNet152, DenseNet201, MobileNetV3, and VGG19, the proposed methodology had the greatest accuracy. The models' respective accuracy on a classification test is as follows: VGG19 scored 97.46 %, ResNet152 scored 98 %, DenseNet201 scored 95.44 %, ResNet50V2 scored 90.04 %, MobileNetV3 scored 97.15 %, and the ensemble model outperformed all of them with an accuracy of 98.8 %. The suggested model demonstrated its advantage over the currently used approaches for classifying BTs from MR images.}
}
@article{ZHANG2015227,
title = {Effective staging of fibrosis by the selected texture features of liver: Which one is better, CT or MR imaging?},
journal = {Computerized Medical Imaging and Graphics},
volume = {46},
pages = {227-236},
year = {2015},
note = {Information Technologies in Biomedicine},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0895611115001251},
author = {Xuejun Zhang and Xin Gao and Brent J. Liu and Kevin Ma and Wen Yan and Long Liling and Huang Yuhong and Hiroshi Fujita},
abstract = {Purpose
Texture patterns of hepatic fibrosis are one of the important biomarkers to diagnose and classify chronic liver disease from initial to end stage on computed tomography (CT) or magnetic resonance (MR) images. Computer-aided diagnosis (CAD) of liver cirrhosis using texture features has become popular in recent research advances. To date, however, properly selecting effective texture features and image parameters is still mostly undetermined and not well-defined. In this study, different types of datasets acquired from CT and MR images are investigated to select the optimal parameters and features for the proper classification of fibrosis.
Methods
A total of 149 patients were scanned by multi-detector computed tomography (MDCT) and 218 patients were scanned using 1.5T and 3T superconducting MR scanners for an abdominal examination. All cases were verified by needle biopsies as the gold standard of our experiment, ranging from 0 (no fibrosis) to 5 (cirrhosis). For each case, at least four sequenced phase images are acquired by CT or MR scanners: pre-contrast, arterial, portal venous and equilibrium phase. For both imaging modalities, 15 texture features calculated from gray level co-occurrence matrix (GLCM) are extracted within an ROI in liver as one set of input vectors. Each combination of these input subsets is checked by using support vector machine (SVM) with leave-one-case-out method to differentiate fibrosis into two groups: non-cirrhosis or cirrhosis. In addition, 10 ROIs in the liver are manually selected in a disperse manner by experienced radiologist from each sequenced image and each of the 15 features are averaged across the 10 ROIs for each case to reduce the validation time. The number of input items is selected from the various combinations of 15 features, from which the accuracy rate (AR) is calculated by counting the percentage of correct answers on each combination of features aggregated to determine a liver stage score and then compared to the gold standard.
Results
According to the accuracy rate (AR) calculated from each combination, the optimal number of texture features to classify liver fibrosis degree ranges from 4 to 7, no matter which modality was utilized. The overall performance calculated by the average sum of maximum AR value of all 15 features is 66.83% in CT images, while 68.14%, and 71.98% in MR images, respectively; among the 15 texture features, mean gray value and entropy are the most commonly used features in all 3 imaging datasets. The correlation feature has the lowest AR value and was removed as an effective feature in all datasets. AR value tends to increase with the injection of contrast agency, and both CT and MR images reach the highest AR performance during the equilibrium phase.
Conclusions
Comparing the accuracy of classification with two imaging modalities, the MR images have an advantage over CT images with regards to AR performance of the 15 selected texture features, while 3T MRI is better than 1.5T MRI to classify liver fibrosis. Finally, the texture analysis is more effective during equilibrium phase than in any of the other phased images.}
}
@article{ABDELTAWAB2020101717,
title = {A deep learning-based approach for automatic segmentation and quantification of the left ventricle from cardiac cine MR images},
journal = {Computerized Medical Imaging and Graphics},
volume = {81},
pages = {101717},
year = {2020},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2020.101717},
url = {https://www.sciencedirect.com/science/article/pii/S0895611120300203},
author = {Hisham Abdeltawab and Fahmi Khalifa and Fatma Taher and Norah Saleh Alghamdi and Mohammed Ghazal and Garth Beache and Tamer Mohamed and Robert Keynton and Ayman El-Baz},
keywords = {Cardiac MR, Cardiac parameters, Deep learning, Left ventricle, Segmentation},
abstract = {Cardiac MRI has been widely used for noninvasive assessment of cardiac anatomy and function as well as heart diagnosis. The estimation of physiological heart parameters for heart diagnosis essentially require accurate segmentation of the Left ventricle (LV) from cardiac MRI. Therefore, we propose a novel deep learning approach for the automated segmentation and quantification of the LV from cardiac cine MR images. We aim to achieve lower errors for the estimated heart parameters compared to the previous studies by proposing a novel deep learning segmentation method. Our framework starts by an accurate localization of the LV blood pool center-point using a fully convolutional neural network (FCN) architecture called FCN1. Then, a region of interest (ROI) that contains the LV is extracted from all heart sections. The extracted ROIs are used for the segmentation of LV cavity and myocardium via a novel FCN architecture called FCN2. The FCN2 network has several bottleneck layers and uses less memory footprint than conventional architectures such as U-net. Furthermore, a new loss function called radial loss that minimizes the distance between the predicted and true contours of the LV is introduced into our model. Following myocardial segmentation, functional and mass parameters of the LV are estimated. Automated Cardiac Diagnosis Challenge (ACDC-2017) dataset was used to validate our framework, which gave better segmentation, accurate estimation of cardiac parameters, and produced less error compared to other methods applied on the same dataset. Furthermore, we showed that our segmentation approach generalizes well across different datasets by testing its performance on a locally acquired dataset. To sum up, we propose a deep learning approach that can be translated into a clinical tool for heart diagnosis.}
}
@article{CHOI2023103132,
title = {LUVI: Lightweight UWB-VIO based relative positioning for AR-IoT applications},
journal = {Ad Hoc Networks},
volume = {145},
pages = {103132},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103132},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523000525},
author = {Hong-Beom Choi and Keun-Woo Lim and Young-Bae Ko},
keywords = {Location-based service, Augmented reality, Ultra-wideband, Indoor positioning system, Internet of Things},
abstract = {In this paper, we propose LUVI, Lightweight UWB-VIO relative positioning method for indoor localization. Recent designs of handheld and embedded devices feature various technologies which have the means to enhance localization performance in indoor environments. These include visual odometry based on cameras and augmented reality, and communication hardware such as UWB. Integration of such technologies to exploit their advantages allows us to compensate for each other's errors in measurement. This improves the overall function of future services, such as visual representation of sensing information from sensors in areas that are not physically visible. However, existing work cannot fully exploit these technologies to high extent, often inducing high errors or wasted resources. LUVI is a novel localization method which estimates the location of a target object using relative coordinates of estimator devices without the aid of definitive coordinates. LUVI focuses on utilization of lightweight management of virtual anchors for localization, with functions that reduce the computing and communication complexity while maintaining the accuracy and improving energy efficiency of the localization. Our work has been fully implemented and tested in several indoor environments, showing robustness to NLOS while significantly reducing computational complexity, and up to 30% lower average error.}
}
@article{DOSHI2017178,
title = {Automatic pharynx and larynx cancer segmentation framework (PLCSF) on contrast enhanced MR images},
journal = {Biomedical Signal Processing and Control},
volume = {33},
pages = {178-188},
year = {2017},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1746809416302130},
author = {Trushali Doshi and John Soraghan and Lykourgos Petropoulakis and Gaetano {Di Caterina} and Derek Grose and Kenneth MacKenzie and Christina Wilson},
keywords = {Head and neck cancer, Automatic segmentation, Magnetic resonance imaging (MRI), Fuzzy c-means clustering, Fuzzy rules, Level set method, Radiotherapy},
abstract = {A novel and effective pharynx and larynx cancer segmentation framework (PLCSF) is presented for automatic base of tongue and larynx cancer segmentation from gadolinium-enhanced T1-weighted magnetic resonance images (MRI). The aim of the proposed PLCSF is to assist clinicians in radiotherapy treatment planning. The initial processing of MRI data in PLCSF includes cropping of region of interest; reduction of artefacts and detection of the throat region for the location prior. Further, modified fuzzy c-means clustering is developed to robustly separate candidate cancer pixels from other tissue types. In addition, region-based level set method is evolved to ensure spatial smoothness for the final segmentation boundary after noise removal using non-linear and morphological filtering. Validation study of PLCSF on 102 axial MRI slices demonstrate mean dice similarity coefficient of 0.79 and mean modified Hausdorff distance of 2.2mm when compared with manual segmentations. Comparison of PLCSF with other algorithms validates the robustness of the PLCSF. Inter- and intra-variability calculations from manual segmentations suggest that PLCSF can help to reduce the human subjectivity.}
}
@article{ZHAO2022108669,
title = {Single MR image super-resolution via channel splitting and serial fusion network},
journal = {Knowledge-Based Systems},
volume = {246},
pages = {108669},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108669},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003070},
author = {Xiaole Zhao and Yulun Zhang and Yun Qin and Qian Wang and Tao Zhang and Tianrui Li},
keywords = {Convolutional neural network, Magnetic resonance imaging, Channel splitting, Super-resolution, Serial fusion},
abstract = {In magnetic resonance imaging (MRI), spatial resolution is an important and critical imaging parameter that represents how much information is contained in a unit space. Acquiring high-resolution MRI data usually takes a long scanning time and is subject to motion artifacts due to hardware, physical, and physiological limitations. Single image super-resolution (SISR) based on deep learning is an effective and promising alternative technique to improve the native spatial resolution of magnetic resonance (MR) images. However, because of the simple diversity and single distribution of training samples, the effective training of deep models with medical training samples and improvement of the tradeoff between model performance and computing overhead are major challenges. In addition, deeper networks are more difficult to effectively train since the information is gradually weakened as the network deepens. In this paper, a novel channel splitting and serial fusion network (CSSFN) is presented for single MR image super-resolution. The proposed CSSFN splits hierarchical features into a series of subfeatures, which are then integrated together in a serial manner. Hence, the network becomes deeper and can discriminatively and reasonably deal with the subfeatures. Moreover, a dense global feature fusion (DGFF) is adopted to integrate the intermediate features, which further promotes the information flow in the network and helps to stabilize model training. Extensive experiments on several typical MR images show the superiority of our CSSFN models to other advanced SISR methods.}
}
@article{PAWAR2021101968,
title = {Domain knowledge augmentation of parallel MR image reconstruction using deep learning},
journal = {Computerized Medical Imaging and Graphics},
volume = {92},
pages = {101968},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101968},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121001178},
author = {Kamlesh Pawar and Gary F. Egan and Zhaolin Chen},
keywords = {Image processing, MR image reconstruction, Deep learning image reconstruction, Parallel MRI, Convolutional neural network},
abstract = {A deep learning (DL) method for accelerated magnetic resonance (MR) imaging is presented that incorporates domain knowledge of parallel MR imaging to augment the DL networks for accurate and stable image reconstruction. The proposed DL method employs a novel loss function consisting of a combination of mean absolute error, structural similarity, and sobel edge loss. The DL model takes both original measurements and images reconstructed by the parallel imaging method as inputs to the network. The accuracy of the proposed method was evaluated using two anatomical regions and six MRI contrasts and was compared with state-of-the-art parallel imaging and deep learning methods. The proposed method significantly outperformed the other methods for all the six different contrasts in terms of structural similarity, peak signal to noise ratio, and normalized mean squared error. The out-of-sample performance of the proposed method was assessed for a truly “unseen” case in a volunteer scan. The method produced images without any artificial features, often occurring in the DL image reconstruction methods. A stability analysis was performed by adding perturbations to the input, which demonstrated that the proposed method is robust and stable with respect to small structural changes, and different undersampling ratios. Comprehensive validation on large datasets demonstrated that incorporation of domain knowledge sufficiently regularizes the DL based image reconstruction and produces accurate and stable image enhancement.}
}
@article{BAO2024169,
title = {Effects of virtual agents on interaction efficiency and environmental immersion in MR environments},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {169-179},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000761},
author = {Yihua Bao and Jie Guo and Dongdong Weng and Yue Liu and Zeyu Tian},
keywords = {Mixed reality, Virtual agents, Interaction performance, Environmental immersion, Virtual environments},
abstract = {Background
Physical entity interactions in mixed reality (MR) environments aim to harness human capabilities in manipulating physical objects, thereby enhancing virtual environment (VEs) functionality. In MR, a common strategy is to use virtual agents as substitutes for physical entities, balancing interaction efficiency with environmental immersion. However, the impact of virtual agent size and form on interaction performance remains unclear.
Methods
Two experiments were conducted to explore how virtual agent size and form affect interaction performance, immersion, and preference in MR environments. The first experiment assessed five virtual agent sizes (25%, 50%, 75%, 100%, and 125% of physical size). The second experiment tested four types of frames (no frame, consistent frame, half frame, and surrounding frame) across all agent sizes. Participants, utilizing a head-mounted display, performed tasks involving moving cups, typing words, and using a mouse. They completed questionnaires assessing aspects such as the virtual environment effects, interaction effects, collision concerns, and preferences.
Results
Results from the first experiment revealed that agents matching physical object size produced the best overall performance. The second experiment demonstrated that consistent framing notably enhances interaction accuracy and speed but reduces immersion. To balance efficiency and immersion, frameless agents matching physical object sizes were deemed optimal.
Conclusions
Virtual agents matching physical entity sizes enhance user experience and interaction performance. Conversely, familiar frames from 2D interfaces detrimentally affect interaction and immersion in virtual spaces. This study provides valuable insights for the future development of MR systems.}
}
@article{PUYOLANTON201796,
title = {A multimodal spatiotemporal cardiac motion atlas from MR and ultrasound data},
journal = {Medical Image Analysis},
volume = {40},
pages = {96-110},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300890},
author = {Esther Puyol-Antón and Matthew Sinclair and Bernhard Gerber and Mihaela Silvia Amzulescu and Hélène Langet and Mathieu De Craene and Paul Aljabar and Paolo Piro and Andrew P. King},
keywords = {Cardiac motion atlas, Multi-view dimensionality reduction},
abstract = {Cardiac motion atlases provide a space of reference in which the motions of a cohort of subjects can be directly compared. Motion atlases can be used to learn descriptors that are linked to different pathologies and which can subsequently be used for diagnosis. To date, all such atlases have been formed and applied using data from the same modality. In this work we propose a framework to build a multimodal cardiac motion atlas from 3D magnetic resonance (MR) and 3D ultrasound (US) data. Such an atlas will benefit from the complementary motion features derived from the two modalities, and furthermore, it could be applied in clinics to detect cardiovascular disease using US data alone. The processing pipeline for the formation of the multimodal motion atlas initially involves spatial and temporal normalisation of subjects’ cardiac geometry and motion. This step was accomplished following a similar pipeline to that proposed for single modality atlas formation. The main novelty of this paper lies in the use of a multi-view algorithm to simultaneously reduce the dimensionality of both the MR and US derived motion data in order to find a common space between both modalities to model their variability. Three different dimensionality reduction algorithms were investigated: principal component analysis, canonical correlation analysis and partial least squares regression (PLS). A leave-one-out cross validation on a multimodal data set of 50 volunteers was employed to quantify the accuracy of the three algorithms. Results show that PLS resulted in the lowest errors, with a reconstruction error of less than 2.3 mm for MR-derived motion data, and less than 2.5  mm for US-derived motion data. In addition, 1000 subjects from the UK Biobank database were used to build a large scale monomodal data set for a systematic validation of the proposed algorithms. Our results demonstrate the feasibility of using US data alone to analyse cardiac function based on a multimodal motion atlas.}
}
@article{HUANG2021101817,
title = {Difficulty-aware hierarchical convolutional neural networks for deformable registration of brain MR images},
journal = {Medical Image Analysis},
volume = {67},
pages = {101817},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101817},
url = {https://www.sciencedirect.com/science/article/pii/S136184152030181X},
author = {Yunzhi Huang and Sahar Ahmad and Jingfan Fan and Dinggang Shen and Pew-Thian Yap},
keywords = {Deformable registration, Brain MRI, Cascaded neural network, Difficulty-aware sampling},
abstract = {The aim of deformable brain image registration is to align anatomical structures, which can potentially vary with large and complex deformations. Anatomical structures vary in size and shape, requiring the registration algorithm to estimate deformation fields at various degrees of complexity. Here, we present a difficulty-aware model based on an attention mechanism to automatically identify hard-to-register regions, allowing better estimation of large complex deformations. The difficulty-aware model is incorporated into a cascaded neural network consisting of three sub-networks to fully leverage both global and local contextual information for effective registration. The first sub-network is trained at the image level to predict a coarse-scale deformation field, which is then used for initializing the subsequent sub-network. The next two sub-networks progressively optimize at the patch level with different resolutions to predict a fine-scale deformation field. Embedding difficulty-aware learning into the hierarchical neural network allows harder patches to be identified in the deeper sub-networks at higher resolutions for refining the deformation field. Experiments conducted on four public datasets validate that our method achieves promising registration accuracy with better preservation of topology, compared with state-of-the-art registration methods.}
}
@article{RAY2023104925,
title = {Uncertainty parameter weighted entropy-based fuzzy c-means algorithm using complemented membership functions for noisy volumetric brain MR image segmentation},
journal = {Biomedical Signal Processing and Control},
volume = {85},
pages = {104925},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.104925},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423003580},
author = {Madhumita Ray and Nabanita Mahata and Jamuna Kanta Sing},
keywords = {Uncertainty parameter, Entropy-based FCM algorithm, 3D brain MR image segmentation, Clustering},
abstract = {In this paper, we propose an uncertainty parameter weighted entropy-based fuzzy c-means clustering algorithm for noisy volumetric (3D) brain MR image segmentation using complemented global and spatially constraint local membership functions. Due to inherent noise and intensity inhomogeneity (IIH), the acquired MR images have blurry tissue boundaries. This leads to a situation where uncertainty arises while labeling a pixel/voxel into its proper tissue region. Further, it magnifies in the regions of different tissue boundaries. The proposed algorithm addresses this issue by introducing a class-level uncertainty parameter for each voxel and weightedly incorporating in the fuzzy objective function using complemented global and spatially constraint local fuzzy membership functions. It also incorporates total uncertainties in the 3D image domain by means of Shannon entropy. The complemented local fuzzy membership function estimates the degree of non-association, constraint by the local region-level intensity distribution. This framework allows the algorithm to utilize the spatial intensity distribution both in locally and globally within the image domain and produce more accurate cluster prototypes. We validate the proposed algorithm qualitatively and quantitatively with 10 3D simulated brain MR images, contaminated by noise and intensity inhomogeneity, 4 volumes of clinical real patient 3D brain MR images and a synthetic 3D image with added Rician noise. The simulation results in several evaluation indices indicate its superiority over state-of-the-art algorithms developed in recent past.}
}
@article{YE200677,
title = {An investigation into the implementation of virtual reality technologies in support of conceptual design},
journal = {Design Studies},
volume = {27},
number = {1},
pages = {77-97},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000347},
author = {J. Ye and R.I. Campbell and T. Page and K.S. Badni},
keywords = {conceptual design, interface design, virtual reality, computer aided design},
abstract = {Virtual reality (VR) technologies provide novel and enhanced modes of human computer interaction that can be used to overcome communication drawbacks that prevail in conventional computer aided design (CAD) systems in the support of conceptual design, principally in concept generation and evaluation. The work reported here focuses on how VR technologies can proffer more natural and intuitive interaction between the designer and the CAD system thus enabling a rapid and more straightforward approach to concept design creation and evaluation. As the result of this research, an innovative conceptual design system called the LUCID system has been developed. LUCID provides designers with a new interaction paradigm that enables them to take fuller advantage of their visual, auditory and tactile sensorial channels in order to create, view, touch, manipulate and listen to CAD digital models more effectively. The LUCID system has shown particular human computer interface (HCI) benefits to designers during conceptual design stages based on the outcome from a user evaluation test.}
}
@article{BORSCI201517,
title = {Empirical evidence, evaluation criteria and challenges for the effectiveness of virtual and mixed reality tools for training operators of car service maintenance},
journal = {Computers in Industry},
volume = {67},
pages = {17-26},
year = {2015},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361514002073},
author = {Simone Borsci and Glyn Lawson and Simon Broome},
keywords = {Augmented and virtual reality, Automotive, Service maintenance, Training effectiveness, Training evaluation},
abstract = {The debate on effectiveness of virtual and mixed reality (VR/MR) tools for training professionals and operators is long-running with prominent contributions arguing that there are several shortfalls of experimental approaches and assessment criteria reported within the literature. In the automotive context, although car-makers were pioneers in the use of VR/MR tools for supporting designers, researchers started only recently to explore the effectiveness of VR/MR systems as mean for driving external operators of service centres to acquire the procedural skills necessary for car maintenance processes. In fact, from 463 journal articles on VR/MR tools for training published in the last thirty years, we identified only eight articles in which researchers experimentally tested the effectiveness of VR/MR tools for training service operators’ skills. To survey the current findings and the deficiencies of these eight studies, we use two main drivers: (i) a well-known framework of organizational training programmes, and (ii) a list of eleven evaluation criteria widely applied by researchers of different fields for assessing the effectiveness of training carried out with VR/MR systems. The analysis that we present allows us to: (i) identify a trend among automotive researchers of focusing their analysis only on car service operators’ performance in terms of time and errors, by leaving unexplored important pre- and post-training aspects that could affect the effectiveness of VR/MR tools to deliver training contents – e.g., people skills, previous experience, cibersickness, presence and engagement, usability and satisfaction and (ii) outline the future challenges for designing and assessing VR/MR tools for training car service operators.}
}
@article{CHEN2017437,
title = {Assessing the use of immersive virtual reality, mouse and touchscreen in pointing and dragging-and-dropping tasks among young, middle-aged and older adults},
journal = {Applied Ergonomics},
volume = {65},
pages = {437-448},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003687017300728},
author = {Jiayin Chen and Calvin Or},
keywords = {Virtual reality, Human-computer interaction, Older adults},
abstract = {This study assessed the use of an immersive virtual reality (VR), a mouse and a touchscreen for one-directional pointing, multi-directional pointing, and dragging-and-dropping tasks involving targets of smaller and larger widths by young (n = 18; 18–30 years), middle-aged (n = 18; 40–55 years) and older adults (n = 18; 65–75 years). A three-way, mixed-factorial design was used for data collection. The dependent variables were the movement time required and the error rate. Our main findings were that the participants took more time and made more errors in using the VR input interface than in using the mouse or the touchscreen. This pattern applied in all three age groups in all tasks, except for multi-directional pointing with a larger target width among the older group. Overall, older adults took longer to complete the tasks and made more errors than young or middle-aged adults. Larger target widths yielded shorter movement times and lower error rates in pointing tasks, but larger targets yielded higher rates of error in dragging-and-dropping tasks. Our study indicated that any other virtual environments that are similar to those we tested may be more suitable for displaying scenes than for manipulating objects that are small and require fine control. Although interacting with VR is relatively difficult, especially for older adults, there is still potential for older adults to adapt to that interface. Furthermore, adjusting the width of objects according to the type of manipulation required might be an effective way to promote performance.}
}
@article{NEGUT2016414,
title = {Task difficulty of virtual reality-based assessment tools compared to classical paper-and-pencil or computerized measures: A meta-analytic approach},
journal = {Computers in Human Behavior},
volume = {54},
pages = {414-424},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215301059},
author = {Alexandra Neguţ and Silviu-Andrei Matu and Florin Alin Sava and Daniel David},
keywords = {Virtual reality, Neuropsychological assessment, Task difficulty},
abstract = {Virtual reality-based assessment tools arise as a promising alternative for classic neuropsychological assessment with an increased level of ecological validity. Because virtual reality cognitive measures recreate tasks that resemble with the demands from the real world it is assumed that they require additional cognitive resources and are more difficult than classical paper-and-pencil or computerized measures. Although research has focused on comparing the performance obtained on virtual reality-based measures with classical paper-and-pencil or computerized measures, no meta-analysis has been conducted on this topic. Thirteen studies met our inclusion criteria: assessed any cognitive process using virtual reality and analogous classical or computerized assessment tools of the same process. Based on a random effects model, the results indicated a moderate effect size in favor of classical and computerized tests (g = −0.77) revealing an increased task difficulty in virtual reality. Overall, results from the current meta-analysis point out that cognitive performance obtained in virtual reality is poorer than the one in classical or computerized assessment which might suggest that tasks embedded in virtual reality have an increased level of complexity and difficulty and require additional cognitive resources.}
}
@article{FANG2022102292,
title = {Distributed cognition based localization for AR-aided collaborative assembly in industrial environments},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {75},
pages = {102292},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102292},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001721},
author = {Wei Fang and Wei Fan and Wei Ji and Lei Han and Shuhong Xu and Lianyu Zheng and Lihui Wang},
keywords = {Distributed localization, Augmented reality, Collaborative AR assembly, Scene cognition},
abstract = {The existing (augmented reality) AR-aided assembly is highly associated with AR devices, which mainly provides guidance for one operator, and it is hard to share augmented assembly instructions for large-scale products which require multiple operators working together. To address this problem, the paper proposes a distributed cognition based localization method for AR-aided collaborative assembly. Firstly, a scene cognition using multi-view acquisition about industrial environments is performed with incremental modeling in advance, providing the foundation for the subsequent pose estimate of multi-AR clients. Then, based on feature extracting and matching against the pre-built shop floor model, a pose recovery of AR-aided system is derived from different views of AR operators in a global coordinate system, followed by a distributed motion tracking with the complementary features of visual and inertial data, resulting in a co-located collaborative AR instruction for assembly. Finally, experiments are carried out to validate the proposed method, and experimental results illustrate that the proposed method can achieve distributed cognition-based localization accurately and robustly. Therefore, shared visual communications among multiple operators are synchronized, and assembly status is aware by all the operators.}
}
@article{HUANG20191,
title = {Design of finger gestures for locomotion in virtual reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {1},
number = {1},
pages = {1-9},
year = {2019},
issn = {2096-5796},
doi = {https://doi.org/10.3724/SP.J.2096-5796.2018.0007},
url = {https://www.sciencedirect.com/science/article/pii/S2096579619300026},
author = {Rachel HUANG and Carisa HARRIS-ADAMSON and Dan ODELL and David REMPEL},
keywords = {Human computer interaction, Virtual environment, Gesture design},
abstract = {Background Within a virtual environment (VE) the control of locomotion (e.g., self-travel) is critical for creating a realistic and functional experience. Usually the direction of locomotion, while using a head-mounted display (HMD), is determined by the direction the head is pointing and the forward or backward motion is controlled with a hand held controllers. However, hand held devices can be difficult to use while the eyes are covered with a HMD. Free hand gestures, that are tracked with a camera or a hand data glove, have an advantage of eliminating the need to look at the hand controller but the design of hand or finger gestures for this purpose has not been well developed. Methods This study used a depth-sensing camera to track fingertip location (curling and straightening the fingers), which was converted to forward or backward self-travel in the VE. Fingertip position was converted to self-travel velocity using a mapping function with three parameters: a region of zero velocity (dead zone) around the relaxed hand position, a linear relationship of fingertip position to velocity (slope or/J) beginning at the edge of the dead zone, and an exponential relationship rather than a linear one mapping fingertip position to velocity (exponent). Using a HMD, participants moved forward along a virtual road and stopped at a target on the road by controlling self-travel velocity with finger flexion and extension. Each of the 3 mapping function parameters was tested at 3 levels. Outcomes measured included usability ratings, fatigue, nausea, and time to complete the tasks. Results Twenty subjects participated but five did not complete the study due to nausea. The size of the dead zone had little effect on performance or usability. Subjects preferred lower β values which were associated with better subjective ratings of control and reduced time to complete the task, especially for large targets. Exponent values of 1.0 or greater were preferred and reduced the time to complete the task, especially for small targets. Conclusions Small finger movements can be used to control velocity of self-travel in VE. The functions used for converting fingertip position to movement velocity influence usability and performance.}
}
@article{ZULFIQAR2023104777,
title = {Multi-class classification of brain tumor types from MR images using EfficientNets},
journal = {Biomedical Signal Processing and Control},
volume = {84},
pages = {104777},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.104777},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423002100},
author = {Fatima Zulfiqar and Usama {Ijaz Bajwa} and Yasar Mehmood},
keywords = {Brain tumor classification, Magnetic resonance imaging (MRI), Convolutional neural networks, Transfer learning, Fine-tuning, EfficientNets},
abstract = {Accurate classification of the type of brain tumor plays an important role in the early diagnosis of the tumor which can be the difference between life and death. Magnetic Resonance Imaging (MRI) is commonly used to capture high-contrast grayscale images of the brain and is a non-invasive method for brain tumor diagnosis. Deep Learning (DL) using Convolutional Neural Networks (CNN) has revolutionized Computer Aided Diagnostic (CAD) systems by producing remarkable results for various medical imaging analysis tasks including brain tumor detection. In this research, a transfer learning-based fine-tuning approach for the classification of brain tumors into three classes i.e. glioma, meningioma, and pituitary tumor using EfficientNets is carried out. Five variants of pre-trained models from the EfficientNets family i.e. EfficientNetB0 – EfficientNetB4 are fine-tuned on publically available CE-MRI “Figshare – Brain Tumor Dataset”. The proposed method of fine-tuning pre-trained EfficientNet is carried out by first loading ImageNet weights to the EfficientNet model followed by the addition of several top layers and a fully connected layer for the classification of brain tumor types. Several experiments are carried out to analyze the robustness of the proposed fine-tuned EfficientNets in comparison to other pre-trained models. Moreover, the effect of data augmentation on the model’s test accuracy is also studied. Finally, the Grad-CAM visualization of the attention maps of the best model is presented that successfully highlights the tumorous region of the brain cell. The proposed method of fine-tuning pre-trained EfficientNet showed significant performance using EfficientNetB2 as a backbone achieving overall test accuracy, precision, recall/sensitivity, and F1-score of 98.86%, 98.65%, 98.77%, and 98.71% respectively. The proposed fine-tuned EfficientNetB2 is lightweight, computationally inexpensive (during training and inference), and generalizes well. The source code is available at https://github.com/FatimaZulfiqar/multi-class-brain-tumor-classification.}
}
@article{MASAD20191027,
title = {Automated measurements of lumbar lordosis in T2-MR images using decision tree classifier and morphological image processing},
journal = {Engineering Science and Technology, an International Journal},
volume = {22},
number = {4},
pages = {1027-1034},
year = {2019},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2215098618321414},
author = {Ihssan S. Masad and Amjed Al-Fahoum and Isam Abu-Qasmieh},
keywords = {Spine, Lumbar lordosis curvature, T2-MRI, Data mining, Decision trees, Texture features, Image segmentation, Morphological image processing, Cobb angle},
abstract = {Lumbar spine’s lordosis is a very important parameter functionally and clinically; it is a key feature in maintaining the sagittal balance, in addition to its crucial role in evaluating the spinal deformities. The main objective of the current study is to present a fully-automated measurement of the lumbar spine’s lordotic curve angle in T2-MR images. This goal has been achieved by the automatic measurement of lordosis radius at the lumbar spine level by computer-aided methods utilizing data mining classification and image segmentation followed by morphological image processing. The spine has been segmented from the entire image using a machine-learning technique that is based on texture features for recognizing the lumbar-spine pattern. The extracted features were fed to C4.5 decision tree classifier for designing the lumbar-spine recognition system. The resultant classifier’s “if-then” rules have been employed for segmenting the spine region from the entire image. Multiple morphological image processes have been applied to the raw segmentation result to enhance the true positive rate and suppressing the false positive rate. The mean radius of lumbar spine’s curvature has been evaluated by fitting the contours average to the closest circle using least-square fitting algorithm which was followed by calculating the lumbar lordosis curvature angle. The proposed approach has been tested and validated on normal and pathological T2-MR spine images and found to perform effectively. The calculation of lumbar lordosis angles showed a strong correlation with the Cobb angle measurements (R = 93.2%).}
}
@article{LU2024103148,
title = {A model-based MR parameter mapping network robust to substantial variations in acquisition settings},
journal = {Medical Image Analysis},
volume = {94},
pages = {103148},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103148},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524000732},
author = {Qiqi Lu and Jialong Li and Zifeng Lian and Xinyuan Zhang and Qianjin Feng and Wufan Chen and Jianhua Ma and Yanqiu Feng},
keywords = {Magnetic resonance imaging, Parameter mapping, Deep learning, Regularization},
abstract = {Deep learning methods show great potential for the efficient and precise estimation of quantitative parameter maps from multiple magnetic resonance (MR) images. Current deep learning-based MR parameter mapping (MPM) methods are mostly trained and tested using data with specific acquisition settings. However, scan protocols usually vary with centers, scanners, and studies in practice. Thus, deep learning methods applicable to MPM with varying acquisition settings are highly required but still rarely investigated. In this work, we develop a model-based deep network termed MMPM-Net for robust MPM with varying acquisition settings. A deep learning-based denoiser is introduced to construct the regularization term in the nonlinear inversion problem of MPM. The alternating direction method of multipliers is used to solve the optimization problem and then unrolled to construct MMPM-Net. The variation in acquisition parameters can be addressed by the data fidelity component in MMPM-Net. Extensive experiments are performed on R2 mapping and R1 mapping datasets with substantial variations in acquisition settings, and the results demonstrate that the proposed MMPM-Net method outperforms other state-of-the-art MR parameter mapping methods both qualitatively and quantitatively.}
}
@article{SHAN20141233,
title = {Automatic atlas-based three-label cartilage segmentation from MR knee images},
journal = {Medical Image Analysis},
volume = {18},
number = {7},
pages = {1233-1246},
year = {2014},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1361841514000899},
author = {Liang Shan and Christopher Zach and Cecil Charles and Marc Niethammer},
keywords = {Cartilage, Atlas, Segmentation, Three-label, Automatic},
abstract = {Osteoarthritis (OA) is the most common form of joint disease and often characterized by cartilage changes. Accurate quantitative methods are needed to rapidly screen large image databases to assess changes in cartilage morphology. We therefore propose a new automatic atlas-based cartilage segmentation method for future automatic OA studies. Atlas-based segmentation methods have been demonstrated to be robust and accurate in brain imaging and therefore also hold high promise to allow for reliable and high-quality segmentations of cartilage. Nevertheless, atlas-based methods have not been well explored for cartilage segmentation. A particular challenge is the thinness of cartilage, its relatively small volume in comparison to surrounding tissue and the difficulty to locate cartilage interfaces – for example the interface between femoral and tibial cartilage. This paper focuses on the segmentation of femoral and tibial cartilage, proposing a multi-atlas segmentation strategy with non-local patch-based label fusion which can robustly identify candidate regions of cartilage. This method is combined with a novel three-label segmentation method which guarantees the spatial separation of femoral and tibial cartilage, and ensures spatial regularity while preserving the thin cartilage shape through anisotropic regularization. Our segmentation energy is convex and therefore guarantees globally optimal solutions. We perform an extensive validation of the proposed method on 706 images of the Pfizer Longitudinal Study. Our validation includes comparisons of different atlas segmentation strategies, different local classifiers, and different types of regularizers. To compare to other cartilage segmentation approaches we validate based on the 50 images of the SKI10 dataset.}
}
@article{SWATI201934,
title = {Brain tumor classification for MR images using transfer learning and fine-tuning},
journal = {Computerized Medical Imaging and Graphics},
volume = {75},
pages = {34-46},
year = {2019},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0895611118305937},
author = {Zar Nawab Khan Swati and Qinghua Zhao and Muhammad Kabir and Farman Ali and Zakir Ali and Saeed Ahmed and Jianfeng Lu},
keywords = {Brain tumor classification, Block-wise fine-tuning, Convolutional neural networks, Deep learning, Magnetic resonance images, Transfer learning},
abstract = {Accurate and precise brain tumor MR images classification plays important role in clinical diagnosis and decision making for patient treatment. The key challenge in MR images classification is the semantic gap between the low-level visual information captured by the MRI machine and the high-level information perceived by the human evaluator. The traditional machine learning techniques for classification focus only on low-level or high-level features, use some handcrafted features to reduce this gap and require good feature extraction and classification methods. Recent development on deep learning has shown great progress and deep convolution neural networks (CNNs) have succeeded in the images classification task. Deep learning is very powerful for feature representation that can depict low-level and high-level information completely and embed the phase of feature extraction and classification into self-learning but require large training dataset in general. For most of the medical imaging scenario, the training datasets are small, therefore, it is a challenging task to apply the deep learning and train CNN from scratch on the small dataset. Aiming this problem, we use pre-trained deep CNN model and propose a block-wise fine-tuning strategy based on transfer learning. The proposed method is evaluated on T1-weighted contrast-enhanced magnetic resonance images (CE-MRI) benchmark dataset. Our method is more generic as it does not use any handcrafted features, requires minimal preprocessing and can achieve average accuracy of 94.82% under five-fold cross-validation. We compare our results not only with the traditional machine learning but also with deep learning methods using CNNs. Experimental results show that our proposed method outperforms state-of-the-art classification on the CE-MRI dataset.}
}
@article{GODETBAR2010492,
title = {HCI and business practices in a collaborative method for augmented reality systems},
journal = {Information and Software Technology},
volume = {52},
number = {5},
pages = {492-505},
year = {2010},
note = {TAIC-PART 2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909002079},
author = {Guillaume Godet-Bar and Dominique Rieu and Sophie Dupuy-Chessa},
keywords = {Information systems, Software engineering, Human–computer interaction, Augmented reality, Collaborative design, Evolution},
abstract = {Context
Every interactive system is composed of a functional core and a user interface. However, the software engineering (SE) and human–computer interaction (HCI) communities do not share the same methods, models or tools. This usually induces a large work overhead when specialists from the two domains try to connect their applicative studies, especially when developing augmented reality systems that feature complex interaction cores.
Objective
We present in this paper the essential activities and concepts of a development method integrating the SE and HCI development practices, from the specifications down to the design, as well as their application on a case study.
Method
The efficiency of the method was tested in a qualitative study involving four pairs of SE and HCI experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system. The effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment (based on the same analysis model), using software engineering metrics.
Results
The first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process, but raised concerns on the handling of conflicting collaborative activities. The second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software (in terms of coupling, stability and complexity).
Conclusion
It is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems. Areas of improvement are also described.}
}
@article{SZALKOWSKI2021104917,
title = {Synthetic digital reconstructed radiographs for MR-only robotic stereotactic radiation therapy: A proof of concept},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104917},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104917},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521007113},
author = {Gregory Szalkowski and Dong Nie and Tong Zhu and Pew-Thian Yap and Jun Lian},
keywords = {Synthetic CT, Robotic radiotherapy, Deep learning},
abstract = {Purpose
To create synthetic CTs and digital reconstructed radiographs (DRRs) from MR images that allow for fiducial visualization and accurate dose calculation for MR-only radiosurgery.
Methods
We developed a machine learning model to create synthetic CTs from pelvic MRs for prostate treatments. This model has been previously proven to generate synthetic CTs with accuracy on par or better than alternate methods, such as atlas-based registration. Our dataset consisted of 11 paired CT and conventional MR (T2) images used for previous CyberKnife (Accuray, Inc) radiotherapy treatments. The MR images were pre-processed to mimic the appearance of fiducial-enhancing images. Two models were trained for each parameter case, using a sub-set of the available image pairs, with the remaining images set aside for testing and validation of the model to identify the optimal patch size and number of image pairs used for training. Four models were then trained using the identified parameters and used to generate synthetic CTs, which in turn were used to generate DRRs at angles 45° and 315°, as would be used for a CyberKnife treatment. The synthetic CTs and DRRs were compared visually and using the mean squared error and peak signal-to-noise ratio against the ground-truth images to evaluate their similarity.
Results
The synthetic CTs, as well as the DRRs generated from them, gave similar visualization of the fiducial markers in the prostate as the true counterparts. There was no significant difference found for the fiducial localization for the CTs and DRRs. Across the 8 DRRs analyzed, the mean MSE between the normalized true and synthetic DRRs was 0.66 ± 0.42% and the mean PSNR for this region was 22.9 ± 3.7 dB. For the full CTs, the mean MAE was 72.9 ± 88.1 HU and the mean PSNR was 31.2 ± 2.2 dB.
Conclusions
Our machine learning-based method provides a proof of concept of a way to generate synthetic CTs and DRRs for accurate dose calculation and fiducial localization for use in radiation treatment of the prostate.}
}
@article{ESFAHLANI201942,
title = {Mixed reality and remote sensing application of unmanned aerial vehicle in fire and smoke detection},
journal = {Journal of Industrial Information Integration},
volume = {15},
pages = {42-49},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X18300773},
author = {Shabnam Sadeghi Esfahlani},
keywords = {Fire detection, Autonomous flight, Crazyflie 2.0, Monocular camera, Computer vision},
abstract = {This paper proposes the development of a system incorporating inertial measurement unit (IMU), a consumer-grade digital camera and a fire detection algorithm simultaneously with a nano Unmanned Aerial Vehicle (UAV) for inspection purposes. The video streams are collected through the monocular camera and navigation relied on the state-of-the-art indoor/outdoor Simultaneous Localisation and Mapping (SLAM) system. It implements the robotic operating system (ROS) and computer vision algorithm to provide a robust, accurate and unique inter-frame motion estimation. The collected onboard data are communicated to the ground station and used the SLAM system to generate a map of the environment. A robust and efficient re-localization was performed to recover from tracking failure, motion blur, and frame lost in the data received. The fire detection algorithm was deployed based on the color, movement attributes, temporal variation of fire intensity and its accumulation around a point. The cumulative time derivative matrix was utilized to analyze the frame-by-frame changes and to detect areas with high-frequency luminance flicker (random characteristic). Color, surface coarseness, boundary roughness, and skewness features were perceived as the quadrotor flew autonomously within the clutter and congested area. Mixed Reality system was adopted to visualize and test the proposed system in a physical environment, and the virtual simulation was conducted through the Unity game engine. The results showed that the UAV could successfully detect fire and flame, autonomously fly towards and hover around it, communicate with the ground station and simultaneously generate a map of the environment. There was a slight error between the real and virtual UAV calibration due to the ground truth data and the correlation complexity of tracking real and virtual camera coordinate frames.}
}
@article{ORTEGA2008367,
title = {Deformable brain atlas: Validation of the location of subthalamic nucleus using T1-weighted MR images of patients operated on for Parkinson’s},
journal = {Computerized Medical Imaging and Graphics},
volume = {32},
number = {5},
pages = {367-378},
year = {2008},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2008.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0895611108000220},
author = {M. Ortega and M.C. Juan and M. Alcañiz and J.A. Gil and C. Monserrat},
keywords = {Brain atlas, Talairach–Tournoux, Elastic registration, Radial basis functions, Active appearance models, Subthalamic nucleus, Parkinson’s disease},
abstract = {Parkinson’s disease is a degenerative disease of the central nervous system. One of the most effective treatments is deep brain stimulation. This technique requires the localization of an objective structure: the subthalamic nucleus. Unfortunately this structure is difficult to locate. In this work the creation of a deformable brain atlas that enables the identification of the subthalamic nucleus in T1-weighted magnetic resonance imaging (MRI) in an automatic, precise and fast way is presented. The system has been validated using data from 10 patients (20 nucleus) operated on for Parkinson’s. Our system offers better results using a Wendland function with an error of 1.8853±0.9959mm.}
}
@article{PETROLO201086,
title = {Effect of a virtual reality interface on the learning curve and on the accuracy of a surgical planner for total hip replacement},
journal = {Computer Methods and Programs in Biomedicine},
volume = {97},
number = {1},
pages = {86-91},
year = {2010},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2009.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260709002922},
author = {Luca Petrolo and Debora Testi and Fulvia Taddei and Marco Viceconti},
keywords = {Virtual reality, Surgical planning, Total hip replacement},
abstract = {The aim of this study is to evaluate the performance of a non-conventional input and output device (virtual reality) in a total hip replacement surgical planner. A test was performed asking five users to position a cup in a defined position. Every user performed the task using three different hardware configurations: (I) conventional mouse and monitor, (II) mouse and auto-stereoscopic monitor, and (III) 12-DOF tracker (haptic device) and auto-stereoscopic monitor. The results were evaluated in terms of root mean square error of the obtained position with respect to the target one and in terms of learning curve. The results showed that the examined VR technology does not show a sufficient positioning accuracy to be considered for clinical assessment.}
}
@incollection{SHANG2009435,
title = {How a process simulator and a rule-based system contribute to virtual reality applications for process safety},
editor = {Jacek Jeżowski and Jan Thullie},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {26},
pages = {435-439},
year = {2009},
booktitle = {19th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70073-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609700732},
author = {Xiaolei Shang and Paul Chung and Jeff Fry and Luca Vezzadini and Mauro Colombo},
keywords = {virtual reality, plant safety, process simulation, rule-based system},
abstract = {The VIRTHUALIS project aims to develop a number of virtual reality applications for improving safety in the process industries. The applications allow human factors experts to study how operators interact with plant, and provide a safe environment in which new safety actions can be tried and tested. Safety applications are built on the SafeVR technology platform, a distributed clientserver virtual reality system. This paper describes how two external modules - a process simulator and a rule-based system - are interfaced to the platform and the benefits they provide both separately and together. The two modules communicate with the platform's server by exchanging messages, conforming to a simple syntax. pSimProxy provides a generic interface to an external process simulator, which in turn delivers the realistic plant behaviour. It handles bidirectional data exchange with and control of the external simulator. It can be configured at run time to use whichever available mechanisms are supported by the actual process simulator that models the plant being simulated. ClipsClient is an expert or rule-based system, based on NASA's CLIPS expert system software that can make inferences about the information contained in the messages. It consists of a set of facts, a number of rules and an inference engine. It can be provided with a number of rules that monitor how operators are running the plant, and react in useful ways to these events. The simulator notifies the server of changes in process parameters through a message. The values may be displayed, for example as gauge readings, in the virtual environment. As operators control the plant, their actions, say opening a valve, are also reported by messages via the server to the process simulation. Messages can also be read by the rule-based system, allowing it to maintain its own representation of the plant. This in turn permits automated expert reasoning on the state of the plant and the actions of its operators which can cause further message to be sent to the server. The rule-based system is therefore, a powerful mechanism for rapidly reconfiguring the application and general rules can be written that only require new facts at run-time to change the behaviour of the entire virtual environment. The message syntaxes, the system architecture and the interfacing of the external modules are described along with examples showing their individual and joint benefits.}
}
@article{DAS2021106074,
title = {Deep neural network for automated simultaneous intervertebral disc (IVDs) identification and segmentation of multi-modal MR images},
journal = {Computer Methods and Programs in Biomedicine},
volume = {205},
pages = {106074},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106074},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721001498},
author = {Pabitra Das and Chandrajit Pal and Amit Acharyya and Amlan Chakrabarti and Saumyajit Basu},
keywords = {Deep learning, Convolutional neural networks, Intervertebral disc, Segmentation, Identification, Region-to-image matching (RIM)},
abstract = {Background and objective: Lower back pain in humans has become a major risk. Classical approaches follow a non-invasive imaging technique for the assessment of spinal intervertebral disc (IVDs) abnormalities, where identification and segmentation of discs are done separately, making it a time-consuming phenomenon. This necessitates designing a robust automated and simultaneous IVDs identification and segmentation of multi-modality MRI images. Methods: We introduced a novel deep neural network architecture coined as ‘RIMNet’, a Region-to-Image Matching Network model, capable of performing an automated and simultaneous IVDs identification and segmentation of MRI images. The multi-modal input data is being fed to the network with a dropout strategy, by randomly disabling modalities in mini-batches. The performance accuracy as a function of the testing dataset was determined. The execution of the deep neural network model was evaluated by computing the IVDs Identification Accuracy, Dice coefficient, MDOC, Average Symmetric Surface Distance, Jaccard Coefficient, Hausdorff Distance and F1 Score. Results:Proposed model has attained 94% identification accuracy, dice coefficient value of 91.7±1% in segmentation and MDOC 90.2±1%. Our model also achieved 0.87±0.02 for Jaccard Coefficient, 0.54±0.04 for ASD and 0.62±0.02 mm Hausdorff Distance. The results have been validated and compared with other methodologies on dataset of MICCAI IVD 2018 challenge. Conclusions: Our proposed deep-learning methodology is capable of performing simultaneous identification and segmentation on IVDs MRI images of the human spine with high accuracy.}
}
@article{GENG2023107440,
title = {HFIST-Net: High-throughput fast iterative shrinkage thresholding network for accelerating MR image reconstruction},
journal = {Computer Methods and Programs in Biomedicine},
volume = {232},
pages = {107440},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107440},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723001050},
author = {Chenghu Geng and Mingfeng Jiang and Xian Fang and Yang Li and Guangri Jin and Aixi Chen and Feng Liu},
keywords = {MRI, Compressed sensing, FISTA, Multi-channel, CNN},
abstract = {Background and Objectives
Compressed sensing (CS) is often used to accelerate magnetic resonance image (MRI) reconstruction from undersampled k-space data. A novelty deeply unfolded networks (DUNs) based method, designed by unfolding a traditional CS-MRI optimization algorithm into deep networks, can provide significantly faster reconstruction speeds than traditional CS-MRI methods while improving image quality.
Methods
In this paper, we propose a High-Throughput Fast Iterative Shrinkage Thresholding Network (HFIST-Net) for reconstructing MR images from sparse measurements by combining traditional model-based CS techniques and data-driven deep learning methods. Specifically, the conventional Fast Iterative Shrinkage Thresholding Algorithm (FISTA) method is expanded as a deep network. To break the bottleneck of information transmission, a multi-channel fusion mechanism is proposed to improve the efficiency of information transmission between adjacent network stages. Moreover, a simple yet efficient channel attention block, called Gaussian context transformer (GCT), is proposed to improve the characterization capabilities of deep Convolutional Neural Network (CNN,) which utilizes Gaussian functions that satisfy preset relationships to achieve context feature excitation.
Results
T1 and T2 brain MR images from the FastMRI dataset are used to validate the performance of the proposed HFIST-Net. The qualitative and quantitative results showed that our method is superior to those compared state-of-the-art unfolded deep learning networks.
Conclusions
The proposed HFIST-Net is capable of reconstructing more accurate MR image details from highly undersampled k-space data while maintaining fast computational speed.}
}
@article{LIM2022102621,
title = {Generalized self-calibrating simultaneous multi-slice MR image reconstruction from 3D Fourier encoding perspective},
journal = {Medical Image Analysis},
volume = {82},
pages = {102621},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522002493},
author = {Eun Ji Lim and Taehoon Shin and Joonyeol Lee and Jaeseok Park},
keywords = {Magnetic resonance imaging, Simultaneous multi-slice, Parallel imaging, Image reconstruction},
abstract = {This work introduces a novel, k-space based one-step solution for simultaneous multi-slice MR image reconstruction from 3D Fourier encoding perspective. With undersampled SMS imaging, image reconstruction suffers from both inter-slice leakages and in-plane aliasing artifacts. Aliasing separation becomes further challenging in the presence of discrepancies between calibration and imaging. To address them, in this work a measured SMS 3D k-space with additional calibrating signals is decomposed into SMS imaging and self-calibrating data sets. Extended controlled aliasing is performed by upsampling the measured data in the kz-direction. A slice-specific null space operator is then learned using extended self-calibration exploiting target slices and additional in-plane-shifted images. Inter-slice leakages and in-plane aliasing artifacts are jointly resolved in a single step by solving a constrained optimization problem in which null space reconstruction consistency is balanced with a Hankel-structured low rank prior while data fidelity in 3D Fourier space is enforced. Retrospective and prospective studies are performed to validate the effectiveness of the proposed method in various regions including knee and L-spine.}
}
@article{GRAJEWSKI2015348,
title = {Improving the Skills and Knowledge of Future Designers in the Field of Ecodesign Using Virtual Reality Technologies},
journal = {Procedia Computer Science},
volume = {75},
pages = {348-358},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.257},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037187},
author = {Damian Grajewski and Jacek Diakun and Radosław Wichniarek and Ewa Dostatni and Paweł Buń and Filip Górski and Anna Karwasz},
keywords = {ecodesign, design for recycling, virtual reality, immersive training ;},
abstract = {Nowadays future designers are required to explore different fields of engineering (e.g. basis of design and operation of machines, methodology of process design, characteristics of phases of a product life cycle, ergonomics, etc.) in order to obtain sufficient knowledge about the product, the problems of its operation and related services. Currently, there is a strong tendency to take into account the impact of a product on the natural environment. The handling of products that are withdrawn from usage is one of the big problems emerging in a modern society. One of the possibilities to solve these problems is taking into account the environmental issues at the very early stage of the product design. Authors created the methodology to include recycling requirements in the phase of design of the product with support of the CAD 3D systems14,15. In details, the method of ecological-oriented product assessment during the design process was implemented in CAD 3D environment in order to improve the skills and knowledge of future designers about environmental aspects of the designed product12. In order to enhance the effectiveness of the training of designers, authors decided to use immersive Virtual Reality (VR) technologies that are more and more often used as advanced training systems. The user/trained person can explore Virtual Environment (VE) for educational and exercise purposes5. Immersive VR technologies are gaining wider use as engineering design tools to support the product design phase because of their ability to deliver an immersive and user friendly environment that can be used as digital test-bed for prototypes of the product30. Paper briefly describes the analysis of the recycling of selected product designed in the CAD 3D system with the support of Virtual Reality technologies that were based on the recycling product model (RpM) and the agent technology12,14,15. The RpM, developed during the geometric modelling phase, includes the data necessary for a comprehensive product recyclability evaluation already at the design stage. This approach allows designer to select appropriate solutions that facilitate future disassembly and to choose materials most suitable in terms of future recycling. The analysis allows to make an assessment of the susceptibility of the product for recycling using the agent system. The agent system, in accordance with the recycling product model (RpM) that was implemented in the 3D CAD system, is an innovative tool for the designer to enable a comprehensive assessment of the product in terms of its susceptibility to recycling. Future designers (students of mechanical engineering fields) gained possibility to improve their skills and knowledge in the field of ecodesign through the immersive trainings of virtual product design for recycling. Applying the Virtual Reality technologies for training of the future designers was aimed at enhancing the effectiveness of training mainly due to immersion and interaction with the virtual environment and to explore the product before it is constructed, as well as provide learning of the environments in which the finished product will be operated. As the example of the recycling-oriented virtual product design, case study of the design of small household appliance is presented. Participants of the immersive training (students) have a possibility to design the product in the Virtual Reality and select the optimal variant, which meets all environmental requirements. Short description of the recycling modeling (creation of connections, defining the extended materials and disassembly attributes) as well as an example of recycling assessment such as calculation of recycling assessment measures or providing tips and suggestions generated by agent system is also presented.}
}
@article{CHOOI2008473,
title = {Design, modelling and testing of magnetorheological (MR) dampers using analytical flow solutions},
journal = {Computers & Structures},
volume = {86},
number = {3},
pages = {473-482},
year = {2008},
note = {Smart Structures},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2007.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0045794907000739},
author = {Weng W. Chooi and S. Olutunde Oyadiji},
keywords = {Magnetorheological (MR) fluids, Magnetorheological dampers, Semi-active dampers, Annular flow, Herschel–Bulkley fluid model, Bingham plastic model},
abstract = {The most widely used configuration of MR dampers incorporates an annular gap through which the magnetically active MR fluid is forced to flow. The damping force, which is also known as the restoring force and opposes the applied external force, is produced by the movement of the MR fluid through this annular gap. The design of this annular gap usually utilises an approximation that represents this assembly as a flow of fluids between two infinitely wide parallel plates. This assumption has been found to be fairly accurate and sufficient for engineering design purposes, especially when the size of the annular gap is small relative to the mean radius of the annulus, which is usually the case for MR dampers. However, it is still important to have expressions that represent the physical processes exactly. In this paper, the general solutions are derived and the corresponding methodology for representing exactly the flow of fluids with a yield stress through annuli is given. Computational fluid dynamics simulations were also carried out to validate the general expressions presented. The general expression is applicable to any models of fluids with a yield stress. An example of the application of the general analytical expressions using the Herschel–Buckley model is given, and the limitations of the parallel-plate approximation is illustrated for configurations whereby the size of the annular gap relative to the mean radius is large. A mathematical model for a double-tube MR damper fabricated at the University of Manchester, UK is developed based on the annular solution and on the compressibility of MR fluid inside the chambers. It is shown that the modelling procedure represents the MR damper satisfactorily.}
}
@article{MASTMEYER2016161,
title = {Efficient patient modeling for visuo-haptic VR simulation using a generic patient atlas},
journal = {Computer Methods and Programs in Biomedicine},
volume = {132},
pages = {161-175},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715300729},
author = {Andre Mastmeyer and Dirk Fortmeier and Heinz Handels},
keywords = {Virtual reality simulation, Efficient CT image segmentation, Atlas-based segmentation, Full body segmentation, Cloud computing},
abstract = {Background and Objective
This work presents a new time-saving virtual patient modeling system by way of example for an existing visuo-haptic training and planning virtual reality (VR) system for percutaneous transhepatic cholangio-drainage (PTCD).
Methods
Our modeling process is based on a generic patient atlas to start with. It is defined by organ-specific optimized models, method modules and parameters, i.e. mainly individual segmentation masks, transfer functions to fill the gaps between the masks and intensity image data. In this contribution, we show how generic patient atlases can be generalized to new patient data. The methodology consists of patient-specific, locally-adaptive transfer functions and dedicated modeling methods such as multi-atlas segmentation, vessel filtering and spline-modeling.
Results
Our full image volume segmentation algorithm yields median DICE coefficients of 0.98, 0.93, 0.82, 0.74, 0.51 and 0.48 regarding soft-tissue, liver, bone, skin, blood and bile vessels for ten test patients and three selected reference patients. Compared to standard slice-wise manual contouring time saving is remarkable.
Conclusions
Our segmentation process shows out efficiency and robustness for upper abdominal puncture simulation systems. This marks a significant step toward establishing patient-specific training and hands-on planning systems in a clinical environment.}
}
@article{RAJINIKANTH201787,
title = {Entropy based segmentation of tumor from brain MR images – a study with teaching learning based optimization},
journal = {Pattern Recognition Letters},
volume = {94},
pages = {87-95},
year = {2017},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2017.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167865517301915},
author = {V. Rajinikanth and Suresh Chandra Satapathy and Steven Lawrence Fernandes and S. Nachiappan},
abstract = {Image processing plays an important role in various medical applications to support the computerized disease examination. Brain tumor, such as glioma is one of the life threatening cancers in humans and the premature diagnosis will improve the survival rate. Magnetic Resonance Image (MRI) is the widely considered imaging practice to record the glioma for the clinical study. Due to its complexity and varied modality, brain MRI needs the automated assessment technique. In this paper, a novel methodology based on meta-heuristic optimization approach is proposed to assist the brain MRI examination. This approach enhances and extracts the tumor core and edema sector from the brain MRI integrating the Teaching Learning Based Optimization (TLBO), entropy value, and level set / active contour based segmentation. The proposed method is tested on the images acquired using the Flair, T1C and T2 modalities. The experimental work is implemented and is evaluated using the CEREBRIX and BRAINIX dataset. Further, TLBO assisted approach is validated on the MICCAI brain tumor segmentation (BRATS) challenge 2012 dataset and achieved better values of Jaccard index, dice co-efficient, precision, sensitivity, specificity and accuracy. Hence the proposed segmentation approach is clinically significant.}
}
@article{LUSTERMANS2022107116,
title = {Optimized automated cardiac MR scar quantification with GAN‐based data augmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107116},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107116},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004977},
author = {Didier R.P.R.M. Lustermans and Sina Amirrajab and Mitko Veta and Marcel Breeuwer and Cian M. Scannell},
keywords = {Deep learning, Cardiac MRI, Myocardial scar quantification, Synthetic data, Generative adversarial networks},
abstract = {Background
The clinical utility of late gadolinium enhancement (LGE) cardiac MRI is limited by the lack of standardization, and time-consuming postprocessing. In this work, we tested the hypothesis that a cascaded deep learning pipeline trained with augmentation by synthetically generated data would improve model accuracy and robustness for automated scar quantification.
Methods
A cascaded pipeline consisting of three consecutive neural networks is proposed, starting with a bounding box regression network to identify a region of interest around the left ventricular (LV) myocardium. Two further nnU-Net models are then used to segment the myocardium and, if present, scar. The models were trained on the data from the EMIDEC challenge, supplemented with an extensive synthetic dataset generated with a conditional GAN.
Results
The cascaded pipeline significantly outperformed a single nnU-Net directly segmenting both the myocardium (mean Dice similarity coefficient (DSC) (standard deviation (SD)): 0.84 (0.09) vs 0.63 (0.20), p < 0.01) and scar (DSC: 0.72 (0.34) vs 0.46 (0.39), p < 0.01) on a per-slice level. The inclusion of the synthetic data as data augmentation during training improved the scar segmentation DSC by 0.06 (p < 0.01). The mean DSC per-subject on the challenge test set, for the cascaded pipeline augmented by synthetic generated data, was 0.86 (0.03) and 0.67 (0.29) for myocardium and scar, respectively.
Conclusion
A cascaded deep learning-based pipeline trained with augmentation by synthetically generated data leads to myocardium and scar segmentations that are similar to the manual operator, and outperforms direct segmentation without the synthetic images.}
}
@article{DELAMO2022107954,
title = {Hybrid recommendations and dynamic authoring for AR knowledge capture and re-use in diagnosis applications},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107954},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010868},
author = {Iñigo Fernández {del Amo} and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Augmented Reality, Failure diagnosis, Authoring systems, Knowledge capture, Ontology-based reporting},
abstract = {In Industry 4.0, integrated data management is an important challenge due to heterogeneity and the lack of structure of numerous existing data sources. A relevant research gap involves human knowledge integration, especially in maintenance operations. Augmented Reality (AR) can bridge this gap, but it requires improved augmented content to enable effective and efficient knowledge capture. This paper proposes dynamic authoring and hybrid recommender methods for accurate AR-based reporting. These methods aim to provide maintainers with augmented data input formats and recommended datasets for enhancing the efficiency and effectiveness of their reporting tasks. The proposed contributions have been validated through experiments and surveys in two failure diagnosis reporting scenarios. Experimental results indicated that the proposed reporting solution can reduce reporting errors by 50% and reporting time by 20% compared to alternative recommender and AR tools. Besides, survey results suggested that testers perceived the proposed reporting solution as more effective and satisfactory for reporting tasks than alternative tools. Thus, proving that the proposed methods can improve the effectiveness and efficiency of diagnosis reporting applications. Finally, this paper proposes future works towards a framework for automatic adaptive authoring in AR knowledge transfer and capture applications for human knowledge integration in the context of Industry 4.0.}
}
@article{VIGNEAULT201895,
title = {Ω-Net (Omega-Net): Fully automatic, multi-view cardiac MR detection, orientation, and segmentation with deep neural networks},
journal = {Medical Image Analysis},
volume = {48},
pages = {95-106},
year = {2018},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2018.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1361841518302998},
author = {Davis M. Vigneault and Weidi Xie and Carolyn Y. Ho and David A. Bluemke and J. Alison Noble},
keywords = {Cardiac magnetic resonance, Semantic segmentation, Deep convolutional neural networks, Spatial transformer networks},
abstract = {Pixelwise segmentation of the left ventricular (LV) myocardium and the four cardiac chambers in 2-D steady state free precession (SSFP) cine sequences is an essential preprocessing step for a wide range of analyses. Variability in contrast, appearance, orientation, and placement of the heart between patients, clinical views, scanners, and protocols makes fully automatic semantic segmentation a notoriously difficult problem. Here, we present Ω-Net (Omega-Net): A novel convolutional neural network (CNN) architecture for simultaneous localization, transformation into a canonical orientation, and semantic segmentation. First, an initial segmentation is performed on the input image; second, the features learned during this initial segmentation are used to predict the parameters needed to transform the input image into a canonical orientation; and third, a final segmentation is performed on the transformed image. In this work, Ω-Nets of varying depths were trained to detect five foreground classes in any of three clinical views (short axis, SA; four-chamber, 4C; two-chamber, 2C), without prior knowledge of the view being segmented. This constitutes a substantially more challenging problem compared with prior work. The architecture was trained using three-fold cross-validation on a cohort of patients with hypertrophic cardiomyopathy (HCM, N=42) and healthy control subjects (N=21). Network performance, as measured by weighted foreground intersection-over-union (IoU), was substantially improved for the best-performing Ω-Net compared with U-Net segmentation without localization or orientation (0.858 vs 0.834). In addition, to be comparable with other works, Ω-Net was retrained from scratch using five-fold cross-validation on the publicly available 2017 MICCAI Automated Cardiac Diagnosis Challenge (ACDC) dataset. The Ω-Net outperformed the state-of-the-art method in segmentation of the LV and RV bloodpools, and performed slightly worse in segmentation of the LV myocardium. We conclude that this architecture represents a substantive advancement over prior approaches, with implications for biomedical image segmentation more generally.}
}
@article{GUO2021101835,
title = {Fully automated 3D segmentation of MR-imaged calf muscle compartments: Neighborhood relationship enhanced fully convolutional network},
journal = {Computerized Medical Imaging and Graphics},
volume = {87},
pages = {101835},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2020.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0895611120301300},
author = {Zhihui Guo and Honghai Zhang and Zhi Chen and Ellen {van der Plas} and Laurie Gutmann and Daniel Thedens and Peggy Nopoulos and Milan Sonka},
keywords = {Calf muscle compartment segmentation, Fully convolutional network, Edge constraint, Magnetic resonance image, 3D},
abstract = {Automated segmentation of individual calf muscle compartments from 3D magnetic resonance (MR) images is essential for developing quantitative biomarkers for muscular disease progression and its prediction. Achieving clinically acceptable results is a challenging task due to large variations in muscle shape and MR appearance. In this paper, we present a novel fully convolutional network (FCN) that utilizes contextual information in a large neighborhood and embeds edge-aware constraints for individual calf muscle compartment segmentations. An encoder–decoder architecture is used to systematically enlarge convolution receptive field and preserve information at all resolutions. Edge positions derived from the FCN output muscle probability maps are explicitly regularized using kernel-based edge detection in an end-to-end optimization framework. Our method was evaluated on 40 T1-weighted MR images of 10 healthy and 30 diseased subjects by fourfold cross-validation. Mean DICE coefficients of 88.00–91.29% and mean absolute surface positioning errors of 1.04–1.66 mm were achieved for the five 3D muscle compartments.}
}
@article{LEE2004719,
title = {Development of a virtual reality environment for somatosensory and perceptual stimulation in the balance assessment of children},
journal = {Computers in Biology and Medicine},
volume = {34},
number = {8},
pages = {719-733},
year = {2004},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2003.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010482503001021},
author = {Hsiao-Yu Lee and Rong-Ju Cherng and Chi-Hsuan Lin},
keywords = {Virtual reality, Balance, Sensory organization test, Singular value decomposition},
abstract = {In this study, we developed a balance assessment system in which the visual stimulus was generated by a virtual reality (VR) technique and somatosensation was obtained from a movable platform. By standing on the movable platform, the center of pressure (COP) was measured to express the subject's postural control in response to varied visual stimuli. From the COP data, a singular value decomposition technique was used to derive the sway area and direction, represented in a polar form. Our system demonstrated the feasibility of using a VR environment in postural control trials and provided more realistic somatosensory and visual inputs.}
}
@article{ZAVOLOVICH2021845,
title = {A Python software to evaluate geometric discrepancies between stereotactic CT and MR images in radiosurgery},
journal = {Procedia Computer Science},
volume = {190},
pages = {845-851},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013545},
author = {Bogdan D. Zavolovich and Irina I. Bannikova and Aleksandra V. Dalechina and Valery V. Kostjuchenko and Pavel N. Ryabov},
keywords = {MR imaging, CT, stereotactic radiosurgery, Elekta MR phantom, Python},
abstract = {Due to its excellent soft tissue contrast, magnetic resonance imaging (MRI) is widely used for target delineation in stereotactic radiosurgery. However, because of a number of factors (inhomogeneity of the constant magnetic field, nonlinearity of the gradient fields, etc.), MR images are more susceptible to spatial distortion as opposed to Computed tomography (CT) images, which could be considered as the reference studies in terms of the accuracy of target localization. It is crucial to assess the geometric errors in MR images as the accuracy of dose delivery strongly depends on MRI quality. One of the ways of MRI distortion evaluation is to scan a phantom with multiple marker points on both CT and MRI scanner and compare coordinates of the points defined on CT and MR images, respectively. Performing this test as a routine part of MRI quality assurance (QA) program for stereotactic radiosurgery is complicated by lack of a software for the automatic calculations of the discrepancies. The aim of our work was to develop a software to calculate discrepancies between stereotactic coordinates of the phantom’s plastic rods defined on the MR and CT images of the Elekta MR phantom. A special Python software was developed and the spatial accuracy of the stereotactic MR images was assessed. The calculated total differences between the coordinates of the CT and T1 weighted MR images, CT and T2 weighted MR images exceeded 1 mm in 3,5%, 0.1% of the points, respectively. Using the software in clinic routines could both speed up the time needed for the test performance and eliminate subjective visual assessment of the discrepancies.}
}
@article{KOLEY2016453,
title = {Delineation and diagnosis of brain tumors from post contrast T1-weighted MR images using rough granular computing and random forest},
journal = {Applied Soft Computing},
volume = {41},
pages = {453-465},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300096},
author = {Subhranil Koley and Anup K. Sadhu and Pabitra Mitra and Basabi Chakraborty and Chandan Chakraborty},
keywords = {MRI, Brain tumor, Rough entropy, Feature extraction, Tumor characterization},
abstract = {This paper presents a new approach of delineation and characterization of four different types of brain tumors viz. Glioblastoma multiforme (GBM), metastasis (MET), meningioma (MG) and granuloma (GN) from magnetic resonance imaging (MRI) slices of post contrast T1-weighted (T1C) sequence to improve the computer assistive diagnostic accuracy. An integrated framework of identification and extraction of tumor region, quantification of histogram, shape and textural features followed through pattern classification by machine learning algorithm has been proposed. Rough entropy based thresholding in granular computing paradigm has been adopted for delineation of tumor area. After accomplishing quantitative validation and comparison with existing methods, experimental results prove the efficiency and applicability of proposed segmentation approach. In the next stage, the extracted lesions have been quantified with 86 features to develop the training dataset. Random forest (RF), an ensemble learning scheme has been implemented, which learns the training data for accurate prediction of the class label of a given input. The performance of RF has been evaluated by statistical measures from 3 fold cross-validation and compared with five different classifiers. The same experiment has been repeated over the reduced set of features generated by correlation based feature selection strategy. Experimental results show the superiority of RF (Sensitivity achieved in %: GBM-96.7, MET-96.2, MG-98.1 and GN-97.7) with the complete set of features. The comparison of proposed methodology with the existing works signifies its applicability and effectiveness. Additionally a 10 fold cross-validation has been accomplished to justify the statistical significance of the classification accuracy achieved from proposed methodology.}
}
@article{NOON2012500,
title = {A system for rapid creation and assessment of conceptual large vehicle designs using immersive virtual reality},
journal = {Computers in Industry},
volume = {63},
number = {5},
pages = {500-512},
year = {2012},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2012.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166361512000371},
author = {Christian Noon and Ruqin Zhang and Eliot Winer and James Oliver and Brian Gilmore and Jerry Duncan},
keywords = {Product configuration, Conceptual design, Virtual reality},
abstract = {Currently, new product concepts are often evaluated by developing detailed virtual part and assembly models with traditional computer aided design (CAD) tools followed by appropriate analyses (e.g., finite element analysis, computational fluid dynamics, etc.). The creation of these models and analyses are tremendously time consuming. If a number of different conceptual configurations have been determined, it may not be possible to model and analyze each of them due to the complexity of these evaluation processes. Thus, promising concepts might be eliminated based solely on insufficient time and resources for assessment. In addition, the virtual models and analyses performed are usually of much higher detail and accuracy than what is needed for such early assessment. By eliminating the time-consuming complexity of a CAD environment and incorporating qualitative assessment tools, engineers could spend more time evaluating concepts that may have been previously abandoned due to time constraints. To address these issues, the Advanced Systems Design Suite (ASDS), was created. The ASDS incorporates a PC user interface with an immersive virtual reality (VR) environment to ease the creation and assessment of conceptual design prototypes individually or collaboratively in an immersive VR environment. Assessment tools incorporate metamodeling approximations and immersive visualization to evaluate the feasibility of each concept. In this paper, the ASDS system and interface along with specifically designed immersive VR assessment tools such as state saving and dynamic viewpoint creation are presented for conceptual large vehicle design. A test case example of redesigning an airplane is presented to explore the feasibility of the proposed system.}
}