@article{KRITTIAN20121029,
title = {A finite-element approach to the direct computation of relative cardiovascular pressure from time-resolved MR velocity data},
journal = {Medical Image Analysis},
volume = {16},
number = {5},
pages = {1029-1037},
year = {2012},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2012.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841512000497},
author = {Sebastian B.S. Krittian and Pablo Lamata and Christian Michler and David A. Nordsletten and Jelena Bock and Chris P. Bradley and Alex Pitcher and Philip J. Kilner and Michael Markl and Nic P. Smith},
keywords = {4D flow, MRI, Relative pressure field, Pressure Poisson equation, Direct flow evaluation},
abstract = {The evaluation of cardiovascular velocities, their changes through the cardiac cycle and the consequent pressure gradients has the capacity to improve understanding of subject-specific blood flow in relation to adjacent soft tissue movements. Magnetic resonance time-resolved 3D phase contrast velocity acquisitions (4D flow) represent an emerging technology capable of measuring the cyclic changes of large scale, multi-directional, subject-specific blood flow. A subsequent evaluation of pressure differences in enclosed vascular compartments is a further step which is currently not directly available from such data. The focus of this work is to address this deficiency through the development of a novel simulation workflow for the direct computation of relative cardiovascular pressure fields. Input information is provided by enhanced 4D flow data and derived MR domain masking. The underlying methodology shows numerical advantages in terms of robustness, global domain composition, the isolation of local fluid compartments and a treatment of boundary conditions. This approach is demonstrated across a range of validation examples which are compared with analytic solutions. Four subject-specific test cases are subsequently run, showing good agreement with previously published calculations of intra-vascular pressure differences. The computational engine presented in this work contributes to non-invasive access to relative pressure fields, incorporates the effects of both blood flow acceleration and viscous dissipation, and enables enhanced evaluation of cardiovascular blood flow.}
}
@article{CIEZA2018352,
title = {Educational Mobile Application of Augmented Reality Based on Markers to Improve the Learning of Vowel Usage and Numbers for Children of a Kindergarten in Trujillo},
journal = {Procedia Computer Science},
volume = {130},
pages = {352-358},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304046},
author = {Edwin Cieza and David Lujan},
keywords = {Mobile app, Learning, Augmented reality, Markers, Development tools},
abstract = {The main objective of this research was to improve the level of understanding of the usage of vowels and numbers for children over 4 years of age in the Juana Alarco de Dammert Nursery School in Trujillo through an educational mobile application that is composed of the unit development platform, monodevelopment, Andriod Studio, Vuforia using the programming language C# that was made based on the development methodology of extreme software programming. The research design is a pre-experimental experiment grade which was composed of 10 children over the age of 4 of the nursery school and was used as a method of data analysis Student T test. In addition, with the implemented application it was possible to increase the level of academic performance of vowel usage by 27.60% and the use of numbers by 22.60%. It was concluded that with the implementation of the educational mobile application of augmented reality, the level of understanding of vowel usage and numbers had improved in the Juana Alarco de Dammart nursery school children.}
}
@article{BUN2017445,
title = {Low – Cost Devices Used in Virtual Reality Exposure Therapy},
journal = {Procedia Computer Science},
volume = {104},
pages = {445-451},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.158},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730159X},
author = {Pawel Bun and Filip Gorski and Damian Grajewski and Radoslaw Wichniarek and Przemyslaw Zawadzki},
keywords = {Virtual reality, Low – cost devices, Virtual reality exposure therapy},
abstract = {The Virtual Reality technology is nowadays dynamically developed, thanks to appearance of low-cost devices and interest by large companies related to entertainment, communication and visualization. It is especially important in medicine, as it allows a much wider access to tools such as Virtual Reality Exposure Therapy. The paper presents possibilities of using low-cost VR devices in curing phobias by exposure to a stress-generating factor in an immersive virtual environment. Several use scenarios are presented for a simple application aimed at exposing a patient to fear of heights. A test group consisted of healthy individuals. To evaluate level of immersion and fear caused by the application, a heart rate monitor was used, to record heartbeat in real time.}
}
@article{SAMEI2020101588,
title = {A partial augmented reality system with live ultrasound and registered preoperative MRI for guiding robot-assisted radical prostatectomy},
journal = {Medical Image Analysis},
volume = {60},
pages = {101588},
year = {2020},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101588},
url = {https://www.sciencedirect.com/science/article/pii/S1361841519301288},
author = {Golnoosh Samei and Keith Tsang and Claudia Kesch and Julio Lobo and Soheil Hor and Omid Mohareri and Silvia Chang and S. Larry Goldenberg and Peter C. Black and Septimiu Salcudean},
keywords = {Image-guidance, Surgery, Ultrasound, Magnetic resonance imaging, Image fusion, Registration},
abstract = {We propose an image guidance system for robot assisted laparoscopic radical prostatectomy (RALRP). A virtual 3D reconstruction of the surgery scene is displayed underneath the endoscope’s feed on the surgeon’s console. This scene consists of an annotated preoperative Magnetic Resonance Image (MRI) registered to intraoperative 3D Trans-rectal Ultrasound (TRUS) as well as real-time sagittal 2D TRUS images of the prostate, 3D models of the prostate, the surgical instrument and the TRUS transducer. We display these components with accurate real-time coordinates with respect to the robot system. Since the scene is rendered from the viewpoint of the endoscope, given correct parameters of the camera, an augmented scene can be overlaid on the video output. The surgeon can rotate the ultrasound transducer and determine the position of the projected axial plane in the MRI using one of the registered da Vinci instruments. This system was tested in the laboratory on custom-made agar prostate phantoms. We achieved an average total registration accuracy of 3.2  ±  1.3 mm. We also report on the successful application of this system in the operating room in 12 patients. The average registration error between the TRUS and the da Vinci system for the last 8 patients was 1.4  ±  0.3 mm and average target registration error of 2.1  ±  0.8 mm, resulting in an in vivo overall robot system to MRI mean registration error of 3.5 mm or less, which is consistent with our laboratory studies.}
}
@article{CHEN2016477,
title = {Augmented reality-based video-modeling storybook of nonverbal facial cues for children with autism spectrum disorder to improve their perceptions and judgments of facial expressions and emotions},
journal = {Computers in Human Behavior},
volume = {55},
pages = {477-485},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215301643},
author = {Chien-Hsu Chen and I-Jui Lee and Ling-Yi Lin},
keywords = {Augmented reality (AR), Visual indicator, Visual supports, Video modeling, Nonverbal social cues},
abstract = {Autism spectrum disorders (ASD) are characterized by a reduced ability to understand the emotions of other people. Increasing evidence indicates that children with ASD might not recognize or understand crucial nonverbal behaviors, which likely causes them to ignore nonverbal gestures and social cues, like facial expressions, that usually aid social interaction. We used an augmented reality (AR)-based video modeling (VM) storybook (ARVMS) to strengthen and attract the attention of children with ASD to nonverbal social cues because they have difficulty adjusting and switching their attentional focus. In this research, AR has multiple functions: it extends the social features of the story, but it also restricts attention to the most important parts of the videos. Evidence-based research shows that AR attracts the attention of children with ASD. However, few studies have combined AR with VM to train children with ASD to mimic facial expressions and emotions to improve their social skills. In addition, we used markerless natural tracking to teach the children to recognize patterns as they focused on the stable visual image printed in the storybook and then extended their attention to an animation of the story. After the three-phase (baseline, intervention, and maintenance) test data had been collected, the results showed that ARVMS intervention provided an augmented visual indicator which had effectively attracted and maintained the attention of children with ASD to nonverbal social cues and helped them better understand the facial expressions and emotions of the storybook characters.}
}
@article{VOGT201946,
title = {Virtual reality interventions for balance prevention and rehabilitation after musculoskeletal lower limb impairments in young up to middle-aged adults: A comprehensive review on used technology, balance outcome measures and observed effects},
journal = {International Journal of Medical Informatics},
volume = {126},
pages = {46-58},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618303010},
author = {Sarah Vogt and Nina Skjæret-Maroni and Dorothee Neuhaus and Jochen Baumeister},
keywords = {Virtual reality, Balance training, Musculoskeletal disorders, Lower limb, Exergames},
abstract = {Background
Balance training is an important aspect in prevention and rehabilitation of musculoskeletal lower limb injuries. Virtual reality (VR) is a promising addition or alternative to traditional training. This review aims to provide a comprehensive overview of VR technology and games employed for balance prevention and rehabilitation, balance outcome measures, and effects for both balance prevention and balance rehabilitation following musculoskeletal lower limb impairments.
Methods
A systematic literature search was conducted in electronic databases to identify all related articles with a longitudinal study design on VR, balance, and prevention or musculoskeletal rehabilitation of the lower limbs in adult subjects between 19 and 65 years.
Results
Eleven articles concerning balance prevention and five articles regarding balance rehabilitation were included. All studies used screen-based VR and off-the-shelf gaming consoles with accompanying games. The Star Excursion Balance Test (SEBT) was the most frequently used outcome measure. Two studies found positive effects of VR balance training in healthy adults, while none reported negative effects. None of the included studies showed a significant difference in balance performance after a VR balance rehabilitation intervention compared to traditional balance training.
Conclusion
Few studies have been published concerning musculoskeletal balance rehabilitation and balance prevention in healthy adult subjects. However, the studies published have shown that VR exercises are equally effective compared to traditional balance training for both domains of application. As there is large variability between studies, recommendations for future research are given to prospectively investigate the use of VR technology for balance training.}
}
@article{LAYONA2018457,
title = {Web based Augmented Reality for Human Body Anatomy Learning},
journal = {Procedia Computer Science},
volume = {135},
pages = {457-464},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.197},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831487X},
author = {Rita Layona and Budi Yulianto and Yovita Tunardi},
keywords = {human anatomy, augmented reality, 3D object, human organ},
abstract = {Human body anatomy becomes an important topic in Biology subject that must be understood since junior high school. Learning materials are mostly available in form of book and anatomy mannequin (puppet), but it is still insufficient enough to help students in understanding human body anatomy. Augmented Reality (AR) is a technology that combines a real thing into virtual environment interactively. This research purpose is to develop an AR application for human body anatomy learning to be more interesting and easier for student to understand. This application enables student to learn human body anatomy with 3D object interaction while previously using textbook and mannequin. Research method in for this study is by using quantitative method that collects data and then develops the prototype to prove the impact. Application development method is done by using waterfall method that includes planning (collect data and analysis), design (user interface and diagram), implementation, and testing. Research result is AR application for human body anatomy learning that contains 3D object, organ explanation and position that can be accessible on web.}
}
@article{PIERDICCA201667,
title = {Smart maintenance of riverbanks using a standard data layer and Augmented Reality},
journal = {Computers & Geosciences},
volume = {95},
pages = {67-74},
year = {2016},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416301662},
author = {Roberto Pierdicca and Emanuele Frontoni and Primo Zingaretti and Adriano Mancini and Eva Savina Malinverni and Anna Nora Tassetti and Ernesto Marcheggiani and Andrea Galli},
keywords = {Buffer strips, Augmented Reality, Environmental monitoring, Mobile visualization, GIS},
abstract = {Linear buffer strips (BS) along watercourses are commonly adopted to reduce run-off, accumulation of bank-top sediments and the leaking of pesticides into fresh-waters, which strongly increase water pollution. However, the monitoring of their conditions is a difficult task because they are scattered over wide rural areas. This work demonstrates the benefits of using a standard data layer and Augmented Reality (AR) in watershed control and outlines the guideline of a novel approach for the health-check of linear BS. We designed a mobile environmental monitoring system for smart maintenance of riverbanks by embedding the AR technology within a Geographical Information System (GIS). From the technological point of view, the system's architecture consists of a cloud-based service for data sharing, using a standard data layer, and of a mobile device provided with a GPS based AR engine for augmented data visualization. The proposed solution aims to ease the overall inspection process by reducing the time required to run a survey. Indeed, ordinary operational survey conditions are usually performed basing the fieldwork on just classical digitized maps. Our application proposes to enrich inspections by superimposing information on the device screen with the same point of view of the camera, providing an intuitive visualization of buffer strip location. This way, the inspection officer can quickly and dynamically access relevant information overlaying geographic features, comments and other contents in real time. The solution has been tested in fieldwork to prove at what extent this cutting-edge technology contributes to an effective monitoring over large territorial settings. The aim is to encourage officers, land managers and practitioners toward more effective monitoring and management practices.}
}
@article{GIMENO20131263,
title = {A new AR authoring tool using depth maps for industrial procedures},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1263-1271},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513001267},
author = {J. Gimeno and P. Morillo and J.M. Orduña and M. Fernández},
keywords = {Augmented reality, Authoring tools, Non-immersive desktop, Kinect, Occlusion},
abstract = {Several augmented reality systems have been proposed for different target fields such as medical, cultural heritage and military. However, most of the current AR authoring tools are actually programming interfaces that are exclusively suitable for programmers. In this paper, we propose an AR authoring tool which provides advanced visual effect, such as occlusion or media contents. This tool allows non-programming users to develop low-cost AR applications, specially oriented to on-site assembly and maintenance/repair tasks. A new 3D edition interface is proposed, using photos and Kinect depth information to improve 3D scenes composition. In order to validate our AR authoring tool, two evaluations have been performed, to test the authoring process and the task execution using AR. The evaluation results show that overlaying 3D instructions on the actual work pieces reduces the error rate for an assembly task by more than a 75%, particularly diminishing cumulative errors common in sequential procedures. Also, the results show how the new edition interface proposed, improves the 3D authoring process making possible create more accurate AR scenarios and 70% faster.}
}
@article{TOSSAVAINEN2003277,
title = {Development of virtual reality stimuli for force platform posturography},
journal = {International Journal of Medical Informatics},
volume = {70},
number = {2},
pages = {277-283},
year = {2003},
note = {MIE 2002 Special Issue},
issn = {1386-5056},
doi = {https://doi.org/10.1016/S1386-5056(03)00034-0},
url = {https://www.sciencedirect.com/science/article/pii/S1386505603000340},
author = {Timo Tossavainen and Martti Juhola and Ilmari Pyykkö and Heikki Aalto and Esko Toppila},
keywords = {Virtual reality, Posturography, Balance},
abstract = {People relying much on vision in the control of posture are known to have an elevated risk of falling. Dependence on visual control is an important parameter in the diagnosis of balance disorders. We have previously shown that virtual reality (VR) methods can be used to produce visual stimuli that affect balance, but suitable stimuli need to be found. In this study, the effect of six different VR stimuli on the balance of 22 healthy test subjects was evaluated using force platform posturography. We report in more detail and expand the results published earlier. According to the tests two of the stimuli have a significant destabilizing effect on balance. In addition a significant displacement effect on the subject's center of pressure (COP) was found. Thus it is shown that the design of VR stimuli to cause different effects on the control of balance is possible.}
}
@article{GALVANDEBARBA2018142,
title = {Self-attribution of distorted reaching movements in immersive virtual reality},
journal = {Computers & Graphics},
volume = {76},
pages = {142-152},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301353},
author = {Henrique {Galvan Debarba} and Ronan Boulic and Roy Salomon and Olaf Blanke and Bruno Herbelin},
keywords = {Virtual reality, Self-attribution, Evaluation},
abstract = {This study explores the extent to which individuals embodied in Virtual Reality tend to self-attribute the movements of their avatar. More specifically, we tested subjects performing goal-directed movements and distorted the mapping between user and avatar movements by decreasing or increasing the amplitude of the avatar hand movement required to reach for a target, while maintaining the apparent amplitude – visual distance – fixed. In two experiments, we asked subjects to report whether the movement that they have seen matched the movement that they have performed, or asked them to classify whether a distortion was making the task easier or harder to complete. Our results show that subjects perform poorly in detecting discrepancies when the nature of the distortion is not made explicit and that subjects are biased to self-attributing distorted movements that make the task easier. These findings, in line with previous accounts on the sense of agency, demonstrate the flexibility of avatar embodiment and open new perspectives for the design of guided interactions in Virtual Reality.}
}
@article{KHANDELWAL2015698,
title = {Detection of Features to Track Objects and Segmentation Using GrabCut for Application in Marker-less Augmented Reality},
journal = {Procedia Computer Science},
volume = {58},
pages = {698-705},
year = {2015},
note = {Second International Symposium on Computer Vision and the Internet (VisionNet’15)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915022012},
author = {Pulkit Khandelwal and P. Swarnalatha and Neha Bisht and S. Prabu},
keywords = {Augmented Reality, Virtual Reality, Histogram of Oriented Gradients, GrabCut, Scale Invariant Feature Transform, Support Vector Machine, Interest Points Detectors, Image Descriptors, Segmentation, Human Body},
abstract = {Augmented Reality applications have hovered itself over various platforms such as desktop and most recently to handheld devices such as mobile phones and tablets. Augmented Reality (AR) systems have mostly been limited to Head Worn Displays with start-ups such as Magic Leap and Occulus Rift making tremendous advancement in such AR and VR research applications facing a stiff competition with Software giant Microsoft which has recently introduced Holo Lens. AR refers to the augmentation or the conglomeration of virtual objects in the real world scenario which has a distinct but close resemblance to Virtual Reality (VR) systems which are computer simulated environments which render physical presence in imaginary world. Developers and hackers round the globe have directed their research interests in the development of AR and VR based applications especially in the domain of advertisement and gaming. Many open source libraries, SDKs and proprietary software are available worldwide for developers to make such systems. This paper describes an algorithm for an AR prototype which uses a marker less approach to track and segment out real world objects and then overlay the same on another real world scene. The algorithm was tested on Desktop. The results are comparable with other existing algorithms and outperform some of them in terms of robustness, speed, and accuracy, precision and timing analysis.}
}
@article{LEE201541,
title = {BoreholeAR: A mobile tablet application for effective borehole database visualization using an augmented reality technology},
journal = {Computers & Geosciences},
volume = {76},
pages = {41-49},
year = {2015},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2014.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300414002830},
author = {Sangho Lee and Jangwon Suh and Hyeong-Dong Park},
keywords = {Borehole, Mobile tablet, Augmented reality, Mobile database, Geological visualization},
abstract = {Boring logs are widely used in geological field studies since the data describes various attributes of underground and surface environments. However, it is difficult to manage multiple boring logs in the field as the conventional management and visualization methods are not suitable for integrating and combining large data sets. We developed an iPad application to enable its user to search the boring log rapidly and visualize them using the augmented reality (AR) technique. For the development of the application, a standard borehole database appropriate for a mobile-based borehole database management system was designed. The application consists of three modules: an AR module, a map module, and a database module. The AR module superimposes borehole data on camera imagery as viewed by the user and provides intuitive visualization of borehole locations. The map module shows the locations of corresponding borehole data on a 2D map with additional map layers. The database module provides data management functions for large borehole databases for other modules. Field survey was also carried out using more than 100,000 borehole data.}
}
@article{TRIBERTI2016163,
title = {Unconscious goal pursuit primes attitudes towards technology usage: A virtual reality experiment},
journal = {Computers in Human Behavior},
volume = {64},
pages = {163-172},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.06.044},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216304721},
author = {Stefano Triberti and Daniela Villani and Giuseppe Riva},
keywords = {Unconscious goal pursuit, Intention, Technology adoption, Technology acceptance, Affordance},
abstract = {Several approaches in technology adoption, such as the Technology Acceptance Model (TAM), ask future users to provide evaluations of technology. Such evaluations are expected to predict actual use behavior. For example, users’ evaluations in terms of perceived usefulness and perceived ease of use are considered meaningful indicators of intention to use the technology, and future usage. However, these approaches still show limited reliability and do not consider other critical aspects, such as situated, unconscious goals and the tendency to perceive related affordances. In order to test the hypothesis that technology evaluation may be influenced by unconscious goals, forty participants were split in two groups. The experimental session included two phases. In the first phase, each group explored a virtual environment that primed a specific goal. In the second phase, participants were asked to evaluate the usefulness and the easiness of use of two versions of the same technology (a mobile devices interface). Results showed that each group evaluated as more useful the version of the technology which featured an affordance related to the respective primed goal. Discussion deals with the possible unconscious influences on attitudes towards technology adoption, and provides operative guidelines to account for them in technology adoption research.}
}
@article{CHO2014258,
title = {Development of virtual reality proprioceptive rehabilitation system for stroke patients},
journal = {Computer Methods and Programs in Biomedicine},
volume = {113},
number = {1},
pages = {258-265},
year = {2014},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2013.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169260713003052},
author = {Sangwoo Cho and Jeonghun Ku and Yun Kyung Cho and In Young Kim and Youn Joo Kang and Dong Pyo Jang and Sun I. Kim},
keywords = {Proprioception feedback, Computer based rehabilitation, Virtual Reality, Stroke, upper-extremity limb},
abstract = {In this study, the virtual reality (VR) proprioception rehabilitation system was developed for stroke patients to use proprioception feedback in upper limb rehabilitation by blocking visual feedback. To evaluate its therapeutic effect, 10 stroke patients (onset>3 month) trained proprioception feedback rehabilitation for one week and visual feedback rehabilitation for another week in random order. Proprioception functions were checked before, a week after, and at the end of training. The results show the click count, error distance and total error distance among proprioception evaluation factors were significantly reduced after proprioception feedback training compared to visual feedback training (respectively, p=0.005, p=0.001, and p=0.007). In addition, subjects were significantly improved in conventional behavioral tests after training. In conclusion, we showed the effectiveness and possible use of the VR to recover the proprioception of stroke patients.}
}
@article{AVAN2022390,
title = {Enhancing VR experience with RBF interpolation based dynamic tuning of stereoscopic rendering},
journal = {Computers & Graphics},
volume = {102},
pages = {390-401},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321002107},
author = {Emre Avan and Tolga K. Capin and Hasmet Gurcay and Ufuk Celikcan},
keywords = {Virtual reality, Stereoscopic rendering, Depth perception, RBF interpolation, Depth authoring},
abstract = {Providing a depth-rich Virtual Reality (VR) experience to users without causing discomfort remains to be a challenge with today’s commercially available head-mounted displays (HMDs), which enforce strict measures on stereoscopic rendering parameters for the sake of keeping visual discomfort to a minimum. However, these measures often lead to an unimpressive VR experience with shallow depth feeling. Using radial basis function interpolation and projection matrix manipulations, we propose a novel method for dynamic tuning of stereoscopic rendering parameters toward enhanced VR experience and demonstrate that it is ready to be used with existing consumer HMDs for automated stereoscopic camera control. We present our method with an approach that is primarily intended for guided VR experiences such as VR walkthroughs or VR rides, assuming a depth layout defined by a path of depth-authored key points in the virtual environment. For this purpose, we also introduce a novel immersive interface for authoring unique 3D stereoscopic cinematographies for VR HMD experiences. The results of the user study indicate that our approach, with proper depth authoring, is able to enhance user experience in terms of overall perceived depth and picture quality while maintaining visual comfort on a par with the HMD’s default arrangement. We also investigate the effects of using depth-of-field blurring in combination with our approach and observe that the quality of the outcome varies based on the overall scene depth.}
}
@article{TSAI2001333,
title = {Virtual reality orthopedic surgery simulator},
journal = {Computers in Biology and Medicine},
volume = {31},
number = {5},
pages = {333-351},
year = {2001},
issn = {0010-4825},
doi = {https://doi.org/10.1016/S0010-4825(01)00014-2},
url = {https://www.sciencedirect.com/science/article/pii/S0010482501000142},
author = {Ming-Dar Tsai and Ming-Shium Hsieh and Shyan-Bin Jou},
keywords = {Volume visualization, Volume manipulation, Orthopedic surgical simulation, Preoperative rehearsal and training, Arthroplasty},
abstract = {This paper describes a highly interactive virtual reality orthopedic surgery simulator. The simulator allows surgeons to use various surgical instruments to operate on virtual rigid anatomic structures, such bones, prostheses and bone grafts, to simulate every procedure on the rigid structures for complex orthopedic surgeries, including arthroplasty, corrective or open osteotomy, open reduction of fractures and amputation. A comparative study of the simulator with paper simulation was performed and showed that interns and residents found the simulator to be a useful learning tool, and that visiting doctors could use it effectively for planning verification and rehearsal of operations.}
}
@article{KHORASANI2023100037,
title = {Hands-on or hands-off: Deciphering the impact of interactivity on embodied learning in VR},
journal = {Computers & Education: X Reality},
volume = {3},
pages = {100037},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000314},
author = {Sara Khorasani and Brandon {Victor Syiem} and Sadia Nawaz and Jarrod Knibbe and Eduardo Velloso},
keywords = {Virtual reality, Embodied learning, VR learning, VR education, VR training, Interaction techniques, Interaction fidelity, Movement in VR, Learning analytics},
abstract = {Studies suggest that Sense of Embodiment (SoE) enabled by VR promotes embodied and active learning. However, it is unclear what features of VR learning environments tap into the concept of embodied learning. For example, interaction techniques, movement and purely observational scenarios in VR can all play a role in facilitating embodied learning. To understand how these mechanisms impact learning, we conducted 2 studies with a total of 64 participants who had no prior experience in the training task. Participants were taught how to use a table saw in 4 conditions and were tested on their task performance in a fully interactive VR assessment. The conditions were analyzed in pairs; 2 conditions with different interaction techniques, 2 conditions with differing ability to move and a cross-study analysis comparing conditions with purely observational learning to interactive learning. We used a mixed methods approach; Analysis of Variance (ANOVA), pairwise comparison of the learning outcomes in each condition as well as thematic analysis of the interview results. We found that some types of “hands-on” interactions can have a detrimental impact on learning and that observational learning can be as impactful as a fully interactive experience. Based on participant interviews, we explored how these mechanisms of the learning environment can impact participants’ learning ability.}
}
@article{DROGEMULLER2020100937,
title = {Examining virtual reality navigation techniques for 3D network visualisations},
journal = {Journal of Computer Languages},
volume = {56},
pages = {100937},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.100937},
url = {https://www.sciencedirect.com/science/article/pii/S2590118419300620},
author = {Adam Drogemuller and Andrew Cunningham and James Walsh and Bruce H. Thomas and Maxime Cordeil and William Ross},
keywords = {Virtual reality, Navigation, Graphs, Visualisation},
abstract = {Research into how virtual reality (VR) can be a beneficial technology for new and emerging large, complex data visualisations for data scientists is ongoing. In this paper, we evaluate three-dimensional VR navigation technique for data visualisations and test their effectiveness with a large graph visualisation. We evaluate two prominent navigation techniques employed in VR (Teleportation and One-Handed Flying) against two less common methods (Two-Handed Flying and Worlds-In-Miniature) and evaluate their performance and effectiveness through a series of tasks. We found Steering Patterns (One-Handed Flying and Two-Handed Flying) to be faster and preferred by participants for completing searching tasks in comparison to Teleportation. Worlds-In-Miniature was the least physically demanding of the navigations, and was preferred by participants for tasks that required an overview of the graph such as triangle counting.}
}
@article{CHU2017241,
title = {Registration and fusion quantification of augmented reality based nasal endoscopic surgery},
journal = {Medical Image Analysis},
volume = {42},
pages = {241-256},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517301263},
author = {Yakui Chu and Jian Yang and Shaodong Ma and Danni Ai and Wenjie Li and Hong Song and Liang Li and Duanduan Chen and Lei Chen and Yongtian Wang},
keywords = {Endoscope, Image-guided surgery, Augmented reality, Image registration, Fusion, Optical tracking},
abstract = {This paper quantifies the registration and fusion display errors of augmented reality-based nasal endoscopic surgery (ARNES). We comparatively investigated the spatial calibration process for front-end endoscopy and redefined the accuracy level of a calibrated endoscope by using a calibration tool with improved structural reliability. We also studied how registration accuracy was combined with the number and distribution of the deployed fiducial points (FPs) for positioning and the measured registration time. A physically integrated ARNES prototype was customarily configured for performance evaluation in skull base tumor resection surgery with an innovative approach of dynamic endoscopic vision expansion. As advised by surgical experts in otolaryngology, we proposed a hierarchical rendering scheme to properly adapt the fused images with the required visual sensation. By constraining the rendered sight in a known depth and radius, the visual focus of the surgeon can be induced only on the anticipated critical anatomies and vessel structures to avoid misguidance. Furthermore, error analysis was conducted to examine the feasibility of hybrid optical tracking based on point cloud, which was proposed in our previous work as an in-surgery registration solution. Measured results indicated that the error of target registration for ARNES can be reduced to 0.77 ± 0.07 mm. For initial registration, our results suggest that a trade-off for a new minimal time of registration can be reached when the distribution of five FPs is considered. For in-surgery registration, our findings reveal that the intrinsic registration error is a major cause of performance loss. Rigid model and cadaver experiments confirmed that the scenic integration and display fluency of ARNES are smooth, as demonstrated by three clinical trials that surpassed practicality.}
}
@article{LEE200888,
title = {Empirical analysis of consumer reaction to the virtual reality shopping mall},
journal = {Computers in Human Behavior},
volume = {24},
number = {1},
pages = {88-104},
year = {2008},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2007.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0747563207000155},
author = {Kun Chang Lee and Namho Chung},
keywords = {Shopping mall, User interface, Virtual reality, Convenience, Enjoyment, Quality assurance, Customer satisfaction},
abstract = {The Internet shopping mall has received wide attention from researchers and practitioners due to the fact that it is one of the most killing applications customers can find on the Internet. Though numerous studies have been performed on various issues of the Internet shopping mall, some research issues relating to the user interface of VR (virtual reality) shopping malls still await further empirical investigation. The objective of this study is to investigate whether the user interface of the VR shopping mall positively affects customer satisfaction in comparison with the ordinary shopping mall. For this purpose, we developed a prototype of the VR shopping mall for which the user interface consists of both 3D graphics and an avatar, using it as an experimental medium. 102 valid questionnaires were gathered from active student users of the ordinary shopping mall, and two research hypotheses were then tested to prove whether the three explanatory variables such as convenience, enjoyment, quality assurance improve in the VR shopping mall, and whether customer satisfaction is also significantly enhanced in the VR shopping mall in comparison with the ordinary shopping mall. Additionally, we conducted the PLS (partial least square) analysis to test whether the customer satisfaction is explained significantly by the three explanatory variables or not.}
}
@article{NG2019278,
title = {Effectiveness of virtual and augmented reality-enhanced exercise on physical activity, psychological outcomes, and physical performance: A systematic review and meta-analysis of randomized controlled trials},
journal = {Computers in Human Behavior},
volume = {99},
pages = {278-291},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302018},
author = {Yu-Leung Ng and Flora Ma and Frederick K. Ho and Patrick Ip and King-wa Fu},
keywords = {Virtual reality, Augmented reality, Physical activity, Physical performance, Systematic review, Meta-analysis, Randomized controlled trials},
abstract = {Virtual reality (VR) and augmented reality (AR)-enhanced exercise training is a novel approach to promoting health. Previous systematic reviews have focused on the effectiveness of VR interventions in clinical settings. The present study was the first systematic review to investigate the effectiveness of exercise-based VR and AR training as preventive measures in improving physical activity, psychological outcomes, and physical performance of a healthy population when compared with traditional programs and no-exercise controls. This study included 22 research articles published between 1997 and 2017, involving 1,184 participants aged 18 to 79. The results showed a large effect on physical activity (Hedges' g = 0.83, SE = 0.18), a small to moderate effect on physical performance (Hedges’ g = 0.31, SE = 0.09), and no significant effect on psychological outcomes. VR training programs were particularly shown to be effective for enhancing frequency of physical activity and strength of physical performance. Only two studies examined the effectiveness of AR training programs on physical performance, and the findings concerning those effects were not separately reported. A list of plausible moderators was tested but that variable was not significantly associated with the effects of VR on the three outcomes. Limitations and future directions are discussed.}
}
@article{THEOKTISTO2005704,
title = {Enhancing collaboration in virtual reality applications},
journal = {Computers & Graphics},
volume = {29},
number = {5},
pages = {704-718},
year = {2005},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2005.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0097849305001330},
author = {Víctor Theoktisto and Marta Fairén},
abstract = {We derive a complete component framework for transforming standalone virtual reality (VR) applications into full-fledged multithreaded collaborative virtual reality environments (CVREs), after characterizing existing implementations into a feature-rich superset. Our main contribution is placing over the existing VR tool a very concise and extensible class framework as an add-on component that provides emerging collaboration features. The enhancements include: a scalable arbitrated peer-to-peer topology for scene sharing; multi-threaded components for graphics rendering, user interaction and network communications; a streaming message protocol for client communications; a collaborative user interface model for session handling; and interchangeable user roles with multicamera perspectives, avatar awareness and shared 3D annotations. We validate the framework by converting the existing ALICE VR Navigator into complete CVRE, with experimental results showing good performance in the collaborative inspection and manipulation of complex models.}
}
@article{KIM2010373,
title = {AR interfacing with prototype 3D applications based on user-centered interactivity},
journal = {Computer-Aided Design},
volume = {42},
number = {5},
pages = {373-386},
year = {2010},
note = {Advanced and Emerging Virtual and Augmented Reality Technologies in Product Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S001044850800198X},
author = {Seungjun Kim and Anind K. Dey},
keywords = {Augmented reality, 3D application prototyping, Human–computer interaction, Ubiquitous computing},
abstract = {Augmented Reality (AR) has been acclaimed as one of the promising technologies for advancing future UbiComp (Ubiquitous Computing) environments. Despite a myriad of AR applications and its influence on the human–computer interaction field, end products that integrate AR are less commonplace than expected. This is due to a lack of being able to forecast in which direction mainstream standardization of AR-oriented platform components will be framed. It is mainly due to a focus on technology issues in implementing reasonable AR solutions, which also results in difficulty in initiating AR research and creating prototype test-beds. Thus, this paper provides a comprehensive review of AR prototyping trends and AR research activities. Through the technical review of a de facto AR prototyping method, we remark upon the key elements of AR techniques, and then present an alternative view of the AR environment centered on end-user advantages in interaction, which is characterized by three features: intuitive observation, informative visualization, and immersive interaction. In addition, we believe that these features can be used to motivate the integration of AR technology into custom-built 3D applications. In this paper, we propose a conceptual schema and an architectural framework for generic AR prototyping. On the basis of these, a video see-through AR interface is integrated into three prototype 3D applications in 3 different domains: engineering systems, geospace, and multimedia. As the case studies for validating our integration, we present three sample AR applications; (a) an AR-interfaced 3D CAE (Computer-Aided Engineering) simulation test-bed, (b) a two-stage distributed Traveler Guidance Service (TGS) test-bed based on a GIS database and AR, and (c) a haptically-enhanced broadcasting test-bed for AR-based 3D media production. For each application, a description of the test-bed implementation, demonstration of the feasibility and usefulness and AR-specific technical challenges are included in this paper.}
}
@article{COUPE2010483,
title = {Robust Rician noise estimation for MR images},
journal = {Medical Image Analysis},
volume = {14},
number = {4},
pages = {483-493},
year = {2010},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2010.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841510000241},
author = {Pierrick Coupé and José V. Manjón and Elias Gedamu and Douglas Arnold and Montserrat Robles and D. Louis Collins},
keywords = {Rician noise, MR imaging, Wavelet, Noise estimation, MRI},
abstract = {In this paper, a new object-based method to estimate noise in magnitude MR images is proposed. The main advantage of this object-based method is its robustness to background artefacts such as ghosting. The proposed method is based on the adaptation of the Median Absolute Deviation (MAD) estimator in the wavelet domain for Rician noise. The MAD is a robust and efficient estimator initially proposed to estimate Gaussian noise. In this work, the adaptation of MAD operator for Rician noise is performed by using only the wavelet coefficients corresponding to the object and by correcting the estimation with an iterative scheme based on the SNR of the image. During the evaluation, a comparison of the proposed method with several state-of-the-art methods is performed. A quantitative validation on synthetic phantom with and without artefacts is presented. A new validation framework is proposed to perform quantitative validation on real data. The impact of the accuracy of noise estimation on the performance of a denoising filter is also studied. The results obtained on synthetic images show the accuracy and the robustness of the proposed method. Within the validation on real data, the proposed method obtained very competitive results compared to the methods under study.}
}
@article{LEEBMANN2003400,
title = {A stochastic analysis of the calibration problem for Augmented Reality systems with see-through head-mounted displays},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {57},
number = {5},
pages = {400-408},
year = {2003},
note = {Challenges in Geospatial Analysis and Visualization},
issn = {0924-2716},
doi = {https://doi.org/10.1016/S0924-2716(02)00168-5},
url = {https://www.sciencedirect.com/science/article/pii/S0924271602001685},
author = {Johannes Leebmann},
keywords = {augmented reality, see-through head-mounted displays, stochastic analysis},
abstract = {This paper presents a closed stochastic solution for the calibration of see-through head-mounted displays (STHMD) for Augmented Reality. An Augmented Reality system (ARS) is based on several components that are affected by stochastic and random errors. One important component is the tracking system. The flock of birds (FOB) tracking system was tested for consistency in position and orientation outputs by establishing constraints that the system was required to meet. The tests for position and orientation were separated to derive uncorrelated quality measures. The tests are self-controlling and do not require any other measuring device. In addition, the image coordinate accuracy also had to be determined to complete the stochastic description of the calibration problem. Based on this stochastic model, different mathematical models were tested to determine whether or not they fit the stochastic model. An overview of different calibration approaches for optical see-through displays is given and a quantitative comparison of the different models is made based on the derived accuracy information.}
}
@article{LAMATA2007273,
title = {SINERGIA laparoscopic virtual reality simulator: Didactic design and technical development},
journal = {Computer Methods and Programs in Biomedicine},
volume = {85},
number = {3},
pages = {273-283},
year = {2007},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2006.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260706002975},
author = {Pablo Lamata and Enrique J. Gómez and Francisco M. Sánchez-Margallo and Óscar López and Carlos Monserrat and Verónica García and Carlos Alberola and Miguel Ángel {Rodríguez Florido} and Juan Ruiz and Jesús Usón},
keywords = {Laparoscopy, Surgical training, Virtual reality, Simulation design, Biomechanical model, Collision detection and handling},
abstract = {VR laparoscopic simulators have demonstrated its validity in recent studies, and research should be directed towards a high training effectiveness and efficacy. In this direction, an insight into simulators’ didactic design and technical development is provided, by describing the methodology followed in the building of the SINERGIA simulator. It departs from a clear analysis of training needs driven by a surgical training curriculum. Existing solutions and validation studies are an important reference for the definition of specifications, which are described with a suitable use of simulation technologies. Five new didactic exercises are proposed to train some of the basic laparoscopic skills. Simulator construction has required existing algorithms and the development of a particle-based biomechanical model, called PARSYS, and a collision handling solution based in a multi-point strategy. The resulting VR laparoscopic simulator includes new exercises and enhanced simulation technologies, and is finding a very good acceptance among surgeons.}
}
@article{FANTINI2021101897,
title = {Automatic MR image quality evaluation using a Deep CNN: A reference-free method to rate motion artifacts in neuroimaging},
journal = {Computerized Medical Imaging and Graphics},
volume = {90},
pages = {101897},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101897},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121000458},
author = {Irene Fantini and Clarissa Yasuda and Mariana Bento and Leticia Rittner and Fernando Cendes and Roberto Lotufo},
keywords = {Medical imaging, Magnetic resonance imaging, Motion artifacts, Deep learning, Convolutional neural network, Transfer learning},
abstract = {Motion artifacts on magnetic resonance (MR) images degrade image quality and thus negatively affect clinical and research scanning. Considering the difficulty in preventing patient motion during MR examinations, the identification of motion artifact has attracted significant attention from researchers. We propose an automatic method for the evaluation of motion corrupted images using a deep convolutional neural network (CNN). Deep CNNs has been used widely in image classification tasks. While such methods require a significant amount of annotated training data, a scarce resource in medical imaging, the transfer learning and fine-tuning approaches allow us to use a smaller amount of data. Here we selected four renowned architectures, initially trained on Imagenet contest dataset, to fine-tune. The models were fine-tuned using patches from an annotated dataset composed of 68 T1-weighted volumetric acquisitions from healthy volunteers. For training and validation 48 images were used, while the remaining 20 images were used for testing. Each architecture was fine-tuned for each MR axis, detecting the motion artifact per patches from the three orthogonal MR acquisition axes. The overall average accuracy for the twelve models (three axes for each of four architecture) was 86.3%. As our goal was to detect fine-grained corruption in the image, we performed an extensive search on lower layers from each of the four architectures, since they filter small regions in the original input. Experiments showed that architectures with fewer layers than the original ones reported the better results for image patches with an overall average accuracy of 90.4%. The accuracies per architecture were similar so we decided to explore all four architectures performing a result consensus. Also, to determine the probability of motion artifacts presence on the whole acquisition a combination of the three axes were performed. The final architecture consists of an artificial neural network (ANN) classifier combining all models from the four shallower architectures, which overall acquisition-based accuracy was 100.0%. The proposed method generalization was tested using three different MR data: (1) MR image acquired in epilepsy patients (93 acquisitions); (2) MR image presenting susceptibility artifact (22 acquisitions); and (3) MR image acquired from different scanner vendor (20 acquisitions). The achieved acquisition-based accuracy on generalization tests (1) 90.3%, (2) 63.6%, and (3) 75.0%) suggests that domain adaptation is necessary. Our proposed method can be rapidly applied to large amounts of image data, providing a motion probability p∈[0,1] per acquisition. This method output can be used as a scale to identify the motion corrupted images from the dataset, thus minimizing the time spent on visual quality control.}
}
@article{ZHOU2018239,
title = {Promoting Knowledge Construction: A Model for Using Virtual Reality Interaction to Enhance Learning},
journal = {Procedia Computer Science},
volume = {130},
pages = {239-246},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918303867},
author = {Yun Zhou and Shangpeng Ji and Tao Xu and Zi Wang},
keywords = {Virtual Reality, Knowledge Construction, Evaluation, Learning Experience, Learning Model},
abstract = {VR technologies, offering powerful immersion and rich interaction, have gained great interest from researchers and practitioners in the field of education. However, current learning theories and models either mainly take into account the technology perspectives, or focus more on the pedagogy. In this paper, we propose a learning model benefiting from both the Human-Computer Interaction aspects and pedagogical aspects. This model takes full account of the impact of different factors including pedagogical contexts, VR roles and scenarios, and output specifications, which would be combined to inform the design and realize VR education applications. Based on this model, we design and implement an educational application of computer assembly under virtual reality using HTC Vive, which is a headset providing immersion experience. To analyze users’ learning behaviors and evaluate their performance and experience, we conduct an evaluation with 32 college students as participants. We design a questionnaire including usability tests and emotion state measures. Results showed that our proposed learning model gave a good guidance for informing the design and use of VR-supported learning application. The use of the natural interaction not only makes the learning interesting and fosters the engagement, but also improves the construction of knowledge in practices.}
}
@article{BASORI201993,
title = {iMars: Intelligent Municipality Augmented Reality Service for Efficient Information Dissemination based on Deep Learning algorithm in Smart City of Jeddah},
journal = {Procedia Computer Science},
volume = {163},
pages = {93-108},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321301},
author = {Ahmad Hoirul Basori and Ahmad Luqman bin {Abdul Hamid} and Andi Besse {Firdausiah Mansur} and Norazah Yusof},
keywords = {Smart City, Deep Learning, Augmented Reality, Municipality Service},
abstract = {A smart city is one of a huge transformation in these millennia, where it combines latest and advanced technology to overcome the daily basis problem of humans. Artificial intelligence with augmented reality has been highlighted as the potential solution along with IOT and another system to support the smart city system. The smarter Jeddah is part of Saudi Vision 2030 where Saudi government tends aims to transform this second biggest city in the kingdom to be a smart city with a ton of applications that connected with IOT and they can monitor people inside the city in real time. The proposed system focused on providing the resident of the Jeddah city with smart apps that can help them on conducting their daily basis activities (tourism) or even in an emergency situation. The geo-location data and other information will be used as a feeder for deep learning to make the system learn and getting smarter to disseminate information to the user based on their needs, location, emergency factors, etc. Resident can access the municipality public facilities such as Jeddah waterfront location and see what the event happens inside. They also can find the location of other attractive location such as Al-Fakieh aquarium and observe the content of aquarium facilities with virtual reality technology. In term of emergency cases: accident, car stopping and another incident; the proposed apps will be able to inform the police patrol nearby or nearest workshop for tyre or car workshop. This app is capable for wide range utilization to support program of the Ministry of Municipal and Rural Affairs to apprehend the smart city integrated application. The simulation test has confidently proved that deep learning algorithm overcome other classifier Naïve Bayes and Random forest with 67.5% recognition rate. The future work can be integrated with IOT sensor based system to enhance the municipality service in smart city.}
}
@article{MEZA20151,
title = {Measuring the potential of augmented reality in civil engineering},
journal = {Advances in Engineering Software},
volume = {90},
pages = {1-10},
year = {2015},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2015.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997815000915},
author = {Sebastjan Meža and Žiga Turk and Matevž Dolenc},
keywords = {Augmented reality, Mobile computing, Computer integrated design, Computer integrated engineering, Civil engineering, Project documentation, Building information modelling, BIM},
abstract = {Recently building information models have substantially improved the explicit semantic content of design information. Information models are used to integrate the initial phases of project development. On the construction site, however, the designs are still mostly represented as line-based paper drawings or projections on portable displays. A generic technology that can integrate information and situate it in time, place and context is augmented reality. The specific research issues addressed are (1) does augmented reality have a potential use in civil engineering, (2) how big – in comparison to other technologies - is this potential and (3) what are the main barriers to its adoption. The generic research issue was to develop a methodology for evaluation of potentials of technology. A prototype was built. It was tested on a real construction site to evaluate the potential of its use using the action-research method. A set of structured interviews with potential users was then conducted to compare the prototype to conventional presentation methods. Using this methodology it has been found out that augmented reality is expected to be as big a step as the transition from 2D line drawings to photorealistic 3D projections. The main barrier to the adoption is immature core virtual reality technology, conservative nature of construction businesses and size of building information models.}
}
@article{GUIMARAES20151373,
title = {Graphical High Level Analysis of Communication in Distributed Virtual Reality Applications},
journal = {Procedia Computer Science},
volume = {51},
pages = {1373-1382},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011515},
author = {Marcelo de Paiva Guimarães and Bruno Barberi Gnecco and Diego Roberto Colombo Dias and Jośe Remo Ferreira Brega and Luis Carlos Trevelin},
keywords = {Analysis, distributed computing, Virtual Reality, message passing, bug, defects},
abstract = {Analysing distributed virtual reality applications communicating through message-passing is challenging. Their development is complex, and knowing if something is wrong depends on the states of each process, defects (bugs) cause software crashes, hangs, and generation of incorrect results. To address this daunting problem we specify functional behavior models (for example, using synchronization barriers and shared variables) for these applications that ensures correctness. We also developed the GTracer tool, which compares the functional behavior models developed with the messages transmitted among processes. GTracer checks for violations of these models automatically and displays the message traffic graphically. It is a tool made for libGlass, a message library for distributed computing. We have been able to find several non-trivial defects during the tests of this tool.}
}
@article{TALO2019176,
title = {Application of deep transfer learning for automated brain abnormality classification using MR images},
journal = {Cognitive Systems Research},
volume = {54},
pages = {176-188},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718310933},
author = {Muhammed Talo and Ulas Baran Baloglu and Özal Yıldırım and U {Rajendra Acharya}},
keywords = {MRI classification, Abnormal brain images, Deep transfer learning, CNN},
abstract = {Magnetic resonance imaging (MRI) is the most common imaging technique used to detect abnormal brain tumors. Traditionally, MRI images are analyzed manually by radiologists to detect the abnormal conditions in the brain. Manual interpretation of huge volume of images is time consuming and difficult. Hence, computer-based detection helps in accurate and fast diagnosis. In this study, we proposed an approach that uses deep transfer learning to automatically classify normal and abnormal brain MR images. Convolutional neural network (CNN) based ResNet34 model is used as a deep learning model. We have used current deep learning techniques such as data augmentation, optimal learning rate finder and fine-tuning to train the model. The proposed model achieved 5-fold classification accuracy of 100% on 613 MR images. Our developed system is ready to test on huge database and can assist the radiologists in their daily screening of MR images.}
}
@article{SARKAR2016527,
title = {Rough Possibilistic Type-2 Fuzzy C-Means clustering for MR brain image segmentation},
journal = {Applied Soft Computing},
volume = {46},
pages = {527-536},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S156849461630028X},
author = {Jnanendra Prasad Sarkar and Indrajit Saha and Ujjwal Maulik},
keywords = {Fuzzy clustering, Rough set, Possibilistic, Type-2 fuzzy set, Statistical test},
abstract = {Pixel clustering in spectral domain is an important approach for the soft-tissue categorization of magnetic resonance (MR) brain images. In this regard, clustering algorithms based on type-1 fuzzy set theory are suitable for the overlapping partitions while the rough set based clustering algorithms deal with uncertainty and vagueness. However, additional degree of fuzziness makes the clustering more challenging for various subtle uncertainties and noisy data in the overlapping areas. Hence, this fact motivates us to propose a hybrid technique, called Rough Possibilistic Type-2 Fuzzy C-Means clustering with the integration of Random Forest. In the proposed method, possibilistic approach handles the noisy data better, whereas the other various uncertainties and inherent vagueness are taken care by type-2 fuzzy set and rough set theories. After clustering, it produces rough and crisp points. Thereafter, such crisp points are used to train the Random Forest classifier in order to classify the rough points for yielding better clustering solution. The performance of the proposed method has been demonstrated in comparison with several other recently proposed methods for MR brain image segmentation. Finally, superiority of the results produced by the proposed hybrid method has also been validated through statistical significance test.}
}
@article{ROLDAN2019305,
title = {A training system for Industry 4.0 operators in complex assemblies based on virtual reality and process mining},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {59},
pages = {305-316},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518303685},
author = {Juan Jesús Roldán and Elena Crespo and Andrés Martín-Barrio and Elena Peña-Tapia and Antonio Barrientos},
keywords = {Industry 4.0, Training system, Assemblies, Virtual reality, Process mining},
abstract = {Industry 4.0 aims at integrating machines and operators through network connections and information management. It proposes the use of a set of technologies in industry, such as data analysis, Internet of Things, cloud computing, cooperative robots, and immersive technologies. This paper presents a training system for industrial operators in assembly tasks, which takes advantage of tools such as virtual reality and process mining. First, expert workers use an immersive interface to perform assemblies according to their experience. Then, process mining algorithms are applied to obtain assembly models from event logs. Finally, trainee workers use an improved immersive interface with hints to learn the assemblies that the expert workers introduced in the system. A toy example has been developed with building blocks and tests have been performed with a set of volunteers. The results show that the proposed training system, based on process mining and virtual reality, is competitive against conventional alternatives. Furthermore, user evaluations are better in terms of mental demand, perception, learning, results, and performance.}
}
@article{JIMENOMORENILLA20131371,
title = {Augmented and Virtual Reality techniques for footwear},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1371-1382},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S016636151300122X},
author = {Antonio Jimeno-Morenilla and Jose Luis Sánchez-Romero and Faustino Salas-Pérez},
keywords = {Augmented reality in footwear sector, Stereoscopic vision for design and display of footwear, 3D gloves for footwear design},
abstract = {The use of 3D imaging techniques has been early adopted in the footwear industry. In particular, 3D imaging could be used to aid commerce and improve the quality and sales of shoes. Footwear customization is an added value aimed not only to improve product quality, but also consumer comfort. Moreover, customisation implies a new business model that avoids the competition of mass production coming from new manufacturers settled mainly in Asian countries. However, footwear customisation implies a significant effort at different levels. In manufacturing, rapid and virtual prototyping is required; indeed the prototype is intended to become the final product. The whole design procedure must be validated using exclusively virtual techniques to ensure the feasibility of this process, since physical prototypes should be avoided. With regard to commerce, it would be desirable for the consumer to choose any model of shoes from a large 3D database and be able to try them on looking at a magic mirror. This would probably reduce costs and increase sales, since shops would not require storing every shoe model and the process of trying several models on would be easier and faster for the consumer. In this paper, new advances in 3D techniques coming from experience in cinema, TV and games are successfully applied to footwear. Firstly, the characteristics of a high-quality stereoscopic vision system for footwear are presented. Secondly, a system for the interaction with virtual footwear models based on 3D gloves is detailed. Finally, an augmented reality system (magic mirror) is presented, which is implemented with low-cost computational elements that allow a hypothetical customer to check in real time the goodness of a given virtual footwear model from an aesthetical point of view.}
}
@article{LEE2016488,
title = {Augmented reality technology combined with three-dimensional holography to train the mental rotation ability of older adults},
journal = {Computers in Human Behavior},
volume = {65},
pages = {488-500},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216306410},
author = {I-Jui Lee and Chien-Hsu Chen and Kuo-Ping Chang},
keywords = {Mental rotation, Augmented reality, 3D holography, Older adults},
abstract = {A decline in the cognitive ability of mental rotation causes a poor sense of spatial direction and environmental cognitive capacity. Currently, training tasks for the elderly thus affected are still presented in 2D form. However, clinical research indicates that this strategy generates a cognitive load that reduces the interest of the trainees and diminishes the effects of training. In contrast, augmented reality (AR) is a rising solution that effectively reduces cognitive load, improves the sense of spatial direction of the elderly, and helps increase interest in training. We recruited 28 elderly (age ≥ 65 years) for this study. Fourteen were randomly assigned to an active Intervention group and were given AR-based 3D hologram (AR-3DH) mental rotation training, and 14 were assigned to a group that used the traditional 2D model. Both groups took ABA-designed pre-and post-tests that required inferring the rotating shapes' states from standard mental rotation tasks. Change scores for the two groups were compared using error rates and reaction times as covariates. After six-week of training, the mental rotation ability of the Intervention group improved through the use of AR-3DH training system during the intervention phase. The practical and developmental implications of the findings are discussed.}
}
@article{JUAN2010756,
title = {Using augmented and virtual reality for the development of acrophobic scenarios. Comparison of the levels of presence and anxiety},
journal = {Computers & Graphics},
volume = {34},
number = {6},
pages = {756-766},
year = {2010},
note = {Graphics for Serious Games Computer Graphics in Spain: a Selection of Papers from CEIG 2009 Selected Papers from the SIGGRAPH Asia Education Program},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2010.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849310001263},
author = {M. Carmen Juan and David Pérez},
keywords = {Augmented reality, Acrophobia},
abstract = {Acrophobia has been treated using exposure in imagination, exposure “in vivo” and Virtual Reality (VR). This paper presents the development of an Augmented Reality (AR) system and a VR system that includes acrophobic scenarios. A study involving both these systems and non-phobic users has been carried out in order to compare the levels of presence and anxiety. In the acrophobic scenario, the floor fell away and the walls rose up. 20 participants took part in this study. After using each system (AR or VR), the participants were asked to fill out an adapted SUS questionnaire [26], and paired t-tests and ANOVA analyses were applied to the data obtained. For the sense of presence and anxiety levels, we did not find differences between the systems using an experiment with enough sensitivity to detect differences as large as d=0.66. For the anxiety level, the results show that there is a significant difference between the level of anxiety felt at the moment before starting the experiment and the level felt during the different stages of the experiment. For the correlation between anxiety and presence, the results show very low correlation between anxiety and presence. These results suggest that AR is likely to be as effective as VR in treating acrophobia.}
}
@article{MICHEL201896,
title = {Attitude estimation for indoor navigation and augmented reality with smartphones},
journal = {Pervasive and Mobile Computing},
volume = {46},
pages = {96-121},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217303371},
author = {Thibaud Michel and Pierre Genevès and Hassen Fourati and Nabil Layaïda},
keywords = {Attitude estimation, Smartphone, Inertial sensors, Augmented reality motions, Magnetic field, Perturbations, Benchmark},
abstract = {We investigate the precision of attitude estimation algorithms in the particular context of pedestrian navigation with commodity smartphones and their inertial/magnetic sensors. We report on an extensive comparison and experimental analysis of existing algorithms. We focus on typical motions of smartphones when carried by pedestrians. We use a precise ground truth obtained from a motion capture system. We test state-of-the-art and built-in attitude estimation techniques with several smartphones, in the presence of magnetic perturbations typically found in buildings. We discuss the obtained results, analyze advantages and limits of current technologies for attitude estimation in this context. Furthermore, we propose a new technique for limiting the impact of magnetic perturbations with any attitude estimation algorithm used in this context. We show how our technique compares and improves over previous works. A particular attention was paid to the study of attitude estimation in the context of augmented reality motions when using smartphones.}
}
@article{WEI2024107983,
title = {CT synthesis from MR images using frequency attention conditional generative adversarial network},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107983},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107983},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000672},
author = {Kexin Wei and Weipeng Kong and Liheng Liu and Jian Wang and Baosheng Li and Bo Zhao and Zhenjiang Li and Jian Zhu and Gang Yu},
keywords = {MR, Synthetic CT, Deep learning, Generative adversarial networks, Attention mechanism},
abstract = {Magnetic resonance (MR) image-guided radiotherapy is widely used in the treatment planning of malignant tumors, and MR-only radiotherapy, a representative of this technique, requires synthetic computed tomography (sCT) images for effective radiotherapy planning. Convolutional neural networks (CNN) have shown remarkable performance in generating sCT images. However, CNN-based models tend to synthesize more low-frequency components and the pixel-wise loss function usually used to optimize the model can result in blurred images. To address these problems, a frequency attention conditional generative adversarial network (FACGAN) is proposed in this paper. Specifically, a frequency cycle generative model (FCGM) is designed to enhance the inter-mapping between MR and CT and extract more rich tissue structure information. Additionally, a residual frequency channel attention (RFCA) module is proposed and incorporated into the generator to enhance its ability in perceiving the high-frequency image features. Finally, high-frequency loss (HFL) and cycle consistency high-frequency loss (CHFL) are added to the objective function to optimize the model training. The effectiveness of the proposed model is validated on pelvic and brain datasets and compared with state-of-the-art deep learning models. The results show that FACGAN produces higher-quality sCT images while retaining clearer and richer high-frequency texture information.}
}
@article{SHEIKHBAHAEI2021102250,
title = {A case study for risk assessment in AR-equipped socio-technical systems},
journal = {Journal of Systems Architecture},
volume = {119},
pages = {102250},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102250},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001715},
author = {Soheila {Sheikh Bahaei} and Barbara Gallina and Marko Vidović},
keywords = {Socio-technical systems, Augmented reality, Risk assessment, ISO 26262, ISO/PAS 21448-SOTIF},
abstract = {Augmented Reality (AR) technologies are used as human–machine interface within various types of safety-critical systems. Several studies have shown that AR improves human performance. However, the introduction of AR might introduce risks due to new types of dependability threats. In order to avoid unreasonable risk, it is required to detect new types of dependability threats (faults, errors, failures). In our previous work, we have designed extensions for the SafeConcert metamodel (a metamodel for modeling socio-technical systems) to capture AR-related dependability threats (focusing on faults and failures). Despite the availability of various modeling techniques, there has been no detailed investigation of providing an integrated framework for risk assessment in AR-equipped socio-technical systems. Hence, in this paper, we provide an integrated framework based on our previously proposed extensions. In addition, in cooperation with our industrial partners, active in the automotive domain, we design and execute a case study. We aim at verifying the modeling and analysis capabilities of our framework and finding out if the proposed extensions are helpful in capturing system risks caused by new AR-related dependability threats. Our conducted qualitative analysis is based on the Concerto-FLA analysis technique, which is included in the CHESS toolset and targets socio-technical systems.}
}
@article{KURNIAWAN201880,
title = {Human Anatomy Learning Systems Using Augmented Reality on Mobile Application},
journal = {Procedia Computer Science},
volume = {135},
pages = {80-88},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314388},
author = {Michael H Kurniawan and  Suharjito and  Diana and Gunawan Witjaksono},
keywords = {Augmented Reality, Human Anatomy Learning, Android Application},
abstract = {Students generally experience difficulties in learning human body anatomy due to constraints to visualize the body anatomy from 2D into 3D image. This research aims to develop a human anatomy learning system using augmented reality technology. By using this system, it is expected that students can easily understand the anatomy of the human body using a 3D image visualization. The method used in this system is augmented reality marker on mobile computing platform. The marker is captured by taking a picture. Then, the captured image is divided into pieces and the pattern is matched with images stored in the database. In this research, we use Floating Euphoria Framework and combine it with the SQLite database. Augmented reality anatomy system of the human body has features that can interactively display the whole body or parts of the human organs. To evaluate the usefulness of the application, we tested the augmented reality anatomy system with high school students and medical students for learning the anatomy of the human body. The results show that the human anatomy learning system with interactive augmented reality visualization helps students learn human anatomy more easily.}
}
@article{TROMBETTA201715,
title = {Motion Rehab AVE 3D: A VR-based exergame for post-stroke rehabilitation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {151},
pages = {15-20},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S016926071730113X},
author = {Mateus Trombetta and Patrícia Paula {Bazzanello Henrique} and Manoela Rogofski Brum and Eliane Lucia Colussi and Ana Carolina Bertoletti {De Marchi} and Rafael Rieder},
keywords = {Stroke, Serious game, Rehabilitation, Unity},
abstract = {Background and objective
Recent researches about games for post-stroke rehabilitation have been increasing, focusing in upper limb, lower limb and balance situations, and showing good experiences and results. With this in mind, this paper presents Motion Rehab AVE 3D, a serious game for post-stroke rehabilitation of patients with mild stroke. The aim is offer a new technology in order to assist the traditional therapy and motivate the patient to execute his/her rehabilitation program, under health professional supervision.
Methods
The game was developed with Unity game engine, supporting Kinect motion sensing input device and display devices like Smart TV 3D and Oculus Rift. It contemplates six activities considering exercises in a tridimensional space: flexion, abduction, shoulder adduction, horizontal shoulder adduction and abduction, elbow extension, wrist extension, knee flexion, and hip flexion and abduction. Motion Rehab AVE 3D also report about hits and errors to the physiotherapist evaluate the patient's progress.
Results
A pilot study with 10 healthy participants (61–75 years old) tested one of the game levels. They experienced the 3D user interface in third-person. Our initial goal was to map a basic and comfortable setup of equipment in order to adopt later. All the participants (100%) classified the interaction process as interesting and amazing for the age, presenting a good acceptance.
Conclusions
Our evaluation showed that the game could be used as a useful tool to motivate the patients during rehabilitation sessions. Next step is to evaluate its effectiveness for stroke patients, in order to verify if the interface and game exercises contribute into the motor rehabilitation treatment progress.}
}
@article{YUAN2005980,
title = {A generalized registration method for augmented reality systems},
journal = {Computers & Graphics},
volume = {29},
number = {6},
pages = {980-997},
year = {2005},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2005.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0097849305001688},
author = {M.L. Yuan and S.K. Ong and A.Y.C. Nee},
keywords = {Augmented reality, Registration, Projective reconstruction, Tracking},
abstract = {In augmented reality systems, registration is one of the most difficult problems currently limiting their applications. In this paper, we propose a generalized registration method using projective reconstruction technique in computer vision. This registration method is composed of embedding and tracking. Embedding involves specifying four points to build the world coordinate system on which a virtual object will be superimposed. In this stage, any arbitrary two unrelated images or any 3×4 projective matrices with rank 3 can be used to calculate the 3D pseudo-projective coordinates of the four specified points. In the tracking process, these 3D pseudo-projective coordinates are used to track the four specified points to compute the registration matrix for augmentation. The proposed method is simple, as only four points need to be specified at the embedding stage, and the virtual object can then be easily augmented onto a real scene from a video sequence. One advantage is that the virtual objects can still be superimposed on the specified regions even when the regions are occluded in the video sequence. Another advantage of the proposed method is that the registration errors can be adjusted in real-time to ensure that they are less than certain thresholds that have been specified at the initial embedding stage. Several experiments have been conducted to validate the performance of the proposed generalized method.}
}
@article{KULIGA2015363,
title = {Virtual reality as an empirical research tool — Exploring user experience in a real building and a corresponding virtual model},
journal = {Computers, Environment and Urban Systems},
volume = {54},
pages = {363-375},
year = {2015},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2015.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S019897151530017X},
author = {S.F. Kuliga and T. Thrash and R.C. Dalton and C. Hölscher},
keywords = {Virtual reality (VR), User experience, Pre-occupancy evaluation, Real building, Research tool, Building usability},
abstract = {Virtual reality (VR) allows for highly-detailed observations, accurate behavior measurements, and systematic environmental manipulations under controlled laboratory circumstances. It therefore has the potential to be a valuable research tool for studies in human–environment interaction, such as building usability studies and post- as well as pre-occupancy building evaluation in architectural research and practice. In order to fully understand VR as a valid environmental representation, it is essential to examine to what extent not only user cognition and behavior, but also users' experiences are analogous in real and virtual environments. This work presents a multi-method approach with two studies that investigated the correspondence of building users' experience in a real conference center and a highly-detailed virtual model of the same building as well as a third study that virtually implemented systematic redesigns to the existing building layout. In the context of reporting users' experiential building evaluations, this article discusses the potential, prerequisites and opportunities for the implementation of virtual environments as an empirical research tool in the field of human–environment interaction. Based on quantitative data, few statistically significant differences between ratings of the real and the virtual building were found; however analyses based on qualitative data revealed differences relating to atmospherics. The main conclusion of this article is that VR has a strong potential to be used as an empirical research tool in psychological and architectural research and that future studies could supplement behavioral validation.}
}
@article{WRZESIEN2015343,
title = {Treating small animal phobias using a projective-augmented reality system: A single-case study},
journal = {Computers in Human Behavior},
volume = {49},
pages = {343-353},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.01.065},
url = {https://www.sciencedirect.com/science/article/pii/S074756321500093X},
author = {Maja Wrzesien and Cristina Botella and Juana Bretón-López and Eva {del Río González} and Jean-Marie Burkhardt and Mariano Alcañiz and María Ángeles Pérez-Ara},
keywords = {Small animal phobias, Augmented reality, In vivo exposure, Clinical efficacy, Projection systems, Visual displays},
abstract = {In vivo exposure is the evidence based treatment for small animal phobias. However, this type of treatment still present a low treatment seek rate and a high drop-out rate, due to its aversive character for the patients. New technologies such as Virtual Reality (VR) and augmented reality (AR) have started to show their potential in anxiety disorders, including small animal phobia treatment and have demonstrated their efficacy. However, these systems still present limitations regarding the possibility to offer an optimal therapy to the phobia sufferers. This study evaluates the clinical efficacy of new AR exposure therapy – a projection-based system (P-ARET) for small animal phobias in the short (post-treatment), and long term (3- and 12-month follow-up). Four patients diagnosed with cockroach phobia participated in this pilot treatment study. The results show that all patients improved significantly in main outcome measures after the treatment. The study also follows a strategy of benchmarking, in which the results obtained from the evaluation of the P-ARET system in clinical setting are compared with two other clinically validated phobia therapies (traditional, in vivo exposure therapy (IVET); virtual reality exposure therapy (VRET); and AR exposure therapy with the use of a head-mounted display (HMD-ARET). The results indicate that the clinical effectiveness of new projection-based AR system for small animal phobia treatment was comparable to those achieved by the therapeutic conditions in other studies. However, the P-ARET system brings some advantages in terms of patient-therapist communication and more natural interaction with the system.}
}
@article{ROUPE201442,
title = {Interactive navigation interface for Virtual Reality using the human body},
journal = {Computers, Environment and Urban Systems},
volume = {43},
pages = {42-50},
year = {2014},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971513000884},
author = {Mattias Roupé and Petra Bosch-Sijtsema and Mikael Johansson},
keywords = {Virtual Reality, Natural User Interface, Navigation, Interaction, Urban planning},
abstract = {The use of Virtual Reality (VR) and interactive real-time rendering in urban planning and building design is becoming more and more common. However, the integration of desktop-VR in the urban planning process suffers from complicated navigation interfaces. In particular, people unfamiliar to gaming environments and computers are less prone to interact with a VR visualization using keyboard and mouse as controlling devices. This paper addresses this issue by presenting an implementation of the XBOX 360 Kinect sensor system, which uses the human body to interact with the virtual environment. This type of interaction interface enables a more natural and user-friendly way of interacting with the virtual environment. The validation of the system was conducted with 60 participants using quantitative and qualitative methods. The result showed that participants perceived the interface as non-demanding and easy to use and the interface was perceived better in relation to mouse/keyboard interaction. The implemented interface supported users to switch between different architecture proposals of an urban plan and the switching positively affected learning, understanding and spatial reasoning of the participants. The study also shows that females perceived the system as less demanding than males. Furthermore, the users associated and related their body (human interaction interface) to VR, which could indicate that they used their body during spatial reasoning. This type of spatial reasoning has been argued to enhance the spatial-perception.}
}
@article{SHIN201864,
title = {Empathy and embodied experience in virtual environment: To what extent can virtual reality stimulate empathy and embodied experience?},
journal = {Computers in Human Behavior},
volume = {78},
pages = {64-73},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217305381},
author = {Donghee Shin},
keywords = {Virtual environment, Immersive storytelling, Virtual reality story, Immersion, Virtual storytelling, Empathy, Embodiment, Presence, Flow},
abstract = {This study investigates the user experience to clarify what it is like to experience stories in VR (virtual reality) and how immersion influences story experiences in immersive storytelling. This study explores the immersive storytelling context, developing and testing a VR experience model that integrates presence, flow, empathy, and embodiment. The results imply that users’ personal traits correlates immersion in VR: user experience in VR depend on individual traits, which in turns influence how strongly users immerse in a VR. The way users view and accept VR stories derives from the way they envisage and intend to experience them. Rather than simply being influenced by technological features, users have intentional and purposeful control over VR stories. The findings of this study suggest that the cognitive processes by which users experience quality, presence, and flow determine how they will empathize with and embody VR stories.}
}
@article{INATOME20182050,
title = {Development of an AR Drawing System with Point Cloud Data suitable for Real-time Gripping Movement by using Kinect},
journal = {Procedia Computer Science},
volume = {126},
pages = {2050-2057},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.247},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312237},
author = {Hiroki Inatome and Masato Soga},
keywords = {Augmented Reality, Point Cloud Data, Sketch Learning, Human Body Figure},
abstract = {Virtual human figures have been used in the conventional human figure sketch learning support system. A learner selects one of the virtual figures for a motif. However, the learner could not intuitively manipulate the virtual human figure in real time to change the orientation of the figure for learner’s favorite viewpoint. Therefore, in this research, we developed a system to change the orientation of the virtual model of human figure intuitively by using the drawing doll as a tangible interface. Specifically, it uses PCL which can acquire point cloud data by RGB-D camera of KINECT, then it acquires and tracks the three-dimensional coordinates of the real object and superimposes and displays the virtual human figure on the real drawing doll. In the verification experiment tracking accuracy was verified and the improvement of the system was discussed.}
}
@article{THEES2020106316,
title = {Effects of augmented reality on learning and cognitive load in university physics laboratory courses},
journal = {Computers in Human Behavior},
volume = {108},
pages = {106316},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106316},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300704},
author = {Michael Thees and Sebastian Kapp and Martin P. Strzys and Fabian Beil and Paul Lukowicz and Jochen Kuhn},
keywords = {Augmented reality, Smartglasses, Cognitive load, Split-attention effect, STEM laboratory courses, Physics laboratory courses},
abstract = {Recent studies emphasize a positive impact of learning with augmented reality (AR) systems in various instructional scenarios. Especially combining real and virtual learning components according to spatial and temporal contiguity principles is claimed to foster learning and to reduce extraneous cognitive processing. We applied these principles to a physics laboratory experiment examining heat conduction where students measure the temperature along heated metal rods via a thermal imaging camera. However, the traditional setup leads to a time delay between measuring and receiving data, and spatially separates relevant visualizations causing resource-consuming search processes. Using see-through smartglasses, traditional displays were transformed into virtual representations which were anchored to corresponding objects of the experimental setup, resulting in an integrated AR view of real-time data. Both traditional and AR-assisted workflows of data collection were investigated in a field study with undergraduate students (N=74) during a graded laboratory course. Performance and cognitive load were assessed as dependent variables. Although the AR condition did not show a learning gain in a conceptual knowledge test, they nonetheless reported a significant lower extraneous cognitive load than the traditional condition. These results contrast with recent findings on AR and integrated formats but reveal a significant impact on cognitive load research.}
}
@article{YAN201911,
title = {A propagation-DNN: Deep combination learning of multi-level features for MR prostate segmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {170},
pages = {11-21},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S016926071831099X},
author = {Ke Yan and Xiuying Wang and Jinman Kim and Mohamed Khadra and Michael Fulham and Dagan Feng},
keywords = {Prostate segmentation, Deep neural network, Multi-level features},
abstract = {Background and objective
Prostate segmentation on Magnetic Resonance (MR) imaging is problematic because disease changes the shape and boundaries of the gland and it can be difficult to separate the prostate from surrounding tissues. We propose an automated model that extracts and combines multi-level features in a deep neural network to segment prostate on MR images.
Methods
Our proposed model, the Propagation Deep Neural Network (P-DNN), incorporates the optimal combination of multi-level feature extraction as a single model. High level features from the convolved data using DNN are extracted for prostate localization and shape recognition, while labeling propagation, by low level cues, is embedded into a deep layer to delineate the prostate boundary.
Results
A well-recognized benchmarking dataset (50 training data and 30 testing data from patients) was used to evaluate the P-DNN. When compared it to existing DNN methods, the P-DNN statistically outperformed the baseline DNN models with an average improvement in the DSC of 3.19%. When compared to the state-of-the-art non-DNN prostate segmentation methods, P-DNN was competitive by achieving 89.9 ± 2.8% DSC and 6.84 ± 2.5 mm HD on training sets and 84.13 ± 5.18% DSC and 9.74 ± 4.21 mm HD on testing sets.
Conclusion
Our results show that P-DNN maximizes multi-level feature extraction for prostate segmentation of MR images.}
}
@article{KIM2019186,
title = {Influences of augmented reality head-worn display type and user interface design on performance and usability in simulated warehouse order picking},
journal = {Applied Ergonomics},
volume = {74},
pages = {186-193},
year = {2019},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2018.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0003687018303387},
author = {Sunwook Kim and Maury A. Nussbaum and Joseph L. Gabbard},
keywords = {Head-worn display, Augmented reality, User interface, Performance},
abstract = {Limited information is available regarding the effective use of workplace head-worn displays (HWD), especially the choices of HWD types and user interface (UI) designs. We explored how different HWD types and UI designs affect perceived workload, usability, visual discomfort, and job performance during a simulated warehouse job involving order picking and part assembly. Sixteen gender-balanced participants completed the simulated job in all combinations of two HWD types (binocular vs. monocular) and four UIs, the latter of which manipulated information mode (text-vs. graphic-based) and information availability (always-on vs. on-demand); a baseline condition was also completed (paper pick list). Job performance, workload, and usability were more affected by UI designs than HWD type. For example, the graphic-based UI reduced job completion time and number of errors by ∼13% and ∼59%, respectively. Participants had no strong preference for either of the HWD types, suggesting that the physical HWD designs tested are suboptimal.}
}
@article{CHEN2024102330,
title = {One model, two brains: Automatic fetal brain extraction from MR images of twins},
journal = {Computerized Medical Imaging and Graphics},
volume = {112},
pages = {102330},
year = {2024},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102330},
url = {https://www.sciencedirect.com/science/article/pii/S0895611124000077},
author = {Jian Chen and Ranlin Lu and Bin Jing and He Zhang and Geng Chen and Dinggang Shen},
keywords = {Twin fetal brains, Brain extraction, MR images, Transfer learning},
abstract = {Fetal brain extraction from magnetic resonance (MR) images is of great importance for both clinical applications and neuroscience studies. However, it is a challenging task, especially when dealing with twins, which are commonly existing in pregnancy. Currently, there is no brain extraction method dedicated to twins, raising significant demand to develop an effective twin fetal brain extraction method. To this end, we propose the first twin fetal brain extraction framework, which possesses three novel features. First, to narrow down the region of interest and preserve structural information between the two brains in twin fetal MR images, we take advantage of an advanced object detector to locate all the brains in twin fetal MR images at once. Second, we propose a Twin Fetal Brain Extraction Network (TFBE-Net) to further suppress insignificant features for segmenting brain regions. Finally, we propose a Two-step Training Strategy (TTS) to learn correlation features of the single fetal brain for further improving the performance of TFBE-Net. We validate the proposed framework on a twin fetal brain dataset. The experiments show that our framework achieves promising performance on both quantitative and qualitative evaluations, and outperforms state-of-the-art methods for fetal brain extraction.}
}
@article{RAI2020100004,
title = {Detection of brain abnormality by a novel Lu-Net deep neural CNN model from MR images},
journal = {Machine Learning with Applications},
volume = {2},
pages = {100004},
year = {2020},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2020.100004},
url = {https://www.sciencedirect.com/science/article/pii/S2666827020300049},
author = {Hari Mohan Rai and Kalyan Chatterjee},
keywords = {Deep Learning, CNN, Artificial Neural Network, Biomedical imaging, Magnetic Resonance Imaging},
abstract = {The identification and classification of tumors in the human mind from MR images at an early stage play a pivotal role in diagnosis such diseases. This work presents the novel Deep Neural network with less number of layers and less complex in designed named U-Net (LU-Net) for the detection of tumors. The work is comprised of classifying the brain MR images into normal and abnormal class from the dataset of 253 images of high pixels. The MR images were 1st resized, cropped, preprocessed, and augmented for the accurate and fast training of deep neural models. The performance of the Lu-Net model is evaluated using five types of statistical assessment metrics Precision, Recall, Specificity, F-score, and Accuracy, and compared with the other two types of model Le-Net and VGG-16. The CNN models were trained and tested on augmented images and validation is performed on 50 untrained data. The overall accuracy of Le-Net, VGG-16 and Proposed model received were 88%, 90%, and 98% respectively.}
}
@article{OLEKSY20173,
title = {Catch them all and increase your place attachment! The role of location-based augmented reality games in changing people - place relations},
journal = {Computers in Human Behavior},
volume = {76},
pages = {3-8},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303771},
author = {Tomasz Oleksy and Anna Wnuk},
keywords = {Place attachment, Gamification, Location-based AR games, Augmented reality, Pokémon Go},
abstract = {We examined how playing a game employing augmented reality (AR) technology increases attachment to the place of playing. Place attachment refers to the relationship between people and places, which has numerous benefits for individual well-being. Popular location-based AR games often include elements that are known to predict place attachment: exploration, social relations or the experience of enjoyment in a place. We argue that positive emotions triggered by playing can influence players’ place attachment via the process of gamification. We tested this hypothesis in a correlational study conducted among Pokémon Go players. Our analyses showed that satisfaction from playing and the social relations made during play positively predict place attachment, but the amount of time spent on playing does not. A series of mediation analyses showed that relations among game satisfaction, social relations, and place attachment were mediated by the appraisal of the place as exciting. This study demonstrated a mechanism of emotional transfer between positive experiences from playing and place attachment, which may prove useful in other domains, such as education, land conservation, or marketing.}
}
@article{LI2021101379,
title = {The impact of CCT on driving safety in the normal and accident situation: A VR-based experimental study},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101379},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101379},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001324},
author = {Xiaojun Li and Jiaxin Ling and Yi Shen and Tong Lu and Shouzhong Feng and Hehua Zhu},
keywords = {CCT, Virtual reality, Reaction time, Driving fatigue, Rear-end accident, Road tunnel},
abstract = {Correlated color temperature (CCT) of the light source inside the road tunnel plays a crucial role in ensuring driving safety, which is demonstrated by previous studies that CCT influences not only visual effects but also non-visual effects. Although conventional laboratory experiments could simulate the CCT environment inside the tunnel to some extent, they fail to restore driving experience, let alone simulate driving behavior in the accident situation. It has largely remained unclear whether and how CCT would influence visual/non-visual performance of the subjects who are performing driving tasks, especially in the accident situation. Motivated by this gap, a virtual-reality-based framework for assessing the influence of CCT on the visual and non/visual performance in normal driving situation and in accident situation was proposed. In this study, tunnel models under seven different CCTs were created and a rear-end accident was designed in the tunnel. By integrating analog driving equipment, all participants were required to perform virtual driving tasks both in the normal situation and in the accident situation. The non-visual performance (driving fatigue) and the visual performance (reaction time) of the participants were collected and analyzed. Results show that the CCT of light source inside the tunnel was significant on the driving fatigue of the driver who was performing driving task, and it also had a significant impact on the visual performance when the driver was faced with a rear-end accident. Detailed experimental methodology, behavioral explanations underlying these findings, validity of results and practical implications are also discussed in the paper.}
}
@article{BAUER2017140,
title = {Anatomical augmented reality with 3D commodity tracking and image-space alignment},
journal = {Computers & Graphics},
volume = {69},
pages = {140-153},
year = {2017},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0097849317301747},
author = {Armelle Bauer and Debanga Raj Neog and Ali-Hamadi Dicko and Dinesh K. Pai and François Faure and Olivier Palombi and Jocelyne Troccaz},
keywords = {User-specific anatomy, Augmented human, Real-time, Motion capture, Augmented reality, Markerless device, Image warping, Handled occlusion and self-occlusion},
abstract = {This paper presents a mirror-like augmented reality (AR) system to display the internal anatomy of the current user. Using a single Microsoft V2.0 Kinect (later on referenced as the Kinect), we animate in real-time a user-specific model of internal anatomy according to the user’s motion and we superimpose it onto the user’s color map. Users can visualize their anatomy moving as if they where looking inside their own bodies in real-time. A new calibration procedure to set up and attach a user-specific anatomy to the Kinect body tracking skeleton is introduced. At calibration time, the bone lengths are estimated using a set of poses. By using Kinect data as input, the practical limitation of skin correspondence in prior work is overcome. The generic 3D anatomical model is attached to the internal anatomy registration skeleton, and warped on the depth image using a novel elastic deformer subject to a closest-point registration force and anatomical constraints. The noise in Kinect outputs precludes direct display of realistic human anatomy. Therefore, to enforce anatomical plausibility, a novel filter to reconstruct plausible motions based on fixed bones lengths as well as realistic angular degrees of freedom (DOFs) and limits are introduced. Anatomical constraints, applied to the Kinect body tracking skeleton joints, are used to maximize the physical plausibility of the anatomy motion while minimizing the distance to the raw data. At run-time, a simulation loop is used to attract the bones toward the raw data. Skinning shaders efficiently drag the resulting anatomy to the user’s tracked motion. Our user-specific internal anatomy model is validated by comparing the skeleton with segmented MRI images. A user study is established to evaluate the believability of the animated anatomy. As an extension of [1], we also propose an image-based algorithm that corrects accumulated inaccuracy of the system steps: motion capture, anatomy transfer, image generation and animation. These inaccuracies show up as occlusion and self-occlusion misalignments of the anatomy regions when superimposed between them and on top of the color map. We also show that the proposed work can efficiently reduce these inaccuracies.}
}
@article{PANG2018130,
title = {Variable universe fuzzy control for vehicle semi-active suspension system with MR damper combining fuzzy neural network and particle swarm optimization},
journal = {Neurocomputing},
volume = {306},
pages = {130-140},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218304958},
author = {Hui Pang and Fan Liu and Zeren Xu},
keywords = {Vehicle semi-active suspension, Variable universe, T–S fuzzy control, FNN, PSO},
abstract = {This study proposes a novel variable universe fuzzy control design for vehicle semi-active suspension system with magnetorheological (MR) damper through the combination of fuzzy neural network (FNN) and particle swarm optimization (PSO). By constructing a quarter-vehicle test rig equipped with MR damper and then collecting the measured data, a non-parametric model of MR damper based on adaptive neuro-fuzzy inference system is first presented. And then a Takagi–Sugeno (T–S) fuzzy controller is designed to achieve the effective control of the input current in MR damper by using the contraction-expansion factors. Furthermore, an appropriate FNN controller is proposed to obtain the contraction-expansion factors, in which particle swarm optimization and back propagation are introduced as the learning and training algorithm for the FNN controller. Lastly, a simulation investigation is provided to validate the proposed control scheme. The results of this study can provide the technical foundation for the development of vehicle semi-active suspension system.}
}
@article{CAI201431,
title = {A case study of Augmented Reality simulation system application in a chemistry course},
journal = {Computers in Human Behavior},
volume = {37},
pages = {31-40},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214002271},
author = {Su Cai and Xu Wang and Feng-Kuang Chiang},
keywords = {Augmented Reality, Chemistry learning, Inquiry-based learning},
abstract = {The comprehension of micro-worlds has always been the focus and the challenge of chemistry learning. Junior high school students’ imaginative abilities are not yet mature. As a result, they are not able to visualize microstructures correctly during the beginning stage of chemistry learning. This study targeted “the composition of substances” segment of junior high school chemistry classes and, furthermore, involved the design and development of a set of inquiry-based Augmented Reality learning tools. Students could control, combine and interact with a 3D model of micro-particles using markers and conduct a series of inquiry-based experiments. The AR tool was tested in practice at a junior high school in Shenzhen, China. Through data analysis and discussion, we conclude that (a) the AR tool has a significant supplemental learning effect as a computer-assisted learning tool; (b) the AR tool is more effective for low-achieving students than high-achieving ones; (c) students generally have positive attitudes toward this software; and (d) students’ learning attitudes are positively correlated with their evaluation of the software.}
}
@article{SZAJNA20203618,
title = {The Production Quality Control Process, Enhanced with Augmented Reality Glasses and the New Generation Computing Support System},
journal = {Procedia Computer Science},
volume = {176},
pages = {3618-3625},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319177},
author = {Andrzej Szajna and Roman Stryjski and Waldemar Woźniak and Norbert Chamier-Gliszczyński and Tomasz Królikowski},
keywords = {Augmented Reality, industry, quality control, AR glasses},
abstract = {The world is accelerating the digitisation of many branches of the economy. In production, it is often called Industry 4.0 - the term invented by Prof. Wahlster in Germany. One branch of ‘Production Digitisation’ is Augmented Reality (AR)- a technology that gives a very convenient and natural human-machine interface. Users are presented with the information requested, right in front of their eyes, and can communicate with the system through simple gestures and speech. The key advantage of AR is having one’s handsfree so that the user can perform his/her regular tasks, unhindered. In the first part of this paper, the concept of using AR glasses, in the quality control process, is presented. The present authors describe a proprietary support system. A specific example is then given in a real production environment on a real production process, comparing the old way with the new AR way of conducting test procedures. An evaluation of its usability is characterised using important factors that can affect the final outcome or features of the product and the manner in which it is applied in real life. In concluding, the authors focus on its advantages, the disadvantages, the technological imperfections, and the limitations and go on to deliberate on the conclusions and further research topics.}
}
@article{OLALDE2013136,
title = {The Importance of Geometry Combined with New Techniques for Augmented Reality},
journal = {Procedia Computer Science},
volume = {25},
pages = {136-143},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012222},
author = {Karle Olalde and Beñat García and Andres Seco},
keywords = {Augmented reality, geometry, geomatics},
abstract = {From the field of the “Geomatics Engineering”[1], better known as “Topographic Engineering” and other fields as “surveyors”, has always driven the knowledge of our environment in order to capture it on a support then allow other users to work, taking decisions on where you’ve never been, and are known early nautical charts, the world map, the cartographic maps of cities and countries, orthophotos and more recently around the digital medium, satellite images, google earth, etc... Augmented reality allows us to make visible developing products that take years but it was very difficult to bring them to the general public (requiring knowledge of cartographic techniques, knowledge of graphic expression, heritage documentation methods, ....) in this article want to show the techniques that allow us to make measurements of complete geometric precision to ensure that the final product is not only attractive but strictly accurate to the real model. We explain how we use robotic total stations with reflectorless measurement, centimeter GPS, 3D scanner, close range photogrammetry[2], all so that our model is strictly accurate. There are models that have intrinsic value in their own actions, no escape ancient Egyptian pyramid is not enough just to model it in an attractive way, but measurements have to be geometrically accurate if you want to test any hypothesis about their mathematical knowledge, astronomical, etc. For all this we believe that both techniques can support each other in achieving the best possible models, augmented reality will showcase showing geometrically exact elements obtained with geomatics techniques that can be displayed to the general public or even to researchers who know they are working real and exact values (an archaeological dig, a pyramid, a sculpture, caves, ...).}
}
@article{DIDONATO201570,
title = {Text legibility for projected Augmented Reality on industrial workbenches},
journal = {Computers in Industry},
volume = {70},
pages = {70-78},
year = {2015},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2015.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166361515000421},
author = {Michele {Di Donato} and Michele Fiorentino and Antonio E. Uva and Michele Gattullo and Giuseppe Monno},
keywords = {Spatial Augmented Reality, Industrial applications, Text legibility, Visualization},
abstract = {Augmented Reality is a promising technology for the product lifecycle development, but it is still not established in industrial facilities. The most relevant issues to be addressed relate to the ergonomics: avoid the discomfort of Head-Worn Displays, allow the operators to have free hands and improve data visualization. In this work we study the possibility to use projection-based Augmented Reality (projected AR), as optimal solution for technical visualization on industrial workbenches. In particular, text legibility in projected AR is difficult to optimize since it is affected by many parameters: environment conditions, text style, material and shape of the target surface. This problem is poorly addressed in literature and in the specific industrial field. We analyze the legibility of a set of colors prescribed by international standards for the industrial environments, on six widely used industrial workbenches surfaces. We compared the performance of 14 subjects using projected AR, with that using a traditional LCD monitor. We collected about 2500 measurements (times and errors) through the use of a test application, followed by qualitative interviews. The results showed that, as regards legibility, projected AR can be used in place of traditional monitors in most of the cases. Another not trivial finding is that the influence on legibility of surface irregularities (e.g., grooves, prominences) is more important than that of surface texturization. A possible limitation for the use of projected AR is given by the blue color, whose performance turned out to be lower than that of other colors with every workbench surface.}
}
@article{DANGELMAIER2005371,
title = {Virtual and augmented reality support for discrete manufacturing system simulation},
journal = {Computers in Industry},
volume = {56},
number = {4},
pages = {371-383},
year = {2005},
note = {The Digital Factory: An Instrument of the Present and the Future},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2005.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361505000266},
author = {Wilhelm Dangelmaier and Matthias Fischer and Jürgen Gausemeier and Michael Grafe and Carsten Matysczok and Bengt Mueck},
keywords = {Virtual reality, Augmented reality, Production planning, Simulation},
abstract = {Nowadays companies operate in a difficult environment: the dynamics of innovations increase and product life cycles become shorter. Furthermore products and the corresponding manufacturing processes get more and more complex. Therefore, companies need new methods for the planning of manufacturing systems. One promising approach in this context is digital factory/virtual production—the modeling and analysis of computer models of the planned factory with the objective to reduce time and costs. For the modeling and analysis various simulation methods and programs have been developed. They are a highly valuable support for planning and visualizing the manufacturing system. But there is one major disadvantage: only experienced and long trained experts are able to operate with these programs. The graphical user interface is very complex and not intuitive to use. This results in an extensive and error-prone modeling of complex simulation models and a time-consuming interpretation of the simulation results. To overcome these weak points, intuitive and understandable man–machine interfaces like augmented and virtual reality can be used. This paper describes the architecture of a system which uses the technologies of augmented and virtual reality to support the planning process of complex manufacturing systems. The proposed system assists the user in modeling, the validation of the simulation model, and the subsequent optimization of the production system. A general application of the VR- and AR-technologies and of the simulation is realized by the development of appropriate linking and integration mechanisms. For the visualization of the arising 3D-data within the VR- and AR-environments, a dedicated 3D-rendering library is used.}
}
@article{HONG2023100029,
title = {VR training program for fire escape: Learning progress predicted by the perception of fire presence, VR operational frustration, and gameplay self-efficacy},
journal = {Computers & Education: X Reality},
volume = {3},
pages = {100029},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100029},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000235},
author = {Jon-Chao Hong and Hsun-Yu Chan and Yun-Hsuang Teng and Kai-Hsin Tai and Chang-Zhen Lin},
keywords = {Virtual reality, Frustration, Fire presence, Gameplay self-efficacy, Fire safety},
abstract = {Most VR fire escape training programs only task learners to observe the procedure of fire escape in different simulated fire scenes. To improve the effectiveness of such training programs for everyone, we tested a “fire escape virtual reality training program” which takes advantage of the feedback on the action to help individuals to learn the necessary and correct steps of fire escape. The virtual program emulates a real fire scene by providing realistic visual and auditory stimuli. A single-group quasi-experimental study was carried out to measure the effectiveness of the program, and a total of 173 seventh- and eighth-grade students from a high school in New Taipei City participated. The results of structural equation modeling showed that 1) gameplay self-efficacy was negatively predicted by frustration, 2) fire presence positively predicted gameplay self-efficacy, and 3) gameplay self-efficacy positively predicted learning progress. The findings suggested that critical life-saving skills such as fire escape skills can be readily acquired and trained through individual virtual reality training programs.}
}
@article{CHOI2023102220,
title = {Intracranial steno-occlusive lesion detection on time-of-flight MR angiography using multi-task learning},
journal = {Computerized Medical Imaging and Graphics},
volume = {107},
pages = {102220},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102220},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000381},
author = {Dongjun Choi and Tackeun Kim and Jinhee Jang and Leonard Sunwoo and Kyong Joon Lee},
keywords = {Intracranial artery, Magnetic resonance angiography, Lesion detection, Multi-task learning},
abstract = {Steno-occlusive lesions in intracranial arteries refer to segments of narrowed or occluded blood vessels that increase the risk of ischemic strokes. Steno-occlusive lesion detection is crucial in clinical settings; however, automatic detection methods have hardly been studied. Therefore, we propose a novel automatic method to detect steno-occlusive lesions in sequential transverse slices on time-of-flight magnetic resonance angiography. Our method simultaneously detects lesions while segmenting blood vessels based on end-to-end multi-task learning, reflecting that the lesions are closely related to the connectivity of blood vessels. We design classification and localization modules that can be attached to arbitrary segmentation network. As blood vessels are segmented, both modules simultaneously predict the presence and location of lesions for each transverse slice. By combining outputs from the two modules, we devise a simple operation that boosts the performance of lesion localization. Experimental results show that lesion prediction and localization performance is improved by incorporating blood vessel extraction. Our ablation study demonstrates that the proposed operation enhances lesion localization accuracy. We also verify the effectiveness of multi-task learning by comparing our approach with those that individually detect lesions with extracted blood vessels.}
}
@article{SUN2023107814,
title = {Like being there: How the soft VR technology could change consumers’ initial decision-making},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107814},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107814},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001656},
author = {Yi Sun and Yaobin Lu and Bin Wang and Weiguo (Patrick) Fan},
keywords = {Virtual reality, Soft VR, Metaverse, Initial decision, Decision-making},
abstract = {With the sweeping wave of digital transformation, Virtual Reality (VR) technology has become one of the key technologies for building the metaverse with its characteristic of lifting the physical interaction restrictions between people and things, which has attracted the extensive attention of scholars and practitioners. However, the VR industry is gradually evolving from traditional device-based hard VR to soft VR, where the market is larger but less researched. Additionally, although most businesses have high expectations for the potential of VR technology, little is known about whether a huge investment in VR will deliver the expected returns. The objective of this study is to explore whether and how soft VR technology can play a significant positive role in promoting consumers' initial decision-making. This study collected and empirically analyzed the data of 51,495 online second-hand condominiums on an online platform for housing transactions and services. The results show that the soft VR technology can significantly increase consumers' attention to the product and prompt their initial decision-making. Moreover, it is interesting that the enhancement effect of soft VR technology will show an equilibrating trend due to the different features of the search products. This study contributes to scholarship and practice by initiating the study of the emerging phenomenon of soft VR and empirically testing its effect on consumers' initial decision-making.}
}
@article{SUAREZWARDEN201517,
title = {Small Sample Size for Test of Training Time by Augmented Reality: An Aeronautical Case},
journal = {Procedia Computer Science},
volume = {75},
pages = {17-27},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036510},
author = {Fernando Suárez-Warden and Myrta Rodriguez and Nicolás Hendrichs and Salvador García-Lumbreras and Eduardo González Mendívil},
keywords = {small sample, estimation, training time, assembly time, Augmented Reality (AR), Operating Characteristic Curves (OCC)},
abstract = {Augmented Reality works for learning (growing tendency) in aeronautical sector demand an evaluation that requires experimental sampling, and this is often an expensive process. Usually, sample size is determined by resources availability constraint, which does not guarantee the level of significance (α) and the margin of error (E) required. This paper introduces a congruent sequence of statistical procedures to determine the estimated work sample size for traineeship in assembly operations. Sample size estimations for an aeronautical case are presented considering various scenarios. A minimum necessary sample size is calculated with statistical rigor via formulation and Operating Characteristic Curves.}
}
@article{LANG2019118,
title = {Mixed reality in production and logistics: Discussing the application potentials of Microsoft HoloLensTM},
journal = {Procedia Computer Science},
volume = {149},
pages = {118-129},
year = {2019},
note = {ICTE in Transportation and Logistics 2018 (ICTE 2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930122X},
author = {Sebastian Lang and Mohammed Saif Sheikh {Dastagir Kota} and David Weigert and Fabian Behrendt},
keywords = {Mixed Reality, Microsoft HoloLens, Production, Logistics, Manufacturing},
abstract = {In 2016, Microsoft released the mixed reality (MR) head-mounted display (HMD) HoloLensTM. As augmented reality (AR) devices, the HoloLensTM can enrich the user’s perceived environment with virtual information. However, Microsoft does not consider the HoloLensTM as AR device, but as the first MR device, since it has some additional features compared to competing AR devices. Especially to mention is the possibility to interact with virtual objects and vice versa. This paper shall provide an insight about the application potentials of MR in the field of production and logistics. For this purpose, we present the findings of a literature review about the current state of research concerning the application of Microsoft HoloLensTM in the field of production and logistics. Furthermore, we present a small HoloLensTM application, which we have developed to evaluate the capabilities of the device. Several persons tested our application and gave us feedback concerning the utility and usability of the HoloLensTM. We provide an evaluation of the user experiences at the end of the paper.}
}
@article{CINAR2022103647,
title = {A hybrid DenseNet121-UNet model for brain tumor segmentation from MR Images},
journal = {Biomedical Signal Processing and Control},
volume = {76},
pages = {103647},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103647},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422001690},
author = {Necip Cinar and Alper Ozcan and Mehmet Kaya},
keywords = {Deep learning, Image processing, Brain tumor segmentation, Artificial neural network models, Image segmentation, UNet, DenseNet121},
abstract = {Several techniques are used to detect brain tumors in the medical research field; however, Magnetic Resonance Imaging (MRI) is still the most effective technique used by experts. Recently, researchers have proposed different MRI techniques to detect brain tumors with the possibility of uploading and visualizing the image. In the current decade, deep learning techniques have shown promising results in every research area, especially in bioinformatics and medical image analysis. This paper aims to segment brain tumors using deep learning methods of MR images. The UNet architecture, one of the deep learning networks, is used as a hybrid model with pre-trained DenseNet121 architecture for the segmentation process. During training and testing of the model, we focus on smaller sub-regions of tumors that comprise the complex structure. The proposed model is validated on BRATS 2019 publicly available brain tumor dataset that contains high-grade and low-grade glioma tumors. The experimental results indicate that our model performs better than other state-of-the-art methods presented in this particular area. Specifically, the best Dice Similarity Coefficient (DSC) are obtained by using the proposed approach to segment whole tumor (WT), core tumor (CT), and enhancing tumor (ET).}
}
@article{PEPERKORN2015542,
title = {Temporal dynamics in the relation between presence and fear in virtual reality},
journal = {Computers in Human Behavior},
volume = {48},
pages = {542-547},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215001260},
author = {Henrik M. Peperkorn and Julia Diemer and Andreas Mühlberger},
keywords = {Virtual reality, Presence, Exposure therapy, Behavioral avoidance test, Anxiety disorders, Specific phobia},
abstract = {Virtual reality (VR) is increasingly investigated as a new medium for exposure therapy, but process variables are not well understood. In particular, presence and fear during VR exposure correlate strongly, but the causal relationship between them remains unclear. We assigned 22 female spider-fearful participants randomly to either a stereoscopic (high presence) or a monoscopic (low presence) condition and exposed them repeatedly to a large virtual spider presented on a Powerwall. Presence and fear were assessed on subjective, physiological, and behavioral levels. Fear reactions were stronger and presence ratings were higher in the stereoscopic than the monoscopic condition. Presence in the first exposure trial correlated significantly with fear in the second exposure trial, while fear in the first exposure trial did not correlate significantly with presence in the second exposure trial. For the following exposure trials, correlations between presence and fear were significant in both directions. Limitations of our study include the small sample and the fact that we did not check diagnostic criteria of specific phobia. This is the first study to show temporal dynamics of the relationship between presence and fear. Initially, presence in VR seems to directly influence fear, while over time, presence and fear appear mutually dependent.}
}
@article{OKTAY2014613,
title = {Computer aided diagnosis of degenerative intervertebral disc diseases from lumbar MR images},
journal = {Computerized Medical Imaging and Graphics},
volume = {38},
number = {7},
pages = {613-619},
year = {2014},
note = {Special Issue on Computational Methods and Clinical Applications for Spine Imaging},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2014.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0895611114000573},
author = {Ayse Betul Oktay and Nur Banu Albayrak and Yusuf Sinan Akgul},
keywords = {Degenerative disc disease, Machine learning, Intervertebral disc, Herniation, Desiccation, Degeneration, Computer aided diagnosis, Lumbar},
abstract = {This paper presents a novel method for the automated diagnosis of the degenerative intervertebral disc disease in midsagittal MR images. The approach is based on combining distinct disc features under a machine learning framework. The discs in the lumbar MR images are first localized and segmented. Then, intensity, shape, context, and texture features of the discs are extracted with various techniques. A Support Vector Machine classifier is applied to classify the discs as normal or degenerated. The method is tested and validated on a clinical lumbar spine dataset containing 102 subjects and the results are comparable to the state of the art.}
}
@article{SANTANAMANCILLA2012721,
title = {Service Oriented Architecture to Support Mexican Secondary Education through Mobile Augmented Reality},
journal = {Procedia Computer Science},
volume = {10},
pages = {721-727},
year = {2012},
note = {ANT 2012 and MobiWIS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.06.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912004498},
author = {Pedro C. Santana-Mancilla and Miguel A. Garc’a-Ruiz and Ricardo Acosta-Diaz and Celso U. Juárez},
keywords = {Augmented reality, Service-Oriented Architecture, Mexican Basic Education, Mobile computing},
abstract = {This paper proposes a mobile augmented reality system that allows Mexican secondary education students to access additional educational contents related to their textbooks. Our system recognizes the images printed in the book as part of regular taught topics and shows multimedia contents that complement the topics covered in the book. These contents are generated through a service-oriented architecture. Initial usability testing of our augmented reality system showed a user satisfaction of 97%.}
}
@article{SAHAYAM2022103939,
title = {Brain tumor segmentation using a hybrid multi resolution U-Net with residual dual attention and deep supervision on MR images},
journal = {Biomedical Signal Processing and Control},
volume = {78},
pages = {103939},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103939},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422004396},
author = {Subin Sahayam and Rahul Nenavath and Umarani Jayaraman and Surya Prakash},
keywords = {Brain tumor segmentation, BraTS 2020 dataset, Multi-resolution, Residual block, Dual attention, Deep supervision},
abstract = {Manual identification of brain tumors in Magnetic Resonance (MR) images is laborious, time-consuming, and human error-prone. Automatic segmentation of brain tumors from MR images aims to bridge the gap. U-Net, a deep learning model, has delivered promising results in generating brain tumor segments. However, the model tends to over-segment the tumor volume than required. It will have a significant impact on deploying the model for practical use. In this work, the baseline U-Net model has been studied with the addition of residual, multi-resolution, dual attention, and deep supervision blocks. The goal of residual blocks is to efficiently extract features to reduce the semantic gap between low-level features from the decoder and high-level features from skip connections. The multiple resolution blocks have been added to extract features and analyze tumors of varying scales. The dual attention mechanism has been incorporated to highlight tumor representations and reduce over-segmentation. Finally, the deep supervision blocks have been added to utilize features from various decoder layers to obtain the target segmentation. The design of the proposed model has been justified with several experiments and ablation studies. The proposed model has been trained and evaluated on the BraTS2020 training and validation datasets. On the validation data, the proposed model has achieved a dice score of 0.60, 0.75, 0.62 for enhancing tumor (ET), whole tumor (WT), and tumor core (TC), respectively, and a Hausdorff 95 score of 46.84, 11.05, and 22.5, respectively. Compared to the baseline U-Net, the proposed model has outperformed WT and TC volumes in the Hausdorff 95 distance metric except for the ET volume.}
}
@article{PRASTAWA2009297,
title = {Simulation of brain tumors in MR images for evaluation of segmentation efficacy},
journal = {Medical Image Analysis},
volume = {13},
number = {2},
pages = {297-311},
year = {2009},
note = {Includes Special Section on Functional Imaging and Modelling of the Heart},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841508001357},
author = {Marcel Prastawa and Elizabeth Bullitt and Guido Gerig},
keywords = {Brain MRI, Segmentation validation, Tumor simulation, Simulation of tumor infiltration, Diffusion tensor imaging, Ground truth, Gold standard},
abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors).}
}
@article{SEERS2022105006,
title = {Virtual outcrop geology comes of age: The application of consumer-grade virtual reality hardware and software to digital outcrop data analysis},
journal = {Computers & Geosciences},
volume = {159},
pages = {105006},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105006},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002892},
author = {Thomas D. Seers and Ali Sheharyar and Stefano Tavani and Amerigo Corradetti},
keywords = {Digital outcrop, Virtual reality, Fracture analysis, MATLAB language, Remote sensing},
abstract = {In this work, the application of consumer-grade virtual reality (VR) hardware and software to the analysis of digital outcrop data is explored. Here, we utilize a widely available VR hardware platform (HTC Vive) and VR based freeform 3D visual arts software package (Google Tilt Brush), to digitize fault traces from photo-textured digital outcrop models of exposures of the Penrith Sandstone Formation (Lacy's Caves), in northwest England. Using MATLAB routines provided herein, triangular meshes output from Tilt Brush are used as the basis for 3D fracture trace map extraction, which in turn, are used to generate fracture properties (trace length, orientation, areal fracture intensity). We compare the results of this analysis to two equivalent datasets obtained from the Lacy's Caves model using digital outcrop analysis deployed via a conventional flat panel display: namely (1) a 3D trace map extracted using optical ray tracing from manually interpreted calibrated images and (2) 3D traces fitted directly the Lacy's Caves textured mesh using manual polyline interpretation within an established digital outcrop analysis software platform (OpenPlot). Fault statistics obtained using VR based analysis are broadly equivalent to those acquired from 3D trace maps extracted using the flat panel display deployed analyses presented herein. In this case study, it was found that VR based digital outcrop analysis provided faster data acquisition than the comparative pixel-based approach, which requires linkage and merging of traces mapped from multiple contiguous images. Manual raster analysis and optical ray tracing did however provide 3D trace maps with significantly higher areal fault intensity, with VR analysis incurring censoring of finer fault traces, due to the limited resolution of the outcrop model textured mesh. Whilst data acquisition times and resultant fault intensities proved similar between the VR and OpenPlot workflows, it was noted anecdotally, that the VR analysis holds some advantages for the operator when interpreting models exhibiting complex geometries, such as mine workings and caves systems, with the clip point implemented within the viewport of conventional digital outcrop analysis software tools obstructing the user from obtaining an optimum view of the outcrop surface. VR based digital outcrop analysis techniques, such as those presented here, provide an immersive analytical environment to the operator. This allows users to fuse powerful 3D visualizations of photo-realistic outcrop models with geological interpretation and data collection, fulfilling the early promise of ‘virtual outcrops’ as an analytical medium that can emulate traditional fieldwork. It is hoped that this study and its associated code library will facilitate the evaluation of emerging VR technologies for digital outcrop applications, by provided access to VR analytical tools for non-specialists in virtual reality systems. Finally, prospects for the use of VR technology within the field of digital outcrop geology, as well as within the wider geosciences, are also discussed.}
}
@article{GRAJEWSKI2013289,
title = {Application of Virtual Reality Techniques in Design of Ergonomic Manufacturing Workplaces},
journal = {Procedia Computer Science},
volume = {25},
pages = {289-301},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012404},
author = {Damian Grajewski and Filip Górski and Przemysław Zawadzki and Adam Hamrol},
keywords = {virtual reality, haptic devices, workplace design, ergonomics},
abstract = {This paper presents possibilities of application of the immersive VR and the haptic technologies during the complex process of design and virtual prototyping of the manufacturing workplaces characterized with a high level of ergonomic quality. Two case studies are presented: a workplace for stud welding and a set of two workplaces, for hole drilling and manual assembly. In the first case study, haptic device with force feedback effect was used to improve ergonomics of main operator activities. In the second case study, immersive approach was used, namely Head-Mounted Device, tracking and gesture recognition systems, to test and improve ergonomics of the whole workplace. Application of VR techniques allows to present the virtual prototype of the workplace in its real operation environment, limiting the need for use of real mock-ups. Therefore, Virtual Reality allows to conduct a number of analyses related to designed prototypes, such as: dimensions of devices and possibilities of adjustment to height of the human operator, and arrangement of control and signaling devices according to the rules of ergonomic design. To conduct these analyses, full interaction between user and workplace must be programmed, including collision detection, kinematics of the devices and possibilities of activating their various functions in relation with other objects in the virtual scene.}
}
@article{LANGLOTZ2011831,
title = {Robust detection and tracking of annotations for outdoor augmented reality browsing},
journal = {Computers & Graphics},
volume = {35},
number = {4},
pages = {831-840},
year = {2011},
note = {Semantic 3D Media and Content},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0097849311001075},
author = {Tobias Langlotz and Claus Degendorfer and Alessandro Mulloni and Gerhard Schall and Gerhard Reitmayr and Dieter Schmalstieg},
keywords = {Augmented reality, Annotation, Tracking, Mobile phone},
abstract = {A common goal of outdoor augmented reality (AR) is the presentation of annotations that are registered to anchor points in the real world. We present an enhanced approach for registering and tracking such anchor points, which is suitable for current generation mobile phones and can also successfully deal with the wide variety of viewing conditions encountered in real life outdoor use. The approach is based on on-the-fly generation of panoramic images by sweeping the camera over the scene. The panoramas are then used for stable orientation tracking, while the user is performing only rotational movements. This basic approach is improved by several new techniques for the re-detection and tracking of anchor points. For the re-detection, specifically after temporal variations, we first compute a panoramic image with extended dynamic range, which can better represent varying illumination conditions. The panorama is then searched for known anchor points, while orientation tracking continues uninterrupted. We then use information from an internal orientation sensor to prime an active search scheme for the anchor points, which improves matching results. Finally, global consistency is enhanced by statistical estimation of a global rotation that minimizes the overall position error of anchor points when transforming them from the source panorama in which they were created, to the current view represented by a new panorama. Once the anchor points are redetected, we track the user's movement using a novel 3-degree-of-freedom orientation tracking approach that combines vision tracking with the absolute orientation from inertial and magnetic sensors. We tested our system using an AR campus guide as an example application and provide detailed results for our approach using an off-the-shelf smartphone. Results show that the re-detection rate is improved by a factor of 2 compared to previous work and reaches almost 90% for a wide variety of test cases while still keeping the ability to run at interactive frame rates.}
}
@article{NEVEN2018335,
title = {SOULMATE - Secure Old people’s Ultimate Lifestyle Mobility by offering Augmented reality Training Experiences},
journal = {Procedia Computer Science},
volume = {141},
pages = {335-342},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.192},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318428},
author = {An Neven and Tom Bellemans and Astrid Kemperman and Pauline van den Berg and Martijn Kiers and Lex van Velsen and Judith Urlings and Davy Janssens and Yves Vanrompay},
keywords = {Virtual trip training, Mobile routing, GPS-based monitoring, Virtual, real-life user experiences, Customizable guardian angel},
abstract = {With ageing, travel becomes more and more complex. Physical and cognitive abilities may decline, and thus, elderly people may experience several problems that hinder them from travelling independently. One of the most difficult issues when addressing the problem of age-related travel restrictions is the heterogeneity of the ageing population. SOULMATE offers a non-intrusive and personalized mobility package that evolves with the end-user across his/her different life stages to ensure him/her to make secure trips. SOULMATE aggregates three types of mobility support: Indoor virtual training of a route, active routing during trips and monitoring by a coach at a distance during trips. For each individual, the desired or necessary functionalities can be chosen, based on his/her specific abilities and travel needs. The SOULMATE solution will be developed iteratively, in co-creation with end-users and stakeholders, and the package will be tested intensively, and evaluated in three countries (Belgium, The Netherlands and Austria) based on usability, technical and business aspects.}
}
@article{URREA2020103447,
title = {Development of a virtual reality simulator for a strategy for coordinating cooperative manipulator robots using cloud computing},
journal = {Robotics and Autonomous Systems},
volume = {126},
pages = {103447},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103447},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307651},
author = {Claudio Urrea and Rodrigo Matteoda},
keywords = {Cloud, Services, Collaborative, Cooperative, Coordination, Virtual reality},
abstract = {This article deals with the development of a simulator that recreates, in virtual reality, a team of Selectively Compliance Assembly Robot Arms (SCARA). This team works cooperatively to fulfill the task of stacking rectangular objects coordinated through a strategy that includes a cloud server responsible for communication between the robots. The execution of the task is based on a leader/follower configuration. In this configuration, the leader performs a computed trajectory constantly reporting its position to the remote server. The remote server, in turn, sends this information back to the follower so this can follow the leader. The application combines MatLab® and Java. The latter is specifically used for communication routines, since its versatility makes it easy to be incorporated into any type of machine. This paper seeks to demonstrate the advantages of incorporating cloud resources into a multi-robot system and how its performance can be tested by means of the application developed.}
}
@article{GARZA2013154,
title = {Augmented Reality Application for the Maintenance of a Flapper Valve of a Fuller-kynion Type M Pump},
journal = {Procedia Computer Science},
volume = {25},
pages = {154-160},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012246},
author = {Luis Eduardo Garza and Gabriel Pantoja and Pablo Ramírez and Hector Ramírez and Nestor Rodríguez and Eduardo González and Raúl Quintal and Juan A. Pérez},
keywords = {Augmented Reality, Virtual Reality, CAD models, maintenance process, mobile devices},
abstract = {The overall purpose of this project is to test the impact and potential benefits of Virtual and Augmented Reality technologies (AR&VR) to improve maintenance operation in industrial equipment. The main function for a Flapper valve of a Fuller-Kinyon type M pump is to prevent that the air generated to convey the bulk material through a conveying pipe flows inside of the material chute through the rotating screw. If this will occur, the material flow will decrease or even stop, causing a reduction of the pump capacity. Thus, it is necessary to maintain calibrated each flapper valve in the plant. This process of maintenance is done once a month, or when necessary, and it needs the pump to be shut down, taking up to four hours to finish the complete process. For this reason, an augmented reality application is being developed, aiming to reduce the consumed time by the maintenance process. Using this application, it is expected to dedicate less time training the new personnel responsible for the maintaining process, displaying tridimensional models, animations, images and text information that would simplify the instructions shown in a printed manual and adding an interactive environment between the users and the information displayed. A mobile device either a tablet or a smartphone is to be used as the hardware that will run the application, allowing the user to take it right to the working area, either in a workshop or directly in field. The information displayed includes CAD models of the pump and its components as well as animations illustrating the instructions to follow in each step of the process. Also, the right tool to use in each step will be indicated following by security warnings when needed. This project was developed according to the collaboration cathedra between CEMEX and ITESM, following the LEAD methodology developed by CEMEX to support the project administration, the general process development was constructed from knowledge and experiences gathered among the different previous AR projects developed at ITESM. This information has been studied and “best practices” has been noted, learned and established to develop and implement AR.}
}
@article{POLVI201633,
title = {SlidAR: A 3D positioning method for SLAM-based handheld augmented reality},
journal = {Computers & Graphics},
volume = {55},
pages = {33-43},
year = {2016},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2015.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849315001806},
author = {Jarkko Polvi and Takafumi Taketomi and Goshiro Yamamoto and Arindam Dey and Christian Sandor and Hirokazu Kato},
keywords = {Handheld augmented reality, 3D manipulation, 3D positioning, SLAM, User evaluation},
abstract = {Handheld Augmented Reality (HAR) has the potential to introduce Augmented Reality (AR) to large audiences due to the widespread use of suitable handheld devices. However, many of the current HAR systems are not considered very practical and they do not fully answer to the needs of the users. One of the challenging areas in HAR is the in-situ AR content creation where the correct and accurate positioning of virtual objects to the real world is fundamental. Due to the hardware limitations of handheld devices and possible restrictions in the environment, the correct 3D positioning of objects can be difficult to achieve we are unable to use AR markers or correctly map the 3D structure of the environment. We present SlidAR, a 3D positioning for Simultaneous Localization And Mapping (SLAM) based HAR systems. SlidAR utilizes 3D ray-casting and epipolar geometry for virtual object positioning. It does not require a perfect 3D reconstruction of the environment nor any virtual depth cues. We have conducted a user experiment to evaluate the efficiency of SlidAR method against an existing device-centric positioning method that we call HoldAR. Results showed that SlidAR was significantly faster, required significantly less device movement, and also got significantly better subjective evaluation from the test participants. SlidAR also had higher positioning accuracy, although not significantly.}
}
@article{ZENG2019176,
title = {Eye-around vibration haptics on VR immersion improvement},
journal = {Virtual Reality & Intelligent Hardware},
volume = {1},
number = {2},
pages = {176-184},
year = {2019},
note = {Haptic Interaction},
issn = {2096-5796},
doi = {https://doi.org/10.3724/SP.J.2096-5796.2018.0014},
url = {https://www.sciencedirect.com/science/article/pii/S2096579619300154},
author = {Tao ZENG and Keyu WEI and Yanlin YU and Yi ZHAO},
keywords = {VR immersion, Haptics, Eye tactile feedback, Vibration actuator},
abstract = {Due to the inherent shortcomings of the hardware, the immersion of visual interaction between the user and the virtual reality (VR) equipment is greatly reduced. In this paper, effects of eye-around vibration haptics on improving the VR immersion were studied. The vibration was generated by flexible vibrators whose performance was evaluated by a laser vibrometer. Fitting the vibrators on the human eye area at different positions and derived by different waveforms and frequencies of the input signal, the effects of vibration on the human vision and comfort of the users were verified. Then, with the selected input signals and fitting locations, different kinds of vibration were applied on the eye area cooperating with virtual reality images or videos to evaluate the changes of immersion. Research results provide references to the modeling of eye tactile feedback and the design of relevant tactile device in improving the VR immersion.}
}
@article{CRESPO2015107,
title = {Virtual Reality Application for Simulation and Off-line Programming of the Mitsubishi Movemaster RV-M1 Robot Integrated with the Oculus Rift to Improve Students Training},
journal = {Procedia Computer Science},
volume = {75},
pages = {107-112},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.226},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503687X},
author = {Raúl Crespo and René García and Samuel Quiroz},
keywords = {Virtual Reality, Robotics, Off-Line Programming, Unity, Oculus Rift, A* algorithm},
abstract = {The overall purpose of this project is to test the impact and potential benefits of Virtual Reality technology by improving the current training methods for the operation of the Mitsubishi Movemaster RV-M1 Robot. The final application aims to achieve this goal by increasing the user's interactivity with the robot's features inside a virtual environment developed using the game engine Unity and the Oculus Rift headset for the virtual visualization. By using this application, on site operation time is decreased mainly by allowing off-line programming of the equipment. It also allows universities without or with limited industrial machinery to provide their students a way to learn and practice on industrial automation and robotics simulation topics without inconvenience. The application is designed to decrease the student's learning curve by displaying a complete virtual environment where the tridimensional model of the robotic arm can be visualized and programmed according to the real model's parameters and specifications. Joint type moving sequences are compiled into a file which afterwards can be transferred to the robot for real testing and execution. The system integrates a set of joysticks that allows the user to program each of the robot's joints as well as display several features of it in the virtual environment such as animations, images and text information that simplify the instructions shown in a printed manual. In order to find an optimized, collision-free movement sequence between two points an A* shortest path algorithm is implemented using some of the built-in tools and plugins available in Unity and its Asset store. As a result of this last feature, the application can create a comparison between the user's input sequence and a computer generated sequence based on the A* algorithm.}
}
@article{MA2024105508,
title = {A novel Residual and Gated Network for prostate segmentation on MR images},
journal = {Biomedical Signal Processing and Control},
volume = {87},
pages = {105508},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105508},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423009412},
author = {Ling Ma and Qiliang Fan and Zhiqiang Tian and Lizhi Liu and Baowei Fei},
keywords = {Prostate cancer, Prostate segmentation, Gated convolutional neural network, Residual convolutional neural network, Magnetic resonance imaging},
abstract = {Reliable and automated segmentation of the prostate in magnetic resonance (MR) images is critical for the diagnosis and treatment of prostate cancer. The unclear boundary of the prostate increases the difficulty of correctly differentiating the prostate region from the surrounding regions. This paper proposes a novel approach with Residual and Gated Network (ResGNet) for prostate segmentation on MR images. The ResGNet block fuses the residual and gated connections in a multi-directional and multi-path learning strategy for enhancing information propagation and preserving important boundary information. The proposed method is conducted on several public databases, and an average DSC of 94.4%, 95-HD of 3.280 mm, and ABD of 0.919 mm are obtained in the 5-fold cross-validation experiment on the PROMISE12 dataset. Experimental results demonstrate that the ResGNet outperforms the state-of-the-art methods and is effective and robust regardless of the variability of MR scanners and acquisition protocols or the size and shape of the prostate gland for the segmentation of healthy or diseased prostates.}
}
@article{JODA201993,
title = {Augmented and virtual reality in dental medicine: A systematic review},
journal = {Computers in Biology and Medicine},
volume = {108},
pages = {93-100},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S001048251930085X},
author = {T. Joda and G.O. Gallucci and D. Wismeijer and N.U. Zitzmann},
keywords = {Systematic review, Augmented reality (AR), Virtual reality (VR), Computer simulation, Dentistry, Oral medicine},
abstract = {Background
The aim of this systematic review was to provide an update on the contemporary knowledge and scientific development of augmented reality (AR) and virtual reality (VR) in dental medicine, and to identify future research needs to accomplish its clinical translation.
Method
A modified PICO-strategy was performed using an electronic (MEDLINE, EMBASE, CENTRAL) plus manual search up to 12/2018 exploring AR/VR in dentistry in the last 5 years. Inclusion criteria were limited to human studies focusing on the clinical application of AR/VR and associated field of interest in dental medicine.
Results
The systematic search identified 315 titles, whereas 87 abstracts and successively 32 full-texts were selected for review, resulting in 16 studies for final inclusion. AR/VR-technologies were predominantly used for educational motor skill training (n = 9 studies), clinical testing of maxillofacial surgical protocols (n = 5), investigation of human anatomy (n = 1), and the treatment of patients with dental phobia (n = 1). Due to the heterogeneity of the included studies, meta-analyses could not be performed.
Conclusions
The overall number of includable studies was low; and scientifically proven recommendations for clinical protocols could not be given at this time. However, AR/VR-applications are of increasing interest and importance in dental under- and postgraduate education offering interactive learning concepts with 24/7-access and objective evaluation. In maxillofacial surgery, AR/VR-technology is a promising tool for complex procedures and can help to deliver predictable and safe therapy outcomes. Future research should focus on establishing technological standards with high data quality and developing approved applications for dental AR/VR-devices for clinical routine.}
}
@article{SUAREZWARDEN2015281,
title = {Assembly Operations Aided by Augmented Reality: An Endeavour toward a Comparative Analysis},
journal = {Procedia Computer Science},
volume = {75},
pages = {281-290},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.249},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037102},
author = {Fernando Suárez-Warden and Eduardo González Mendívil and Ciro A. Rodríguez and Salvador Garcia-Lumbreras},
keywords = {augmented reality (AR), complex assembly, experimental conditions, maintenance, aeronautical assembly, small sample.},
abstract = {Costly complex assembly operations supported by augmented reality demands endeavors to achieve a comparative and experimental depiction of different circumstances for assays to analyze fluctuating conclusions where the situation with limited number of tries has compelled to recognize that variations exist depending on test conditions. Then developments in emergent technologies, for training in Maintenance and Repair Operations (MRO), must be evaluated. Scenarios are focused on assessing an AR request, by stating a confrontation of declarations about assembly time and comparisons respect to outstanding authors who conduct assembly operations. An assembly operations case study establishes a small sample size during experimentation.}
}
@article{JULIANTINO2023310,
title = {The development of virtual healing environment in VR platform},
journal = {Procedia Computer Science},
volume = {216},
pages = {310-318},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022190},
author = {Chris Juliantino and Mega Putri Nathania and Religiana Hendarti and Herru Darmadi and Bonny A Suryawinata},
keywords = {Android, Environment, Smartphone, Virtual Reality, Virtual space},
abstract = {The purpose of this research is to build a healing environment that can be experienced through virtual reality (VR) to reduce stress levels of the user because of the lengthy period of COVID-19 pandemic that forced people to stay at home. The healing environment is created based on several theories on the principles of Healing Environments related to Architectural design. And as for the development of the simulation software, it is using the method of Extreme Programming that is based on Agile principle that combined with Architectural design flow. The VR is targeted to run optimally in low-end devices with cardboard and common Bluetooth controller so that it can be accessed inclusively. The evaluation is conducted using a user acceptance test with expert judgment and survey to 32 respondents. The findings are that they enjoyed the simulation because of the guidance is clear and the virtual environment looks more real compared to others that looks cartoonish.}
}
@article{BAL2016267,
title = {Computer Hardware Course Application through Augmented Reality and QR Code Integration: Achievement Levels and Views of Students},
journal = {Procedia Computer Science},
volume = {102},
pages = {267-272},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.400},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325807},
author = {Erkan Bal and Hüseyin Bicen},
keywords = {Augmented Reality, QR Code, Computer Hardware, Achievement, View},
abstract = {This study examined the effect of Augmented Reality and QR Code Integration on achievements and views of undergraduate students taking computer course. Study group of the research included 50 voluntary students taking compulsory computer course studying in the department of Guidance and Psychological Counseling at Near East University. Experimental research design was used in the study. In the beginning of the term, students were seperated into two groups including experimental and control group consisting of 25 students in each group. Pre-test was administered to the two groups in the first lecture and lecture was given with traditional methods, computer features were defined and functions of computer features were explained to the 1st group. Three-dimensional images of hardware features in the hardware chapter of computer course were explained through augmented reality and QR technology in laboratory environment with computer, projection and voice systems to the 2nd group. Students examined computer hardware with QR code cards and augmented reality technology during the lecture from their own mobile devices and all lectures were given by this way until the end of the semester. At the end of the term, post-test was administered to the experimental and control group and a questionnaire related with augmented reality and computer hardware course application with QR code integration was also applied to the experimental group. Results obtained from the study revealed that achievement level of experimental group is higher. In this context, results showed that computer hardware course application with augmented reality and QR code integration has a positive effect on academic achievements of students and students have positive views towards this course. Based on the results, it is expected that method used in this study might contribute to the integration of technology into education and therefore technology and education could support each other.}
}
@article{HAN2012841,
title = {Assessment of cognitive flexibility in real life using virtual reality: A comparison of healthy individuals and schizophrenia patients},
journal = {Computers in Biology and Medicine},
volume = {42},
number = {8},
pages = {841-847},
year = {2012},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2012.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010482512000996},
author = {Kiwan Han and In {Young Kim} and Jae-Jin Kim},
keywords = {Virtual reality, Cognitive flexibility, Decision making, Schizophrenia},
abstract = {To date, cognitive flexibility has been measured only using neuropsychological tasks, and has not been tested using more ecologically valid task due to methodological limitations. In this study, a virtual reality task was developed to evaluate cognitive flexibility in a real life situation and performance on this task was compared between 30 healthy individuals and 30 schizophrenia patients. Compared to healthy controls, a greater number of schizophrenia patients made concrete decisions, and their decision-making times were negatively correlated with the severity of their negative symptoms. These findings indicate that virtual reality can be an ecologically valid measurement of cognitive flexibility.}
}
@article{PENUMUDI2020103010,
title = {The effects of target location on musculoskeletal load, task performance, and subjective discomfort during virtual reality interactions},
journal = {Applied Ergonomics},
volume = {84},
pages = {103010},
year = {2020},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2019.103010},
url = {https://www.sciencedirect.com/science/article/pii/S0003687019302194},
author = {Sai Akhil Penumudi and Veera Aneesh Kuppam and Jeong Ho Kim and Jaejin Hwang},
keywords = {Virtual environment, Hand gestures, Head-mounted display, Task performance},
abstract = {The objective of this study was to evaluate the effect of different target locations on musculoskeletal loading and task performance during virtual reality (VR) interactions. A repeated-measures laboratory study with 20 participants (24.2 ± 1.5 years; 10 males) was conducted to compare biomechanical exposures (joint angle, moment, and muscle activity in the neck and shoulder), subjective discomfort, and task performance (speed and accuracy) during two VR tasks (omni-directional pointing and painting tasks) among different vertical target locations (ranged from 15° above to 30° below eye height). The results showed that neck flexion/extension angle and moment, shoulder flexion angle and moment, shoulder abduction angle, muscle activities of neck and shoulder muscles, and subjective discomfort in the neck and shoulder significantly varied by target locations (p's < 0.001). The target locations at 15° above and 30° below eye height demonstrated greater shoulder flexion (up to 52°), neck flexion moment (up to 2.7Nm), anterior deltoid muscle activity, and subjective discomfort in the neck and shoulder as compared to the other locations. This result indicates that excessive vertical target locations should be avoided to reduce musculoskeletal discomfort and injury risks during VR interactions. Based on relatively lower biomechanical exposures and trade-off between neck and shoulder postures, vertical target location between eye height and 15° below eye height could be recommended for VR use.}
}
@article{TONNIS2013997,
title = {Representing information – Classifying the Augmented Reality presentation space},
journal = {Computers & Graphics},
volume = {37},
number = {8},
pages = {997-1011},
year = {2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313001313},
author = {Marcus Tönnis and David A. Plecher and Gudrun Klinker},
keywords = {Augmented Reality, Taxonomy, Information Presentation},
abstract = {Augmented Reality has a wide-ranging presentation space. In addition to presenting virtual information in a 3D space, such information can also be placed in relation to physical objects, locations or events. Decomposing this presentation space – or more exactly, the principles of how information is represented in Augmented Reality – into unique and independent dimensions provides a fundamental spectrum of options. First, this decomposition facilitates a fine-grained analysis of effects on human understanding. Second, multiple factors, given by multiple differences between different presentation systems with respect to more than one such principle, can be determined and properly addressed. Third, this decomposition facilitates a determination of new fields of research by identifying not-yet-used concepts. Since the beginning of Augmented Reality research, a growing number of applications have emerged that exploit various ways to represent information. This paper resumes this development and presents a set of independent dimensions covering representation principles of virtual information related to a physical environment: the temporality of virtual information, dimensionality, the frame of reference, mounting/registration and the type of reference. The suitability of the devised dimensions is tested by categorizing a wide variety of AR applications. The categorized data is analyzed for the most-often and less-frequently used combinations of classes. In particular, the classes that have not yet been used exhibit the potential to allow future work that investigates new options for information presentation.}
}
@article{IMOTTESJO2018120,
title = {The Urban CoBuilder – A mobile augmented reality tool for crowd-sourced simulation of emergent urban development patterns: Requirements, prototyping and assessment},
journal = {Computers, Environment and Urban Systems},
volume = {71},
pages = {120-130},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517305008},
author = {Hyekyung Imottesjo and Jaan-Henrik Kain},
keywords = {Urban design and planning, Emergent planning, Multi-stakeholder inclusion, Outdoor mobile augmented reality, Urban simulative game},
abstract = {Policy and research argue for multi-stakeholder inclusion in design and planning to increase urban qualities and resilience. Communicative planning and agent-based modelling are two approaches facilitating such inclusion, but both have shortcomings. In this paper, a third complementary approach is explored: rule-based emergent planning supported through mobile augmented reality (MAR) and gamification. Such an approach would serve to crowdsource data on how people collectively build their city under different types of planning rules, mimicking emergent development patterns but, currently, there is a lack of functioning participative outdoor MAR tools. The objectives of this paper are to a) identify a set of specifications detailing the necessary performance of a MAR tool; b) describe the development of a prototype MAR tool; and c) assess this prototype MAR tool through pilot application. A literature review was carried out to identify tool requirements. An iterative research by design approach was applied to turn these specifications into a functioning MAR tool: the Urban CoBuilder. The tool was then piloted in a series of tests. The findings suggest that the MAR tool makes it possible for multiple stakeholders to design urban environments on site and that crowdsourced data on collective results of individual design and planning decisions can be gathered. Although the immersive qualities of the Urban CoBuilder were highly appreciated, further development is needed. The realism of planning rules, building types and functions has to be strengthened, the techniques for positioning the MAR model in relation to real space need improvement, and the gaming mechanisms should be enhanced to make gameplay attractive for a large number of stakeholders.}
}
@article{SINGH2020105572,
title = {Segmentation of prostate zones using probabilistic atlas-based method with diffusion-weighted MR images},
journal = {Computer Methods and Programs in Biomedicine},
volume = {196},
pages = {105572},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105572},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719321674},
author = {Dharmesh Singh and Virendra Kumar and Chandan J. Das and Anup Singh and Amit Mehndiratta},
keywords = {Prostate zonal segmentation, 3D registration, Probabilistic atlas, Partial volume correction, Diffusion-weighted imaging},
abstract = {Background and objective
Accurate segmentation of prostate and its zones constitute an essential preprocessing step for computer-aided diagnosis and detection system for prostate cancer (PCa) using diffusion-weighted imaging (DWI). However, low signal-to-noise ratio and high variability of prostate anatomic structures are challenging for its segmentation using DWI. We propose a semi-automated framework that segments the prostate gland and its zones simultaneously using DWI.
Methods
In this paper, the Chan-Vese active contour model along with morphological opening operation was used for segmentation of prostate gland. Then segmentation of prostate zones into peripheral zone (PZ) and transition zone (TZ) was carried out using in-house developed probabilistic atlas with partial volume (PV) correction algorithm. The study cohort included MRI dataset of 18 patients (n = 18) as our dataset and methodology were also independently evaluated using 15 MRI scans (n = 15) of QIN-PROSTATE-Repeatability dataset. The atlas for zones of prostate gland was constructed using dataset of twelve patients of our patient cohort. Three-fold cross-validation was performed with 10 repetitions, thus total 30 instances of training and testing were performed on our dataset followed by independent testing on the QIN-PROSTATE-Repeatability dataset. Dice similarity coefficient (DSC), Jaccard coefficient (JC), and accuracy were used for quantitative assessment of the segmentation results with respect to boundaries delineated manually by an expert radiologist. A paired t-test was performed to evaluate the improvement in zonal segmentation performance with the proposed PV correction algorithm.
Results
For our dataset, the proposed segmentation methodology produced improved segmentation with DSC of 90.76 ± 3.68%, JC of 83.00 ± 5.78%, and accuracy of 99.42 ± 0.36% for the prostate gland, DSC of 77.73 ± 2.76%, JC of 64.46 ± 3.43%, and accuracy of 82.47 ± 2.22% for the PZ, and DSC of 86.05 ± 1.50%, JC of 75.80 ± 2.10%, and accuracy of 91.67 ± 1.56% for the TZ. The segmentation performance for QIN-PROSTATE-Repeatability dataset was, DSC of 85.50 ± 4.43%, JC of 75.00 ± 6.34%, and accuracy of 81.52 ± 5.55% for prostate gland, DSC of 74.40 ± 1.79%, JC of 59.53 ± 8.70%, and accuracy of 80.91 ± 5.16% for PZ, and DSC of 85.80 ± 5.55%, JC of 74.87 ± 7.90%, and accuracy of 90.59 ± 3.74% for TZ. With the implementation of the PV correction algorithm, statistically significant (p<0.05) improvements were observed in all the metrics (DSC, JC, and accuracy) for both prostate zones, PZ and TZ segmentation.
Conclusions
The proposed segmentation methodology is stable, accurate, and easy to implement for segmentation of prostate gland and its zones (PZ and TZ). The atlas-based segmentation framework with PV correction algorithm can be incorporated into a computer-aided diagnostic system for PCa localization and treatment planning.}
}
@article{GAZCON201830,
title = {Fieldwork in Geosciences assisted by ARGeo: A mobile Augmented Reality system},
journal = {Computers & Geosciences},
volume = {121},
pages = {30-38},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417313237},
author = {Nicolás F. Gazcón and Juan M. {Trippel Nagel} and Ernesto A. Bjerg and Silvia M. Castro},
keywords = {Fieldwork, Augmented reality, GIS, Mobile visualization, Geo-data},
abstract = {Gathering data on the field is a crucial task in many disciplines. Moreover, in the Geosciences the GPS positioning is relevant to the data acquisition as well as to the planning of the fieldwork. Nowadays, mobile devices are widespread, constituting an accessible platform consisting of single devices coupled with different sensors for data acquisition, such as GPS, compass, magnetometer, etc. Emerging technologies such as Augmented Reality (AR) enable the use of these devices as a software platform to complement the perception of our surrounding environment. Despite the fact that there are several AR applications focused on different aspects of Geosciences, a comprehensive fieldwork oriented system, which can be easily adapted to the community needs, is still missing. Based on a multidisciplinary work between professionals from Geosciences and Computer Sciences, we designed and developed a framework based AR system named ARGeo. The proposed system constitutes a complementing tool for the geologist's fieldwork carefully designed to be used in remote sites, using only the mobile device hardware and without the need of an internet connection for data acquisition. ARGeo complements the fieldwork observation and the precise recording of geological data needed, e.g., for the interpretation of folded structures. It allows the user to register geo-tagged information such as points of interests (i.e. references on the field or the fold itself) using natural interactions. In addition, it provides two novel features: an interactive dipping plane measurement from distant points of view and a back-in-time visualization of previous recorded positions, in order to help understanding the spatial relations among the recorded samples. Pilot studies were conducted with domain experts of geology and they agree on the potential of the system. Results point out that ARGeo naturally integrates mobile devices and AR to the geologists' fieldwork. Its possible extension with new features to the Geosciences as well as to other application fields is detailed.}
}
@incollection{PREIM2014625,
title = {Chapter 18 - Image-Guided Surgery and Augmented Reality},
editor = {Bernhard Preim and Charl Botha},
booktitle = {Visual Computing for Medicine (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {625-663},
year = {2014},
isbn = {978-0-12-415873-3},
doi = {https://doi.org/10.1016/B978-0-12-415873-3.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158733000183},
author = {Bernhard Preim and Charl Botha},
keywords = {Intraoperative registration, Calibration, Tracking, Intraoperative visualization, Head-mounted display, Augmented microscope, Augmented endoscope, Projection-based augmented reality, Workflow analysis, Validation},
abstract = {Surgeons and interventional radiologists benefit from intraoperative guidance to deliver the treatment precisely, as it was planned. Printouts from a planning system are often of limited value, since they are hard to translate to the intraoperative situation. This chapter focuses on techniques that are used during an intervention. Hence, they are called intraoperative techniques. In intraoperative visualization, a lot of research is motivated by the challenges of minimally-invasive surgery and interventions, such as needle insertion, laparoscopic and endoscopic interventions. In particular, we introduce augmented reality (AR) techniques where relevant information from preoperative images is superimposed with live images from surgery. Metaphorically, many people refer to this technology as X-ray vision, i.e., the surgeon is able to see through the skin or an organ and observes the operative site before he or she actually arrives there. Such technologies were introduced in the 1990s in industrial applications, such as the assembly and maintenance of aircrafts and cars. Workers were supported by placing blueprints onto surfaces. In a similar way, planned resection lines, surgical targets or risk structures may be displayed along with organ surfaces, the skull or other anatomical structures. Thus, the target regions, e.g., a deep-seated tumor, may be reached faster with reduced risks of harming critical structures. Thus, implants in total knee replacement, hip or shoulder replacement may be fitted more accurately and, hopefully, the rate of revision surgery is reduced. In very difficult cases, e.g., a large deep-seated tumor close to vital structures, navigation and intraoperative visualization may even shift the border of operability. Major application areas are ENT and neurosurgery as well as orthopedics. Intraoperative visualization requires an appropriate dataset of the patient, preprocessing, e.g., segmentations, a sufficiently accurate registration procedure that maps the patient’s dataset to the patient himself. Once this registration is obtained and carefully validated, visual output from the dataset and surgical instruments can be mapped to the patient. These instruments need to be tracked by a navigation system. Thus, surgeons can locate instruments on the patient (with their eyes) and in the dataset (through the registration mapping). This setup with a direct correlation between the patient’s dataset and the patient is referred to as image-guided surgery (IGS). We also want to make the reader aware of potential and real drawbacks which are due to a complex processing chain that may involve significant errors. Moreover, the use of the underlying technology is challenging and the necessary setup times are often significant.}
}
@article{LI2023102471,
title = {An AR-assisted Deep Reinforcement Learning-based approach towards mutual-cognitive safe human-robot interaction},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102471},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001533},
author = {Chengxi Li and Pai Zheng and Yue Yin and Yat Ming Pang and Shengzeng Huo},
keywords = {Smart manufacturing, Human robot interaction, Augmented reality, Deep reinforcement learning, Manufacturing safety},
abstract = {With the emergence of Industry 5.0, the human-centric manufacturing paradigm requires manufacturing equipment (robots, etc.) interactively assist human workers to deal with dynamic and complex production tasks. To achieve symbiotic human–robot interaction (HRI), the safety issue serves as a prerequisite foundation. Regarding the growing individualized demand of manufacturing tasks, the conventional rule-based safe HRI measures could not well address the safety requirements due to inflexibility and lacking synergy. To fill the gap, this work proposes a mutual-cognitive safe HRI approach including worker visual augmentation, robot velocity control, Digital Twin-enabled motion preview and collision detection, and Deep Reinforcement Learning-based robot collision avoidance motion planning in the Augmented Reality-assisted manner. Finally, the feasibility of the system design and the performance of the proposed approach are validated by establishing and executing the prototype HRI system in a practical scene.}
}
@article{IMBERT2013364,
title = {Adding Physical Properties to 3D Models in Augmented Reality for Realistic Interactions Experiments},
journal = {Procedia Computer Science},
volume = {25},
pages = {364-369},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012490},
author = {Nicolas Imbert and Frederic Vignat and Charlee Kaewrat and Poonpong Boonbrahm},
keywords = {Augmented Reality, Physical Properties, Realistic Interaction},
abstract = {Augmented Reality is the combination of virtual objects (created by computer i.e. video, texts or 3D computer models) overlay on top of real world image. Applications of Augmented Reality can be ranged from advertising, edutainment, education, engineering, medicine to industrial manufacturing. In basic applications, like in advertisement or games, users only see the actions or interact with part of the screen designed for initiate some actions. In order to make users have realistic experiences, the interaction amongst virtual objects in Augmented Reality must be restricted to the law of Physics. Virtual objects can have their own dimensions, volumes or weights. When interaction between virtual objects occurred, the collision for example, they should not penetrate each other. The objects will react to each other by the law of Physics. With this concept, all kinds of experiments can be tested or practiced without spending a lot of fortunes with the real setup ranging from simple science experiment, medical training or even assembly process of equipment. In this research, Unity 3D game engine is used on Vuforia platform. Unity is a fully integrated development engine for creating games and other interactive 3D content and Vuforia platform make it possible to write a single native application that runs on almost all smartphones and tablets. To test the concept, 8 pieces of virtual 3D puzzle modules were created using 8 markers. Each virtual module was assigned with physical properties such dimensions, shapes and positions. When assemble the puzzle, each piece of the marker must be able to move around so that the virtual modules can fit to each other. By lifting and rotating the markers, the virtual module will snap with the other proper virtual part, forming virtual 3D puzzle. The virtual module will not penetrate each other because they have their own territory due to their dimensions. With this experiment, users will have a realistic feeling on assembling the virtual model. The concept can be implemented for experiments that are dangerous or expensive to setup. Experiments related to interaction between objects such as physics and chemistry experiments, engineering and medical training are the main targets for using this kind of technology.}
}
@article{YEH201830,
title = {A multiplayer online car racing virtual-reality game based on internet of brains},
journal = {Journal of Systems Architecture},
volume = {89},
pages = {30-40},
year = {2018},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118301619},
author = {Shih-Ching Yeh and Chung-Lin Hou and Wei-Hao Peng and Zhen-Zhan Wei and Shiuan Huang and Edward Yu-Chen Kung and Longsong Lin and Yi-Hung Liu},
keywords = {Brain-computer interface, Virtual reality, EEG, Mental arithmetic, Concentration, Car racing game},
abstract = {Development of brain-computer interface (BCI)-controlled virtual reality (VR) games has received increasing attention. Yet, the up-to-date BCI-VR systems were still based on one single BCI and a virtual environment (VE). In this paper, we propose and implement a novel BCI-controlled VR (BCI-VR) game based on a structure of internet of brains (IoB) allowing multiple players from different sites to play a car racing game online. Electroencephalographic (EEG) and electromyographic (EMG) signals from different sites’ BCIs are uploaded to a high-performance cloud server where the car-controlled algorithms are performed. During the online car racing period, the players mentally control the speeds of their chosen cars by means of concentration, and the concentration level can be adjusted by performing a mental arithmetic (MA) task with different levels of difficulty. Two linear and two nonlinear EEG features, including theta band power (BP), beta BP, Higuchi's fractal dimension (HFD), and Katz's FD (KFD), are used to transform the concentration level to speeds of four different cars. The players can also sensitively trigger the car in the VE to jump by performing a slight teeth-gritting task to generate easy-to-detect EMG signals. Six subjects participated in this study to test the performance of the proposed hybrid (EEG plus EMG) BCI-VR car racing game. The results indicate that theta BP and HFD are more sensitive to the MA-induced concentration in comparison with beta BP and KFD. Through the test of online car racing game, the results also demonstrated the feasibility that different players play the game in the same VE through multiple BCI control at different sites. More importantly, our BCI-VR implementation has a high usability (only two electrodes are required; calibration needs only 64 s) and high feasibility (high average scores of the control, sensory, and distraction factors in a 30-item post-experimental presence questionnaire).}
}
@article{DESHPANDE2018760,
title = {The effects of augmented reality on improving spatial problem solving for object assembly},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {760-775},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618300971},
author = {Abhiraj Deshpande and Inki Kim},
keywords = {Augmented reality, Spatial problem solving, Mental representation, Ready-to-assemble furniture},
abstract = {The capability of Augmented Reality (AR) technology to track and visualize relations of objects in space has led to diverse industry applications to support complex engineering tasks. Object assembly is one of them. For an AR aid to support Ready-to-Assemble (RTA) furniture particularly, the challenge is to effectively design the visual features and mode of interaction, so that the first-time users can quickly conceive spatial relations of its parts. However, AR developers and engineers do not have sufficient guidelines to achieve such performance-driven goals. The scientific evidence and account of how one could cognitively benefit in object assembly can be useful to guide them. This experimental research developed an Augmented Reality (AR) application on the Microsoft HoloLens™ headset, and tested it on the first-time users of RTA furniture. The controlled experiments and behavioral analyses of fourteen participants in working out the two RTA furniture with different assembly complexity showed that, the application was effective to improve spatial problem-solving abilities. Especially, the positive effects of the AR-based supports stood out against the assembly of higher complexity. The findings have implications for the information design of advanced AR applications to support assembly tasks with high demands of spatial knowledge. In addition, the extensive analyses of the participants’ performance, behaviors, cognitive workload, and subjective responses helped identify usability issues with the Microsoft HoloLens™.}
}
@article{KIM2020101349,
title = {The impact of innovation and gratification on authentic experience, subjective well-being, and behavioral intention in tourism virtual reality: The moderating role of technology readiness},
journal = {Telematics and Informatics},
volume = {49},
pages = {101349},
year = {2020},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101349},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320300083},
author = {Myung Ja Kim and Choong-Ki Lee and Michael W. Preis},
keywords = {Virtual reality (VR), Innovation diffusion, Uses and gratifications, Technology readiness, Subjective well-being, Tourism},
abstract = {Despite the increasing amount of attention paid to virtual reality (VR) tourism and the rising importance of VR tourism, a theoretically integrated model of behavior has not been developed. To fill this void, we build and test a framework based on both innovation diffusion and uses and gratifications theories to explain why people participate in VR tourism. The moderating role of VR tourists’ technology readiness (optimism and innovativeness) between subjective well-being and behavioral intention is also examined. Results demonstrate that authentic experience and subjective well-being are affected by simplicity, benefit, compatibility (attributes of innovation diffusion), informativeness, social interactivity, and playfulness (uses and gratifications attributes). Behavioral intention is more positively influenced by subjective well-being than by authentic experience. The moderating role of technology readiness between subjective well-being and behavioral intention is stronger in individuals with high optimism and innovativeness than their counterparts with low optimism and innovativeness.}
}