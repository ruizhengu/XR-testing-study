@article{WIERZBICKI2005295,
title = {Erratum to: “Validation of dynamic heart models obtained using non-linear registration for virtual reality training, planning, and guidance of minimally invasive cardiac surgeries” [Medical Image Analysis 8 (2004) 387–401]},
journal = {Medical Image Analysis},
volume = {9},
number = {3},
pages = {295},
year = {2005},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2004.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841504000957},
author = {Marcin Wierzbicki and Maria Drangova and Gerard Guiraudon and Terry Peters}
}
@article{NORI201572,
title = {The virtual reality Walking Corsi Test},
journal = {Computers in Human Behavior},
volume = {48},
pages = {72-77},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215000497},
author = {Raffaella Nori and Laura Piccardi and Matteo Migliori and Antonella Guidazzoli and Francesca Frasca and Daniele {De Luca} and Fiorella Giusberti},
keywords = {Corsi Test, Human navigation, Virtual reality, Gender differences, Topographical memory},
abstract = {We compared the performance of men and women on a modified and a virtual version of the Walking Corsi Test (WalCT). The WalCT is a large version of the Corsi Block-Tapping Task that requires learning a path and then recalling it. It has been proved to measure topographical memory. The main aim of the study was to compare the effects of real and virtual reality learning environment on the acquisition of spatial information. A secondary aim was to detect the presence of gender-related differences in the two environments. Specifically, we expected that men would perform better in both environments. Eighty college students (40 men) were assigned to real or virtual environments and had to learn four different paths. Gender differences emerged in both environments: men outperformed women in both the real and the virtual reality environment. Results did not show difference in virtual and real environment supporting the equivalence of the two tests to measure topographical memory. Gender-related differences are interpreted in light of Coluccia and Louse’s model, according to which men outperform women when tasks require a high visuo-spatial working memory load and the different spatial strategy used by men and women.}
}
@article{BRAN20201487,
title = {In-vehicle Visualization of Data by means of Augmented Reality},
journal = {Procedia Computer Science},
volume = {176},
pages = {1487-1496},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.159},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320597},
author = {Emanuela Bran and Dragos Florin Sburlan and Dorin Mircea Popovici and Crenguta Mădălina Puchianu and Elena Băutu},
keywords = {In-vehicle System, Augmented Reality, Visualization, Multi-modal Interaction},
abstract = {Recently, in-vehicle systems have considered integrating Augmented Reality (AR) to serve various purposes. The information offered to the driver by an in-vehicle system should be presented such that it can be easily interpreted without negatively affecting the driver’s attention for the driving task. The interface must be accessible, visible and easily legible, while also being symbolically relevant. In this paper, we explain the requirements and design decisions that lead to the proposal of our current AR-based visualization component of our in-vehicle system. The 3D AR visualization scheme we propose enhances the automatic adaptive notification filtering module of the in-vehicle system previously proposed. We integrated voice and gesture commands in order to manage common actions during driving, such as navigation system functions, control of the radio or of the air conditioner, or generic actions regarding incoming notifications. The system is tested in an artificial setting in the laboratory, using a simulated prototype vehicle deck and windshield.}
}
@article{MOURTZIS2021103383,
title = {Integrated and adaptive AR maintenance and shop-floor rescheduling},
journal = {Computers in Industry},
volume = {125},
pages = {103383},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103383},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306175},
author = {Dimitris Mourtzis and John Angelopoulos and Vasilios Zogopoulos},
keywords = {Maintenance, Scheduling, Augmented reality, Machine monitoring, Industry 4.0},
abstract = {By virtue of the Digitalization of Manufacturing Systems, the importance of interconnecting multiple digital systems is becoming a necessity. By extension, the need for connecting the shop-floor technician to the rest of the production system is highlighted more than ever. Internet and Communication technologies (ICT) may connect the technician with data collected from the machine sensors, knowledge repositories and integrated production software. Mobile devices offer highly usable interfaces that can be exploited to involve the operator in this process, digitalize their inputs and also visualize machine information on top of the physical system through Augmented Reality (AR). Towards that direction, an application that allows the operator to monitor machine status and call event-driven AR remote machine maintenance and rescheduling based on maintenance time estimation is developed. The framework is tested and validated in a laboratory-base machine shop.}
}
@article{CAO2015115,
title = {Feature selection for low bit rate mobile augmented reality applications},
journal = {Signal Processing: Image Communication},
volume = {36},
pages = {115-126},
year = {2015},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2015.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0923596515001010},
author = {Yi Cao and Christian Ritz and Raad Raad},
keywords = {Feature selection, Mobile augmented reality, Retrieval accuracy, Low bit-rate transmission},
abstract = {Mobile augmented reality applications rely on automatically matching a captured visual scene to an image in a database. This is typically achieved by deriving a set of features from the captured image, transmitting them through a network and then matching with features derived from a database of reference images. A fundamental problem is to select as few and robust features as possible such that the matching accuracy is invariant to distortions caused by camera capture whilst minimizing the bit rate required for their transmission. In this paper, novel feature selection methods are proposed, based on the entropy of the image content, entropy of extracted features and the Discrete Cosine Transformation (DCT) coefficients. The methods proposed in the descriptor domain and DCT domain achieve better retrieval accuracy under low bit rate transmission than state-of-the-art peak based feature selection used within the MPEG-7 Compact Descriptor for Visual Search (CDVS). The robustness of the proposed methods is evaluated under controlled single distortion and the retrieval performance is verified from image retrieval experiments and results for a realistic dataset with complex real world capturing distortion. Results show that the proposed method can improve the matching accuracy for various detectors. On the one side, feature selection can achieve low bit rate transmission, on the other side, when coping with distorted images, it can result in a higher matching accuracy than using all features. Hence, even if all the features can be transmitted in high transmission bandwidth scenarios, feature selection should still be applied to the distorted query image to ensure high matching accuracy.}
}
@article{LOUPESCANDE20141049,
title = {Needs’ elaboration between users, designers and project leaders: Analysis of a design process of a virtual reality-based software},
journal = {Information and Software Technology},
volume = {56},
number = {8},
pages = {1049-1061},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000895},
author = {Emilie Loup-Escande and Jean-Marie Burkhardt and Olivier Christmann and Simon Richir},
keywords = {Ergonomics, Design process, Emerging technologies, Needs},
abstract = {Context
The participation of users in the design process is recognized as a positive and a necessary element as artifacts suit their needs. Two complementary approaches of users’ involvement co-exist: the user-centered design and the participatory design. These approaches involve learning process from users to designers and vice versa. However, there has no research in design of virtual reality (VR)-based software dealing with how the elaboration of needs is actually distributed in time and among users, designers and project leaders, as well as how it is actually supported by tools and methods.
Objective
This paper aims to observe, in a real design project of a virtual reality-based software, how the various stakeholders (users, designers, project leaders) actually participate by sharing and pulling pieces of information from the process of needs elaboration, and how these contributions evolve throughout the decisions made in the course of the project.
Method
Our method, based on the observation of the practices in collective design, allows us to collect and analyze the relationship between each possible action (e.g., elicitation), each stakeholder who initiates these actions (e.g., users) and each phase of the design process (e.g., evaluation phase), and the dynamics of the construction of needs.
Results
Our results detail how the elicited needs are dealt with by designers, users and/or project leaders: (1) we show a strong contribution of users in the design, compared to others stakeholders, (2) among the needs elicited by users, most have been validated by the designers, (3) some elicited needs could have been firstly rejected and finally validated and implemented.
Conclusion
We identify the reasons which justify and explain our results confronting them to the literature. We underline the conditions have been satisfied in our study in order to involve effectively users in the design of emerging technologies.}
}
@article{SUKIRMAN2019247,
title = {Self-Evacuation Drills by Mobile Virtual Reality Application to Enhance Earthquake Preparedness},
journal = {Procedia Computer Science},
volume = {157},
pages = {247-254},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.164},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919310828},
author = { Sukirman and Reza Arif Wibisono and  Sujalwo},
keywords = {virtual reality, VR application, mobile VR, earthquake preparedness, self-evacuation drills},
abstract = {Traditional evacuation drills by self-rescue practice are a common method used to educate society in communities, companies or schools in Indonesia which involving participants in large numbers. However, the participants in this approach are not always committed and doing it seriously in some cases. Besides, the participants were also unable to feel the earthquake sensation because no shaking at all when practice evacuation drills. For this reason, it is necessary to design a tool that can be used to practice by themselves with no involving many people, but the sensation of shaking can be felt by the users, one way is using Virtual Reality (VR) technology. This study aims to develop VR-based applications that can create a sensation of shaking such as an earthquake and can be used individually. The method used in this study is initial research, development, testing and analyzing, and recommendations. The development carried out by Unity 3D software which is equipped with GoogleVR plugin. The testing was conducted on 14 students of SMK Batik Batik 1 Surakarta, Indonesia, whose average age of 16 years old and the habit of playing games is 2-3 hours per day on average. From the experiments conducted, 71% stated that earthquake simulations using VR applications are more interesting than animated video, because they can interact with virtual objects directly and the simulations are more realistic. Thus, it can be concluded that the VR application can be used for earthquake evacuation exercises independently, further hope that user preparedness will be better in dealing with natural disasters, especially earthquakes}
}
@article{HEINRICH2021210,
title = {Estimating depth information of vascular models: A comparative user study between a virtual reality and a desktop application},
journal = {Computers & Graphics},
volume = {98},
pages = {210-217},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001138},
author = {Florian Heinrich and Vikram Apilla and Kai Lawonn and Christian Hansen and Bernhard Preim and Monique Meuschke},
keywords = {Medical visualization, Virtual reality, Empirical user studies},
abstract = {Vascular structures are assessed, e.g., in tumor surgery to understand the influence of a planned resection on the vascular supply and venous drainage. The understanding of complex branching vascular structures may benefit from immersive virtual reality (VR) visualization. Therefore, the estimation of distance, depth and shape information is a crucial task to support diagnosis and therapy decisions. Depending on the visualization techniques used, perceptual issues can influence this process and may thus lead to false conclusions. Many studies were carried out to study depth perception for different variants of vessel visualization. However, these studies are restricted to desktop applications. Since VR exhibits specific perceptual problems, we aim at an understanding of the appropriateness of vessel visualization techniques in VR. Therefore, this paper presents a user study that investigates the effects of three commonly used visualization techniques on depth perception. The set of visualization techniques comprises Phong shading, pseudo-chromadepth and fog shading. An immersive VR setup of the study using a head-mounted display (HMD) was compared to a traditional desktop setup. Results suggest that depth judgments are less error-prone and more certain in VR than in desktop environments. Moreover, depth-enhancing visualization techniques had greater effects in the desktop study compared to the VR study.}
}
@article{BROSCH2021100078,
title = {Model-based segmentation using neural network-based boundary detectors: Application to prostate and heart segmentation in MR images},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100078},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000396},
author = {Tom Brosch and Jochen Peters and Alexandra Groth and Frank Michael Weber and Jürgen Weese},
keywords = {Model-based segmentation, Boundary detection, Neural networks, Prostate, Heart, MRI},
abstract = {Model-based segmentation (MBS) is a variant of active surfaces and active shape models that has successfully been used to segment anatomical structures such as the heart or the brain. We propose to integrate neural networks (NNs) into MBS for boundary detection. We formulate boundary detection as a regression task and use a NN to predict the distances between a surface mesh and the corresponding boundary points. The proposed approach has been applied to two tasks — prostate segmentation in MR images and the segmentation of the left and right ventricle in MR images. For the first task, data from the Prostate MR Image Segmentation 2012 (PROMISE12) challenge has been used. For the second task, a diverse database with cardiac MR images from six clinical sites has been used. We compare the results to the popular U-net approaches using the nnU-net implementation that is among the top performing segmentation algorithms in various challenges. In cross-validation experiments, the mean Dice scores are very similar and no statistically significant difference is observed. On the PROMISE12 test set, nnU-net Dice scores are significantly better. This is achieved by using an ensemble of 2D and 3D U-nets to generate the final segmentation, a concept that may also be adapted to NN-based boundary detection in the future. While the U-net provides a voxel labeling, our approach provides a 3D surface mesh with pre-defined mesh topology, establishes correspondences with respect to the reference mesh, avoids isolated falsely segmented regions and ensures proper connectivity of different regions.}
}
@article{ZHANG2021101259,
title = {Investigating the influence of route turning angle on compliance behaviors and evacuation performance in a virtual-reality-based experiment},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101259},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101259},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000148},
author = {Ming Zhang and Jinjing Ke and Liyang Tong and Xiaowei Luo},
keywords = {Immersive virtual reality, Evacuation in buildings, Route turning angle, Behavioral compliance, Evacuation performance},
abstract = {Route turning is one of the most essential and ubiquitous physical features in the complex building environment. Under the influence of route turning, evacuees’ approaching perspective to an emergency sign could vary, affecting their information perception and behavioral compliance during the evacuation. Although conventional simulation methods assess the effectiveness of the emergency sign in the visible region, they fail to consider evacuees’ wayfinding behaviors and interaction with the emergency sign. It remains unclear whether the route turning angle affects evacuees’ compliance for detecting and responding to the emergency sign. To investigate such an influence, a virtual-reality-based method for assessing human evacuation behaviors in building fire evacuations was proposed. In this study, two evacuation routes with different turning angles in a shopping mall were created and implemented in a virtual-reality environment, and 67 subjects participated in the immersive virtual-reality-based experiment. All participants took the two routes to find the nearest exit for evacuation in a fire event, aiming to evaluate the effect of the route turning angle on the evacuation process. The participants were asked to complete a questionnaire at the end of the experiment. Next, statistical analyses were conducted on evacuation results, information perception, and evacuation performance of the participants. The results indicated the route turning angle significantly affected participants' behavioral compliance with emergency signs. The results also suggested the route turning angle was influential on participants’ information perception and evacuation performance. Besides, a significant effect on rotation change, wayfinding pause, and speed deviation were observed. This study validates the effectiveness of investigating evacuees’ interaction with emergency signs using virtual-reality technology and has potential implications for complex building path planning and evacuation simulation modeling.}
}
@article{FENG2020101118,
title = {An immersive virtual reality serious game to enhance earthquake behavioral responses and post-earthquake evacuation preparedness in buildings},
journal = {Advanced Engineering Informatics},
volume = {45},
pages = {101118},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101118},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300872},
author = {Zhenan Feng and Vicente A. González and Robert Amor and Michael Spearpoint and Jared Thomas and Rafael Sacks and Ruggiero Lovreglio and Guillermo Cabrera-Guerrero},
keywords = {Immersive virtual reality, Serious games, Earthquake emergencies, Evacuation training},
abstract = {Enhancing the earthquake behavioral responses and post-earthquake evacuation preparedness of building occupants is beneficial to increasing their chances of survival and reducing casualties after the mainshock of an earthquake. Traditionally, training approaches such as seminars, posters, videos or drills are applied to enhance preparedness. However, they are not highly engaging and have limited sensory capabilities to mimic life-threatening scenarios for the purpose of training potential participants. Immersive Virtual Reality (IVR) and Serious Games (SG) as innovative digital technologies can be used to create training tools to overcome these limitations. In this study, we propose an IVR SG-based training system to improve earthquake behavioral responses and post-earthquake evacuation preparedness. Auckland City Hospital was chosen as a case study to test our IVR SG training system. A set of training objectives based on best evacuation practice has been identified and embedded into several training scenarios of the IVR SG. Hospital staff (healthcare and administrative professionals) and visitors were recruited as participants to be exposed to these training scenarios. Participants’ preparedness has been measured along two dimensions: 1) Knowledge about best evacuation practice; 2) Self-efficacy in dealing with earthquake emergencies. Assessment results showed that there was a significant knowledge and self-efficacy increase after the training. In addition, participants acknowledged that it was easy, helpful, and engaging to learn best evacuation practice knowledge through the IVR SG training system.}
}
@article{BOGES202012,
title = {Virtual reality framework for editing and exploring medial axis representations of nanometric scale neural structures},
journal = {Computers & Graphics},
volume = {91},
pages = {12-24},
year = {2020},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2020.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0097849320300789},
author = {Daniya Boges and Marco Agus and Ronell Sicat and Pierre J. Magistretti and Markus Hadwiger and Corrado Calì},
keywords = {Ultrastructural analysis, Medial axis representation, Immersive environments, Virtual reality in neuroscience},
abstract = {We present a novel virtual reality (VR) based framework for the exploratory analysis of nanoscale 3D reconstructions of cellular structures acquired from rodent brain samples through serial electron microscopy. The system is specifically targeted on medial axis representations (skeletons) of branched and tubular structures of cellular shapes, and it is designed for providing to domain scientists: i) effective and fast semi-automatic interfaces for tracing skeletons directly on surface-based representations of cells and structures, ii) fast tools for proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) natural methods for interactive exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. Neuroscientists currently use the system for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat. The framework runs in a standard PC and has been tested on two different display and interaction setups: PC-tethered stereoscopic head-mounted display (HMD) with 3D controllers and tracking sensors, and a large display wall with a standard gamepad controller. We report on a user study that we carried out for analyzing user performance on different tasks using these two setups.}
}
@article{RODRIGUES2022106831,
title = {Usability, acceptance, and educational usefulness study of a new haptic operative dentistry virtual reality simulator},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106831},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106831},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722002139},
author = {Pedro Rodrigues and Artur Esteves and João Botelho and Vanessa Machado and Carlos Zagalo and Ezequiel Roberto Zorzal and José João Mendes and Daniel Simões Lopes},
keywords = {Operative dentistry, Virtual reality, Tooth drilling, Haptic feedback, 3D Sound, Simulator},
abstract = {Background
Dental preclinical training has been traditionally centered onverbal instructions and subsequent execution on phantom heads and plastic training models. However, these present present limitations. Virtual Reality (VR) and haptic simulators have been proposed with promising results and advantages and have showed usefullness in the preclinical training environment. We designed DENTIFY, a multimodal immersive simulator to assist Operative Dentistry learning, which exposes the user to different virtual clinical scenarios while operating a haptic pen to simulate dental drilling.
Objective
The main objective is to assess DENTIFY’s usability, acceptance, and educational usefulness to dentists, in order to make the proper changes and, subsequently, to test DENTIFY with undergraduate preclinical dental students.
Methods
DENTIFY combines an immersive head mounted VR display, a haptic pen in which the pen itself has been replaced by a 3D printed model of a dental turbine and a controller with buttons to adjust and select the scenario of the simulation, along with 3D sounds of real dental drilling. The user’s dominant hand operated the virtual turbine on the VR-created scenario, while the non-dominant hand is used to activate the simulator and case selection. The simulation sessions occurred in a controlled virtual environment. We evaluated DENTIFY’s usability and acceptance over the course of 13 training sessions with dental professionals, after the users performed a drilling task in virtual dental tissues.
Results
The conducted user acceptance indicates that DENTIFY shows potencial enhancing learning in operative dentistry as it promotes self-evaluation and multimodal immersion on the dental drilling experience.
Conclusions
DENTIFY presented significant usability and acceptance from trained dentists. This tool showed to have teaching and learning (hence, pedagogical) potential in operative dentistry.}
}
@article{HAO2024100669,
title = {Action capture and VR interactive system for online experimental teaching},
journal = {Entertainment Computing},
volume = {50},
pages = {100669},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000375},
author = {Gang Hao and Ling Cao},
keywords = {Online teaching, Action capture, Virtual reality, Feature localization, Joint judgment, Face detection},
abstract = {The development of technology has intensified people’s interest in virtual reality technology. Virtual reality technology is also widely used in fields such as teaching and healthcare. To improve the real-time and operability of virtual reality interaction systems, a motion capture and virtual reality interaction system design for online experimental teaching is proposed. The active apparent model is used for facial feature point localization, and the joint capture method is improved by combining threshold segmentation and forward kinematics algorithms. Experimental data confirms that compared to Kinect method, the improved hand joint detection method has higher joint positioning accuracy, with a loss rate of less than 40 % for hand joints. The average judgment accuracy of the six facial expression tests is 82 %, 81 %, 79 %, 78 %, 80 %, and 81 %, respectively. The comprehensive recognition accuracy of facial expression recognition is 80.17 %. The read, write, and update times for each frame of the virtual reality interaction system are 102.6 ms and 427 ms, respectively. The system memory usage is 920.5 M, and the CPU and GPU usage rates are 25.2 % and 4.4 %, respectively. The system can run smoothly. The virtual reality system can run continuously for 24 h, with strong operability and low difficulty in expanding system functions, achieving design goals.}
}
@article{LANIER201970,
title = {Virtual reality check: Statistical power, reported results, and the validity of research on the psychology of virtual reality and immersive environments},
journal = {Computers in Human Behavior},
volume = {100},
pages = {70-78},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302419},
author = {Madison Lanier and T. Franklin Waddell and Malte Elson and Daniel J. Tamul and James D. Ivory and Andrew Przybylski},
keywords = {Virtual reality, meta-Science, Statistical power, Questionable research practices, Quantitative methodology},
abstract = {Virtual reality (VR) is a popular subject of scientific study across a variety of academic fields. In the present study we evaluate methodological trends in behavioral research on VR with respect to data collection practices, statistical reporting, and data availability. In line with this goal, we conducted a meta-scientific analysis of 61 articles encompassing a total of 1122 statistical tests and highlight three emergent trends that inform our understanding of past and future studies focused on VR. Conclusions from analysis of the data include a high incidence of errors in statistical reporting, and a general lack of transparency with respect to the availability of study data. Transparency in data analysis, increased statistical power, and more careful reporting of statistical outcomes are suggested to heighten methodological rigor and improve reproducibility in the field of VR research.}
}
@article{WIERZBICKI2004387,
title = {Validation of dynamic heart models obtained using non-linear registration for virtual reality training, planning, and guidance of minimally invasive cardiac surgeries},
journal = {Medical Image Analysis},
volume = {8},
number = {3},
pages = {387-401},
year = {2004},
note = {Medical Image Computing and Computer-Assisted Intervention - MICCAI 2003},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2004.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S1361841504000386},
author = {Marcin Wierzbicki and Maria Drangova and Gerard Guiraudon and Terry Peters},
keywords = {Deformable models, Non-linear image registration, Registration validation, Pre-interventional planning, Image-guided surgery},
abstract = {Current minimally invasive techniques for beating heart surgery are associated with three major limitations: the shortage of realistic and safe training methods, the process of selecting port locations for optimal target coverage from X-rays and angiograms, and the sole use of the endoscope for instrument navigation in a dynamic and confined 3D environment. To supplement the current surgery training, planning and guidance methods, we continue to develop our Virtual Cardiac Surgery Planning environment (VCSP) – a virtual reality, patient-specific, thoracic cavity model derived from 3D pre-procedural images. In this work, we create and validate dynamic models of the heart and its components. A static model is first generated by segmenting one of the image frames in a given 4D data set. The dynamics of this model are then extracted from the remaining image frames using a non-linear, intensity-based registration algorithm with a choice of six different similarity metrics. The algorithm is validated on an artificial CT image set created using an excised porcine heart, on CT images of canine subjects, and on MR images of human volunteers. We found that with the appropriate choice of similarity metric, our algorithm extracts the motion of the epicardial surface in CT images, or of the myocardium, right atrium, right ventricle, aorta, left atrium, pulmonary arteries, vena cava and epicardial surface in MR images, with a root mean square error in the 1 mm range. These results indicate that our method of modeling the motion of the heart is easily adaptable and sufficiently accurate to meet the requirements for reliable cardiac surgery training, planning, and guidance.}
}
@article{AJORLOO2020365,
title = {MinDFul: Using double links for stabilizing mmWave wireless channels for application to autonomous vehicles and augmented reality},
journal = {Procedia Computer Science},
volume = {175},
pages = {365-372},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317336},
author = {Hossein Ajorloo and Cormac J. Sreenan and Roberto Bomfin and Martin Danneberg and Gerhard Fettweis},
keywords = {Millimetre wave (mmWave), Scheduling algorithm, Virtual reality (VR) games, Autonomous vehicles, Wireless communication, Beamsteering protocol, Steerable antenna array, Real-time system, Experimental testbed, 5G},
abstract = {Applications that require short-range ultra-high bitrate communication, such as cable removal in virtual reality games and communication between autonomous vehicles, are examining solutions such as millimetre wave wireless (mmWave). When using mmWave, steerable directional antennas are used to mitigate the severe signal power attenuation common with high frequencies. Nonetheless, even small movements in the user device can cause a sudden drop in data-rate down (even to 0 bits/s) making mmWave channels unstable and unusable. To make the channel more stable for the aforementioned applications, which are vulnerable due to frequent blockages and fast movement, we designed and developed a robust solution based on a double link mmWave system. We duplicate the radio transceivers (RT) of a user device (UD) to increase the probability of finding line of sight to an access point (AP) representing the other side of the communication channel. The AP selects one RT of the UD for communication, based on continuous measurement of quality compared to the channel of the other RT. This concept was implemented in a laboratory environment and evaluated using a series of controlled experiments. The experiments serve to validate that using double links is feasible, and is considerably more robust and it can double the link utilization, compared to only using one mmWave link. These results show great promise for the concept, by demonstrating that using multiple mmWave links yields ultra-high bit-rate wireless communication with no disruption, even in the presence of blockages and mobility.}
}
@article{BRUNNSTROM2020116005,
title = {Latency impact on Quality of Experience in a virtual reality simulator for remote control of machines},
journal = {Signal Processing: Image Communication},
volume = {89},
pages = {116005},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.116005},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520301648},
author = {Kjell Brunnström and Elijs Dima and Tahir Qureshi and Mathias Johanson and Mattias Andersson and Mårten Sjöström},
keywords = {Quality of Experience (QoE), Virtual reality, Latency, Head-Mounted Displays (HMD), Forestry crane},
abstract = {In this article, we have investigated a VR simulator of a forestry crane used for loading logs onto a truck. We have mainly studied the Quality of Experience (QoE) aspects that may be relevant for task completion, and whether there are any discomfort related symptoms experienced during the task execution. QoE experiments were designed to capture the general subjective experience of using the simulator, and to study task performance. The focus was to study the effects of latency on the subjective experience, with regards to delays in the crane control interface. Subjective studies were performed with controlled delays added to the display update and hand controller (joystick) signals. The added delays ranged from 0 to 30 ms for the display update, and from 0 to 800 ms for the hand controller. We found a strong effect on latency in the display update and a significant negative effect for 800 ms added delay on latency in the hand controller (in total approx. 880 ms latency including the system delay). The Simulator Sickness Questionnaire (SSQ) gave significantly higher scores after the experiment compared to before the experiment, but a majority of the participants reported experiencing only minor symptoms. Some test subjects ceased the test before finishing due to their symptoms, particularly due to the added latency in the display update.}
}
@article{PUYUELO2013171,
title = {Experiencing Augmented Reality as an Accessibility Resource in the UNESCO Heritage Site Called “La Lonja”, Valencia},
journal = {Procedia Computer Science},
volume = {25},
pages = {171-178},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S187705091301226X},
author = {Marina Puyuelo and José Luís Higón and Lola Merino and Manuel Contero},
keywords = {Accessibility, Heritage sites, Vitual Reality, Augmented Reality, Interactivity, Interface design},
abstract = {This paper presents the design of an augmented reality application for the Gothic Silk Market Building called the “Lonja de la Seda” in Valencia (UNESCO Monument Heritage Site, 1996) and the results of the experiments carried out “in-situ” to observe its usability as an accessibility resource. The objective of this project has been to use and validate augmented reality (AR) as a tool to increase the accessibility to the architectural elements of this monumental setting. The AR application aims to resolve the perception issues derived from poor lighting, the distance in relation to the multiple details, access, etc. 145 individuals of different ages and diverse origin, making the visit to the monument with or without guide used the AR application. When visitors had used the application as they wished, interviews were conducted individually in order to receive their feedback about the AR experience. Users enjoyed identifying and selecting the motifs they were going to visualize with the AR tool. In general, the experience was very positively evaluated by participants, promoting a favourable and renewed image of the monument}
}
@article{LOPEZCHAVEZ2020104226,
title = {A comparative case study of 2D, 3D and immersive-virtual-reality applications for healthcare education},
journal = {International Journal of Medical Informatics},
volume = {141},
pages = {104226},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104226},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620304330},
author = {Omar {López Chávez} and Luis-Felipe Rodríguez and J. Octavio Gutierrez-Garcia},
keywords = {Healthcare education, Virtual reality, Comparative case study},
abstract = {Background and objective
The workings of medical educational tools are implemented using a myriad of approaches ranging from presenting static content to immersing students in gamified virtual-reality environments. The objective of this paper is to explore whether and how different approaches for designing medical educational tools affect students’ learning performance.
Materials and methods
Four versions of an educational tool for the study of clinical cases were implemented: a 2D version, a gamified 2D version, a gamified 3D version, and a gamified immersive-virtual-reality version. All complying with the same functional requirements. Each version was used and evaluated by an independent group of students. The participants (n = 78) evaluated the applications regarding usefulness, usability, and gamification. Afterward, the students took an exam to assess the retention of information on the clinical cases presented.
Results
One-sample Wilcoxon signed-rank tests confirmed that the participants perceived that it was at least quite likely that gamification helped improved their learning. In addition, based on the participants’ perception, the gamification of the immersive-virtual-reality version helped the most to improve their learning performance in comparison with the gamified 2D and 3D versions.
Conclusions
Regardless of whether different versions of a medical educational tool (complying with the same functional requirements) are perceived as equally useful and usable, the design approach (either 2D, 3D, or immersive-virtual-reality with or without gamification) affects students’ retention of information on clinical cases.}
}
@article{SAMPAIO2016894,
title = {Pedagogical Strategies for the Integration of Augmented Reality in ICT Teaching and Learning Processes},
journal = {Procedia Computer Science},
volume = {100},
pages = {894-899},
year = {2016},
note = {International Conference on ENTERprise Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.240},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324097},
author = {Daniel Sampaio and Pedro Almeida},
keywords = {Information and Communication Technologies, Educational Technologies, Augmented Reality, Digital skills, Traching/Learning strategies, Google Cardbords},
abstract = {The technologies that are being gradually introduced in educational contexts enable students to diversify the ways for knowledge building. However, the exploitation of new technologies in the classroom is always a challenge for all interveners in the educational process. The arrival of a new technology, as is the case of augmented reality devices, captures the teachers’ attention. It creates the expectation that its uses may provide students with new ways to interact, new possibilities of collaboration between students and between students and teachers and potentially an increase in the motivation for learning. In this context we may find some examples of this technological introduction in learning scenarios. But the referred expectations and the most suitable strategies for its use need to be validated and new ones to be explored. This is the purpose of this research. This project is being carried with sixty-two students from the 8th grade in the Information and Communication Technologies subject. It follows an action-research methodology and for the implementation of the several research cycles that follows this methodology different prototypes for students to use in classroom were created and are being evaluated. The prototypes range from simple technological integrations to more complex ones with the introduction of an augmented reality system. The study aims to identify and explore the pedagogical strategies by evaluating them in real teaching and learning scenarios, particularly in terms of competences developed and student motivation levels, as well as on the ways of integrating different devices for augmented reality. The preliminary results show that students understand the support and contents provided by the prototypes and feel higher motivation when using it in the assignments’.}
}
@article{NOLIN2016327,
title = {ClinicaVR: Classroom-CPT: A virtual reality tool for assessing attention and inhibition in children and adolescents},
journal = {Computers in Human Behavior},
volume = {59},
pages = {327-333},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216300759},
author = {Pierre Nolin and Annie Stipanicic and Mylène Henry and Yves Lachapelle and Dany Lussier-Desrochers and Albert “Skip” Rizzo and Philippe Allain},
keywords = {ClinicaVR, Classroom-CPT, Inhibition, Validity, Reliability, Children, Virtual reality, Virtual classroom},
abstract = {Having garnered interest both in clinic and research areas, the Virtual Classroom (Rizzo et al., 2000) assesses children's attention in a virtual context. The Digital MediaWorks team (www.dmw.ca) has evolved the original basic classroom concept over a number of iterations to form the ClinicaVR Suite containing the Classroom-CPT as one of its components. The present study has three aims: investigate certain validity and reliability aspects of the tool; examine the relationship between performance in the virtual test and the attendant sense of presence and cybersickness experienced by participants; assess potential effects of gender and age on performance in the test. The study was conducted with 102 children and adolescents from Grade 2 to Grade 10. All participants were enrolled in a regular school program. Results support both concurrent and construct validity as well as temporal stability of ClinicaVR: Classroom-Continuous Performance Test (CPT). Gender exerted no effect on performance, while age did. The test did not cause much cybersickness. We recommend ClinicaVR: Classroom-CPT as an assessment tool for selective and sustained attention, and inhibition, in clinic and research domains.}
}
@article{PRAKOSA2021104366,
title = {Impact of augmented-reality improvement in ablation catheter navigation as assessed by virtual-heart simulations of ventricular tachycardia ablation},
journal = {Computers in Biology and Medicine},
volume = {133},
pages = {104366},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104366},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001608},
author = {Adityo Prakosa and Michael K. Southworth and Jennifer N. {Avari Silva} and Jonathan R. Silva and Natalia A. Trayanova},
keywords = {Ablation, Ventricular tachycardia, Computer simulation, Augmented reality},
abstract = {Background
Recently, an augmented reality (AR) solution allows the physician to place the ablation catheter at the designated lesion site more accurately during cardiac electrophysiology studies. The improvement in navigation accuracy may positively affect ventricular tachycardia (VT) ablation termination, however assessment of this in the clinic would be difficult. Novel personalized virtual heart technology enables non-invasive identification of optimal lesion targets for infarct-related VT. This study aims to evaluate the potential impact of such catheter navigation accuracy improvement in virtual VT ablations.
Methods
2 MRI-based virtual hearts with 2 in silico induced VTs (VT 1, VT 2) were included. VTs were terminated with virtual “ground truth” endocardial ablation lesions. 106 navigation error values that were previously assessed in a clinical study evaluating the improvement of ablation catheter navigation accuracy guided with AR (53 with, 53 without) were used to displace the “ground truth” ablation targets. The corresponding ablations were simulated based on these errors and VT termination for each simulation was assessed.
Results
In 54 VT 1 ablation simulations, smaller error with AR significantly resulted in more VT termination (25) compared to the error without AR (16) (P < 0.01). In 52 VT 2 ablation simulations, no significant difference was observed from error with (11) and without AR (13) (P = 0.58). The substrate characteristic may impact the effect of improved accuracy to an improved VT termination.
Conclusion
Virtual heart shows that the increased catheter navigation accuracy provided by AR guidance can affect the VT termination.}
}
@article{ALSABBAG2022101473,
title = {Interactive defect quantification through extended reality},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101473},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101473},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002238},
author = {Zaid Abbas Al-Sabbag and Chul Min Yeum and Sriram Narasimhan},
keywords = {Visual inspection, Extended reality, Augmented reality, Damage detection},
abstract = {In this study, a new visual inspection method that can interactively detect and quantify structural defects using an Extended Reality (XR) device (headset) is proposed. The XR device, which is at the core of this method, supports an interactive environment using a holographic overlay of graphical information on the spatial environment and physical objects being inspected. By leveraging this capability, a novel XR-supported inspection pipeline, called eXtended Reality-based Inspection and Visualization (XRIV), is developed. Key tasks supported by this method include detecting visual damage from sensory data acquired by the XR device, estimating its size, and visualizing (overlaying) information on the spatial environment. The crucial step of real-time interactive segmentation—detection and pixel-wise damage boundary refinement—is achieved using a feature Back-propagating Refinement Scheme (f-BRS) algorithm. Then, a ray-casting algorithm is applied to back-project the 2D image pixel coordinates of the damage region to their 3D world coordinates for damage area quantification in real-world (physical) units. Finally, the area information is overlaid and anchored to the scene containing damage for visualization and documentation. The performance of XRIV is experimentally demonstrated by measuring surface structural damage of an in-service concrete bridge with less than 10% errors for two different test cases, and image processing latency of 2–3 s (or 0.5 s per seed point) from f-BRS. The proposed XRIV pipeline underscores the advantages of real-time interaction between expert users and the XR device through immersive visualization so that a human–machine collaborative workflow can be established to obtain better inspection outcomes in terms of accuracy and robustness.}
}
@article{BEHZADAN200890,
title = {General-purpose modular hardware and software framework for mobile outdoor augmented reality applications in engineering},
journal = {Advanced Engineering Informatics},
volume = {22},
number = {1},
pages = {90-105},
year = {2008},
note = {Intelligent computing in engineering and architecture},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2007.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034607000481},
author = {Amir H. Behzadan and Brian W. Timm and Vineet R. Kamat},
abstract = {This paper presents a reusable, general-purpose, mobile augmented reality (AR) framework developed to address the critical and repetitive challenges specific to visualization in outdoor AR. In all engineering applications of AR developed thus far, basic functionality that supports accurate user registration, maximizes the range of user motion, and enables data input and output has had to be repeatedly re-implemented. This is primarily due to the fact that designed methods have been traditionally custom created for their respective applications and are not generic enough to be readily shared and reused by others. The objective of this research was to remedy this situation by designing and implementing a reusable and pluggable hardware and software framework that can be used in any AR application without the need to re-implement low-level communication interfaces with selected hardware. The underlying methods of hardware communication as well as the object-oriented design (OOD) of the reusable interface are presented. Details on the validation of framework reusability and pluggability are also described.}
}
@article{CHEN2015124,
title = {Development of a surgical navigation system based on augmented reality using an optical see-through head-mounted display},
journal = {Journal of Biomedical Informatics},
volume = {55},
pages = {124-131},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046415000702},
author = {Xiaojun Chen and Lu Xu and Yiping Wang and Huixiang Wang and Fang Wang and Xiangsen Zeng and Qiugen Wang and Jan Egger},
keywords = {Surgical navigation, Augmented reality, Optical see-through HMD, Intra-operative motion tracking},
abstract = {The surgical navigation system has experienced tremendous development over the past decades for minimizing the risks and improving the precision of the surgery. Nowadays, Augmented Reality (AR)-based surgical navigation is a promising technology for clinical applications. In the AR system, virtual and actual reality are mixed, offering real-time, high-quality visualization of an extensive variety of information to the users (Moussa et al., 2012) [1]. For example, virtual anatomical structures such as soft tissues, blood vessels and nerves can be integrated with the real-world scenario in real time. In this study, an AR-based surgical navigation system (AR-SNS) is developed using an optical see-through HMD (head-mounted display), aiming at improving the safety and reliability of the surgery. With the use of this system, including the calibration of instruments, registration, and the calibration of HMD, the 3D virtual critical anatomical structures in the head-mounted display are aligned with the actual structures of patient in real-world scenario during the intra-operative motion tracking process. The accuracy verification experiment demonstrated that the mean distance and angular errors were respectively 0.809±0.05mm and 1.038°±0.05°, which was sufficient to meet the clinical requirements.}
}
@article{JETTER201818,
title = {Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits?},
journal = {Computers in Human Behavior},
volume = {87},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.054},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830222X},
author = {Jérȏme Jetter and Jörgen Eimecke and Alexandra Rese},
keywords = {Augmented reality, Industrial applications, Key performance indicator, Ready for market, Technology acceptance, Technology acceptance model},
abstract = {Augmented reality (AR) tools are poised to have great potential for organizations when it comes to complex processes in the field of industrial applications – like construction or maintenance in the automotive industry. The human-centred technology displays context-specific 3-D information in a real environment related to a specific targeted object. Immersive experiences are expected to boost task efficiency, the quality of training and maintenance purposes. However, ready-for-market AR tools are still rarely used and benefits seldom demonstrated. This paper focuses on the key performance indicators (KPIs) that are able to benchmark the impact of using ready-for-market AR tools on automotive maintenance performance. After a comprehensive literature review on the benefits of AR for several industrial applications in design, education and training, KPIs were extracted and evaluated by experts from the automotive industry. They were used in an empirical study – based on the technology acceptance model (TAM) – with users evaluating the KPIs before and after training situations. In addition, ‘Perceived Usefulness’ and ‘Intention to Use’ were investigated. Significant enhancements of all KPIs were observed and novice users were identified as a potential target group.}
}
@article{TSAI2018171,
title = {Augmented reality display based on user behavior},
journal = {Computer Standards & Interfaces},
volume = {55},
pages = {171-181},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917300247},
author = {Chung-Hsien Tsai and Jiung-Yao Huang},
keywords = {Augmented reality, Context-aware, Behavior perception, User interface},
abstract = {The development and commercialization of smart glasses in recent years have made the exploration of one's surroundings with mobile augmented reality (MAR) browsers anytime and anywhere more practical. However, users often suffer from issues such as cognitive overload and inconvenient interactions when operating MAR browsers on smart glasses owing to the constraints of the screen resolution and size. To overcome these problems, this paper presents a user-behavior-driven augmented content display approach called iDisplay. First, user behaviors were modeled while the smart glasses were used. A user behavior perception algorithm that infers the current state of the user by crosschecking his/her past behavior and feature data extracted from the built-in sensors of the smart glasses was then developed. Five augmented content display patterns corresponding to the modeled user's behavior states were designed accordingly. To verify that iDisplay can be based upon the perceived user states to adaptively manage the smart glasses display, a prototype system was built to conduct a series of experiments. The experimental results show that iDisplay can accurately infer user states and manage augmented content display accordingly. A user study also shows that iDisplay can successfully reduce the user's cognitive load and split attention when searching for specific point-of-interest information while moving. Furthermore, all subjects claimed that iDisplay causes less dizziness during the experiments than the native overview + detail augmented reality interface.}
}
@article{CAPUTO2018225,
title = {The Smart Pin: An effective tool for object manipulation in immersive virtual reality environments},
journal = {Computers & Graphics},
volume = {74},
pages = {225-233},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318300840},
author = {Fabio M. Caputo and Marco Emporio and Andrea Giachetti},
keywords = {Mid-air manipulation technique, Virtual reality, Hand tracking, User tests},
abstract = {In this paper, we present a novel method for object manipulation in immersive virtual environments. The proposed technique exploits a novel widget, called “Smart Pin”, to enable the user to select, translate, rotate and scale objects relying entirely on the positional tracking of a single hand. It is, therefore, suitable for several real-world applications where handheld devices are not available, the surrounding environmental conditions make orientation tracking hard, and the non-dominant hand may be involved in different tasks. We evaluated the method with users on two classical tasks such as detail search and docking, comparing it with a successful two-handed manipulation technique (Handlebar). The measured execution efficiency obtained with the two methods was similar, but most users preferred the Smart Pin for its gestural comfort and ease of use.}
}
@article{MARQUES201896,
title = {Deep spherical harmonics light probe estimator for mixed reality games},
journal = {Computers & Graphics},
volume = {76},
pages = {96-106},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301377},
author = {Bruno Augusto Dorta Marques and Esteban Walter Gonzalez Clua and Cristina Nader Vasconcelos},
keywords = {Mixed reality, Augmented reality, Virtual reality, Games, Lighting estimation, Deep learning},
abstract = {The recent developments in virtual and mixed reality by the video game and entertainment industries are responsible for increasing user’s visual immersion and provide a better user experience in games and other interactive simulations. However, the interaction between the user and simulated environment still relies on game controllers or other unnatural handheld devices. In the mixed reality context, the usage of more natural and immersive alternative to the game controllers, such as the user’s hands, may drastically increase the game interface experience, allowing a personalized visual feedback of the user’s interactions in the real-time simulation. There are basically two approaches for including the user’s hand: a 3D reconstruction based method, typically based on depth cameras, or an image-based approach, composing the virtual scene with the real images of the user’s hands. In the composition of the user’s hands and virtual elements, perceptual discrepancies in the illumination of objects may occur, generating an inconsistency in the illumination of the mixed reality environment. A consistent illumination of the environment greatly improves the user’s immersion in the mixed reality application. One way to ensure consistent illumination is by estimating the real-world illumination and use this information to adapt the virtual world lighting setting. We present the Spherical Harmonics Light Probe Estimator, a deep learning based technique that estimates the lighting setting of the real-world environment. The method uses a single RGB image and does not requires prior knowledge of the scene. The estimator outputs a light probe of the real-world lighting, represented by 9 spherical harmonics coefficients. The estimated light probe is used to create a composite image containing both real and virtual elements in an environment with a consistent illumination. We validate the technique through synthetic tests achieving an RMS error of 0.0573. We show the usage of the method in an augmented virtuality application.}
}
@article{SIESS201920,
title = {User color temperature preferences in immersive virtual realities},
journal = {Computers & Graphics},
volume = {81},
pages = {20-31},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S009784931930041X},
author = {Andreas Siess and Matthias Wölfel},
keywords = {Virtual reality, Color temperature, Perception, Cognition,},
abstract = {Virtual reality is currently experiencing a renaissance, with more and more fields of application emerging and more and more users spending an increasing amount of time using head mounted displays (HMD). While technical parameters such as field of view or pixel density are heavily researched, some fundamental aspects of perception and in particular the effects on human cognition and physical constitution are less investigated. One of these aspects to be addressed in this study is the perceived color temperature—a stimulus to which every color-seeing person reacts both consciously and subconsciously. A total of 86 test persons were asked to adjust the color temperature according to their personal taste in five photorealistic scenarios; once using a PC screen and once using a HMD. Between these two devices significant differences in the personal preferences of the test persons were found. This study will open a broad discussion about the effects of color temperature as an omnipresent stimulus and how external factors like daytime or season may effect these preferences in immersive environments.}
}
@article{AKCAYIR2016334,
title = {Augmented reality in science laboratories: The effects of augmented reality on university students’ laboratory skills and attitudes toward science laboratories},
journal = {Computers in Human Behavior},
volume = {57},
pages = {334-342},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215303253},
author = {Murat Akçayır and Gökçe Akçayır and Hüseyin Miraç Pektaş and Mehmet Akif Ocak},
keywords = {Augmented reality, Science laboratory, Higher education, Laboratory skills, Attitude},
abstract = {This study investigated the effects of the use of augmented reality (AR) technologies in science laboratories on university students' laboratory skills and attitudes towards laboratories. A quasi-experimental pre-test/post-test control group design was employed. The participants were 76 first-year university students, aged 18–20 years old. They were assigned to either an experimental or a control group. Qualitative and quantitative data collection tools were used. The experimental results obtained following the 5-week application revealed that the AR technology significantly enhanced the development of the university students' laboratory skills. AR technology both improved the students’ laboratory skills and helped them to build positive attitudes towards physics laboratories. The statements of the students and the instructor regarding other effects of AR technology on science laboratories, both negative and positive, are also discussed.}
}
@article{CAVALCANTI2019100,
title = {Usability and effects of text, image and audio feedback on exercise correction during augmented reality based motor rehabilitation},
journal = {Computers & Graphics},
volume = {85},
pages = {100-110},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S009784931930158X},
author = {Virgínia Carrazzone Cavalcanti and Maria Iziane de Santana Ferreira and Veronica Teichrieb and Ricardo Rossiter Barioni and Walter Franklin Marques Correia and Alana Elza Fontes {Da Gama}},
keywords = {Augmented reality, Motor rehabilitation, Error correction feedback, Usability, Serious games, Kinect},
abstract = {This paper aims to evaluate text, image and audio feedback regarding usability, exercise correction and user preferences when using Augmented Reality (AR) to guide movements execution in rehabilitation applications. For that, this study used an AR rehabilitation tool, named ARkanoidAR, which attempts to correct the wrong movement performed by a user through text, image and audio feedback, while he/she is playing a game controlled based on the therapy protocol defined by a physiotherapist for that particular user. The ARkanoidAR uses a Kinect sensor to track the human body and its movements, and it is completely configurable according to patient therapy needs. To evaluate feedback effects individually, this study collected quantitative data to measure success rate, which represents the amount of time that the user executed the movements correctly when using the application with each feedback individually activated. Experiments were composed of five phases: control, text, image, audio and control. The feedback sequence was randomized before the tests in order to avoid bias. Quantitative results show that users understood the instructions for performing the movements correctly since the success rate along time increased. Tests also included observation and qualitative analysis through questionnaires and a semi-structured interview to evaluate the usability perception of users. The qualitative data helped to detect different types of problems for each feedback based on users’ answers. This study was carried out with healthy people, and did not predict the particularities and limitations of physiotherapy patients.}
}
@article{YU2024108039,
title = {HSA-net with a novel CAD pipeline boosts both clinical brain tumor MR image classification and segmentation},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {108039},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108039},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524001239},
author = {Zekuan Yu and Xiang Li and Jiaxin Li and Weiqiang Chen and Zhiri Tang and Daoying Geng},
keywords = {Brain tumor, Medical image segmentation, Computer-aided-diagnostic pipelines},
abstract = {Brain tumors are among the most prevalent neoplasms in current medical studies. Accurately distinguishing and classifying brain tumor types accurately is crucial for patient treatment and survival in clinical practice. However, existing computer-aided diagnostic pipelines are inadequate for practical medical use due to tumor complexity. In this study, we curated a multi-centre brain tumor dataset that includes various clinical brain tumor data types, including segmentation and classification annotations, surpassing previous efforts. To enhance brain tumor segmentation accuracy, we propose a new segmentation method: HSA-Net. This method utilizes the Shared Weight Dilated Convolution module (SWDC) and Hybrid Dense Dilated Convolution module (HDense) to capture multi-scale information while minimizing parameter count. The Effective Multi-Dimensional Attention (EMA) and Important Feature Attention (IFA) modules effectively aggregate task-related information. We introduce a novel clinical brain tumor computer-aided diagnosis pipeline (CAD) that combines HSA-Net with pipeline modification. This approach not only improves segmentation accuracy but also utilizes the segmentation mask as an additional channel feature to enhance brain tumor classification results. Our experimental evaluation of 3327 real clinical data demonstrates the effectiveness of the proposed method, achieving an average Dice coefficient of 86.85 % for segmentation and a classification accuracy of 95.35 %. We also validated the effectiveness of our proposed method using the publicly available BraTS dataset.}
}
@article{ROMPAPAS201924,
title = {Towards large scale high fidelity collaborative augmented reality},
journal = {Computers & Graphics},
volume = {84},
pages = {24-41},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849319301359},
author = {Damien Constantine Rompapas and Christian Sandor and Alexander Plopski and Daniel Saakes and Joongi Shin and Takafumi Taketomi and Hirokazu Kato},
keywords = {Augmented reality, High fidelity, Game design, Large scale interaction},
abstract = {In recent years, there has been an increasing amount of Collaborative Augmented Reality (CAR) experiences, classifiable by the deployed scale and the fidelity of the experience. In this paper, we create HoloRoyale, the first large scale high fidelity (LSHF) CAR experience. We do this by first exploring the LSHF CAR design space, drawing on technical implementations and design aspects from AR and video games. We then create and implement a software architecture that improves the accuracy of synchronized poses between multiple users. Finally, we apply our target experience and technical implementation to the explored design space. A core design component of HoloRoyale is the use of visual repellers as crowd control elements to guide players away from undesired areas. To evaluate the effectiveness of the employed visual repellers in a LSHF CAR context we conducted a user study, deploying HoloRoyale in a 12.500 m2 area. The results from the user study suggest that visual repellers are effective crowd control elements that do not significantly impact the user’s overall immersion. Overall our main contribution is the exploration of a design space, discussing several means to address the challenges of LSHF CAR, the creation of a system capable of LSHF CAR interactions along with an experience that has been fitted to the design space, and an indepth study that verifies a key design aspect for LSHF CAR. As such, our work is the first to explore the domain of LSHF CAR and provides insight into designing experiences in other AR domains.}
}
@article{CARDOS2017371,
title = {Virtual reality exposure therapy in flight anxiety: A quantitative meta-analysis},
journal = {Computers in Human Behavior},
volume = {72},
pages = {371-380},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217301565},
author = {Roxana A.I. Cardoş and Oana A. David and Daniel O. David},
keywords = {Virtual reality, Exposure, Flight anxiety, Meta-analysis},
abstract = {Background
Flight anxiety and the fear-related avoidance draw serious personal and financial negative consequences. Although classical exposure techniques for flight anxiety are widely used, they involve significant limits. Efforts to develop the benefits and access to evidence-based psychotherapies have led to a new method of delivering exposure technique, namely virtual reality exposure therapy (VRET). Until now, there has been no meta-analysis which assumed as a primary objective the analysis of VRET effectiveness in flight anxiety.
Methods
The present meta-analysis aimed to investigate the efficacy of VRET interventions for flight anxiety compared to various control conditions, at post-test and follow-up. We conducted a quantitative review of 11 randomized studies, we examined potential moderators of the efficacy of interventions and we investigated the presence and the degree of publication bias.
Results
Results pointed out significant overall efficiency of VRET in flight anxiety at post-test and follow-up. Analysis highlighted the superiority of VRET vs. control conditions at post-test and follow-up and the superiority of VRET vs. classical evidence-based interventions at post-test and follow-up. Results revealed similar efficacy between VRET and exposure based interventions at post-test, and showed better treatment gains over time when using VRET vs. exposure based interventions. Moderation analyses revealed that low quality trials, with smaller and younger samples led to a larger effect size of VRET for flight anxiety. Also, outcome types, the number of exposure sessions and follow-up intervals were significant moderators of the efficiency of VRET in this disorder.
Conclusions
The present meta-analysis supports the efficiency of VRET in flight anxiety and encourages the use of this type of exposure both in clinical practice and research field.}
}
@article{DORTAMARQUES2022257,
title = {Spatially and color consistent environment lighting estimation using deep neural networks for mixed reality},
journal = {Computers & Graphics},
volume = {102},
pages = {257-268},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001710},
author = {Bruno Augusto {Dorta Marques} and Esteban Walter {Gonzalez Clua} and Anselmo Antunes Montenegro and Cristina {Nader Vasconcelos}},
keywords = {Lighting estimation, Spherical harmonics, Deep learning, Environment map, Mixed reality, Augmented Reality},
abstract = {The representation of consistent mixed reality (XR) environments requires adequate real and virtual illumination composition in real-time. Estimating the lighting of a real scenario is still a challenge. Due to the ill-posed nature of the problem, classical inverse-rendering techniques tackle the problem for simple lighting setups. However, those assumptions do not satisfy the current state-of-art in computer graphics and XR applications. While many recent works solve the problem using machine learning techniques to estimate the environment light and scene’s materials, most of them are limited to geometry or previous knowledge. This paper presents a CNN-based model to estimate complex lighting for mixed reality environments with no previous information about the scene. We model the environment illumination using a set of spherical harmonics (SH) environment lighting, capable of efficiently represent area lighting. We propose a new CNN architecture that inputs an RGB image and recognizes, in real-time, the environment lighting. Unlike previous CNN-based lighting estimation methods, we propose using a highly optimized deep neural network architecture, with a reduced number of parameters, that can learn high complex lighting scenarios from real-world high-dynamic-range (HDR) environment images. We show in the experiments that the CNN architecture can predict the environment lighting with an average mean squared error (MSE) of 7.85× 10−4 when comparing SH lighting coefficients. We validate our model in a variety of mixed reality scenarios. Furthermore, we present qualitative results comparing relights of real-world scenes.}
}
@article{XU2023100568,
title = {Design and application of VR-based college English game teaching},
journal = {Entertainment Computing},
volume = {46},
pages = {100568},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100568},
url = {https://www.sciencedirect.com/science/article/pii/S187595212300023X},
author = {Yuecheng Xu and Gawa Bao and Xiaokai Duan},
keywords = {DNN, English teaching, Game, HMM, VR},
abstract = {With the deepening of international exchanges, colleges and universities have higher and higher requirements for the quality of English teaching. In order to improve the quality of English teaching in colleges and universities, the research proposes to design and construct an English game education and teaching system based on virtual reality technology. First, the modern VR technology is used to build the initial English game teaching system, and then the HMM-DNN algorithm is introduced to improve the virtual system. In the process, the CMUShinx speech recognition platform is used to build a robot control command speech recognition system, in order to obtain a good voice acoustic model and achieve correct speech recognition in the virtual environment. Finally, the performance analysis of the built system is carried out to verify the feasibility of the system model. The results show that the step response coefficient of the model built by the research institute can be kept within 1 M for a long time; When the speech type is White and Cafe1, the correct recognition rate of the constructed model for English speech is as high as 98 % and 95 %, and is always higher than other algorithms; In the comparison of the accuracy of students' oral speech recognition scores by different models, the lowest error of the research model is 2.23 %, which is significantly smaller than the other models. The above results indicate that the overall teaching effect of the system built by the research institute is stable, and it has high feasibility for college students' English teaching, which helps to further improve students' interest and ability in English learning, and is of great significance for college English talent training.}
}
@article{CANESSA2014227,
title = {Calibrated depth and color cameras for accurate 3D interaction in a stereoscopic augmented reality environment},
journal = {Journal of Visual Communication and Image Representation},
volume = {25},
number = {1},
pages = {227-237},
year = {2014},
note = {Visual Understanding and Applications with RGB-D Cameras},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S104732031300031X},
author = {Andrea Canessa and Manuela Chessa and Agostino Gibaldi and Silvio P. Sabatini and Fabio Solari},
keywords = {RGB-D cameras, Human–computer interactions, Calibration and data pre-processing, Kinect device, Virtual reality, Spatially variant depth correction, Eyes′ tracking, Mixed reality},
abstract = {A Human–machine interaction system requires precise information about the user’s body position, in order to allow a natural 3D interaction in stereoscopic augmented reality environments, where real and virtual objects should coherently coexist. The diffusion of RGB-D sensors seems to provide an effective solution to such a problem. Nevertheless, the interaction with stereoscopic 3D environments, in particular in peripersonal space, requires a higher degree of precision. To this end, a reliable calibration of such sensors and an accurate estimation of the relative pose of different RGB-D and visualization devices are crucial. Here, robust and straightforward procedures to calibrate a RGB-D camera, to improve the accuracy of its 3D measurements, and to co-register different calibrated devices are proposed. Quantitative measures validate the proposed approach. Moreover, calibrated devices have been used in an augmented reality system, based on a dynamic stereoscopic rendering technique that needs accurate information about the observer’s eyes position.}
}
@article{KOSTOV2022896,
title = {Designing a Framework for Collaborative Mixed Reality Training},
journal = {Procedia Computer Science},
volume = {200},
pages = {896-903},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002964},
author = {Georgi Kostov and Josef Wolfartsberger},
keywords = {multi-device, collaboration, tool, virtual reality, augmented reality, network, human-computer interaction},
abstract = {Training simulations in Virtual Reality (VR) benefit from the technology’s interactive and intuitive nature. Typically, the VR user is isolated in the virtual environment and an external instructor or spectators cannot support the learning progress. Previous research work presents solutions for isolated use cases, but lacks specific guidelines for the implementation of collaborative multi-device systems. We propose a method for expanding collaborative extended reality (XR) applications to multiple platforms to support collaborative learning, but also other types of applications such as remote collaboration and maintenance. In order to test our approach, we expanded an existing application to four different platforms - Steam VR, Microsoft Mixed Reality, smartphone/tablet and desktop computer. Based on lessons learned from our prototype, we propose a solution for multi-device networking and interaction on each platform. The main strengths of our approach are in the decoupling of local and networked objects and the common interface for interaction, which accommodates multiple platforms. We believe our approach can provide useful insights into collaborative training and serve as a good starting point for future projects.}
}
@article{ZHANG20101167,
title = {A multi-regional computation scheme in an AR-assisted in situ CNC simulation environment},
journal = {Computer-Aided Design},
volume = {42},
number = {12},
pages = {1167-1177},
year = {2010},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2010.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448510001442},
author = {J. Zhang and S.K. Ong and A.Y.C. Nee},
keywords = {Augmented reality, CNC simulation, Tracking, Registration, Template matching},
abstract = {Computer numerical control (CNC) simulation systems based on 3D graphics have been well researched and developed for NC tool path verification and optimization. Although widely used in the manufacturing industries, these CNC simulation systems are usually software-centric rather than machine tool-centric. The user has to adjust himself from the 3D graphic environment to the real machining environment. Augmented reality (AR) is a technology that supplements a real world with virtual information, where virtual information is augmented on to real objects. This paper builds on previous works of integrating the AR technology with a CNC machining environment using tracking and registration methodologies, with an emphasis on in situ simulation. Specifically configured for a 3-axis CNC machine, a multi-regional computation scheme is proposed to render a cutting simulation between a real cutter and a virtual workpiece, which can be conducted in situ to provide the machinist with a familiar and comprehensive environment. A hybrid tracking method and an NC code-adaptive cutter registration method are proposed and validated with experimental results. The experiments conducted show that this in situ simulation system can enhance the operator’s understanding and inspection of the machining process as the simulations are performed on real machines. The potential application of the proposed system is in training and machining simulation before performing actual machining operations.}
}
@article{IFTENE2018166,
title = {Enhancing the Attractiveness of Learning through Augmented Reality},
journal = {Procedia Computer Science},
volume = {126},
pages = {166-175},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.220},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311943},
author = {Adrian Iftene and Diana Trandabăț},
keywords = {Augmented Reality, eLearning, Autism, Usability Testing},
abstract = {Over the last years, augmented reality was used in various domains, from medical, industrial design, modeling and production, robot teleoperation, military, entertainment, leisure activities to translation, facial recognition, assistance while driving, interior and exterior design, virtual friends, internet of things and eLearning. In eLearning, the combination between classical and augmented content (the later coming with 3D models, images, sounds, animations, Internet browsing, etc.) can help the teacher to better explain the content of the courses. In this paper, we present four augmented reality applications, created with the aim to improve communication and collaboration skills (two of them) and to ease the learning of biology and geography (the other two). The motivation behind these applications is to enhance the attractiveness of the classes, allow students to retrain new information more easily and reduce the stress behind tests when presented as games.}
}
@article{HU2023102574,
title = {AR-based deep learning for real-time inspection of cable brackets in aircraft},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102574},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102574},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000509},
author = {Jingyu Hu and Gang Zhao and Wenlei Xiao and Rupeng Li},
keywords = {Augmented reality, Deep learning, Aeronautics intelligent manufacturing, Assembly inspection, CAD model checking},
abstract = {In the process of aircraft assembly, there exist numerous and ubiquitous cable brackets that shall be installed on frames and subsequently need to be manually verified with CAD models. Such a task is usually performed by special operators, hence is time-consuming, labor-intensive, and error-prone. In order to save the inspection time and increase the reliability of results, many researchers attempt to develop intelligent inspection systems using robotic, AR, or AI technologies. However, there is no comprehensive method to achieve enough portability, intelligence, efficiency, and accuracy while providing intuitive task assistance for inspectors in real time. In this paper, a combined AR+AI system is introduced to assist brackets inspection in a more intelligent yet efficient manner. Especially, AR-based Mask R-CNN is proposed by skillfully integrating markerless AR into deep learning-based instance segmentation to generate more accurate and fewer region proposals, and thus alleviates the computation load of the deep learning program. Based on this, brackets segmentation can be performed robustly and efficiently on mobile devices such as smartphones or tablets. By using the proposed system, CAD model checking can be automatically performed between the segmented physical brackets and the corresponding virtual brackets rendered by AR in real time. Furthermore, the inspection results can be directly projected on the corresponding physical brackets for the convenience of maintenance. To verify the feasibility of the proposed method, experiments are carried out on a full-scale mock-up of C919 aircraft main landing gear cabin. The experimental results indicate that the inspection accuracy is up to 97.1%. Finally, the system has been deployed in the real C919 aircraft final-assembly workshop. The preliminary evaluation reveals that the proposed real-time AR-assisted intelligent inspection approach is effective and promising for large-scale industrial applications.}
}
@article{SCHIOPU2021101575,
title = {Virus tinged? Exploring the facets of virtual reality use in tourism as a result of the COVID-19 pandemic},
journal = {Telematics and Informatics},
volume = {60},
pages = {101575},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101575},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321000149},
author = {Andreea F. Schiopu and Remus I. Hornoiu and Mihaela A. Padurean and Ana-Maria Nica},
keywords = {VR use in tourism, Technology Acceptance Model (TAM), Perceived substitutability of VR},
abstract = {Several studies have investigated the use of virtual reality (VR) in tourism, but none has taken an epidemiological outlook. This research examined the use of VR in tourism through the lenses of an extended TAM model in times of COVID-19 pandemic. The premise was that, in this context, people would prefer less risky experiences and would see VR as a substitute for traditional travel. The data used was collected through a within-subjects experiment, which proved that intention to use VR in tourism increased under the COVID-19 effect. This study tested a conceptual model that showed this intention was influenced by the perceived ease of use, perceived usefulness, and perceived substitutability of VR, all mediated by people’s interest in VR use in tourism. The perceived authenticity of VR experience determined the perceived substitutability of VR. This paper has theoretical and practical implications. In the long term, promoting tourism-related VR activities might reduce the risk of virus spreading, lessen the pressure imposed on this sector by such epidemic episodes, and increase its sustainability.}
}
@article{FIORENTINO2014270,
title = {Augmented reality on large screen for interactive maintenance instructions},
journal = {Computers in Industry},
volume = {65},
number = {2},
pages = {270-278},
year = {2014},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513002340},
author = {Michele Fiorentino and Antonio E. Uva and Michele Gattullo and Saverio Debernardis and Giuseppe Monno},
keywords = {Computer aided task guidance, Augmented reality, Large screen instruction, Maintenance},
abstract = {We present an empirical study that evaluates the effectiveness of technical maintenance assisted with interactive augmented reality instructions. Our approach consists in an augmented visualization on a large screen and a combination of multiple fixed and mobile cameras. We used commercially available solutions. In our test, 14 participants completed a set of 4 maintenance tasks based on manual inspections of a motorbike engine. Tool selection, removal of bolts, and part dis\assembly, are supported by visual labels, 3D virtual models and 3D animations. All participants executed similar operations in two modalities: paper manuals and augmented instructions. Statistical analyses proved that augmented instructions reduced significantly participants’ overall execution time and error rate.}
}
@article{ALKADRI2021104770,
title = {Utilizing a multilayer perceptron artificial neural network to assess a virtual reality surgical procedure},
journal = {Computers in Biology and Medicine},
volume = {136},
pages = {104770},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104770},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521005643},
author = {Sami Alkadri and Nicole Ledwos and Nykan Mirchi and Aiden Reich and Recai Yilmaz and Mark Driscoll and Rolando F. {Del Maestro}},
keywords = {Multilayered artificial neural network, Feature importance, Virtual reality, Surgical simulation, Surgical education, Performance metric, Surgical expertise, Anterior cervical discectomy and fusion},
abstract = {Background
Virtual reality surgical simulators are a safe and efficient technology for the assessment and training of surgical skills. Simulators allow trainees to improve specific surgical techniques in risk-free environments. Recently, machine learning has been coupled to simulators to classify performance. However, most studies fail to extract meaningful observations behind the classifications and the impact of specific surgical metrics on the performance. One benefit from integrating machine learning algorithms, such as Artificial Neural Networks, to simulators is the ability to extract novel insights into the composites of the surgical performance that differentiate levels of expertise.
Objective
This study aims to demonstrate the benefits of artificial neural network algorithms in assessing and analyzing virtual surgical performances. This study applies the algorithm on a virtual reality simulated annulus incision task during an anterior cervical discectomy and fusion scenario.
Design
An artificial neural network algorithm was developed and integrated. Participants performed the simulated surgical procedure on the Sim-Ortho simulator. Data extracted from the annulus incision task were extracted to generate 157 surgical performance metrics that spanned three categories (motion, safety, and efficiency).
Setting
Musculoskeletal Biomechanics Research Lab; Neurosurgical Simulation and Artificial Intelligence Learning Center, McGill University, Montreal, Canada.
Participants
Twenty-three participants were recruited and divided into 3 groups: 11 post-residents, 5 senior and 7 junior residents.
Results
An artificial neural network model was trained on nine selected surgical metrics, spanning all three categories and achieved 80% testing accuracy.
Conclusions
This study outlines the benefits of integrating artificial neural networks to virtual reality surgical simulators in understanding composites of expertise performance.}
}
@article{ZHAO201793,
title = {Augmented reality for enhancing tele-robotic system with force feedback},
journal = {Robotics and Autonomous Systems},
volume = {96},
pages = {93-101},
year = {2017},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2017.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0921889016306066},
author = {Zhou Zhao and Panfeng Huang and Zhenyu Lu and Zhengxiong Liu},
keywords = {Augmented reality, Teleoperation, Robot, Force feedback, Haptic},
abstract = {In the teleoperation, the force feedback is indispensable, which can enhance the sense of presence of the operator and help the operator accomplish tasks comfortably. The time delay is one of the main challenges that influence the stability of the teleoperation systems, which leads to the discontinuous operation. Thus building a local virtual model in the master side is an effective way to solve this problem. In this paper, a new method is presented to reconstruct the virtual model of the remote object. The virtual model can estimate the real-time force feedback to the operator and eliminate the effects of the time delay. Then the tele-robotic system based on augmented reality technology is set up in our laboratory. In the tele-robotic system, the dynamic parameters including damping and stiffness of the virtual model are constantly updated by utilizing the positions and forces information from sensors of the remote robot site. Finally, the effectiveness of the proposed method and the correctness of the visual model parameters are verified by two experiments.}
}
@article{KHALEGHI2022100504,
title = {Toward using effective elements in adults’ amblyopia treatment in a virtual reality-based gamified binocular application},
journal = {Entertainment Computing},
volume = {43},
pages = {100504},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100504},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000283},
author = {Ali Khaleghi and Zahra Aghaei and Fateme Hosseinnia},
keywords = {Amblyopia, Gamification, Adult lazy eye treatment, Binocular game, Virtual reality, Smartphones},
abstract = {The common method for treating amblyopia (lazy eye) in adults is binocular therapy implemented as simple computer games. However, it lacks the maximum therapeutic elements effective in treating amblyopia and uses expensive stereoscopes and virtual reality (VR) glasses, limiting widespread use. Also, gamification can be incorporated into these games to keep patients’ motivation over a stable period. The features that have the potential to be included in binocular games are identified and included in a VR-based gamified binocular application with a more straightforward implementation than the games presented so far. The game is implemented on smartphones to increase accessibility. Color and size of game components, CAM stimulator, and binocular therapy using VR were identified and incorporated in a VR zombie first-person shooter game. Eight experts evaluated the game with a questionnaire designed based on Nielsen's heuristics evaluation and therapeutic aspects. The initial tests with experts indicated the game’s potential application as a therapeutic tool for improving visual acuity and vision’s contrast and depth. Also, the game has the potential application in increasing motivation and interaction. After rigorous evaluations with patients, the game can be used outside clinics by adjusting it to the patients’ eyes in advance and remotely.}
}
@article{LAWSON2015240,
title = {The use of virtual reality and physical tools in the development and validation of ease of entry and exit in passenger vehicles},
journal = {Applied Ergonomics},
volume = {48},
pages = {240-251},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S000368701400297X},
author = {Glyn Lawson and Paul Herriotts and Louise Malcolm and Katharina Gabrecht and Setia Hermawati},
keywords = {Entry exit, Automotive},
abstract = {Ease of entry and exit is important for creating a positive first impression of a car and increasing customer satisfaction. Several methods are used within vehicle development to optimise ease of entry and exit, including CAD reviews, benchmarking and buck trials. However, there is an industry trend towards digital methods to reduce the costs and time associated with developing physical prototypes. This paper reports on a study of entry strategy in three properties (buck, car, CAVE) in which inconsistencies were demonstrated by people entering a vehicle representation in the CAVE. In a second study industry practitioners rated the CAVE as worse than physical methods for identifying entry and exit issues, and having lower perceived validity and reliability. However, the resource issues associated with building bucks were recognised. Recommendations are made for developing the CAVE and for combinations of methods for use at different stages of a vehicle's development.}
}
@article{LI2021106776,
title = {MDFA-Net: Multiscale dual-path feature aggregation network for cardiac segmentation on multi-sequence cardiac MR},
journal = {Knowledge-Based Systems},
volume = {215},
pages = {106776},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106776},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000393},
author = {Feiyan Li and Weisheng Li and Sheng Qin and Linhong Wang},
keywords = {Multiscale dual-path feature aggregation network, Multi-sequence CMR, Cardiac segmentation},
abstract = {Deep convolutional neural networks have shown great potential in medical image segmentation. However, automatic cardiac segmentation is still challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this paper, we propose a multiscale dual-path feature aggregation network (MDFA-Net) to solve misclassification and shape discontinuity problems. The proposed network is aimed to maintain a realistic shape of the segmentation results and divided into two parts: the first part is a non-downsampling multiscale nested network (MN-Net) which restrains the cardiac continuous shape and maintains the shallow information, and the second part is a non-symmetric encoding and decoding network (nSED-Net) that can retain deep details and overcome misclassification. We conducted four-fold cross-validation experiments on balanced steady-state, free precession cine cardiac magnetic resonance (bSSFP cine CMR) sequence, edema-sensitive T2-weighted, black blood spectral presaturation attenuated inversion-recovery (T2-SPAIR) CMR sequence and late gadolinium enhancement (LGE) CMR sequence which include 45 cases in each sequence. The data are provided by the organizer of the Multi-sequence Cardiac MR Segmentation Challenge (MS-CMRSeg 2019) in conjunction with 2019 Medical Image Computing and Computer Assisted Interventions (MICCAI). We also conducted external validation experiments on the data of 2020 MICCAI myocardial pathology segmentation challenge (MyoPS 2020). Whether it is a four-fold cross-validation experiment or an external validation experiment, the proposed method ranks first or second in the segmentation tasks of multi-sequence CMR images. The subjective evaluation also shows the same results as the objective evaluation metrics. The code will be posted at https://github.com/fly1995/MDFA-Net/.}
}
@article{INCETAN2021101990,
title = {VR-Caps: A Virtual Environment for Capsule Endoscopy},
journal = {Medical Image Analysis},
volume = {70},
pages = {101990},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.101990},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000360},
author = {Kağan İncetan and Ibrahim Omer Celik and Abdulhamid Obeid and Guliz Irem Gokceler and Kutsev Bengisu Ozyoruk and Yasin Almalioglu and Richard J. Chen and Faisal Mahmood and Hunter Gilbert and Nicholas J. Durr and Mehmet Turan},
keywords = {Capsule endoscopy, Deep reinforcement learning, Area coverage, Disease classification, Synthetic data generation},
abstract = {Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation, automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes, but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of normal and abnormal tissue conditions (e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope designs (e.g., mono, stereo, dual and 360∘ camera), and the type, number, strength, and placement of internal and external magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop, optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems. To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results demonstrate the usefulness and effectiveness of the proposed virtual platform in developing algorithms that quantify fractional coverage, camera trajectory, 3D map reconstruction, and disease classification. All of the code, pre-trained weights and created 3D organ models of the virtual environment with detailed instructions how to setup and use the environment are made publicly available at https://github.com/CapsuleEndoscope/VirtualCapsuleEndoscopy and a video demonstration can be seen in the supplementary videos (Video-I).}
}
@article{URIEL2020389,
title = {Improving the understanding of Basic Sciences concepts by using Virtual and Augmented Reality},
journal = {Procedia Computer Science},
volume = {172},
pages = {389-392},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.165},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314952},
author = {Cukierman Uriel and Silvestri Sergio and González Carolina and González Mariano and Dellepiane Paola and Agüero Martín},
keywords = {Basic Sciences, Virtual, Augmented Reality, Student-Centered Learning},
abstract = {One of the most important problems in the early years of engineering programs is the difficulty that students have to understand Basic Sciences concepts, their application in technology and, particularly, in engineering. One of the ways to solve this situation is by implementing methodologies that promote an "Active and Student-Centered Learning" (ASCL). There are many techniques to develop this kind of approach, some of them are facilitated and/or enhanced through the use of technology. One of the most developed technologies in recent times and with greater expectations for the near future is Virtual and Augmented Reality (VAR). Our objective with this project is to develop and test VAR applications for improving the understanding of Basic Sciences concepts in freshmen courses.}
}
@article{OPREA201977,
title = {A visually realistic grasping system for object manipulation and interaction in virtual reality environments},
journal = {Computers & Graphics},
volume = {83},
pages = {77-86},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849319301098},
author = {Sergiu Oprea and Pablo Martinez-Gonzalez and Alberto Garcia-Garcia and John A. Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez},
keywords = {Human-computer interaction, Virtual reality, User studies},
abstract = {Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.}
}
@article{TRAPPEY2022100331,
title = {VR-enabled engineering consultation chatbot for integrated and intelligent manufacturing services},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100331},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200005X},
author = {Amy J.C. Trappey and Charles V. Trappey and Min-Hua Chao and Chun-Ting Wu},
keywords = {Chatbot, Virtual reality, Power transformer, Natural language processing, Manufacturing service, Product-service system},
abstract = {Industrial equipment manufacturing, such as electric power transformers, require highly customized design, assembly, installation and maintenance services for customers. Engineering consultation is an essential manufacturing service which requires deep domain knowledge. This research develops and implements the framework and the prototype of a Virtual Reality (VR) enabled intelligent engineering consultation chatbot system with natural language communication capabilities. The consultation chatbot case focuses on the power transformer knowledge domain. The system is constructed using a comprehensive knowledge base of Frequently Asked Questions (FAQs) and answers from a power transformer manufacturer. A total of 490,000 technical articles (with more than 1.1 billion words) from Wikipedia and a comprehensive domain-specific document set are used to train a word embedding model for retrieving questions and answers (QA) from the FAQ knowledge base. Immersive VR technology provides a highly interactive QA chatbot with realistic graphical views for informative engineering counselling. The case example demonstrates the accuracy of the proposed VR-enabled chatbot for transformer QAs exceeds 91%.}
}
@article{CICCARELLI2022540,
title = {Interface and interaction design principles for Mixed Reality applications: the case of operator training in wire harness activities},
journal = {Procedia Computer Science},
volume = {204},
pages = {540-547},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008043},
author = {Marianna Ciccarelli and Agnese Brunzini and Alessandra Papetti and Michele Germani},
keywords = {Mixed Reality, Augmented Reality, Human-Computer Interaction, Operator training, Industry 4.0, Wire harness, User Interface},
abstract = {Operator 4.0 has to deal with a vast amount of product variants and production data especially within the mass customization paradigm, high mental demanding tasks, and smart production systems. Technologies capable of supporting his training and his work become fundamental, such as the extended reality (XR). Its increasing use in industrial applications, however, opens up new challenges related to interface and interaction design, which can determine the success of both the use and development experience. The lack of guidelines for designing interfaces for mixed reality (MR) applications is what this paper aims to address. Design requirements for MR interfaces are presented and applied in the context of operator training in wire harness activities. Different interaction modes and user interfaces have been developed to evaluate the most suitable and user-friendly one for the operator. A pilot test was conducted to assess the applications’ usability and potentialities with satisfactory results.}
}
@article{LEE2022106709,
title = {Development of three-dimensional visualisation technology of aerodynamic environment in fattening pig house using CFD and VR technology},
journal = {Computers and Electronics in Agriculture},
volume = {194},
pages = {106709},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106709},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922000266},
author = {Sang-Yeon Lee and Jun-Gyu Kim and Rack-Woo Kim and Uk-Hyeon Yeo and In-Bok Lee},
keywords = {Computational fluid dynamics, Fattening pig house, Visualisation technology, Virtual reality, Virtual reality simulator},
abstract = {In this study, a virtual reality (VR) simulator to visualise the aerodynamic environment of a fattening pig house was developed as educational materials for farmers and consultants. The aerodynamic environments inside a fattening pig house were firstly analysed according to various environmental conditions using computational fluid dynamics (CFD). Using the validated CFD model (Kim et al., 2019), the aerodynamic environment inside the fattening pig house was analysed with 54 cases of environmental conditions in winter and 60 cases of environmental conditions in summer. And then, the VR simulator was developed by visualising the CFD-computed data of aerodynamic environments in virtual space. The three-dimensional fattening pig house models were designed following the CFD-computed cases, and the three-dimensional pig model was developed with real shape and texture. The virtual space was organized by arranging the three-dimensional image models of the fattening pig house and fattening pig. A C language-based code was also used to extract the CFD-computed results for developing visualisation of the aerodynamic environment inside the fattening pig house. Visualisation was realized using contour plot, two-dimensional vector flow, and smoke effect in the virtual space. In the case of the contour plot, a scalar of air temperature, relative humidity, and gas concentration were expressed using color on the active plane. A two-dimensional vector flow represented two-dimensional flows on the active plane. From streamline data, the smoke effect was developed to describe the airflow from the air inlet. In this study, a tablet-shaped user interface (UI) was created so that the user can directly select the desired cases. Through a performance test, the optimal number of frames was determined. Finally, the VR simulator was developed to effectively describe the aerodynamic environments inside the fattening pig house.}
}
@article{LEE2020103229,
title = {Experiencing immersive virtual reality in museums},
journal = {Information & Management},
volume = {57},
number = {5},
pages = {103229},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103229},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618310280},
author = {Hyunae Lee and Timothy Hyungsoo Jung and M.Claudia {tom Dieck} and Namho Chung},
keywords = {Experience economy, Absorption, Immersion, Virtual reality, Museum, Tourism},
abstract = {Virtual Reality (VR) has been regarded as a highly effective technology that enables people to gain enjoyable and immersive information about museum collections. Drawing from the four realms of the experience economy, we assume absorptive experiences influence immersive experiences, overall museum VR tour experience, and intention to visit a museum. The results show that all the hypotheses are supported. Furthermore, we compared and tested the proposed model and its rival model (postulating the direct influence of the four realms of the experience economy on museum VR experience) and found that the proposed model is better than the rival model.}
}
@article{KLEPACZKO2016293,
title = {Simulation of MR angiography imaging for validation of cerebral arteries segmentation algorithms},
journal = {Computer Methods and Programs in Biomedicine},
volume = {137},
pages = {293-309},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304016},
author = {Artur Klepaczko and Piotr Szczypiński and Andreas Deistung and Jürgen R. Reichenbach and Andrzej Materka},
keywords = {MR angiography, Vessel segmentation, Cerebral vasculature modeling, MRI simulation, Quantitative validation},
abstract = {Background and objective
Accurate vessel segmentation of magnetic resonance angiography (MRA) images is essential for computer-aided diagnosis of cerebrovascular diseases such as stenosis or aneurysm. The ability of a segmentation algorithm to correctly reproduce the geometry of the arterial system should be expressed quantitatively and observer-independently to ensure objectivism of the evaluation.
Methods
This paper introduces a methodology for validating vessel segmentation algorithms using a custom-designed MRA simulation framework. For this purpose, a realistic reference model of an intracranial arterial tree was developed based on a real Time-of-Flight (TOF) MRA data set. With this specific geometry blood flow was simulated and a series of TOF images was synthesized using various acquisition protocol parameters and signal-to-noise ratios. The synthesized arterial tree was then reconstructed using a level-set segmentation algorithm available in the Vascular Modeling Toolkit (VMTK). Moreover, to present versatile application of the proposed methodology, validation was also performed for two alternative techniques: a multi-scale vessel enhancement filter and the Chan–Vese variant of the level-set-based approach, as implemented in the Insight Segmentation and Registration Toolkit (ITK). The segmentation results were compared against the reference model.
Results
The accuracy in determining the vessels centerline courses was very high for each tested segmentation algorithm (mean error rate = 5.6% if using VMTK). However, the estimated radii exhibited deviations from ground truth values with mean error rates ranging from 7% up to 79%, depending on the vessel size, image acquisition and segmentation method.
Conclusions
We demonstrated the practical application of the designed MRA simulator as a reliable tool for quantitative validation of MRA image processing algorithms that provides objective, reproducible results and is observer independent.}
}
@article{CHOI2023107644,
title = {A single stage knowledge distillation network for brain tumor segmentation on limited MR image modalities},
journal = {Computer Methods and Programs in Biomedicine},
volume = {240},
pages = {107644},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107644},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723003097},
author = {Yoonseok Choi and Mohammed A. Al-masni and Kyu-Jin Jung and Roh-Eul Yoo and Seong-Yeong Lee and Dong-Hyun Kim},
keywords = {Brain tumor segmentation, Missing modality, Knowledge distillation, Barlow twins, nnU-Net, Glioblastoma},
abstract = {Background and objective
Precisely segmenting brain tumors using multimodal Magnetic Resonance Imaging (MRI) is an essential task for early diagnosis, disease monitoring, and surgical planning. Unfortunately, the complete four image modalities utilized in the well-known BraTS benchmark dataset: T1, T2, Fluid-Attenuated Inversion Recovery (FLAIR), and T1 Contrast-Enhanced (T1CE) are not regularly acquired in clinical practice due to the high cost and long acquisition time. Rather, it is common to utilize limited image modalities for brain tumor segmentation.
Methods
In this paper, we propose a single stage learning of knowledge distillation algorithm that derives information from the missing modalities for better segmentation of brain tumors. Unlike the previous works that adopted a two-stage framework to distill the knowledge from a pre-trained network into a student network, where the latter network is trained on limited image modality, we train both models simultaneously using a single-stage knowledge distillation algorithm. We transfer the information by reducing the redundancy from a teacher network trained on full image modalities to the student network using Barlow Twins loss on a latent-space level. To distill the knowledge on the pixel level, we further employ a deep supervision idea that trains the backbone networks of both teacher and student paths using Cross-Entropy loss.
Results
We demonstrate that the proposed single-stage knowledge distillation approach enables improving the performance of the student network in each tumor category with overall dice scores of 91.11% for Tumor Core, 89.70% for Enhancing Tumor, and 92.20% for Whole Tumor in the case of only using the FLAIR and T1CE images, outperforming the state-of-the-art segmentation methods.
Conclusions
The outcomes of this work prove the feasibility of exploiting the knowledge distillation in segmenting brain tumors using limited image modalities and hence make it closer to clinical practices.}
}
@article{ZHANG2020368,
title = {Cloud-to-end Rendering and Storage Management for Virtual Reality in Experimental Education},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {368-380},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300541},
author = {Hongxin Zhang and Jin Zhang and Xue Yin and Kan Zhou and Zhigeng Pan and Abdennour {El Rhalibi}},
keywords = {Clout-to-end render, Cloud storage, Virtual reality, Experimental education},
abstract = {Background
Real-time 3D rendering and interaction is important for virtual reality (VR) experimental education. Unfortunately, standard end-computing methods prohibitively escalate computational costs. Thus, reducing or distributing these requirements needs urgent attention, especially in light of the COVID-19 pandemic
Methods
In this study, we design a cloud-to-end rendering and storage system for VR experimental education comprising two models: background and interactive. The cloud server renders items in the background and sends the results to an end terminal in a video stream. Interactive models are then lightweight-rendered and blended at the end terminal. An improved 3D warping and hole-filling algorithm is also proposed to improve image quality when the user’s viewpoint changes.
Results
We build three scenes to test image quality and network latency. The results show that our system can render 3D experimental education scenes with higher image quality and lower latency than any other cloud rendering systems.
Conclusions
Our study is the first to use cloud and lightweight rendering for VR experimental education. The results demonstrate that our system provides good rendering experience without exceeding computation costs.}
}
@article{SERRANO2013648,
title = {Using virtual reality and mood-induction procedures to test products with consumers of ceramic tiles},
journal = {Computers in Human Behavior},
volume = {29},
number = {3},
pages = {648-653},
year = {2013},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2012.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0747563212003044},
author = {Berenice Serrano and Cristina Botella and Rosa M. Baños and Mariano Alcañiz},
keywords = {Virtual reality, Product testing, Mood-induction procedures, Relaxation, Sense of presence, Consumers},
abstract = {This work describes a Virtual Reality Environment (VRE), through which users are able to view and test ceramic tile products. Users’ virtual interfacing with the products generated emotional experiences that allowed them to feel “engaged” with the products. Users could choose between different kinds of products and test them out in order to know how they would look in a real-world context. In the VRE several mood-induction Procedures for inducing relaxation were included. The VRE was tested with respect to its ability to induce relaxation and sense of presence in 26 participants. It was also analyzed the level of satisfaction. Measures included the Visual Analogue Scale, the Self-Assessment Manikin, the Presence Self-Assessment Manikin, and a Satisfaction Scale. The results showed that the VRE was effective among participants in inducing relaxation and high sense of presence. In addition, participants’ satisfaction with the experience in the VRE was high. The VRE thus could be a useful tool for showing and testing products and for evoking a positive emotional association while users are interacting.}
}
@article{GUPTA2020112,
title = {Classification of patients with tumor using MR FLAIR images},
journal = {Pattern Recognition Letters},
volume = {139},
pages = {112-117},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2017.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167865517304051},
author = {Tanvi Gupta and Tapan K. Gandhi and R.K. Gupta and B.K. Panigrahi},
keywords = {MRI, SVM, FLAIR, Tumor},
abstract = {Magnetic Resonance Imaging (MRI) is a fast growing imaging tool for neurodiagnosis. The radiologists time is at a premium due to increase in patient studies each having a large data set. This can be aided by classification using machine learning techniques. This paper evaluates its utility for accurate and rapid diagnosis of cerebral tumors. Two hundred subjects were classified into normal and abnormal using volumetric Fluid Attenuated Inversion Recovery (FLAIR) acquisition. The images are normalized to obtain 12 useful slices to be considered as the patient feature set for classification. Discrete Wavelet Transform (DWT) is used for feature extraction and Principal Component Analysis (PCA) is used for feature selection. Various classifiers like Support Vector Machine (SVM), k-Nearest Neighbor (k-NN), CART (Classification and Regression Tree) and Random forest are tested. Applying K-fold cross validation in each train-test ratios, we obtained ceiling level classification accuracy with coherent sensitivity and specificity using only linear SVM (negating the use of PCA). An accuracy of 88% is obtained with a sensitivity of 84% and specificity of 92% with 62.28 s computation time. The algorithm is robust to be tested in clinical settings.}
}
@article{DELIMA2022100515,
title = {Adaptive virtual reality horror games based on Machine learning and player modeling},
journal = {Entertainment Computing},
volume = {43},
pages = {100515},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100515},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000398},
author = {Edirlei Soares {de Lima} and Bruno M.C. Silva and Gabriel Teixeira Galam},
keywords = {Horror Games, Virtual Reality, Player Modeling, Adaptive Games},
abstract = {Fear is a basic human emotion that can be triggered by different situations, which vary from person to person. However, game developers usually design horror games based on a general knowledge about what most players fear, which does not guarantee a satisfying horror experience for every-one. When a horror game aims at intensifying the fear evoked in individual players, having useful information about the fears of the current player is vital to promote more frightening experiences. This work presents a new method to create adaptive virtual reality horror games, which combines player modeling techniques and an adaptive agent-based system that can identify what individual players fear and adapt the content of the game to intensify the fear evoked in players. The main contributions of this work are: (1) a new method to identify individual player’s fears using only gameplay data and machine learning techniques; and (2) a new agent-based adaptive game system that can track the horror intensity experienced by players and moderate the use of the horror elements feared by individual players in the game. The results show that the proposed method is capable of correctly identifying players’ fears (average accuracy of 79.4% for new players). In addition, results of a user study and statistical significance tests (ANOVA and post-hoc analyses) suggest that our method can intensify the fear evoked in players and positively improve immersion and flow.}
}
@article{TATIC20171,
title = {The application of augmented reality technologies for the improvement of occupational safety in an industrial environment},
journal = {Computers in Industry},
volume = {85},
pages = {1-10},
year = {2017},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516302718},
author = {Dušan Tatić and Bojan Tešić},
keywords = {Augmented reality, Occupational safety, Mobile devices, Virtual reality, Real world interaction},
abstract = {In many branches of industry, occupational safety experts identified two main causes of worker injuries related to the usage of modern electro-mechanical machines and systems: inadequate training and insufficient work experience, and monotonicity of the tasks often performed repeatedly. In this paper, we present a system based on augmented reality (AR) technologies that can be useful in reducing these factors of risk at work and decreasing the error rate and preventing injuries. The system that is implemented on mobile devices is intended to project augmented reality instructions directly at the work place. A worker is led by the AR-system step by step through various work and safety procedures that should be performed. Each procedure consists of steps specified by a series of instructions accessed through an interactive check list. To ensure the safeness, if a confirmation is missing because of a skipped, incompletely, or wrongly performed step of a procedure, the AR-system blocks further implementation of the procedure and returns the worker to the previous step until the correct actions are carried out. At the same time, interactive work with the checklist breaks the monotonicity of the job. The system is personalized according to skills of a worker by taking into account his professional training and work experience. Depending on that it is determined the amount of data to be displayed to a worker helping even less skilled workers to perform a task. As a case study, the proposed approach is implemented as an instructional and occupational safety system for work at a universal lathe, which is an element of many technological processes of Thermal Power Plant Ugljevik in the Republika Srpska, Bosnia and Herzegovina, where this AR-system was experimentally implemented and verified.}
}
@article{GUI20121565,
title = {Morphology-driven automatic segmentation of MR images of the neonatal brain},
journal = {Medical Image Analysis},
volume = {16},
number = {8},
pages = {1565-1579},
year = {2012},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2012.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1361841512000989},
author = {Laura Gui and Radoslaw Lisowski and Tamara Faundez and Petra S. Hüppi and François Lazeyras and Michel Kocher},
keywords = {Automatic segmentation, Magnetic resonance imaging, Neonatal brain, Watershed segmentation, Mathematical morphology},
abstract = {The segmentation of MR images of the neonatal brain is an essential step in the study and evaluation of infant brain development. State-of-the-art methods for adult brain MRI segmentation are not applicable to the neonatal brain, due to large differences in structure and tissue properties between newborn and adult brains. Existing newborn brain MRI segmentation methods either rely on manual interaction or require the use of atlases or templates, which unavoidably introduces a bias of the results towards the population that was used to derive the atlases. We propose a different approach for the segmentation of neonatal brain MRI, based on the infusion of high-level brain morphology knowledge, regarding relative tissue location, connectivity and structure. Our method does not require manual interaction, or the use of an atlas, and the generality of its priors makes it applicable to different neonatal populations, while avoiding atlas-related bias. The proposed algorithm segments the brain both globally (intracranial cavity, cerebellum, brainstem and the two hemispheres) and at tissue level (cortical and subcortical gray matter, myelinated and unmyelinated white matter, and cerebrospinal fluid). We validate our algorithm through visual inspection by medical experts, as well as by quantitative comparisons that demonstrate good agreement with expert manual segmentations. The algorithm’s robustness is verified by testing on variable quality images acquired on different machines, and on subjects with variable anatomy (enlarged ventricles, preterm- vs. term-born).}
}
@article{HARUN2020660,
title = {Experience Fleming’s rule in Electromagnetism Using Augmented Reality: Analyzing Impact on Students Learning},
journal = {Procedia Computer Science},
volume = {172},
pages = {660-668},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314150},
author = { Harun and Neha Tuli and Archana Mantri},
keywords = {Augmented Reality, Electromagnetism, Experimentation, Fleming’s rule, Virtual Reality},
abstract = {Education trends are changing from traditional teaching to Electronic-learning and Mobile-learning. Through the development of information and communication technology, many e-learning systems were developed but, they lack in terms of user interaction. This affects student’s learning, motivation, performance, thinking level, technical skills, and expertise. The accessibility of practical labs in science and engineering education with expensive equipment and apparatus is restricted for a limited amount of time for large number of students. According to the previous research, it was found that there are fewer studies in the field of electromagnetism by using Augmented Reality (AR) application. Moreover, students are facing difficulties to understand Fleming’s rule and for this, no practical experiment is available in the physics lab. The solution to overcome these problems is through the use of Augmented Reality (AR) and Virtual Reality (VR) labs. In this research the Concept of Fleming’s rule in Electromagnetism is explained by using AR technique and the visualization of magnetic field lines, electric current and force exerted on current-carrying conductor has been done. The efficiency of the proposed system was calculated and team-based activity among secondary and applied engineering students was analyzed. The knowledge gain by the students was measured using the pre-test and post-test method. The outcomes of this research revealed that AR technology was more efficient in enhancing students’ knowledge in the field of Electromagnetism. The investigation also suggests that AR application helps the students to gain higher flow understanding levels than those attained by a web-based application.}
}
@article{JENA2021104293,
title = {Maximum 3D Tsallis entropy based multilevel thresholding of brain MR image using attacking Manta Ray foraging optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {103},
pages = {104293},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104293},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001408},
author = {Bibekananda Jena and Manoj Kumar Naik and Rutuparna Panda and Ajith Abraham},
keywords = {Machine intelligence, Soft computing, Multilevel thresholding, Brain MRI},
abstract = {Nevertheless, the accuracy of a multilevel image thresholding technique using 1D or 2D Tsallis entropy is limited. To overcome this, we propose a maximum 3D Tsallis entropy-based multilevel thresholding method. The idea of 3D Tsallis entropy is introduced. Opposed to the 1D/2D Tsallis entropy, the 3D Tsallis entropy-based approach is more robust, it performs well even in the case of the low signal-to-noise-ratio and contrast. Manta Ray Foraging Optimization (MRFO) algorithm is a newly introduced algorithm to solve the optimization problem by imitating the foraging technique of Manta Ray fish in the ocean using a mathematical model. Due to insufficient energy levels of search agents in MRFO, they fail to avoid local minima and fall on it. To make the algorithm more effective for the segmentation application, we introduce a new algorithm coined as attacking Manta Ray foraging optimization (AMRFO). A set of classical benchmark functions together with composite functions (CEC 2014) is used to validate the proposed AMRFO algorithm. Statistical analysis is implicitly carried out using Wilcoxon’s signed-rank test and Friedman’s mean rank test. Interestingly, the results show that the proposed AMRFO is superior to the state-of-the-art optimization algorithms. Moreover, the proposed method is also compared with 1D/2D Tsallis entropy-based approaches. To experiment, 100 test images from the AANLIB MR Image dataset are considered. Our method outperforms 1D/2D Tsallis entropy-based approaches. The proposed scheme would be useful for the segmentation of multi-spectral color images.}
}
@article{IQBAL2021103841,
title = {Augmented reality in robotic assisted orthopaedic surgery: A pilot study},
journal = {Journal of Biomedical Informatics},
volume = {120},
pages = {103841},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103841},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421001702},
author = {Hisham Iqbal and Fabio Tatti and Ferdinando {Rodriguez y Baena}},
keywords = {Augmented Reality, Surgical Workflows, Human-Machine Interfacing},
abstract = {Background
The research and development of augmented-reality (AR) technologies in surgical applications has seen an evolution of the traditional user-interfaces (UI) utilised by clinicians when conducting robot-assisted orthopaedic surgeries. The typical UI for such systems relies on surgeons managing 3D medical imaging data in the 2D space of a touchscreen monitor, located away from the operating site. Conversely, AR can provide a composite view overlaying the real surgical scene with co-located virtual holographic representations of medical data, leading to a more immersive and intuitive operator experience.
Materials and Methods
This work explores the integration of AR within an orthopaedic setting by capturing and replicating the UI of an existing surgical robot within an AR head-mounted display worn by the clinician. The resulting mixed-reality workflow enabled users to simultaneously view the operating-site and real-time holographic operating informatics when carrying out a robot-assisted patellofemoral-arthroplasty (PFA). Ten surgeons were recruited to test the impact of the AR system on procedure completion time and operating surface roughness.
Results and Discussion
The integration of AR did not appear to require subjects to significantly alter their surgical techniques, which was demonstrated by non-significant changes to the study’s clinical metrics, with a statistically insignificant mean increase in operating time (+0.778 s, p = 0.488) and a statistically insignificant change in mean surface roughness (p = 0.274). Additionally, a post-operative survey indicated a positive consensus on the usability of the AR system without incurring noticeable physical distress such as eyestrain or fatigue.
Conclusions
Overall, these study results demonstrated a successful integration of AR technologies within the framework of an existing robot-assisted surgical platform with no significant negative effects in two quantitative metrics of surgical performance, and a positive outcome relating to user-centric and ergonomic evaluation criteria.}
}
@article{KAMATHE201841,
title = {A novel method based on independent component analysis for brain MR image tissue classification into CSF, WM and GM for atrophy detection in Alzheimer’s disease},
journal = {Biomedical Signal Processing and Control},
volume = {40},
pages = {41-48},
year = {2018},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1746809417302069},
author = {Rupali S. Kamathe and Kalyani R. Joshi},
keywords = {Independent component analysis, Band expansion process, Brain MRI, Support vector machines, Alzheimer’s disease},
abstract = {Brain Magnetic Resonance Image (MRI) plays a vital role in diagnosis of diseases like Brain Tumor, Alzheimer, Multiple Sclerosis, Schizophrenia and other White Matter Lesions. In most of the cases accurate segmentation of Brain MRI into tissue types like Cerebro-Spinal Fluid (CSF), White Matter (WM) and Grey Matter (GM) is of interest. The diagnostic accuracy of expert and non-expert Radiologists can be improved with accurate and automated tissue segmentation and classification system. Such system can also be used for trainees to understand the individual tissue distribution in MRI scans. In this paper, we propose a novel automated tissue segmentation and classification method based on Independent Component Analysis (ICA) with Band Expansion Process (BEP) and Support Vector Machine (SVM) classifier which with input as T1, T2 and Proton Density (PD) scans of patient, provides output as CSF, WM and GM indicating the possible atrophy in brain which can help in diagnosis of Alzheimer’s disease (AD). The objective of this work is to test the effectiveness of ICA with different input images generated using BEP for accurate brain tissue segmentation by validating results with different quality metrics. The novel method for generating input images for ICA has been implemented and segmented tissues are used for atrophy detection. The BEP+ICA+Thresholding+‘SVM trained with Grey Level Co-occurrence Matrix (GLCM) based texture features’ is giving 100% tissue classification accuracy for test samples under consideration.}
}
@incollection{HARTMANN2019323,
title = {16 - Virtual reality and immersive approaches to contextual food testing},
editor = {Herbert L. Meiselman},
booktitle = {Context},
publisher = {Woodhead Publishing},
pages = {323-338},
year = {2019},
isbn = {978-0-12-814495-4},
doi = {https://doi.org/10.1016/B978-0-12-814495-4.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144954000167},
author = {Christina Hartmann and Michael Siegrist},
keywords = {Virtual reality, Food behavior, Consumer, Immersive environment, Food},
abstract = {The goal of this chapter is to summarize existing studies that have used different forms of virtual reality (VR) in the domain of consumer research on foods. Throughout this chapter, we refer to articles that were identified via a systematic literature search. Some studies focused on the applicability and validity of VR in the food context, while others tested the products’ sensory appeal in different immersive environments. We describe two of our own studies in which we first tested the validity of a virtual food buffet, and then tested whether participants apply the same search strategies in VR as they do in reality food shopping. The applicability of VR to evoke food-related emotions is discussed, and the limitations of VR in food behavior research that must be overcome are highlighted. Research gaps are identified, and the potential for VR applications in food behavior research is highlighted.}
}
@article{LAMB2022100003,
title = {The moderating role of creativity and the effect of virtual reality on stress and cognitive demand during preservice teacher learning},
journal = {Computers & Education: X Reality},
volume = {1},
pages = {100003},
year = {2022},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2022.100003},
url = {https://www.sciencedirect.com/science/article/pii/S2949678022000034},
author = {Richard Lamb and Jonah Firestone},
keywords = {Preservice teacher preparation, Teacher education, Virtual reality, Stress, Protective factors, Sensors, fNIRS},
abstract = {Virtual reality (VR) has received considerable attention related to its use in teacher development over the last decade. Despite this attention, there is insufficient understanding of how specific underlying cognitive system responses and autonomic nervous system responses moderate the use of VR and the associated learning outcomes in the development of preservice teachers. This work intends to explore and evaluate preservice service teachers’ (PSTs) experiences using VR and the effects of creativity, mental flexibility (MF), acute stress, and cognitive demand (CD). Forty-eight undergraduate college students in year two of their teacher preparation program were recruited for the study. Each of the preservice teachers was assigned randomly to one of two conditions, microteaching (n ​= ​24) or VR (n ​= ​24). The use of these two conditions allowed the researchers to compare the effects of creativity and MF [measured using the Torrance Test of Creative Thinking] on cognitive demand and stress responses [measured using heart rate variability and electrodermal activity]. Results from analysis of the hemodynamic data and stress response data illustrate that the protective factors of creativity and MF may moderate success in VR and the reduction of cognitive demand and stress when VR is used to develop skills related to teaching.}
}
@article{KIM2007842,
title = {A recursive camera resectioning technique for off-line video-based augmented reality},
journal = {Pattern Recognition Letters},
volume = {28},
number = {7},
pages = {842-853},
year = {2007},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2006.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167865506002947},
author = {Jong-Sung Kim and Ki-Sang Hong},
keywords = {Augmented reality, Camera resectioning, Unscented particle filter},
abstract = {In this paper, we propose a new recursive framework for camera resectioning and apply it to off-line video-based augmented reality. Our method is based on an unscented particle filter and an independent Metropolis–Hastings chain, which deal with nonlinear dynamic systems without local linearization, and lead to more accurate results than other nonlinear filters. The proposed method has some desirable properties for camera resectioning: Since it does not rely on erroneous linear solutions, initialization problems do not occur, in contrast to the previous resectioning methods. Jittering error can be reduced by considering consistency and coherency between adjacent frames in our recursive framework. Our method is fairly accurate comparable to nonlinear optimization methods, which in general have higher levels of computation and complexity. As a result, the proposed algorithm outperforms the standard camera resectioning algorithm. We verify the effectiveness of our method through several experiments using synthetic and real image sequences comparing the estimation performance with other linear and nonlinear methods.}
}
@article{GOULDING2012103,
title = {Construction industry offsite production: A virtual reality interactive training environment prototype},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {1},
pages = {103-116},
year = {2012},
note = {Network and Supply Chain System Integration for Mass Customization and Sustainable Behavior},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000851},
author = {Jack Goulding and Wafaa Nadim and Panagiotis Petridis and Mustafa Alshawi},
keywords = {Construction industry, Game engines, Offsite production, Training, Virtual environment, Simulation},
abstract = {The ‘traditional’ construction industry has constantly been challenged to improve its inherent problematic practices. Offsite production (OSP), under the umbrella of modern methods of construction (MMC), has been acknowledged as a means to help improve construction industry performance as well as meet new market demands through the provision of improved, adaptable, and sustainable buildings. However, the deployment of OSP systems, if not managed properly, may adversely affect the end result and be counterproductive. It is therefore imperative that the construction industry stakeholders learn and appreciate the specifics, merits, as well as the risks associated with OSP systems in order to achieve the desired outcomes and consequently improve industry performance. On-the-job-training (OJT) is usually sought to facilitate ‘experiential’ learning, which is argued to be particularly effective where a great deal of independence is granted to the task performer. However, OJT has been criticised for being expensive, limited, and sometimes devoid of the actual training context. In order to address the problems encountered with OJT, several virtual reality (VR) solutions have been proposed. This paper introduces one such VR solution prototype, in order to provide a risk-free environment for learning without the ‘do-or-die’ consequences often faced on real construction projects. The proffered solution provides a unique VR environment for practicing new working conditions associated with OSP practices. While the ‘scenes’ of the VR environment take place on a construction site, the environment predominantly targets professionals, such as project managers, construction managers, architects, designers, suppliers and manufacturers, to allow multidisciplinary learning to occur, and hence overcome ‘knowledge silos’ or ‘knowledge compartmentation’. The VR environment enables unforeseen problems often caused by professionals’ decisions, faulty work, and health and safety issues to occur; where the implications of which can be evaluated in respect of time, cost and resources. The VR environment proposed does not aim to resolve problems associated with OSP per se, rather aims to allow ‘things to go wrong’ and consequently allows users not only to ‘experience’ the resulting implications but also to reflect on those implications as part of the learning process. This paper discusses and presents the prototype for the first development phase of the VR interactive training environment. While the prototype was tested and validated with domain experts from industry, the research community, and academia from different EU countries, the data used in developing the prototype was constrained to one project in the UK which may limit the generalisability of results.}
}
@article{MIAO2020105132,
title = {Virtual reality-based measurement of ocular deviation in strabismus},
journal = {Computer Methods and Programs in Biomedicine},
volume = {185},
pages = {105132},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.105132},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719309733},
author = {Yinan Miao and Jun Young Jeon and Gyuhae Park and Sang Woo Park and Hwan Heo},
keywords = {Virtual reality, Pupil tracking, Computer vision, Strabismus, Cover tests},
abstract = {Background and objective
Strabismus is an eye movement disorder in which shows the abnormal ocular deviation. Cover tests have mainly been used in the clinical diagnosis of strabismus for treatment. However, the whole process depends on the doctor's level of experience, which could be subjected to several factors. In this study, an automated technique for measurement of ocular deviation using a virtual reality (VR) device is developed.
Methods
A VR display system in which the screens that have the fixation target are changed alternately between on and off stages is used to simulate the normal strabismus diagnosis steps. Patients watch special-designed 3D scenes, and their eye motions are recorded by two infrared (IR) cameras. An image-processing-based pupil tracking technique is then applied to track their eye movement. After recording eye motion, two strategies for strabismus angle estimation are implemented: direct measurement and stepwise approximation. The direct measurement converts the eye movement to a strabismus angle after considering the eyeball diameter, while the stepwise approximation measures the ocular deviation through the feedback calibration process.
Results
Experiments are carried out with various strabismus patients. The results are compared to those of their doctors’ measurement, which shows good agreement.
Conclusions
The results clearly indicate that these techniques could identify ocular deviation with high accuracy and efficiency. The proposed system can be applied in small space and has high tolerance for the unexpected head movements compared with other camera-based system.}
}
@article{SERRANO20161,
title = {Virtual reality and stimulation of touch and smell for inducing relaxation: A randomized controlled trial},
journal = {Computers in Human Behavior},
volume = {55},
pages = {1-8},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215300856},
author = {Berenice Serrano and Rosa M. Baños and Cristina Botella},
keywords = {Virtual reality, Mood-induction, Relaxation, Touch, Smell, Augmented virtuality},
abstract = {The aim of this study was to test the efficacy of a mood-induction procedure in a Virtual Reality (VR-MIP) environment for inducing relaxation and generating sense of presence, and to test whether the stimulation of the senses of touch and smell improves the efficacy of this VR-MIP. A controlled study was carried out with four experimental conditions. All of them included the VR-MIP to induce relaxation, but varying the senses stimulated. The sample consisted of 136 participants randomly assigned to one of the four experimental conditions. Emotions and sense of presence were evaluated. The results showed statistical differences before and after mood-induction and a high sense of presence in all groups. However, no statistical differences were found among the four groups on emotions and sense of presence. The results showed that the VR-MIP was effective; however, the stimulation of the senses of touch and smell did not show significate improve of the mood-induction or the sense of presence. It was identified a trend in favor of the groups where the sense of touch was stimulated, they seemed more relaxed and the sense of presence was higher. We hypothesized that the stimulation of sense of touch, could improve the efficacy when using VR-MIP because it provides more sensory information.}
}
@article{WU2020102802,
title = {Hand pose estimation in object-interaction based on deep learning for virtual reality applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102802},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102802},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300523},
author = {Min-Yu Wu and Pai-Wen Ting and Ya-Hui Tang and En-Te Chou and Li-Chen Fu},
keywords = {Hand pose estimation, Deep learning, Convolutional neural network, Spherical part model},
abstract = {Hand Pose Estimation aims to predict the position of joints on a hand from an image, and it has become popular because of the emergence of VR/AR/MR technology. Nevertheless, an issue surfaces when trying to achieve this goal, since a hand tends to cause self-occlusion or external occlusion easily as it interacts with external objects. As a result, there have been many projects dedicated to this field for a better solution of this problem. This paper develops a system that accurately estimates a hand pose in 3D space using depth images for VR applications. We propose a data-driven approach of training a deep learning model for hand pose estimation with object interaction. In the convolutional neural network (CNN) training procedure, we design a skeleton-difference loss function, which effectively can learn the physical constraints of a hand. Also, we propose an object-manipulating loss function, which considers knowledge of the hand-object interaction, to enhance performance. In the experiments we have conducted for hand pose estimation under different conditions, the results validate the robustness and the performance of our system and show that our method is able to predict the joints more accurately in challenging environmental settings. Such appealing results may be attributed to the consideration of the physical joint relationship as well as object information, which in turn can be applied to future VR/AR/MR systems for more natural experience.}
}
@article{DACOSTA2004173,
title = {The acceptance of virtual reality devices for cognitive rehabilitation: a report of positive results with schizophrenia},
journal = {Computer Methods and Programs in Biomedicine},
volume = {73},
number = {3},
pages = {173-182},
year = {2004},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(03)00066-X},
url = {https://www.sciencedirect.com/science/article/pii/S016926070300066X},
author = {Rosa Maria Esteves Moreira {da Costa} and Luı́s Alfredo Vidal {de Carvalho}},
keywords = {Virtual reality, Virtual environment, Cognitive rehabilitation, Schizophrenia},
abstract = {This study presents a process of virtual environment development supported by a cognitive model that is specific to cognitive deficits of diverse disorders or traumatic brain injury, and evaluates the acceptance of computer devices by a group of schizophrenic patients. The subjects that participated in this experiment accepted to work with computers and immersive glasses and demonstrated a high level of interest in the proposed tasks. No problems of illness have been observed. This experiment indicated that further research projects must be carried out to verify the value of virtual reality technology for cognitive rehabilitation of psychiatric patients. The results of the current study represent a small but necessary step in the realization of that potential.}
}
@article{MONTEIRO20231,
title = {Exploring the user experience of hands-free VR interaction methods during a Fitts’ task},
journal = {Computers & Graphics},
volume = {117},
pages = {1-12},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932300242X},
author = {Pedro Monteiro and Hugo Coelho and Guilherme Gonçalves and Miguel Melo and Maximino Bessa},
keywords = {Hands-free, HCI, Immersive Virtual Reality, Interaction, Usability},
abstract = {Despite advancements in interaction with immersive Virtual Reality (VR) systems, using hand gestures for all interactions still imposes some challenges, especially in interactions with graphical user interfaces that are usually performed with point-and-click interfaces. Therefore, exploring the use of alternative hands-free methods for selection is essential to overcome usability problems and provide natural interaction for users. The results and insights gained from this exploration can lead to enhanced user experiences in VR applications. This study aims to contribute to the literature with the evaluation of the usability of the most commonly used hands-free methods for selection and system control tasks in immersive VR and their impact on standard and validated experience and usability metrics, namely the sense of presence, cybersickness, system usability, workload, and user satisfaction. A Fitts’ selection task was performed using a within-subjects design by nine participants experienced in VR. The methods evaluated were the handheld controllers, the head gaze, eye gaze, and voice commands for pointing at the targets, and dwell time and voice commands to confirm the selections. Results show that the methods provide similar levels of sense of presence and low cybersickness while showing low workload values and high user satisfaction, matching the experience of traditional handheld controllers for non-multimodal approaches. The assisted eye gaze with dwell was the preferred hands-free method and the one with the highest values of usability. Still, developers should minimize the number of gaze movements to reduce fatigue. The evaluation also showed that using a multimodal approach for selections, especially using the voice, decreases user satisfaction and increases users’ frustration.}
}
@article{QIN2021101620,
title = {Attractiveness of game elements, presence, and enjoyment of mobile augmented reality games: The case of Pokémon Go},
journal = {Telematics and Informatics},
volume = {62},
pages = {101620},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101620},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321000599},
author = {Yan Qin},
keywords = {Game enjoyment, Game elements, Presence, Self-determination theory, Mobile AR game, Pokémon Go},
abstract = {Building on the enjoyment-as-need-satisfaction model, this study examines the attractiveness of various game elements of Pokémon Go to its players and the associations among the attractiveness of game elements, players’ need satisfaction, sense of presence, enjoyment, and length of gameplay. Participants (N = 719) who had played Pokémon Go filled out an online survey. A research model with the aforementioned constructs was tested using structural equation modeling. This study found that autonomy- and relatedness-supportive elements associated with autonomy and relatedness need satisfaction of the player, which related to presence significantly. The satisfaction of autonomy and competence needs related to game enjoyment, which influenced gameplay length. Theoretical and practical implications were discussed.}
}
@article{SVEINSSON2021105836,
title = {ARmedViewer, an augmented-reality-based fast 3D reslicer for medical image data on mobile devices: A feasibility study},
journal = {Computer Methods and Programs in Biomedicine},
volume = {200},
pages = {105836},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720316692},
author = {Bragi Sveinsson and Neha Koonjoo and Matthew S Rosen},
keywords = {Augmented reality, Medical imaging, Mobile device},
abstract = {Background and objective: Medical images obtained by methods such as magnetic resonance imaging (MRI) or computed tomography (CT) are typically displayed as a stack of 2D slices, comprising a 3D volume. Often, the anatomy of interest does not fall neatly into the slice plane but rather extends obliquely through several slices. Reformatting the data to show the anatomy in one slice in conventional medical imaging software can require expertise and time. In this work, we present ARmedViewer, a medical image viewing app designed for mobile devices that uses augmented reality technology to display medical image data. An arbitrary plane for displaying the data can be chosen quickly and intuitively by moving the mobile device. Methods: The app ARmedViewer, compiled for an iOS device, was designed to allow a user to easily select from a list of 3D image datasets consisting of header information and image data. The user decides where to place the data, which can be overlaid on actual human anatomy. After loading the dataset, the user can move and rotate the data as desired. 15 users compared the user experience of the app to a common image viewer by answering two user surveys each, one custom and one standardized. The utility of the app was also tested by having two users find a plane through a 3D dataset that displayed 3 randomly placed lesions. This operation was timed and compared between the app and a standard medical image viewer. Results: ARmedViewer was successfully developed and run on an iPhone XS. User interfaces for selecting, placing, moving, reslicing, and displaying the data were operated with ease, even by naïve users. The custom user survey indicated that freely selecting a slice through the data was significantly more intuitive and easier using the app than using a conventional image viewer on a computer workstation, and changing the viewing angle was also significantly more intuitive. The standardized survey indicated a significantly better user experience for the app in several categories, and never significantly worse. The timed reslicing experiments demonstrated the app being faster than the standard image viewer by an average factor of 9. Conclusions: The newly developed ARmedViewer is a portable software tool for easily displaying 3D medical image data overlaid on human anatomy, allowing for easy choice of the viewing plane by intuitively moving the mobile device.}
}
@article{TURKAN201790,
title = {Mobile augmented reality for teaching structural analysis},
journal = {Advanced Engineering Informatics},
volume = {34},
pages = {90-100},
year = {2017},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616302944},
author = {Yelda Turkan and Rafael Radkowski and Aliye Karabulut-Ilgu and Amir H. Behzadan and An Chen},
abstract = {Structural analysis is an introductory core course that is taught in every civil engineering program as well as in most architectural and construction engineering programs. Previous research unveils students' deficits in understanding the behavior of structural elements in a three-dimensional (3D) context due to the shortcomings of traditional lecturing approaches, which put too much emphasis on the analysis of individual structural members, thereby falling short in providing a solid, easy-to-follow, and holistic approach to analyzing complex structures with a large number of interconnected elements. In this paper, the authors introduce a new pedagogy for teaching structural analysis that incorporates mobile augmented reality (AR) and interactive 3D visualization technology. The goal of this study is to enhance the contents used in structural analysis textbooks and on worksheets by visualizing discrete structural members employing AR along with interactive 3D models in order to illustrate how the structures behave under different loading conditions. Students can interactively change the load and observe the reaction resulting from this change with the instant feedback provided by the AR interface. The feasibility of AR concepts and interaction metaphors, as well as the potential of using AR for teaching structural analysis are investigated, specifically by focusing on challenges regarding content integration and interaction. An AR application is designed and developed, and a pilot study is conducted in a junior level structural analysis class to assess the pedagogical impact and the design concepts employed by the AR tool. Control and test groups are deployed, and students’ performance is measured using pre- and post-tests. The results of the pilot study indicate that the utilized AR design concepts have potential to contribute to students’ learning by providing interactive and 3D visualization features, which support constructive engagement and retention of information in students.}
}
@article{LIU201873,
title = {Joint foveation-depth just-noticeable-difference model for virtual reality environment},
journal = {Journal of Visual Communication and Image Representation},
volume = {56},
pages = {73-82},
year = {2018},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318301871},
author = {Di Liu and Yingbin Wang and Zhenzhong Chen},
keywords = {Just-noticeable-difference, Depth, Foveation, Stereoscopic images, Virtual reality environment},
abstract = {In this paper, we develop a joint foveation-depth just-noticeable-difference (FD-JND) model to quantify the perceptual redundancy of image in the VR display environment. The proposed FD-JND model is developed with considerations on the effects of both foveation and depth. More specifically, experiments for the VR environment on synthesized stimuli are conducted based on luminance masking and contrast masking and the FD-JND model is developed accordingly. Subjective quality discrimination experiments between the noise contaminated images and original ones validate favorableness of the proposed FD-JND model.}
}
@article{HSIEH200291,
title = {Virtual reality simulator for osteotomy and fusion involving the musculoskeletal system},
journal = {Computerized Medical Imaging and Graphics},
volume = {26},
number = {2},
pages = {91-101},
year = {2002},
issn = {0895-6111},
doi = {https://doi.org/10.1016/S0895-6111(01)00034-9},
url = {https://www.sciencedirect.com/science/article/pii/S0895611101000349},
author = {Ming-Shium Hsieh and Ming-Dar Tsai and Wen-Chien Chang},
keywords = {Osteotomy and fusion, Three-dimensional environment, Surgery simulation, Virtual reality, Teaching, training and planning},
abstract = {In this study, the three-dimensional virtual reality simulation system described herein provides preoperative simulation to verify that the osteotomy and fusion procedures chosen to treat musculoskeletal defects are appropriate. The system also provides an excellent means of training surgeons in new operations without putting patients at risk, and may be especially useful for difficult surgical procedures often performed in orthopedics, craniofacial disease, or plastic and reconstructive surgery departments. The system can be used to teach intern and train resident doctors, and is a planning tool for visiting staff.}
}
@article{BACCA201549,
title = {Mobile Augmented Reality in Vocational Education and Training},
journal = {Procedia Computer Science},
volume = {75},
pages = {49-58},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.203},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036649},
author = {Jorge Bacca and Silvia Baldiris and Ramon Fabregat and  Kinshuk and Sabine Graf},
keywords = {Augmented Reality, Vocational Education and Training, Motivation, Universal Design for Learning, Special Educational Needs},
abstract = {In Vocational Education and Training (VET) institutions, teachers face important difficulties in the teaching process due to a wide variety of student's special educational needs as well as student's lack of: the adequate level of basic competence, motivation, concentration, attention, confidence and background knowledge, among other aspects. Regarding the attention to these aspects, many studies have reported positive impact of Augmented Reality (AR) applications in primary, secondary and higher education in terms of student's motivation, learning gains, collaboration, interaction, learning attitudes and enjoyment, among others. However, very little has been done in terms of AR applications in VET as well as their impact on wide variety of student's special educational needs such as learning difficulties. This paper introduces a marker-based mobile AR application named Paint-cAR for supporting the learning process of repairing paint on a car in the context of a vocational education programme of car's maintenance. The application was developed using a methodology for developing mobile AR applications for educational purposes from a collaborative creation process (Co-Creation) and based on the Universal Design for Learning (UDL). A cross-sectional evaluation study was conducted to validate the Paint-cAR application in a real scenario.}
}
@article{HIGUERATRUJILLO2017398,
title = {Psychological and physiological human responses to simulated and real environments: A comparison between Photographs, 360° Panoramas, and Virtual Reality},
journal = {Applied Ergonomics},
volume = {65},
pages = {398-409},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0003687017301175},
author = {Juan Luis Higuera-Trujillo and Juan {López-Tarruella Maldonado} and Carmen {Llinares Millán}},
keywords = {Virtual reality, 360° Panorama, Validity, Psychological human responses, Physiological human responses},
abstract = {Psychological research into human factors frequently uses simulations to study the relationship between human behaviour and the environment. Their validity depends on their similarity with the physical environments. This paper aims to validate three environmental-simulation display formats: photographs, 360° panoramas, and virtual reality. To do this we compared the psychological and physiological responses evoked by simulated environments set-ups to those from a physical environment setup; we also assessed the users' sense of presence. Analysis show that 360° panoramas offer the closest to reality results according to the participants' psychological responses, and virtual reality according to the physiological responses. Correlations between the feeling of presence and physiological and other psychological responses were also observed. These results may be of interest to researchers using environmental-simulation technologies currently available in order to replicate the experience of physical environments.}
}
@article{HAN2022101634,
title = {Process and Outcome-based Evaluation between Virtual Reality-driven and Traditional Construction Safety Training},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101634},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101634},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000994},
author = {Yu Han and Jinru Yang and Yongsheng Diao and Ruoyu Jin and Brian Guo and Zulfikar Adamu},
keywords = {Construction safety, Embodied cognition, Safety training, Virtual reality, Immersive technology, Hazard recognition},
abstract = {The emerging digital technologies such as virtual reality (VR) provide an alternative platform for construction safety training. In order to explore how digital-driven technologies affect the effectiveness of safety training, there is a need to empirically test the differences in performance between digital 3D/VR safety training and traditional 2D/paper approach. This research conducted a performance evaluation that emphasises both the training process and learning outcomes of trainees based on researchers’ self-developed immersive construction safety training platform. Data related to physiological indicators such as skin resistance were collected to measure safety performance before and after the training. The detailed measurement indicators included nine categories (e.g., immersion, inspiration) to form a holistic list of evaluation dimensions. The findings revealed that VR-driven immersive safety training outperformed the traditional way for trainees in terms of both process and outcome-based indicators. Results confirmed that safety training was no longer constrained by understanding or memorizing 2D information (texts and images). Instead, trainees experienced a stronger sense of embodied cognition through the immersive experience and multi-sensory engagement by interacting with the VR-driven system. By engaging the theory of embodied cognition, this research provides both the empirical evidence and in-depth analysis of how immersive virtual safety training outperforms traditional training in terms of both training process and outcomes.}
}
@article{PARK2020101887,
title = {Deep learning-based smart task assistance in wearable augmented reality},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {63},
pages = {101887},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101887},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518306240},
author = {Kyeong-Beom Park and Minseok Kim and Sung Ho Choi and Jae Yeol Lee},
keywords = {Smart task assistance, Wearable augmented reality, Deep learning-based task assistance, AR smart glasses},
abstract = {Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.}
}
@article{YEH2014311,
title = {Machine learning-based assessment tool for imbalance and vestibular dysfunction with virtual reality rehabilitation system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {116},
number = {3},
pages = {311-318},
year = {2014},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2014.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169260714001667},
author = {Shih-Ching Yeh and Ming-Chun Huang and Pa-Chun Wang and Te-Yung Fang and Mu-Chun Su and Po-Yi Tsai and Albert Rizzo},
keywords = {Vestibular dysfunction, Machine learning, Virtual reality, Assessment},
abstract = {Background and objective
Dizziness is a major consequence of imbalance and vestibular dysfunction. Compared to surgery and drug treatments, balance training is non-invasive and more desired. However, training exercises are usually tedious and the assessment tool is insufficient to diagnose patient's severity rapidly.
Methods
An interactive virtual reality (VR) game-based rehabilitation program that adopted Cawthorne–Cooksey exercises, and a sensor-based measuring system were introduced. To verify the therapeutic effect, a clinical experiment with 48 patients and 36 normal subjects was conducted. Quantified balance indices were measured and analyzed by statistical tools and a Support Vector Machine (SVM) classifier.
Results
In terms of balance indices, patients who completed the training process are progressed and the difference between normal subjects and patients is obvious.
Conclusions
Further analysis by SVM classifier show that the accuracy of recognizing the differences between patients and normal subject is feasible, and these results can be used to evaluate patients’ severity and make rapid assessment.}
}
@article{LAATO2021106816,
title = {Why playing augmented reality games feels meaningful to players? The roles of imagination and social experience},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106816},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106816},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221001394},
author = {Samuli Laato and Sampsa Rauti and A.K.M. Najmul Islam and Erkki Sutinen},
keywords = {Augmented reality, Location-based games, Meaning, Meaningfulness, Imagination, Pokémon GO},
abstract = {Augmented reality (AR) games such as location-based games add virtual content on top of the real world. We investigate why playing these games feels meaningful to players by focusing on the dimensions of imagination and sociality. We theorise a structural model that we test with data collected from a global sample of players of the popular AR game Pokémon GO (N = 515). Our findings show that nostalgic feelings about Pokémon increased imagining AR content in the real world. Surprisingly, using imagination in this way was a much stronger predictor of affection towards the fictional pokémon creatures than nostalgia. The affection towards the fictional creatures, in turn, increased the meaningfulness of playing. Regarding the social factors, community identification and social self-efficacy increased players' sense of meaningfulness of playing. As our study's main design implications, we highlight the importance of socially shared narratives and harnessing the players' imagination to support a sense of meaningfulness of playing.}
}
@article{ONG2020101820,
title = {Augmented reality-assisted robot programming system for industrial applications},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {61},
pages = {101820},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101820},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300250},
author = {S.K. Ong and A.W.W. Yew and N.K. Thanigaivel and A.Y.C. Nee},
keywords = {Augmented reality, Robot programming, Human-robot interaction},
abstract = {Robots are important in high-mix low-volume manufacturing because of their versatility and repeatability in performing manufacturing tasks. However, robots have not been widely used due to cumbersome programming effort and lack of operator skill. One significant factor prohibiting the widespread application of robots by small and medium enterprises (SMEs) is the high cost and necessary skill of programming and re-programming robots to perform diverse tasks. This paper discusses an Augmented Reality (AR) assisted robot programming system (ARRPS) that provides faster and more intuitive robot programming than conventional techniques. ARRPS is designed to allow users with little robot programming knowledge to program tasks for a serial robot. The system transforms the work cell of a serial industrial robot into an AR environment. With an AR user interface and a handheld pointer for interaction, users are free to move around the work cell to define 3D points and paths for the real robot to follow. Sensor data and algorithms are used for robot motion planning, collision detection and plan validation. The proposed approach enables fast and intuitive robotic path and task programming, and allows users to focus only on the definition of tasks. The implementation of this AR-assisted robot system is presented, and specific methods to enhance the performance of the users in carrying out robot programming using this system are highlighted.}
}
@article{KIDO2021101281,
title = {Assessing future landscapes using enhanced mixed reality with semantic segmentation by deep learning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101281},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101281},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000367},
author = {Daiki Kido and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Mixed reality, Dynamic occlusion handling, Landscape index estimation, Landscape design, Video communication, Deep learning},
abstract = {Architecture, engineering, and construction projects need to be promoted in harmony with the natural environment and with the aim of preserving people’s living environment. At the planning and design stage, decision-makers and stakeholders share and assess landscape images during and after construction in order to avoid as much uncertainty as possible when performing environmental impact assessment. Given the lack of a standard visualization method for future landscapes that do not yet exist, mixed reality (MR), which overlays virtual content onto a real scene, has attracted attention in the field of landscape design. One challenge in MR is occlusion, which occurs when virtual objects obscure physical objects that should be rendered in the foreground. In MR-based landscape visualization, the distance between the MR camera and real objects located in front of the virtual objects might vary and might be large, causing difficulty for existing occlusion handling methods. In the process of landscape design, an evidence-based approach has also become important. Landscape index estimation using semantic segmentation by deep learning, which can recognize the surrounding environment, has been actively studied for landscape assessment. In this study, semantic segmentation by deep learning was integrated into an MR system to enable dynamic occlusion handling and landscape index estimation for both existing and designed landscape assessment. This system can be operated on a mobile device with video communication over the internet by connecting to real-time semantic segmentation on a high-performance personal computer. The applicability of the developed system is demonstrated through accuracy verification and case studies.}
}
@article{ERGAN2022101476,
title = {Developing an integrated platform to enable hardware-in-the-loop for synchronous VR, traffic simulation and sensor interactions},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101476},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101476},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002263},
author = {Semiha Ergan and Zhengbo Zou and Suzana Duran Bernardes and Fan Zuo and Kaan Ozbay},
keywords = {Hardware in the loop, Traffic simulation, Work Zone safety, Virtual reality, Sensing},
abstract = {Hardware-in-the-loop (HIL) is a testing paradigm where physical sensors (e.g., monitoring sensors such as cameras and proximity sensors, and alarm sensors such as screaming traffic cones) are connected to a virtual test system that simulates reality (e.g., virtual work zone with simulated dangerous situations). This paradigm is well-suited for conducting user studies for work zone safety because virtual test systems can be implemented using virtual reality (VR), which allows for safe and realistic testing of a sensing system without putting workers in danger along with avoiding high upfront costs needed to generate physical research testbeds. However, when recreating physical work zones in VR, researchers face various challenges while representing traffic patterns in VR, such as the lack of bi-directional communication between traffic simulation platforms and user behaviors in VR, hardware compatibility and integration issues, and customization inflexibility during implementation. Researchers, who need to develop such platforms for research studies that involve high-risk exposure to participants, are in need of evaluating the options available for bringing together the components of such platforms. This study provides an overview of alternative ways to integrate components of platforms that enable hardware-in-the-loop for synchronous VR, traffic simulation, and sensor interactions to position researchers to make decisions based on the pros and cons of each alternative. This paper also presents the implementation of such an integrated platform that allows a two-way interface between traffic simulation and VR environments for work zone safety analysis. Outcomes of this work will lay out the steps in implementing the integrated and immersive platform to be used in work zone safety studies based on the guidance presented.}
}
@article{MASOOD2020103112,
title = {Adopting augmented reality in the age of industrial digitalisation},
journal = {Computers in Industry},
volume = {115},
pages = {103112},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519301186},
author = {Tariq Masood and Johannes Egger},
keywords = {Augmented reality, Manufacturing, Technology, Organisation, Environment, TOE, Challenges, Implementation, Critical success factors, Industry 4.0, Industrial experiments, Digital},
abstract = {Industrial augmented reality (IAR) is one of the key pillars of the industrial digitalisation concepts, which connects workers with the physical world through overlaying digital information. Augmented reality (AR) market is increasing but still its adoption levels are low in industry. While companies strive to learn and adopt AR, there are chances that they fail in such endeavours due to lack of understanding key challenges and success factors in this space. This study identifies critical success factors and challenges for IAR implementation projects based on field experiments. The broadly used technology, organisation, environment (TOE) framework was used as a theoretical basis for the study, while 22 experiments were conducted for validation. It is found that, while technological aspects are of importance, organisational issues are more relevant for industry, which has not been reflected to the same extent in the literature.}
}
@article{LU2021101685,
title = {Finding the difference: Measuring spatial perception of planning phases of high-rise urban developments in Virtual Reality},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101685},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101685},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521000922},
author = {Xi Lu and Adam Tomkins and Sigrid Hehl-Lange and Eckart Lange},
keywords = {Gaze tracking, Virtual reality, Spatial perception, Planning phase, High-rise, Pearl River Delta},
abstract = {Planning is a process in which the contents of planning is gradually refined. However, research in planning communication and perception is often conducted using contrasting scenarios, e.g. by comparing a with/without case. It is not surprising that drastic differences in planning content and representation result in significant differences in perception. Instead, and as a reflection of sequential and gradually evolving projects in planning practice, we are focusing on two planning phases with only subtle differences (2015 and 2018) for a new high-rise development district in Guangzhou. We introduce 3D gaze-tracking and spatial perception experiments to investigate how participants respond to virtual representations of the two planning phases. The results provide implications for planning and design practice and suggest more substantial roles for the general public in participatory planning processes.}
}
@article{ESHAGHI201780,
title = {Vibration analysis and optimal design of multi-layer plates partially treated with the MR fluid},
journal = {Mechanical Systems and Signal Processing},
volume = {82},
pages = {80-102},
year = {2017},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0888327016300814},
author = {Mehdi Eshaghi and Ramin Sedaghati and Subhash Rakheja},
keywords = {Vibration, MR fluid, Partial treatment, Sandwich plate, Optimization},
abstract = {The vibration characteristics of a sandwich plate partially treated with the Magneto-Rheological (MR) fluid are investigated numerically and experimentally considering different boundary conditions and intensities of the magnetic flux. A cantilevered sandwich plate consisting of an aluminum host layer with nine equal cavities for the MR fluid treatments and a constraining layer was fabricated for experimental characterizations and validations of the finite-element (FE) model. The dynamic responses of the untreated plate and the partially treated plate, where only one of the nine cavities in the core layer was filled with the MR fluid (MRF 132DG), were measured under harmonic excitation applied at the fixed support. The finite element model of the sandwich plate, developed using the classical plate theory (CPT) and first-order shear deformation theory (FSDT) is verified using the experimental data. The FE model considers the effect of slippage between the top and bottom layers of the structure. The validated FE model is subsequently used to investigate the effects of partial MR fluid treatments, magnetic flux intensity and boundary conditions on the dynamic response characteristics of the structure. The effect of shear deformation on the vibration properties of the MR sandwich plate is further highlighted. Finally, three optimization problems are formulated to identify optimal locations for the MR fluid treatments so as to maximize the variations in the natural frequencies and damping ratios in response to magnetic field. The solution of the optimization problem, attained using the genetic algorithm (GA) suggested that the MR fluid applied to locations with noticeable shear strain can maximize the stiffness variations and damping of the structure, significantly.}
}
@article{BORGERS2010377,
title = {A virtual reality tool to measure shoppers’ tenant mix preferences},
journal = {Computers, Environment and Urban Systems},
volume = {34},
number = {5},
pages = {377-388},
year = {2010},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2010.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0198971510000359},
author = {Aloys Borgers and Menno Brouwer and Tristan Kunen and Joran Jessurun and Ingrid Janssen},
keywords = {Virtual reality, Shopping centres, Tenant mix, Measuring preferences},
abstract = {Tenant mix is an important key to the success of a shopping centre. Rules dictating the ideal tenant mix are still based on assumptions about consumer preferences and consumer behaviour. However, research into consumers’ tenant mix preferences seems to become more prominent in this context. As standard questionnaires are inadequate to measure preferences regarding the locational components of tenant mix, the purpose of this project is to develop and test an enhanced virtual reality tool to elicit shoppers’ preferences regarding tenant mix. The tool can be used in the case of designing or developing new shopping centres as well as in the case of refurbishing or restructuring existing shopping centres. A prototype version of the VR-tool has been tested in a real world context. A Dutch neighbourhood centre, planned to be refurbished in the near future, was selected as a test case. The findings show that the VR-tool can be used to measure shoppers’ preferences regarding tenant selection and tenant placement. Taking into consideration these preferences may increase the attractiveness of a shopping centre, and therewith the competitive position of the centre in the region.}
}
@article{LIU2018154,
title = {JET: Joint source and channel coding for error resilient virtual reality video wireless transmission},
journal = {Signal Processing},
volume = {147},
pages = {154-162},
year = {2018},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0165168418300185},
author = {Zhi Liu and Susumu Ishihara and Ying Cui and Yusheng Ji and Yoshiaki Tanaka},
keywords = {Virtual reality, VR, 360 video, Source and channel coding, Video streaming, Error resilient},
abstract = {Virtual reality (VR) provides users extraordinary viewing experience and draws more and more attentions from both industry and academia. In this paper, we propose JET: Joint source and channel coding for Error resilient virtual reality video wireless Transmission, where we jointly investigate how to conquer the problem of source video’s huge size, how to efficiently satisfy a user’s view switch request and how to handle packet loss. Specifically, we first divide a VR video into smaller video tiles. Upon a user’s view switch request, the tiles corresponding to the part that the user is requesting, referred to as the field of view (FoV), and the part that the user may switch to before new video can be received, are delivered over a wireless network. We consider unequal error protection (UEP) for FoV and the rest part and formulate the inherent error resilient VR video transmission problem into a joint source and channel coding problem. In particular, we optimize the tile partition, quantization parameter and Forward Error Correction (FEC) packet allocation to maximize a user’s received video quality. We also propose a low-complexity heuristic algorithm to solve this optimization problem. Extensive simulations are conducted and simulation results verify the superior performance.}
}
@article{BAYAHYA2019275,
title = {Virtual Reality in Dementia Diseases},
journal = {Procedia Computer Science},
volume = {163},
pages = {275-282},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.109},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321489},
author = {Areej Y. Bayahya and Wadee AlHalabi and Sultan H. Al-Amri},
keywords = {Dementia, Geriatric Medicine, Memory, Virtual Reality},
abstract = {Most medical research that related to detection and training patients with dementia diseases are based on the traditional clinic tests with high cost devices in hospitals such as Magnetic Resonance Imaging (MRI). Accordingly, there is an expanding requirement for utilizing automated techniques in dementia's to completely use the advantages circumstances by advanced innovations. Subsequently, it was fitting to investigate the more current, more successful applications that receiving the psychological techniques with automated strategies. One of the techniques depends on virtual conditions which are given extra aces to psychological test and permit patients to immersive in a controlled domain. This paper proposed a serious game, intellectual electronic test for assessment of dementia's patients dependent on 3D virtual environments platform.}
}
@article{IZAGUIRRE2024e00318,
title = {Exploring cultural heritage and archaeological research from a VR-based approach},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00318},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00318},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000031},
author = {Joaquín Ignacio Izaguirre and Alejandro Andrés Ferrari and Félix Alejandro Acuto},
keywords = {Virtual reality, Video games, Cultural heritage, South Andean archaeology},
abstract = {In this paper, we approach an archaeological site in the Southern Andes using Virtual Reality (VR) environments to enhance cultural heritage and academic research. We will detail how we used a video game engine to model the light, landscape, sound dispersion, and architecture and create VR experiences. We will also discuss the results of pilot tests and interviews that we did to evaluate the success of such VR experiences. Even though we believe these experiences can positively impact the field of research and the protection of tangible and intangible heritage, we must carefully consider how to design VR environments and experiences.}
}
@article{ALSABBAG2022101709,
title = {Enabling human–machine collaboration in infrastructure inspections through mixed reality},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101709},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101709},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001677},
author = {Zaid Abbas Al-Sabbag and Chul Min Yeum and Sriram Narasimhan},
keywords = {Visual inspection, Mixed reality, Human–machine collaboration, Damage detection},
abstract = {In this study, we propose a novel end-to-end system called Human–Machine Collaborative Inspection (HMCI) to enable collaboration between inspectors with Mixed Reality (MR) headsets and a robotic data collection platform (robot) for structural inspections. We utilize the MR headset’s holographic display and precise head tracking to allow inspectors to visualize and localize information (e.g., structural defect) on the real scenes, which are gathered by the robot and processed by an offsite computational server. The primary use case of HMCI is to enable the inspector to visualize, supervise, and improve results produced by automated defect detection algorithms in near real-time. The workflow in HMCI starts with collecting images and depth data to generate 3D maps of the site from the robot. A technique called single-shot localization is developed to create visual anchors for real-time spatial alignment between the robot and the MR headset. The 3D map and images are then sent to the computational server for analysis to detect defects and their locations. Then, the information is received by the MR headset and overlaid on the actual scenes to visualize it with spatial context. An experimental study is conducted in a lab environment to demonstrate HMCI using Microsoft HoloLens 2 (HL2) as the MR headset and Turtlebot2 as the robot. We start with the reconstruction of a 3D environment using a 3D depth sensor (Azure Kinect) on Turtlebot2 and visually detect fiducial markers as regions-of-interest (replicating structural damage) along a predefined inspection path. Then, regions-of-interest are successfully anchored to the real scene and visualized through HL2. To our knowledge, HMCI is one of the first human–machine collaborative systems that can integrate robots and inspectors with the MR headset, which has been developed, tested, and presented for structural inspection.}
}