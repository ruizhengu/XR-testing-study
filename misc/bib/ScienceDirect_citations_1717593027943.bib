@article{PFEIFER2023107816,
title = {More than meets the eye: In-store retail experiences with augmented reality smart glasses},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107816},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107816},
url = {https://www.sciencedirect.com/science/article/pii/S074756322300167X},
author = {Pauline Pfeifer and Tim Hilken and Jonas Heller and Saifeddin Alimamy and Roberta {Di Palma}},
keywords = {Augmented reality, Augmented reality smart glasses, Embodiment, Retail experience, Purchase intentions},
abstract = {Augmented reality smart glasses (ARSGs) promise to enhance consumer experiences and decision-making when deployed as in-store retail technologies. However, research to date has not studied in-store use cases; instead, it has focused primarily on consumers' potential adoption of these devices for everyday use. Nor have prior studies compared ARSG uses with the now-common use of AR on touchscreen devices. The current research addresses these knowledge gaps by examining whether ARSGs outperform AR on touchscreen devices in the context of in-store retail experiences. Testing with an actual retail application (n = 308) shows that ARSGs are superior to AR on touchscreen devices for evoking consumers’ perceptions of immersion and mental intangibility. Furthermore, this superiority leads consumers to evaluate their shopping experiences more positively in terms of their decision comfort, satisfaction, and ease of evaluation, with significantly positive effects on their purchase intentions. These results highlight the relevance of implementing ARSGs in-store and provide retailers with recommendations for effective ARSG strategies.}
}
@article{CEBECI202423,
title = {Gaze-directed and saliency-guided approaches of stereo camera control in interactive virtual reality},
journal = {Computers & Graphics},
volume = {118},
pages = {23-32},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002492},
author = {Berk Cebeci and Mehmet Bahadir Askin and Tolga K. Capin and Ufuk Celikcan},
keywords = {Virtual reality, Head mounted displays, Stereoscopic rendering, Depth perception, Visual comfort},
abstract = {Despite remarkable advances in virtual reality (VR) technologies, serious challenges remain in making extended VR sessions with head-mounted displays (HMDs) thoroughly comfortable. 3D stereo imagery can cause discomfort and eye fatigue due to poor stereo camera settings that result in extreme disparities and vergence-accommodation conflicts. The default stereoscopic parameters of consumer HMDs produce images with shallow depth to circumvent these issues. In this work, we propose a methodology to utilize the gaze-directed and visual saliency-guided paradigms for automatic stereo camera control in real-time interactive VR by employing the basics of stereo grading. We evaluate these two approaches at different levels of interaction, first through a user study and then through a performance benchmark. The results show that the gaze-directed approach outperforms the saliency-guided approach in the VEs tested and both methods are able to convey a better overall depth feeling than the default HMD setting without hindering visual comfort. It is also shown that both approaches lead to a significant overall enhancement of the VR experience in the more interactive VE.}
}
@article{CHEN2020518,
title = {A multichannel human-swarm robot interaction system in augmented reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {6},
pages = {518-533},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300905},
author = {Mingxuan Chen and Ping Zhang and Zebo Wu and Xiaodan Chen},
keywords = {Human-swarm interaction, Augmented reality, Multichannel integration},
abstract = {Background
A large number of robots have put forward the new requirements for humanrobot interaction. One of the problems in human-swarm robot interaction is how to naturally achieve an efficient and accurate interaction between humans and swarm robot systems. To address this, this paper proposes a new type of human-swarm natural interaction system.
Methods
Through the cooperation between three-dimensional (3D) gesture interaction channel and natural language instruction channel, a natural and efficient interaction between a human and swarm robots is achieved.
Results
First, A 3D lasso technology realizes a batch-picking interaction of swarm robots through oriented bounding boxes. Second, control instruction labels for swarm-oriented robots are defined. The instruction label is integrated with the 3D gesture and natural language through instruction label filling. Finally, the understanding of natural language instructions is realized through a text classifier based on the maximum entropy model. A head-mounted augmented reality display device is used as a visual feedback channel.
Conclusions
The experiments on selecting robots verify the feasibility and availability of the system.}
}
@article{DONG201345,
title = {Collaborative visualization of engineering processes using tabletop augmented reality},
journal = {Advances in Engineering Software},
volume = {55},
pages = {45-55},
year = {2013},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2012.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997812001287},
author = {Suyang Dong and Amir H. Behzadan and Feng Chen and Vineet R. Kamat},
keywords = {Tabletop augmented reality, Visualization, Simulation, Planar tracking library, Validation, Collaboration},
abstract = {3D computer visualization has emerged as an advanced problem-solving tool for engineering education and practice. For example in civil engineering, the integration of 3D/4D CAD models in the construction process helps to minimize the misinterpretation of the spatial, temporal, and logical aspects of construction planning information. Yet despite the advances made in visualization, the lack of collaborative problem-solving abilities leaves outstanding challenges that need to be addressed before 3D visualization can become widely accepted in the classroom and in professional practice. The ability to smoothly and naturally interact in a shared workspace characterizes a collaborative learning process. This paper introduces tabletop Augmented Reality to accommodate the need to collaboratively visualize computer-generated models. A new software program named ARVita is developed to validate this idea, where multiple users wearing Head-Mounted Displays and sitting around a table can all observe and interact with dynamic visual simulations of engineering processes. The applications of collaborative visualization using Augmented Reality are reviewed, the technical implementation is covered, and the program’s underlying tracking libraries are presented.}
}
@article{BIGNE2024108104,
title = {Furnishing your home? The impact of voice assistant avatars in virtual reality shopping: A neurophysiological study},
journal = {Computers in Human Behavior},
volume = {153},
pages = {108104},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223004557},
author = {Enrique Bigne and Carla Ruiz and Rafael Curras-Perez},
keywords = {Virtual reality, Voice assisted avatars, Arousal, Eye-tracking, Clickstream data},
abstract = {This study provides insights into the informational cues consumers use in virtual reality (VR)-based retail shopping experiences. The aim of the study is to identify the number and type (extrinsic versus intrinsic) of informational cues that most attract consumers' visual attention, and that are most important in their purchase decision-making in a VR store, with special emphasis on the role of voice assistant (VA) avatars. A sample of 152 Spanish consumers participated in a laboratory-based 2 × 2 between-subjects experiment. The study's main stimulus was a recreation, in both 2D and VR, of a living room in a home. The participants were asked to view the recreation using either a computer screen (2D) or a head-mounted display (HMD). Clickstream data, neurophysiological measures (eye-tracking and GSR) and self-reported measures were used to test the hypotheses. We found that consumers used more informational cues in the product choice process in the 2D online store than in the VR store, but that the VR store generated higher flow state; that the type of cue used depended on the type of platform and that the presence of VA avatars did not influence the number of informational cues consumers used but made them pay more visual attention to the products, and evoked higher arousal.}
}
@article{PAES2023103953,
title = {Investigating the relationship between three-dimensional perception and presence in virtual reality-reconstructed architecture},
journal = {Applied Ergonomics},
volume = {109},
pages = {103953},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103953},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022002769},
author = {Daniel Paes and Javier Irizarry and Mark Billinghurst and Diego Pujoni},
keywords = {Virtual reality, Virtual environments, Human factors, Three-dimensional perception, Presence, Design technology},
abstract = {Identifying and characterizing the factors that affect presence in virtual environments has been acknowledged as a critical step to improving Virtual Reality (VR) applications in the built environment domain. In the search to identify those factors, the research objective was to test whether three-dimensional perception affects presence in virtual environments. A controlled within-group experiment utilizing perception and presence questionnaires was conducted, followed by data analysis, to test the hypothesized unidirectional association between three-dimensional perception and presence in two different virtual environments (non-immersive and immersive). Results indicate no association in either of the systems studied, contrary to the assumption of many scholars in the field but in line with recent studies on the topic. Consequently, VR applications in architectural design may not necessarily need to incorporate advanced stereoscopic visualization techniques to deliver highly immersive experiences, which may be achieved by addressing factors other than depth realism. As findings suggest that the levels of presence experienced by users are not subject to the display mode of a 3D model (whether immersive or non-immersive display), it may still be possible for professionals involved in the review of 3D models (e.g., designers, contractors, clients) to experience high levels of presence through non-stereoscopic VR systems provided that other presence-promoting factors are included.}
}
@article{BOEDIONO2023718,
title = {Markerless Augmented Reality Application for Indonesian Traditional House Education},
journal = {Procedia Computer Science},
volume = {227},
pages = {718-725},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.576},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301743X},
author = {Jonathan Apriliano Saputra Boediono and Muhammad Rizqi Aulia and Fairuz Iqbal Maulana},
keywords = {Augmented Reality, Traditional House, Rumah Adat, ARCore, Application},
abstract = {This paper proposes the development of a markerless augmented reality (AR) application for Indonesian traditional house education. The application aims to provide an immersive and interactive way of visualizing traditional houses from various regions of Indonesia, with the potential to raise public awareness about the importance of preserving traditional culture. The prototyping method was employed as the most appropriate approach to software development, and the ARFoundation platform was used to integrate plane detection into the AR application. The results showed that 80% of tested devices were compatible with ARCore, and the AR plane detection could detect up to 250cm from the device position. In addition, a feedback survey conducted on 17 testers showed that the application design and mechanism brought a good experience to users, with 53% stating that the app is easy to use. This research project contributes to the development of technology-based solutions to promote Indonesian culture and preserve traditional houses, especially for younger generations who may be less familiar with traditional culture.}
}
@article{ZHANG2023107217,
title = {Online view enhancement for exploration inside medical volumetric data using virtual reality},
journal = {Computers in Biology and Medicine},
volume = {163},
pages = {107217},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107217},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523006820},
author = {Hongkun Zhang and Lifeng Zhu and Qingxiang Zhang and Yunhai Wang and Aiguo Song},
keywords = {Medical volumetric data, Volume rendering, Virtual reality},
abstract = {Background and Objective:
Medical image visualization is an essential tool for conveying anatomical information. Ray-casting-based volume rendering is commonly used for generating visualizations of raw medical images. However, exposing a target area inside the skin often requires manual tuning of transfer functions or segmentation of original images, as preset parameters in volume rendering may not work well for arbitrary scanned data. This process is tedious and unnatural. To address this issue, we propose a volume visualization system that enhances the view inside the skin, enabling flexible exploration of medical volumetric data using virtual reality.
Methods:
In our proposed system, we design a virtual reality interface that allows users to walk inside the data. We introduce a view-dependent occlusion weakening method based on geodesic distance transform to support this interaction. By combining these methods, we develop a virtual reality system with intuitive interactions, facilitating online view enhancement for medical data exploration and annotation inside the volume.
Results:
Our rendering results demonstrate that the proposed occlusion weakening method effectively weakens obstacles while preserving the target area. Furthermore, comparative analysis with other alternative solutions highlights the advantages of our method in virtual reality. We conducted user studies to evaluate our system, including area annotation and line drawing tasks. The results showed that our method with enhanced views achieved 47.73% and 35.29% higher accuracy compared to the group with traditional volume rendering. Additionally, subjective feedback from medical experts further supported the effectiveness of the designed interactions in virtual reality.
Conclusions:
We successfully address the occlusion problems in the exploration of medical volumetric data within a virtual reality environment. Our system allows for flexible integration of scanned medical volumes without requiring extensive manual preprocessing. The results of our user studies demonstrate the feasibility and effectiveness of walk-in interaction for medical data exploration.}
}
@article{JEUNG2023107323,
title = {Augmented reality-based surgical guidance for wrist arthroscopy with bone-shift compensation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {230},
pages = {107323},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107323},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722007040},
author = {Deokgi Jeung and Kyunghwa Jung and Hyun-Joo Lee and Jaesung Hong},
keywords = {Wrist arthroscopy, Augmented reality, Bone-shift compensation, In vivo computed tomography},
abstract = {Background and objectives
Intraoperative joint condition is different from preoperative CT/MR due to the motion applied during surgery, inducing an inaccurate approach to surgical targets. This study aims to provide real-time augmented reality (AR)-based surgical guidance for wrist arthroscopy based on a bone-shift model through an in vivo computed tomography (CT) study.
Methods
To accurately visualize concealed wrist bones on the intra-articular arthroscopic image, we propose a surgical guidance system with a novel bone-shift compensation method using noninvasive fiducial markers. First, to measure the effect of traction during surgery, two noninvasive fiducial markers were attached before surgery. In addition, two virtual link models connecting the wrist bones were implemented. When wrist traction occurs during the operation, the movement of the fiducial marker is measured, and bone-shift compensation is applied to move the virtual links in the direction of the traction. The proposed bone-shift compensation method was verified with the in vivo CT data of 10 participants. Finally, to introduce AR, camera calibration for the arthroscope parameters was performed, and a patient-specific template was used for registration between the patient and the wrist bone model. As a result, a virtual bone model with three-dimensional information could be accurately projected on a two-dimensional arthroscopic image plane.
Results
The proposed method was possible to estimate the position of wrist bone in the traction state with an accuracy of 1.4 mm margin. After bone-shift compensation was applied, the target point error was reduced by 33.6% in lunate, 63.3% in capitate, 55.0% in scaphoid, and 74.8% in trapezoid than those in preoperative wrist CT. In addition, a phantom experiment was introduced simulating the real surgical environment. AR display allowed to expand the field of view (FOV) of the arthroscope and helped in visualizing the anatomical structures around the bones.
Conclusions
This study demonstrated the successful handling of AR error caused by wrist traction using the proposed method. In addition, the method allowed accurate AR visualization of the concealed bones and expansion of the limited FOV of the arthroscope. The proposed bone-shift compensation can also be applied to other joints, such as the knees or shoulders, by representing their bone movements using corresponding virtual links. In addition, the movement of the joint skin during surgery can be measured using noninvasive fiducial markers in the same manner as that used for the wrist joint.}
}
@article{FU2020294,
title = {High-order Taylor expansion based image space transform method for real-time augmented reality},
journal = {Computer Communications},
volume = {153},
pages = {294-301},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419318559},
author = {Feiran Fu and Ming Fang and Huamin Yang and Zhe Li},
keywords = {Real-time augment reality, Space transformation, Taylor expansion, Tracking},
abstract = {This paper proposes a real-time augmented reality method based on Taylor expansion formula. This method has the advantage that the pixel relationship in the discrete image space is converted to a continuous high-order Taylor space and maintains pixel invariance. After conversion to Taylor space, the ability to resist dramatic changes in illumination between frames is enhanced and robust to intra-frame local illumination mutations. In the spatial transformation process, differential low-order features are used to represent higher-order features, multiplied by appropriate feature coefficients. These coefficients are first determined theoretically and then experimentally verified, allowing us to obtain high-order feature information and approximate the original pixel values based on the features. We then applied this technique to color-based mean shift tracking problems to achieve promising results.}
}
@article{GUO201841,
title = {Using virtual reality to support the product’s maintainability design: Immersive maintainability verification and evaluation system},
journal = {Computers in Industry},
volume = {101},
pages = {41-50},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517303950},
author = {Ziyue Guo and Dong Zhou and Jiayu Chen and Jie Geng and Chuan Lv and Shengkui Zeng},
keywords = {Virtual reality, Maintainability, Product Design},
abstract = {Maintainability is an important characteristic of complex products. Good maintainability design can save substantial costs and reduce incidents and accidents throughout the product life cycle. Therefore, maintainability should be given full consideration in the early design stages. However, the current desktop tools are not effective to aid in maintainability design. Virtual reality (VR), a state-of-the-art of computer science, generates a virtual environment in which the user can have intuitive feelings and interact with virtual objects. In this paper, an immersive maintainability verification and evaluation system (IMVES) based on virtual reality is proposed. The goal of the system is to develop a cost-effective, rapid and precise method to improve the maintainability design in the early design stages. IMVES enables the user to interact with maintenance objects and conduct immersive simulations. Based on the developed methods, the data generated during simulation can be gathered to analyze the maintainability status. A case study applying IMVES into an aero-engine project is presented to demonstrate the effectiveness and feasibility of the system. Compared with desktop-based methods, the IMVES could provide a more efficient way to conduct a maintenance simulation. Furthermore, the credibility and objectivity of the maintainability evaluation are also improved.}
}
@article{DASDEMIR2022103942,
title = {Cognitive investigation on the effect of augmented reality-based reading on emotion classification performance: A new dataset},
journal = {Biomedical Signal Processing and Control},
volume = {78},
pages = {103942},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103942},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422004414},
author = {Yaşar Daşdemir},
keywords = {Augmented reality, Real reading, AR reading, EEG, Emotion, Metaverse},
abstract = {As a result of stimulating the basic human senses, emotional states occur in humans. Of these senses, the visual sense is the most basic human sense. This sense perceives visual stimuli and elicits emotional states. Augmented Reality (AR) applications also work with these visual stimuli. This study investigates how AR systems, which are among immersive environments, are effective in distinguishing the emotional states of students in book reading activities, with the support of Electroencephalography (EEG). The BOOKAR dataset obtained within the scope of this study is among the first AR-supported datasets in emotion recognition using physiological signals with immersive methods. To reveal the emotional states of the readers, texts that stimulate emotional states such as disgusting, happy, neutral, and 2-dimensional pictures are presented within these texts. In the AR-based reading section, the 3-dimensional models of these 2-dimensional pictures and the rig-processed conditions of these models are presented as a stimulus to the reader. The results show that AR-based reading has a significant discriminatory effect, especially on the valence-arousal emotional states of readers, and achieves higher classification performance than real reading. The results also show that the proposed method is good at classifying emotional states from EEG signals with accuracy scores close to 100%. It has been observed that the designed AR application also meets the usability characteristics. The proposed emotion recognition method in AR applications has significant potential for integration into various Metaverse-based applications.}
}
@article{XIAO2020291,
title = {Multimodal interaction design and application in augmented reality for chemical experiment},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {291-304},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300589},
author = {Mengting Xiao and Zhiquan Feng and Xiaohui Yang and Tao Xu and Qingbei Guo},
keywords = {Augmented reality, Gesture recognition, Intelligent equipment, Multimodal Interaction, Augmented Reality Chemistry Lab},
abstract = {Background
Augmented reality classrooms have become an interesting research topic in the field of education, but there are some limitations. Firstly, most researchers use cards to operate experiments, and a large number of cards cause difficulty and inconvenience for users. Secondly, most users conduct experiments only in the visual modal, and such single-modal interaction greatly reduces the users' real sense of interaction. In order to solve these problems, we propose the Multimodal Interaction Algorithm based on Augmented Reality (ARGEV), which is based on visual and tactile feedback in Augmented Reality. In addition, we design a Virtual and Real Fusion Interactive Tool Suite (VRFITS) with gesture recognition and intelligent equipment.
Methods
The ARGVE method fuses gesture, intelligent equipment, and virtual models. We use a gesture recognition model trained by a convolutional neural network to recognize the gestures in AR, and to trigger a vibration feedback after a recognizing a fivefinger grasp gesture. We establish a coordinate mapping relationship between real hands and the virtual model to achieve the fusion of gestures and the virtual model.
Results
The average accuracy rate of gesture recognition was 99.04%. We verify and apply VRFITS in the Augmented Reality Chemistry Lab (ARCL), and the overall operation load of ARCL is thus reduced by 29.42%, in comparison to traditional simulation virtual experiments.
Conclusions
We achieve real-time fusion of the gesture, virtual model, and intelligent equipment in ARCL. Compared with the NOBOOK virtual simulation experiment, ARCL improves the users' real sense of operation and interaction efficiency.}
}
@article{RUBO2024107915,
title = {Social stress in an interaction with artificial agents in virtual reality: Effects of ostracism and underlying psychopathology},
journal = {Computers in Human Behavior},
volume = {153},
pages = {107915},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107915},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002662},
author = {Marius Rubo and Simone Munsch},
keywords = {Social stress, Virtual reality, Ostracism, Social exclusion, General psychopathology, Anxiety, Social anxiety, Cyberball},
abstract = {Social stress can emerge from situational as well as dispositional factors. Here we tested in direct juxtaposition how ostracism in a Cyberball game and underlying psychopathology levels both influence subjective and objective stress markers in an interaction with artificial agents in virtual reality (VR) in 80 participants from a student population. Ostracism led to moderately enhanced subjective stress and negative mood but not to alterations on objective markers of stress. By contrast, underlying psychopathology levels were associated with substantially stronger alterations on subjective stress markers and were additionally associated with reduced eye gaze at virtual agents' heads, larger pupil size, larger high-frequency pupil-size variability, higher heart rate and reduced high-frequency heart-rate variability. Effects for social anxiety, general anxiety and depression levels were overall similar with largest effects on objective stress markers linked to general anxiety. These findings contest the suitability of the Cyberball game as a model for real-life ostracism but demonstrate the utility of gaze and physiological data in predicting psychopathology levels during interactions in VR. These findings furthermore highlight the need for data security solutions when using social VR applications where rich data streams are exposed publicly.}
}
@article{FENG2020101134,
title = {Towards a customizable immersive virtual reality serious game for earthquake emergency training},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101134},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101134},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301051},
author = {Zhenan Feng and Vicente A. González and Carol Mutch and Robert Amor and Anass Rahouti and Anouar Baghouz and Nan Li and Guillermo Cabrera-Guerrero},
keywords = {Adaptive game-based learning, Immersive virtual reality, Serious games, Customization, Earthquake training},
abstract = {Earthquake emergencies require a variety of behavioral responses in order to ensure the safety of occupants, which is different from simply exiting a building in fire emergencies. This makes it more complex to train building occupants in order to acquire skills that align to best practices for immediate earthquake response and post-earthquake evacuation. In recent years, Immersive Virtual Reality (IVR) and Serious Games (SGs) have become popular as training tools for earthquake emergencies. IVR SGs have been introduced to train individuals for specific building layouts or settings with fixed training objectives. However, the lack of flexibility in existing IVR SGs makes it challenging to have widespread uptake as trainees require different training objectives, pedagogical strategies, context, and content. As a result, the effectiveness of IVR SGs training is jeopardized if the customization ability is limited. To overcome this limitation, this paper presents a customization framework for IVR SGs suited to earthquake emergency training, using the concept of adaptive game-based learning. Trainees can receive training in context by customizing virtual environments, storylines, and teaching methods. A case study was undertaken to validate the proposed framework. Results showed the potential to carry out the customization process with ease, to generate a customized training experience, and to deliver the customized training for optimum learning.}
}
@article{GHORBANI2022100458,
title = {Towards an intelligent assistive system based on augmented reality and serious games},
journal = {Entertainment Computing},
volume = {40},
pages = {100458},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100458},
url = {https://www.sciencedirect.com/science/article/pii/S1875952121000550},
author = {Fatemeh Ghorbani and Mahsa {Farshi Taghavi} and Mehdi Delrobaei},
keywords = {Augmented reality, Fuzzy decision-making, Intelligent assistive technology, Internet of Things, Serious game},
abstract = {Age-related cognitive impairment is generally characterized by gradual memory loss and decision-making difficulties. The aim of this study is to investigate multi-level support and suggest relevant helping means for the elderly with mild cognitive impairment as well as their caregivers as the primary end-users. This work reports preliminary results on an intelligent assistive system, achieved through the integration of Internet of Things, augmented reality, and adaptive fuzzy decision-making methods. The proposed system operates in different modes, including automated and semi-automated modes. The former helps the user complete their daily life activities by showing augmented reality messages or making automatic changes; while the latter allows manual changes after the real-time assessment of the user’s cognitive state based on the augmented reality serious game score. We have also evaluated the accuracy of the serious game score with 37 elderly participants and compared it with users’ paper-based cognitive test results. We further noted that there is an acceptable correlation between the paper-based test and users’ serious game scores. Moreover, we observed that the system response in the semi-automated mode causes less data loss compared with the automated mode, as the number of active devices decreases.}
}
@article{CHIANG2022107125,
title = {Augmented reality in vocational training: A systematic review of research and applications},
journal = {Computers in Human Behavior},
volume = {129},
pages = {107125},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107125},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221004489},
author = {Feng-Kuang Chiang and Xiaojing Shang and Lu Qiao},
keywords = {Augmented reality (AR), Vocational training, On-the-job training, AR application, AR system},
abstract = {Augmented reality (AR) technology is widely used in various fields. However, there are few systematic reviews on the application of AR in vocational training. To fill this research gap, the current study reviewed the application of AR technology in the training of various industries over a 20 year period (2000–2021). Through cross-referencing and abstract reading, 80 relevant studies were selected for the final analysis from two perspectives: the improvement of vocational skills (including application area, target audience, training objectives, and effects) and AR training technology (including AR application, AR training system, and device). Furthermore, CiteSpaceV was employed to analyze the research hotspots and trends of AR vocational training. The results indicated that AR training is frequently applied in the industry, vocational education and medical fields. Among these industries, AR has been most frequently used in medical training, industrial maintenance, and assembly. Furthermore, AR glasses, simulators, the Unity3D game engine, 360° panorama, AR systems and apps are becoming widely used for training tasks. The benefits of these systems have been identified. We also explored the impact of AR on vocational training results. Based on 17 empirical studies, this study summarized the results and advantages of AR vocational training. This verified that AR has a high promotion effect on vocational training when the meta-analysis method is used. Future researchers can study how vocational skills education can be combined with these new intelligent technologies to design more mature teaching practice cases.}
}
@article{GU2023107571,
title = {Cross-modality image translation: CT image synthesis of MR brain images using multi generative network with perceptual supervision},
journal = {Computer Methods and Programs in Biomedicine},
volume = {237},
pages = {107571},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107571},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723002365},
author = {Xianfan Gu and Yu Zhang and Wen Zeng and Sihua Zhong and Haining Wang and Dong Liang and Zhenlin Li and Zhanli Hu},
keywords = {Computed tomography (CT), Magnetic resonance imaging (MRI), Radiotherapy, MR-To-CT synthesis, Cross-modality, Cycle-consistency},
abstract = {Background: Computed tomography (CT) and magnetic resonance imaging (MRI) are the mainstream imaging technologies for clinical practice. CT imaging can reveal high-quality anatomical and physiopathological structures, especially bone tissue, for clinical diagnosis. MRI provides high resolution in soft tissue and is sensitive to lesions. CT combined with MRI diagnosis has become a regular image-guided radiation treatment plan. Methods: In this paper, to reduce the dose of radiation exposure in CT examinations and ameliorate the limitations of traditional virtual imaging technologies, we propose a Generative MRI-to-CT transformation method with structural perceptual supervision. Even though structural reconstruction is structurally misaligned in the MRI-CT dataset registration, our proposed method can better align structural information of synthetic CT (sCT) images to input MRI images while simulating the modality of CT in the MRI-to-CT cross-modality transformation. Results: We retrieved a total of 3416 brain MRI-CT paired images as the train/test dataset, including 1366 train images of 10 patients and 2050 test images of 15 patients. Several methods (the baseline methods and the proposed method) were evaluated by the HU difference map, HU distribution, and various similarity metrics, including the mean absolute error (MAE), structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and normalized cross-correlation (NCC). In our quantitative experimental results, the proposed method achieves the lowest MAE mean of 0.147, highest PSNR mean of 19.27, and NCC mean of 0.431 in the overall CT test dataset. Conclusions: In conclusion, both qualitative and quantitative results of synthetic CT validate that the proposed method can preserve higher similarity of structural information of the bone tissue of target CT than the baseline methods. Furthermore, the proposed method provides better HU intensity reconstruction for simulating the distribution of the CT modality. The experimental estimation indicates that the proposed method is worth further investigation.}
}
@article{LARAPRIETO201559,
title = {An Innovative Self-learning Approach to 3D Printing Using Multimedia and Augmented Reality on Mobile Devices},
journal = {Procedia Computer Science},
volume = {75},
pages = {59-65},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036674},
author = {Vianney Lara-Prieto and Efraín Bravo-Quirino and Miguel Ángel Rivera-Campa and José Enrique Gutiérrez-Arredondo},
keywords = {3D Printing, Augmented Reality, Self-learning},
abstract = {Technology is evolving rapidly and it is becoming harder to keep up its pace. At a university environment, important investments are required so that students and professors can have access to novel technology. However, it is also fundamental to know how to use this novel technology to exploit its benefits. Tecnológico de Monterrey, Campus Monterrey, recently acquired several 3D printers so that the students could get familiar with this rapid prototyping technology and use them to deliver better quality projects. Nevertheless, the new challenge was to administrate the 3D printing process including 3D model verification, STL file generation, printing time and raw material. Since students were not familiar with 3D printing, the expert had to spend a lot of time with them explaining the whole process from the modelling to the printing stage, and verifying their work to make an efficient use of 3D printing time and materials. To overcome this situation, it was decided to make use of augmented reality and multimedia applications to generate tutorials for self-learning the whole process of 3D printing. Nowadays, the wide spread of mobile devices and wireless technologies brings a huge potential to e-learning changing dramatically the traditional instructor-oriented scheme. The learning process can take place in an informal setting having the tutorials available on mobile devices by just scanning a quick response code, usually known as QR code. The first QR code enables a link to download the free augmented reality Layar app. Then, several images can be scanned using Layar to open each of the video tutorials. These videos explain graphically step by step of all the processes involved and give different options of software to be used. The tutorials are easy to follow so that any engineering or design student can learn from them. This case-study offers an innovative learning approach that fosters self-learning and a more efficient use of technology resources.}
}
@article{SHOSHANI2023107546,
title = {From virtual to prosocial reality: The effects of prosocial virtual reality games on preschool Children's prosocial tendencies in real life environments},
journal = {Computers in Human Behavior},
volume = {139},
pages = {107546},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107546},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222003661},
author = {Anat Shoshani},
keywords = {Virtual reality, Children, Prosocial, Games, Positive emotions, Preschool},
abstract = {Virtual Reality (VR) technology can provide new opportunities to promote prosocial learning in young children. However, little empirical research has examined how this technology can impact preschool children's prosocial behavior. To respond to this need, two experiments investigated how VR prosocial games affect preschool children's prosocial behavior in real-life settings. Positive affect and sense of competence were examined as potential mediators between the VR prosocial play and real-life prosocial behavior. In the first experiment, 4-to 6-year-olds (N = 166) were randomly assigned to play a prosocial, violent, or neutral VR game. After the game, helping behaviors towards the experimenter were tested on a behavioral task. In the second experiment, 4-to 6-year-olds (N = 173) were randomly assigned to a prosocial, positive affect, or neutral VR game condition, and their sharing behavior with peers was examined on a dictator game. Across experiments, children in the pro-social game condition exhibited more helping and sharing behaviors than children in the violent, positive affect, or neutral conditions. Positive affect mediated the effect of VR prosocial play on prosocial behavior; the effect of competence was not significant. The contribution of gamified VR environments to facilitating prosocial development during the preschool age is discussed.}
}
@article{VERHULST2021106951,
title = {Do VR and AR versions of an immersive cultural experience engender different user experiences?},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106951},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106951},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002740},
author = {Isabelle Verhulst and Andy Woods and Laryssa Whittaker and James Bennett and Polly Dalton},
keywords = {Virtual reality, Augmented reality, User experience, Presence, Enjoyment, Engagement},
abstract = {Although Virtual Reality (VR) and Augmented Reality (AR) user experiences have received large amounts of recent research interest, a direct comparison of different immersive technologies' user experiences has not often been conducted. This study compared user experiences of one VR and two AR versions of an immersive gallery experience ‘Virtual Veronese’, measuring multiple aspects of user experience, including enjoyment, presence, cognitive, emotional and behavioural engagement, using a between-subjects design, at the National Gallery in London, UK. Analysis of the self-reported survey data (N = 368) showed that enjoyment was high on all devices, with the Oculus Quest (VR) receiving higher mean scores than both AR devices, Magic Leap and Mira Prism. In relation to presence, the elements ‘spatial presence’, ‘involvement’, and ‘sense of being there’ received a higher mean score on the Oculus Quest than on both AR devices, and on ‘realism’ the Oculus Quest scored significantly higher than the Magic Leap. Cognitive engagement was similar between the three devices, with only ‘I knew what to do’ being rated higher for Quest than Mira Prism. Emotional engagement was similar between the devices. Behavioural engagement was high on all devices, with only ‘I would like to see more experiences like this’ being higher for Oculus Quest than Mira Prism. Negative effects including nausea were rarely reported. Differences in user experiences were likely partly driven by differences in immersion levels between the devices.}
}
@article{HUANG2023102245,
title = {Semi-supervised hybrid spine network for segmentation of spine MR images},
journal = {Computerized Medical Imaging and Graphics},
volume = {107},
pages = {102245},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102245},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000630},
author = {Meiyan Huang and Shuoling Zhou and Xiumei Chen and Haoran Lai and Qianjin Feng},
keywords = {Spine segmentation, MR images, Semi-supervised learning, Cross tri-attention},
abstract = {Automatic segmentation of vertebral bodies (VBs) and intervertebral discs (IVDs) in 3D magnetic resonance (MR) images is vital in diagnosing and treating spinal diseases. However, segmenting the VBs and IVDs simultaneously is not trivial. Moreover, problems exist, including blurry segmentation caused by anisotropy resolution, high computational cost, inter-class similarity and intra-class variability, and data imbalances. We proposed a two-stage algorithm, named semi-supervised hybrid spine network (SSHSNet), to address these problems by achieving accurate simultaneous VB and IVD segmentation. In the first stage, we constructed a 2D semi-supervised DeepLabv3+ by using cross pseudo supervision to obtain intra-slice features and coarse segmentation. In the second stage, a 3D full-resolution patch-based DeepLabv3+ was built. This model can be used to extract inter-slice information and combine the coarse segmentation and intra-slice features provided from the first stage. Moreover, a cross tri-attention module was applied to compensate for the loss of inter-slice and intra-slice information separately generated from 2D and 3D networks, thereby improving feature representation ability and achieving satisfactory segmentation results. The proposed SSHSNet was validated on a publicly available spine MR image dataset, and remarkable segmentation performance was achieved. Moreover, results show that the proposed method has great potential in dealing with the data imbalance problem. Based on previous reports, few studies have incorporated a semi-supervised learning strategy with a cross attention mechanism for spine segmentation. Therefore, the proposed method may provide a useful tool for spine segmentation and aid clinically in spinal disease diagnoses and treatments. Codes are publicly available at: https://github.com/Meiyan88/SSHSNet.}
}
@article{CHEKHOVSKOY2022223,
title = {The Use of Virtual Reality Technologies in the Specialists’ Training in the Field of Information Security},
journal = {Procedia Computer Science},
volume = {213},
pages = {223-231},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017513},
author = {Yuriy Chekhovskoy and Kirill Plaksiy and Andrey Nikiforov and Natalia Miloslavskaya},
keywords = {virtual reality, specialists’ training, information security, virtual reality testbed, network security},
abstract = {The paper presents a study of ready-made solutions and ways to use virtual reality (VR) technologies in training specialists. Due to the lack of tools that meet the needs of such preparation in the field of information security (IS), when students study network attacks and ways of protection against them, a training and laboratory complex using the Unity development environment for educational purposes were developed. It is intended for a detailed study of current network attacks implementation and development of protection measures against them. The educational and laboratory bench includes a VR testbed and its algorithmic support successfully tested in the educational process of the the National Research Nuclear University MEPhI (Moscow Engineering Physics Institute) within the framework of the discipline "Secure Information Systems".}
}
@article{ASISH202275,
title = {Detecting distracted students in educational VR environments using machine learning on eye gaze data},
journal = {Computers & Graphics},
volume = {109},
pages = {75-87},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322001856},
author = {Sarker Monojit Asish and Arun K. Kulshreshth and Christoph W. Borst},
keywords = {Education, Virtual Reality, Eye Tracking, Machine Learning, Deep Learning, Distraction Detection},
abstract = {Virtual Reality (VR) has been found useful to improve engagement and retention level of students, for some topics, compared to traditional learning tools such as books, and videos. However, a student could still get distracted and disengaged due to a variety of factors including stress, mind-wandering, unwanted noise, and external alerts. Student eye gaze data could be useful for detecting these distracted students. Gaze data-based visualizations have been proposed in the past to help a teacher monitor distracted students. However, it is not practical for a teacher to monitor a large number of student indicators while teaching. To help filter students based on distraction level, we propose an automated system based on machine learning to classify students based on their distraction level. The key aspects are: (1) we created a labeled eye gaze dataset from an educational VR environment, (2) we propose an automatic system to gauge a student’s distraction level from gaze data, and (3) we apply and compare several classifiers for this purpose. Each classifier classifies distraction, per educational activity section, into one of three levels (low, mid or high). Our results show that Random Forest (RF) classifier had the best accuracy (98.88%) compared to the other models we tested. Additionally, a personalized machine learning model using either RF, kNN, or Extreme Gradient Boosting (XGBoost) model was found to improve the classification accuracy significantly.}
}
@article{CHALILMADATHIL2017501,
title = {An investigation of the efficacy of collaborative virtual reality systems for moderated remote usability testing},
journal = {Applied Ergonomics},
volume = {65},
pages = {501-514},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0003687017300376},
author = {Kapil {Chalil Madathil} and Joel S. Greenstein},
keywords = {Collaborative virtual reality systems, Moderated usability testing, Remote testing},
abstract = {Collaborative virtual reality-based systems have integrated high fidelity voice-based communication, immersive audio and screen-sharing tools into virtual environments. Such three-dimensional collaborative virtual environments can mirror the collaboration among usability test participants and facilitators when they are physically collocated, potentially enabling moderated usability tests to be conducted effectively when the facilitator and participant are located in different places. We developed a virtual collaborative three-dimensional remote moderated usability testing laboratory and employed it in a controlled study to evaluate the effectiveness of moderated usability testing in a collaborative virtual reality-based environment with two other moderated usability testing methods: the traditional lab approach and Cisco WebEx, a web-based conferencing and screen sharing approach. Using a mixed methods experimental design, 36 test participants and 12 test facilitators were asked to complete representative tasks on a simulated online shopping website. The dependent variables included the time taken to complete the tasks; the usability defects identified and their severity; and the subjective ratings on the workload index, presence and satisfaction questionnaires. Remote moderated usability testing methodology using a collaborative virtual reality system performed similarly in terms of the total number of defects identified, the number of high severity defects identified and the time taken to complete the tasks with the other two methodologies. The overall workload experienced by the test participants and facilitators was the least with the traditional lab condition. No significant differences were identified for the workload experienced with the virtual reality and the WebEx conditions. However, test participants experienced greater involvement and a more immersive experience in the virtual environment than in the WebEx condition. The ratings for the virtual environment condition were not significantly different from those for the traditional lab condition. The results of this study suggest that participants were productive and enjoyed the virtual lab condition, indicating the potential of a virtual world based approach as an alternative to conventional approaches for synchronous usability testing.}
}
@article{SEONG2022102994,
title = {Corroborating the effect of positive technology readiness on the intention to use the virtual reality sports game “Screen Golf”: Focusing on the technology readiness and acceptance model},
journal = {Information Processing & Management},
volume = {59},
number = {4},
pages = {102994},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001091},
author = {Bo-Hyun Seong and Chang-Yu Hong},
keywords = {Technology readiness, Technology readiness and acceptance model, Screen golf, Perceived usefulness, Perceived ease of use, Intentions to use},
abstract = {This study investigates the positive and behavioral topic of screen golf, which is often regarded as the most commercially successful virtual reality sports game. Our team analyzed the decision-making process related to screen golf through the widely used the technology readiness and acceptance model to explain the relationships among technology readiness, belief in technology acceptance, and use intentions. The proposed model fit the data satisfactorily, and several of our hypotheses were supported. Structural equation modeling tested the nine hypotheses established based on a literature review, analyzing 350 valid responses obtained through online surveys. Perceived utility (ß = .519**) was the most influential factor in individuals’ plans to participate in the virtual sport. This means that practical considerations such as correcting individuals’ posture and improving their abilities should be prioritized when creating screen golf programs.}
}
@article{TSUJIMOTO2024106054,
title = {Server-enabled mixed reality for flood risk communication: On-site visualization with digital twins and multi-client support},
journal = {Environmental Modelling & Software},
volume = {177},
pages = {106054},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106054},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224001154},
author = {Ryoma Tsujimoto and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Distributed computer systems, Mobile augmented reality, Real-time visualization, Remote rendering, Risk communication, Web based},
abstract = {This study introduces a novel mobile mixed reality (MR) system to improve communication among stakeholders regarding climate-change-driven flood risk. This MR system enables stakeholders to visualize flood risks on common mobile devices, such as smartphones and tablets, offering perspectives from their respective environments. The system overcomes the computational limitations of previous mobile MR systems by rendering the MR images on a server. Furthermore, distributing users across multiple servers enables many users to simultaneously perform server-enabled MR, facilitating widespread engagement in flood risk communication. The system uses a viewport transformation to accurately display virtual floods and correct for real-time posture changes. A prototype tested in a high-risk flood area achieved real-time MR visualization at more than 16 frames per second, even on multiple devices. Future research will aim to promote stakeholder participation through more accurate visualization of virtual floods and improved interactive information sharing among users.}
}
@article{KASAPAKIS2023100020,
title = {Virtual reality in education: The impact of high-fidelity nonverbal cues on the learning experience},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100020},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000144},
author = {Vlasios Kasapakis and Elena Dzardanova and Androniki Agelada},
keywords = {Virtual reality, Avatars, Motion capture, Non-verbal cues, Education},
abstract = {Integrating variant Information and Communication Technologies (ICTs) into the learning process provides students and teachers alike with specialized tools which eliminate the distance between them and create a classroom-like experience. Virtual Reality (VR) is bound to not only match the qualities of interpersonal communication for distance-learning but reconfigure the learning process altogether, providing students and teachers with novel hyper-tools and methods for presentation and interaction. Virtual Reality Learning Environments (VRLEs) are already being designed, developed, and tested out as an educational tool. Among the less investigated aspects of VRLEs is the impact of avatars and characters Nonverbal Cues (NVCs) on the students' learning experience. This study presents the development of a prototype which uses off-the-shelf technologies commonly used in Social Virtual Reality (SVR) platforms to capture a real professor's body motion and gaze, along with his facial expressions, in real-time, during the delivery of a real lecture. The recorded data are later solved onto a high-fidelity avatar delivering the same lecture in a VRLE. A between-groups study including ninety-six (96) participants, all university students, revealed no correlation between the professor's avatar NVCs fidelity and perceived usability, realism, usefulness, and social presence, and no differences in knowledge acquisition as well.}
}
@article{CHUANG2002189,
title = {A virtual reality-based system for hand function analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {69},
number = {3},
pages = {189-196},
year = {2002},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(01)00190-0},
url = {https://www.sciencedirect.com/science/article/pii/S0169260701001900},
author = {Tien-Yow Chuang and Wei-Shin Huang and Shu-Chiung Chiang and Yun-An Tsai and Ji-Liang Doong and Henrich Cheng},
keywords = {Virtual realities, Hand function, Motor retraining, Spatial relation task, Dataglove, Root mean square},
abstract = {The goal of this study was to demonstrate the usability and usefulness of virtual reality technology in assessing hand functions. Ten healthy, non-disabled right-handed adult volunteers were recruited. Each volunteer used a dataglove to insert three-dimensional virtual representations of a cylinder and a prism into the target holes. To verify the reliability of the tests, each subject was retested twice. The performance testing assessed the visual-motor coordination a person needs to achieve a task accurately and within a set time. For each trial, the root mean square (RMS) value of the hand movement trajectory was projected onto the X, Y, and Z axes. This projection enabled us to measure the extent of the genuine, summative displacement of the manipulating hand. The reproducibility of the virtual reality assessment was analyzed using the intraclass correlation (ICC) approach. The total ICC values of 10 subjects demonstrated a high task completion time and RMS on the X and Z axes for the transferring of the prism. However, the values were low for the transferring of the cylinder. Because the individual coefficients of variations (CVs) varied widely in the moving of both the cylinder and the prism, the total (CVs) showed a high reading for the task completion time. Although rehabilitation clinics routinely carry out peg-moving exercises for disabled patients, our model provides a valuable quantitative real time and off-line measure of whole hand functions.}
}
@article{BARSASELLA2021105892,
title = {Effects of Virtual Reality Sessions on the Quality of Life, Happiness, and Functional Fitness among the Older People: A Randomized Controlled Trial from Taiwan},
journal = {Computer Methods and Programs in Biomedicine},
volume = {200},
pages = {105892},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105892},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720317259},
author = {Diana Barsasella and Megan F. Liu and Shwetambara Malwade and Cooper J Galvin and Eshita Dhar and Chia-Chi Chang and Yu-Chuan Jack Li and Shabbir Syed-Abdul},
keywords = {virtual reality, older people, randomized controlled trial, quality of life, happiness, functional fitness},
abstract = {Background and Objective
Ageing is a complex process with physical, psychological, and social changes, which can lead to diseases and disability, and further reduce happiness levels. Virtual reality (VR) is an emerging technology with the potential to improve overall well-being, quality of life (QoL), muscle activity and balance. Our study aimed to determine the influence of VR sessions on the QoL, happiness, and functional fitness components of an elderly cohort.
Methods
A non-blinded randomized controlled trial was conducted. Sixty participants, who visited the active ageing center at the university were randomized into two groups- intervention and control. The intervention group received VR experience sessions for 15 min twice a week for a duration of 6 weeks, while the control group received no sessions. Participants filled out a questionnaire for QoL assessment and happiness assessment. They were also tested for several functional fitness components. Both questionnaires and fitness tests were conducted at the beginning and at the end of study.
Results
QoL improved by some metrics assessed (Pain/Discomfort and Anxiety/Depression). Happiness significantly improved in the intervention group relative to the control group. Among the functional fitness tests, the back scratch test 1st and back scratch test 2nd were measured to be significantly improved in the intervention group in comparison to control group.
Conclusions
VR sessions have potential to influence the well-being and functional fitness of older adults and further support the process of healthy and active ageing. Future considerations could focus on supporting more physical and psychological aspects of the older people through VR content.
Trial registration
NCT04166747.}
}
@article{WANG2023451,
title = {Eye-shaped keyboard for dual-hand text entry in virtual reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {5},
pages = {451-469},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000438},
author = {Kangyu Wang and Yangqiu Yan and Hao Zhang and Xiaolong Liu and Lili Wang},
keywords = {Virtual reality, Human computer interaction, Text entry, Shaped keyboard, QWERTY-like key sequence, Dual-hand cooperation},
abstract = {We propose an eye-shaped keyboard for high-speed text entry in virtual reality (VR), having the shape of dual eyes with characters arranged along the curved eyelids, which ensures low density and short spacing of the keys. The eye-shaped keyboard references the QWERTY key sequence, allowing the users to benefit from their experience using the QWERTY keyboard. The user interacts with an eye-shaped keyboard using rays controlled with both the hands. A character can be entered in one step by moving the rays from the inner eye regions to regions of the characters. A high-speed auto-complete system was designed for the eye-shaped keyboard. We conducted a pilot study to determine the optimal parameters, and a user study to compare our eye-shaped keyboard with the QWERTY and circular keyboards. For beginners, the eye-shaped keyboard performed significantly more efficiently and accurately with less task load and hand movement than the circular keyboard. Compared with the QWERTY keyboard, the eye-shaped keyboard is more accurate and significantly reduces hand translation while maintaining similar efficiency. Finally, to evaluate the potential of eye-shaped keyboards, we conducted another user study. In this study, the participants were asked to type continuously for three days using the proposed eye-shaped keyboard, with two sessions per day. In each session, participants were asked to type for 20min, and then their typing performance was tested. The eye-shaped keyboard was proven to be efficient and promising, with an average speed of 19.89 words per minute (WPM) and mean uncorrected error rate of 1.939%. The maximum speed reached 24.97 WPM after six sessions and continued to increase.}
}
@article{HUANG2023107815,
title = {Effects of virtual reality on creative performance and emotions: A study of brainwaves},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107815},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107815},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001668},
author = {Huai-Te Huang and Yu-Shan Chang},
keywords = {Virtual reality, Creative performance, Emotions, Brainwaves},
abstract = {Creativity is the basis for innovation and a source of national and industrial competitiveness. Thus, pedagogical enhancement of creativity is desirable, particularly via application of new technologies such as virtual reality (VR). The purpose of this study was to examine the effects of VR on creativity (creative outcomes and the creative process), emotions, and brainwaves using a nonequivalent-group pre-post-test quasi-experimental design involving 76 tenth-grade students. An experimental group received training that incorporated VR elements, and a comparison group was taught via lectures combined with multimedia presentations. The main conclusions are as follows: VR exerted large positive effects on the elaboration, vividness, and novelty creative outcomes; VR exerted above-moderate positive effects on feasibility but not novelty; VR exhibited small effect sizes in terms of enjoyment, anger, and anxiety, but a large effect size for boredom, and these emotions positively influenced creativity; and beta and gamma brainwave activity during the ideation and elaboration stages, and delta brainwave activity during the latter stage, were significantly lower in the experimental than comparison group.}
}
@article{BAGASSI2020103291,
title = {Human-in-the-loop evaluation of an augmented reality based interface for the airport control tower},
journal = {Computers in Industry},
volume = {123},
pages = {103291},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103291},
url = {https://www.sciencedirect.com/science/article/pii/S016636152030525X},
author = {Sara Bagassi and Francesca {De Crescenzio} and Sergio Piastra and Carlo A. Persiani and Mohamed Ellejmi and Alan R. Groskreutz and Jorge Higuera},
abstract = {An innovative airport control tower concept based on the use of modern augmented reality technologies has been developed and validated by means of human-in-the-loop experiments in a simulated environment. An optical-based augmented reality interface underpins the proposed concept that consists in providing air traffic control operators in the airport control tower with complete head-up information, as opposed to the current mix of information retrieval through both head-up real view and head-down interfaces. Specific measurement of the time spent by the operator working in either head-up or head-down position, show that the proposal has a clear effect in stimulating the air traffic control operator to work in a head-up position more than in a head-down position, with positive effects on his/her situational awareness and perceived workload, especially when dealing with low visibility conditions operational scenarios.}
}
@article{JABBARPOUR2022105277,
title = {Unsupervised pseudo CT generation using heterogenous multicentric CT/MR images and CycleGAN: Dosimetric assessment for 3D conformal radiotherapy},
journal = {Computers in Biology and Medicine},
volume = {143},
pages = {105277},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105277},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522000695},
author = {Amir Jabbarpour and Seied Rabi Mahdavi and Alireza {Vafaei Sadr} and Golbarg Esmaili and Isaac Shiri and Habib Zaidi},
keywords = {MRI-Only radiotherapy, Brain tumors, Unsupervised deep learning, CycleGAN},
abstract = {Purpose
Absorbed dose calculation in magnetic resonance-guided radiation therapy (MRgRT) is commonly based on pseudo CT (pCT) images. This study investigated the feasibility of unsupervised pCT generation from MRI using a cycle generative adversarial network (CycleGAN) and a heterogenous multicentric dataset. A dosimetric analysis in three-dimensional conformal radiotherapy (3DCRT) planning was also performed.
Material and methods
Overall, 87 T1-weighted and 102 T2-weighted MR images alongside with their corresponding computed tomography (CT) images of brain cancer patients from multiple centers were used. Initially, images underwent a number of preprocessing steps, including rigid registration, novel CT Masker, N4 bias field correction, resampling, resizing, and rescaling. To overcome the gradient vanishing problem, residual blocks and mean squared error (MSE) loss function were utilized in the generator and in both networks (generator and discriminator), respectively. The CycleGAN was trained and validated using 70 T1 and 80 T2 randomly selected patients in an unsupervised manner. The remaining patients were used as a holdout test set to report final evaluation metrics. The generated pCTs were validated in the context of 3DCRT.
Results
The CycleGAN model using masked T2 images achieved better performance with a mean absolute error (MAE) of 61.87 ± 22.58 HU, peak signal to noise ratio (PSNR) of 27.05 ± 2.25 (dB), and structural similarity index metric (SSIM) of 0.84 ± 0.05 on the test dataset. T1-weighted MR images used for dosimetric assessment revealed a gamma index of 3%, 3 mm, 2%, 2 mm and 1%, 1 mm with acceptance criteria of 98.96% ± 1.1%, 95% ± 3.68%, 90.1% ± 6.05%, respectively. The DVH differences between CTs and pCTs were within 2%.
Conclusions
A promising pCT generation model capable of handling heterogenous multicenteric datasets was proposed. All MR sequences performed competitively with no significant difference in pCT generation. The proposed CT Masker proved promising in improving the model accuracy and robustness. There was no significant difference between using T1-weighted and T2-weighted MR images for pCT generation.}
}
@article{CUNHA2022426,
title = {Using Virtual Reality in the Development of an Index-Engine of Physical and Emotional Sustainability},
journal = {Procedia Computer Science},
volume = {196},
pages = {426-433},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022559},
author = {Carlos R. Cunha and Alcina Nunes and Paula O. Fernandes and José Bragada and Luís Pires and Maria José and Pedro Magalhães},
keywords = {Virtual Reality, Machine Learning, Model, Emotional Sustainability, Well-being, Health},
abstract = {Nowadays, there is a growing need for rehabilitation associated with demographic changes and health trends, with an increase in the prevalence of non-communicable diseases and an aging population. Although well-being is classically associated with a set of physical activities, the reality of today’s society often shows that there is a lack of time available to develop activities that promote well-being. This fact is aggravated in aging populations by less mobility and greater isolation. In this domain, information and communication technologies have greatly contributed to helping healthcare providers to develop new understanding, measurement and action strategies that promote the physical and emotional well-being of their patients. Accordingly, the role of Virtual Reality has been shown to be promising for the generation of well-being. This article, as part of a broader funding project, reviews the state of the art of the role that Virtual Reality has played in the promotion of well-being, being an exercise of conceptualization that converges in the proposal of a conceptual model - an Index-Engine of Physical and Emotional Sustainability, which will be prototyped and field tested in the future.}
}
@article{RADANOVIC2023101960,
title = {Aligning the real and the virtual world: Mixed reality localisation using learning-based 3D–3D model registration},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101960},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101960},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000885},
author = {Marko Radanovic and Kourosh Khoshelham and Clive Fraser},
keywords = {Augmented reality (AR), Mixed reality (MR), Large-scale localisation, Indoor positioning, 3D building models, Deep learning},
abstract = {Existing camera localisation methods for indoor augmented and mixed reality (AR/MR) are almost exclusively image based. The main issue with image-based methods is that they do not scale well and, as a consequence, AR/MR applications are mostly limited to small-scale room experiences. To tackle the challenge of large-scale indoor AR/MR localisation, we propose a novel framework for AR/MR localisation based solely on 3D–3D model registration. The localisation is performed by an automated registration of a low-density model of the surroundings created by the device to the existing point cloud of the environment based on learning-based keypoint detection and description. Our solution takes advantage of recent significant improvements in automated coarse-to-fine 3D–3D model registration methods. Unlike the existing image-based AR/MR localisation methods, which are restricted to small room-sized environments, the proposed 3D registration-based approach is applicable to large environments and is robust to changes in colour and illumination of the scene. We perform extensive testing and analysis of the approach with real-world experiments and datasets using a prototype developed for the Microsoft HoloLens. Experimental results show high localisation reliability and accuracy, with a mean translation error of 2.8 cm and a mean rotation error of 0.30°. The method performs well in a large-scale environment (300 m2) and shows good robustness to changes in scene geometry.}
}
@article{ALARCONYAQUETTO2021104404,
title = {Effect of augmented reality books in salivary cortisol levels in hospitalized pediatric patients: A randomized cross-over trial},
journal = {International Journal of Medical Informatics},
volume = {148},
pages = {104404},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104404},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000307},
author = {Dulce E. Alarcón-Yaquetto and Jean P. Tincopa and Daniel Guillén-Pinto and Nataly Bailon and César P. Cárcamo},
keywords = {Cortisol, Pediatrics, Hospitalization, Stress, Augmented reality},
abstract = {Objective
This study sought to assess the effect of reading augmented reality (AR) books on salivary cortisol levels in hospitalized pediatric patients compared to reading a standard children’s book.
Methods
This was a randomized, two-period, cross-over trial in hospitalized children aged 7–11 years. AR books currently in the market were used as intervention. Complete block randomization was used to randomize the order of the intervention. Children allocated to the ‘AR-first’ group received the book, a tablet and were left to interact independently with the technology for an hour. After a 48 -h wash-out period, children received a standard book. ‘Standard-book-first’ group received only the standard book and after wash-out received the tablet and the AR book. Salivary cortisol and a validated visual analogue scale (VAS) for psychological stress were assessed at the beginning and at the end of each intervention.
Results
A total of 29 children were recruited in the study. One was lost during follow up. Cortisol levels decreased after the AR intervention (P = 0.019). Nevertheless, the decrease was not greater than the one associated to reading the standard book. VAS scores increased after the AR intervention (P < 0.001).
Discussion
There is evidence of order and sequence effects that might explain results. First assessment of AR-based interventions on stress. Results justify further research.
Conclusions
There was no evidence that reading AR books diminished cortisol levels more than reading a standard book. AR-books improved VAS score for psychological stress compared to a standard book.}
}
@article{NICOLAU2009494,
title = {An augmented reality system for liver thermal ablation: Design and evaluation on clinical cases},
journal = {Medical Image Analysis},
volume = {13},
number = {3},
pages = {494-506},
year = {2009},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2009.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841509000103},
author = {S.A. Nicolau and X. Pennec and L. Soler and X. Buy and A. Gangi and N. Ayache and J. Marescaux},
keywords = {Augmented reality, Computer-guided system, Liver punctures, 3D/2D registration, Breathing motion},
abstract = {We present in this paper an augmented reality guidance system for liver thermal ablation in interventional radiology. To show the relevance of our methodology, the system is incrementally evaluated on an abdominal phantom and then on patients in the operating room. The system registers in a common coordinate system a preoperative image of the patient and the position of the needle that the practitioner manipulates. The breathing motion uncertainty is taken into account with a respiratory gating technique: the preoperative image and the guidance step are synchronized on expiratory phases. In order to fulfil the real-time constraints, we have developed and validated algorithms that automatically process and extract feature points. Since the guidance interface is also a major component of the system effectiveness, we validate the overall targeting accuracy on an abdominal phantom. This experiment showed that a practitioner can reach a predefined target with an accuracy of 2mm with an insertion time below one minute. Finally, we propose a passive evaluation protocol of the overall system in the operating room during five interventions on patients. These experiments show that the system can provide a guidance information during expiratory phases with an error below 5mm.}
}
@article{BLACK2024184,
title = {Mixed reality human teleoperation with device-agnostic remote ultrasound: Communication and user interaction},
journal = {Computers & Graphics},
volume = {118},
pages = {184-193},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000037},
author = {David Black and Mika Nogami and Septimiu Salcudean},
keywords = {Human computer interaction, Mixed reality, Augmented reality, Teleoperation, Tele-ultrasound},
abstract = {For many applications, remote guidance and telerobotics provide great advantages. For example, tele-ultrasound can bring much-needed expert healthcare to isolated communities. However, existing tele-guidance methods have serious limitations including either low precision for video conference-based systems, or high complexity and cost for telerobotics. A new concept called human teleoperation leverages mixed reality, haptics, and high-speed communication to provide tele-guidance that gives an expert nearly-direct remote control without requiring a robot. This paper provides an overview of the human teleoperation concept and its application to tele-ultrasound. The concept and its impact are discussed. A new approach to remote streaming and control of point-of-care ultrasound systems independent of their manufacturer is described, as is a high-speed communication system for the HoloLens 2 that is compatible with ResearchMode API sensor stream access. Details of these systems are shown in supplementary video demonstrations. Novel interaction methods enabled by HoloLens 2-based pose tracking are also introduced and tests of the communication and user interaction are presented. The results show continued improvement of the system compared to previous work in instrumentation, HCI, and communication. The system thus has good potential for tele-ultrasound, as well as possible other applications of human teleoperation including remote maintenance, inspection, and training. The remote ultrasound streaming and control application is made available open source.}
}
@article{SARICA2023104965,
title = {A dense residual U-net for multiple sclerosis lesions segmentation from multi-sequence 3D MR images},
journal = {International Journal of Medical Informatics},
volume = {170},
pages = {104965},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104965},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622002799},
author = {Beytullah Sarica and Dursun Zafer Seker and Bulent Bayram},
keywords = {Multiple sclerosis (MS), MS lesion segmentation, MRI, U-net, Convolutional neural networks, Deep learning, Residual blocks},
abstract = {Multiple Sclerosis (MS) is an autoimmune disease that causes brain and spinal cord lesions, which magnetic resonance imaging (MRI) can detect and characterize. Recently, deep learning methods have achieved remarkable results in the automated segmentation of MS lesions from MRI data. Hence, this study proposes a novel dense residual U-Net model that combines attention gate (AG), efficient channel attention (ECA), and Atrous Spatial Pyramid Pooling (ASPP) to enhance the performance of the automatic MS lesion segmentation using 3D MRI sequences. First, convolution layers in each block of the U-Net architecture are replaced by residual blocks and connected densely. Then, AGs are exploited to capture salient features passed through the skip connections. The ECA module is appended at the end of each residual block and each downsampling block of U-Net. Later, the bottleneck of U-Net is replaced with the ASSP module to extract multi-scale contextual information. Furthermore, 3D MR images of Fluid Attenuated Inversion Recovery (FLAIR), T1-weighted (T1-w), and T2-weighted (T2-w) are exploited jointly to perform better MS lesion segmentation. The proposed model is validated on the publicly available ISBI2015 and MSSEG2016 challenge datasets. This model produced an ISBI score of 92.75, a mean Dice score of 66.88%, a mean positive predictive value (PPV) of 86.50%, and a mean lesion-wise true positive rate (LTPR) of 60.64% on the ISBI2015 testing set. Also, it achieved a mean Dice score of 67.27%, a mean PPV of 65.19%, and a mean sensitivity of 74.40% on the MSSEG2016 testing set. The results show that the proposed model performs better than the results of some experts and some of the other state-of-the-art methods realized related to this particular subject. Specifically, the best Dice score and the best LTPR are obtained on the ISBI2015 testing set by using the proposed model to segment MS lesions.}
}
@article{GAMBERINI2015104,
title = {Psychological response to an emergency in virtual reality: Effects of victim ethnicity and emergency type on helping behavior and navigation},
journal = {Computers in Human Behavior},
volume = {48},
pages = {104-113},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215000540},
author = {Luciano Gamberini and Luca Chittaro and Anna Spagnolli and Claudio Carlesso},
keywords = {Emergency, Virtual reality, Helping behavior, Navigation behavior, Racial discrimination, Validation},
abstract = {Virtual environments are increasingly used for emergency training, but tend to focus mainly on teaching prescribed emergency procedures. However, social psychology literature highlights several factors that can bias individual response to an emergency in the real world, and would be worth considering in virtual training systems. In this paper, we focus on withdrawal of help due to racial discrimination and explore the potential of virtual environments to trigger this bias in emergency situations. We also test if a virtual emergency is actually reacted to as an emergency. We use an immersive virtual environment (IVE) where a victim issues help requests during two different emergency situations (time pressure or fire). While experiencing the emergency, white participants (N=96) receive a request for help from a black or white virtual human. The results show a psychological response to the virtual experience consistent with an emergency situation (increased state anxiety and increased frequency of collisions with objects in the environment) and biased by racial discrimination in help provision. In addition, racial discrimination increases under time pressure, but not in a fire. The implications for virtual training are discussed.}
}
@article{KIM2012184,
title = {Implementation of Martian virtual reality environment using very high-resolution stereo topographic data},
journal = {Computers & Geosciences},
volume = {44},
pages = {184-195},
year = {2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2011.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0098300411003475},
author = {Jung-Rack Kim and Shih-Yuan Lin and Jeong-Woo Hong and Young-Hwi Kim and Chin-Kang Park},
keywords = {Mars, High resolution DTM, Ortho-image, Virtual reality, Stereo analysis, Geomorphology},
abstract = {Topography over terrestrial or other planetary surfaces is an important base data for virtual reality construction. In particular, with inaccessible topography such as the Martian surface, virtual reality provides great value not only for public interaction but also for scientific research. For the latter application, since field surveys are essential for the geological and geomorphological researches, the virtual reality environment created based on verified topographic products provides an alternative solution for planetary research. The performance of virtual reality implementation over a planetary surface can be assessed by two major factors: (1) The geodetically controlled base topographic products, such as DTM and ortho-image, and (2) Technological integration of topographic products into virtual reality software and hardware. For the first aspect, the multi-resolution stereo analysis approach has already provided a solid basis so that specific topographic data sets over testing areas were generated by the hierarchical processor. To address the second problem, a parallel processor with multiple screen display combining 3D display software was employed in this research. As demonstrated in this paper, the constructed Martian virtual environment showed highly detailed features over the Athabasca Valles (one of former potential Mars Exploration Rover landing sites) and Eberswalde crater (one of the main original landing candidates for the NASA’s rover mission scheduled to launch in late 2011). The employment of such virtual reality environments is expected to be a powerful simulator after integrating a 3D Martian model, engineering and environment constraints for Martian geological and geomorphic researches including landing site selection and rover navigation.}
}
@article{ARCELOPERA2021397,
title = {Training birdsong recognition using virtual reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {5},
pages = {397-406},
year = {2021},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S209657962100067X},
author = {Carlos Arce-Lopera and María José Arias and Gustavo Corrales},
keywords = {Human computer interaction, Virtual environment, Birdsong, Audio training, User engagement},
abstract = {Background
In mega-biodiverse environments, where different species are more likely to be heard than seen, species monitoring is generally performed using bioacoustics methodologies. Furthermore, since bird vocalizations are reasonable estimators of biodiversity, their monitoring is of great importance in the formulation of conservation policies. However, birdsong recognition is an arduous task that requires dedicated training in order to achieve mastery, which is costly in terms of time and money due to the lack of accessibility of relevant information in field trips or even specialized databases. Immersive technology based on virtual reality (VR) and spatial audio may improve species monitoring by enhancing information accessibility, interaction, and user engagement.
Methods
This study used spatial audio, a Bluetooth controller, and a head-mounted display (HMD) to conduct an immersive training experience in VR. Participants moved inside a virtual world using a Bluetooth controller, while their task was to recognize targeted birdsongs. We measured the accuracy of recognition and user engagement according to the User Engagement Scale.
Results
The experimental results revealed significantly higher engagement and accuracy for participants in the VR-based training system than in a traditional computer-based training system. All four dimensions of the user engagement scale received high ratings from the participants, suggesting that VR-based training provides a motivating and attractive environment for learning demanding tasks through appropriate design, exploiting the sensory system, and virtual reality interactivity.
Conclusions
The accuracy and engagement of the VR-based training system were significantly high when tested against traditional training. Future research will focus on developing a variety of realistic ecosystems and their associated birds to increase the information on newer bird species within the training system. Finally, the proposed VR-based training system must be tested with additional participants and for a longer duration to measure information recall and recognition mastery among users.}
}
@article{BEHZADAN2015252,
title = {Augmented reality visualization: A review of civil infrastructure system applications},
journal = {Advanced Engineering Informatics},
volume = {29},
number = {2},
pages = {252-267},
year = {2015},
note = {Infrastructure Computer Vision},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2015.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034615000324},
author = {Amir H. Behzadan and Suyang Dong and Vineet R. Kamat},
keywords = {Engineering visualization, Augmented reality, Excavation safety, Building damage reconnaissance, Visual simulation, Education},
abstract = {In Civil Infrastructure System (CIS) applications, the requirement of blending synthetic and physical objects distinguishes Augmented Reality (AR) from other visualization technologies in three aspects: (1) it reinforces the connections between people and objects, and promotes engineers’ appreciation about their working context; (2) it allows engineers to perform field tasks with the awareness of both the physical and synthetic environment; and (3) it offsets the significant cost of 3D Model Engineering by including the real world background. This paper reviews critical problems in AR and investigates technical approaches to address the fundamental challenges that prevent the technology from being usefully deployed in CIS applications, such as the alignment of virtual objects with the real environment continuously across time and space; blending of virtual entities with their real background faithfully to create a sustained illusion of co-existence; and the integration of these methods to a scalable and extensible computing AR framework that is openly accessible to the teaching and research community. The research findings have been evaluated in several challenging CIS applications where the potential of having a significant economic and social impact is high. Examples of validation test beds implemented include an AR visual excavator-utility collision avoidance system that enables workers to “see” buried utilities hidden under the ground surface, thus helping prevent accidental utility strikes; an AR post-disaster reconnaissance framework that enables building inspectors to rapidly evaluate and quantify structural damage sustained by buildings in seismic events such as earthquakes or blasts; and a tabletop collaborative AR visualization framework that allows multiple users to observe and interact with visual simulations of engineering processes.}
}
@article{WEI2024100121,
title = {Communicating environment protection from plastic waste via VR: Effects of realism and spatial presence on risk perception},
journal = {Telematics and Informatics Reports},
volume = {13},
pages = {100121},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100121},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000070},
author = {Ran Wei and Shuhua Zhou and Renyi He and Kanni Huang},
keywords = {Virtual reality, Environmental protection, Plastic pollution, Realism, Spatial presence, Pro-environmental attitude},
abstract = {This study tested the utility of virtual reality technologies in building public awareness of an environmental issue—the growing pollution of plastic waste in oceans. We conducted an experiment to test whether viewing of VR video would produce two anticipated immersive experiences (e.g., perceived realism and spatial presence) in severely polluted oceans, and to further examine their effects on participants’ pro-environmental attitudes with regards to reducing plastic waste. Results showed that VR viewing led to higher perceived realism and spatial presence in comparisons with 2D video and audio-only conditions; perceived realism contributed significantly to pro-environmental attitude, whereas spatial presence did not. The higher the perceived realism, the stronger the pro-environmental attitude. Implications of the findings for using VR technologies for effective environmental communication are discussed.}
}
@article{PRANOTO2023757,
title = {Augmented reality navigation application to promote tourism to local state attraction “Lawang Sewu”},
journal = {Procedia Computer Science},
volume = {216},
pages = {757-764},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022712},
author = {Hady Pranoto and Peter Pranata Saputra and Melki Sadekh and Herru Darmadi and Yanfi Yanfi},
keywords = {Attraction, Augmented Reality, Mobile Application},
abstract = {The writing of this paper and the creation of this application is a means to improve and add on facilities on the local state-run attraction. This subject matter was picked in conjunction with the decline of tourism in the said historical site due to a pandemic that rankles Indonesia's tourism industry and drives it to the ground. Augmented Reality as a method of deliverance was picked due to its popularity and freshness in the tourism market. The research method was conducted starting with analysis, development, implementation, then evaluation. Analysis was done using sourcing available to the public and a questioner. Implementation was done using Unity, Vuforia, and Maya. And with the positive responses of 66.6 percent of the sample market finding enjoyment in the use of this application in their exploration during User Acceptance testing with the existing prototype. This shows that the resulting use of this application improves user enjoyment in experiencing the state attraction.}
}
@article{FUSCO2023102212,
title = {Enhancing hurricane risk perception and mitigation behavior through customized virtual reality},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102212},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102212},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003403},
author = {Giovanna Fusco and Jin Zhu},
keywords = {Customized Virtual Reality, Hurricane, Residential Building, Risk Perception, Mitigation},
abstract = {Hurricanes are one of the most destructive natural disasters, and proper preparation and mitigation are essential to enhance resilience. However, homeowners may have inaccurate risk perceptions, resulting in inadequate mitigation decision-making. Virtual reality (VR) damage visualization has the potential to communicate hurricane risk effectively and improve disaster resilience. Previous studies utilizing VR for hurricanes have primarily focused on disaster response such as evacuation, and often presented generalized visualization consistent across all users. This study presents a methodology for offering individual users a customized tool to experience the impacts of hypothetical hurricane events on their residential structures and explore the effects of various building retrofit actions in customized 3D virtual environments. The methodology was tested with 25 participants. Participants experienced structural damage and utility disruptions caused by hurricanes in their own residential homes in a customized, immersive virtual environment. Participants’ pre- and post-VR risk perceptions, as well as their choices of mitigation actions were surveyed. The results indicate that customized VR was effective in enhancing risk awareness and that participants were able to reduce their structural damages in hypothetical hurricane events by adopting various mitigation measures after the customized VR experience. The outcomes suggest that customized visualization of potential damages in virtual environments can effectively help disaster preparation and mitigation.}
}
@article{ISIK2024483,
title = {Integrating extended reality in industrial maintenance: a game-based framework for compressed air system training},
journal = {Procedia Computer Science},
volume = {232},
pages = {483-492},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000486},
author = {Birkan Isik and Gulbahar Emir Isik and Miroslav Zilka},
keywords = {Extended reality, Serious game, Maintenance training, Compressed Air System},
abstract = {In industrial environments, maintenance technologies developed under Industry 5.0 are important to ensure trouble-free production and reduce downtime. Compressed Air Systems (CASs) are an integral part of industrial production processes and require competent and correct maintenance. Maintenance personnel should be equipped with continuous and up-to-date training and their knowledge should be tested. There is a significant gap in the field of interactive educational games to improve the technical skills of the care staff, especially in the field of CAS. This research proposes a theoretical framework for a game based on Extended Reality (XR) technologies adapted for CAS maintenance training. The game leverages the trio of Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) to provide immersive and interactive learning encounters aimed at improving technical proficiency, problem-solving skills, and safety awareness among maintenance workers. The aim is to take advantage of the rewards of game-oriented learning, such as enhanced user engagement and knowledge absorption, to increase the effectiveness of training. In this context, the CAS game framework was created with three frameworks: education, game, and CAS. The study concludes by highlighting the potential benefits, research areas, and technologies that underpin the proposed structure.}
}
@article{BOTTANI2021103429,
title = {Wearable and interactive mixed reality solutions for fault diagnosis and assistance in manufacturing systems: Implementation and testing in an aseptic bottling line},
journal = {Computers in Industry},
volume = {128},
pages = {103429},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103429},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000361},
author = {Eleonora Bottani and Francesco Longo and Letizia Nicoletti and Antonio Padovano and Giovanni Paolo Carlo Tancredi and Letizia Tebaldi and Marco Vetrano and Giuseppe Vignali},
keywords = {Augmented reality, Smart technologies, System usability scale, Industry 4.0, Wearable technologies, Industrial safety},
abstract = {Thanks to the spread of technologies stemming from the fourth industrial revolution, also the topic of fault diagnosis and assistance in industrial contexts has benefited. Indeed, several smart tools were developed for assisting with maintenance and troubleshooting, without interfering with operations and facilitating tasks. In line with that, the present manuscript aims at presenting a web smart solution with two possible applications installed on an Android smartphone and Microsoft HoloLens. The solution aims at alerting the operators when an alarm occurs on a machine through notifications, and then at providing the instructions needed for solving the alarm detected. The two devices were tested by the operators of an industrial aseptic bottling line consisting of five machines in real working conditions. The usability of both devices was positively rated by these users based on the System Usability Scale (SUS) and additional appropriate statements. Moreover, the in situ application brought out the main difficulties and interesting issues for the practical implementation of the solutions tested.}
}
@article{ZHAO2023107736,
title = {Exploring 360-degree virtual reality videos for CSR communication: An integrated model of perceived control, telepresence, and consumer behavioral intentions},
journal = {Computers in Human Behavior},
volume = {144},
pages = {107736},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107736},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223000870},
author = {Wen Zhao and Yang Cheng and Yen-I Lee},
keywords = {360-Degree virtual reality, CSR communication, Perceived control, CSR advocacy, Purchase intention},
abstract = {Recent years have seen 360-degree virtual reality (VR) being used as a valuable tool in communicating corporate social responsibility (CSR) campaigns. Built upon the wealth of literature on stimulus–organism–response (S–O–R) theory, this study proposed to test a model integrating perceived control of the 360-degree CSR VR technology, telepresence, engagement with VR 360-degree video (VR engagement), CSR advocacy, and purchase intentions. To that end, a nationally representative sample was recruited for a survey that exposed participants to a digital CSR campaign that uses 360-degree VR to increase mental health awareness. Results showed a significant association between participants' perceived control over the VR 360-degree CSR campaign and telepresence as well as their engagement with the VR 360-degree video. Moreover, telepresence and VR engagement were significant mediators in predicting consumers’ willingness to promote social initiatives (CSR advocacy) and purchase intentions. Theoretical and practical implications of the study were discussed.}
}
@article{FUCHS2022103590,
title = {A collaborative knowledge-based method for the interactive development of cabin systems in virtual reality},
journal = {Computers in Industry},
volume = {136},
pages = {103590},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103590},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001974},
author = {Mara Fuchs and Florian Beckert and Jörn Biedermann and Björn Nagel},
keywords = {Aerospace, Aircraft cabin, Virtual reality, Knowledge-based modeling, Digital design},
abstract = {Progressive digitization in the development phase of systems is leading to shorter development times and lower costs. At the same time, the interactions in more complex systems are increasing and become more nested, which affects the understanding of system dependencies for humans as well as modeling these. This results in the challenge of digitizing the knowledge (rules, regulations, requirements, etc.) required to describe the system and its interrelationships. An example of such a system is the aircraft. In practice, usually, the technical design of the cabin and its systems is done separately from the preliminary aircraft design and the cabin results will be integrated late in the aircraft development process. In this paper, a proposal is given for a conceptual design method that enables a cabin systems layout based on preliminary aircraft design data (parameter set). Therefore, a central data model is developed that links cabin components to several disciplines to enable an automated layout. Here, knowledge is stored in an ontology. Linking the ontology with design rules and importing external parameters, missing information needed for preliminary design of cabin systems can be generated. The design rules are based on requirements, safety regulations as well as expert knowledge for design interpretation that has been collected and formalized. Using the ontology, an XML data structure can be instantiated which contains all information about properties, system relationships and requirements. So, the metadata and results of heterogenous domain-specific models and software tools are accessible for all experts of the layout process in a holistic manner and ensure data consistency. Using this XML data structure, a 3D virtual cabin mockup is created in which users have the possibility to interact with cabin modules and system components via controllers. This virtual development platform enables an interaction with complex product data sets like the XML file by visualizing metadata and analysis results along with the cabin geometry, making it even better comprehensible and processable for humans. So, various new cabin system designs can be iterated, evaluated, and optimized at low cost before the concepts are validated in a real prototype. For this, the virtual environment provides a platform that integrates all related disciplines, experts, research partners or the entire supply chain to improve communication among all stakeholders by directly participating and intervening in the evaluation and optimization process. Moreover, the use of VR is being investigated as a new technology in pre-design phase to exploit the potential of knowledge acquisition in immersive environments early in the development stage.}
}
@article{SILVA2023100022,
title = {Effect of an augmented reality app on academic achievement, motivation, and technology acceptance of university students of a chemistry course},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100022},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000168},
author = {Mónica Silva and Karina Bermúdez and Karina Caro},
keywords = {Academic level, Motivation, Technology acceptance, Chemistry, Augmented reality},
abstract = {The purpose of the current research was to explore the effect of an augmented reality app on the academic level, motivation, and technology acceptance of students of a university-level chemistry course. The study followed a pre/post-test design with a control group. At the end of a lecture on carbon bonds, we requested 95 university students to develop three models using modeling clay. The experimental group used the augmented reality app, while the control group used 2D pictures. The academic achievement increased for the students who used the augmented reality app. Motivation scores were not different between the control and experimental group. Our results indicate that augmented reality technology could be helpful in an academic setting.}
}
@article{FIGEYS2023105235,
title = {Challenges and promises of mixed-reality interventions in acquired brain injury rehabilitation: A scoping review},
journal = {International Journal of Medical Informatics},
volume = {179},
pages = {105235},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105235},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623002538},
author = {Mathieu Figeys and Farnaz Koubasi and Doyeon Hwang and Allison Hunder and Antonio Miguel-Cruz and Adriana {Ríos Rincón}},
keywords = {Augmented reality, Mixed reality, Extended reality, Acquired brain injury, Stroke, Rehabilitation},
abstract = {Background
Acquired brain injury (ABI) can lead to significant impairments and difficulties in everyday life, necessitating the need for rehabilitation. Mixed-reality (MR) technologies have revolutionized the delivery of neurorehabilitation therapies. However, inconsistencies in research methodology, diverse study populations and designs, and exaggerated claims in the research, media, and private consumer sectors have impacted the knowledge base of the field, including within the context of ABI rehabilitation.
Objective
This scoping review aims to explore MR-systems in ABI rehabilitation, while assessing the evidence base and technology readiness levels of these systems.
Methods
Seven databases were searched for studies, which were screened and analyzed by two independent raters. The types of MR systems, levels of evidence, and technology readiness levels were extracted and analyzed using descriptive analyses.
Results
Twenty-six studies were included in the review, all of which focused on ABI etiologies stemming from strokes. Across studies, upper-limb motor rehabilitation was the most common rehabilitation target of MR interventions, followed by gait, cognition, and lower-extremity functioning. At present, overall results indicate low evidence for MR-applications in ABI rehabilitation, with a median technology readiness level of 6, corresponding to system prototypes being tested in relevant environments.
Conclusion
Although challenges regarding system usability and design were reported, results appear promising with ongoing research. With variability across studies, technologies, and populations, determining the effectiveness of MR interventions in ABI remains a challenge, necessitating the need for ongoing innovation, research, and development of these systems.}
}
@article{KANG2021106875,
title = {Does virtual reality affect behavioral intention? Testing engagement processes in a K-Pop video on YouTube},
journal = {Computers in Human Behavior},
volume = {123},
pages = {106875},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106875},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221001989},
author = {Seok Kang and Sophia Dove and Hannah Ebright and Serenity Morales and Hyungjoon Kim},
keywords = {Virtual reality, Audience engagement, Celebrity, BTS},
abstract = {The current study tested the effects of a celebrity's virtual reality (VR) video on YouTube on audience transportation, parasocial interaction, identification, worship, and behavioral intention of social campaign participation. In addition to VR effects, fans and nonfans were compared with each other for engagement. The media content of BTS, a Korean pop boy band, was experimented in six groups: VR video with a headset, 360-degree video, 2D video, audio, news article, and control. A posttest only experiment was conducted among 142 participants in total. Results found that the VR with a headset group showed a significant difference from the news article group in transportation into BTS. The VR with a headset group had higher parasocial interaction with BTS than the control group. BTS fans were more likely than nonfans to show significant transportation, parasocial interaction, identification, worship, and behavioral intention.}
}
@article{CHEN2022107073,
title = {Deep learning-based medical image segmentation of the aorta using XR-MSF-U-Net},
journal = {Computer Methods and Programs in Biomedicine},
volume = {225},
pages = {107073},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107073},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004540},
author = {Weimin Chen and Hongyuan Huang and Jing Huang and Ke Wang and Hua Qin and Kelvin K.L. Wong},
keywords = {XR model, Cardiac aorta segmentation, MSF model, U-Net, CT, MRI},
abstract = {Purpose
This paper proposes a CT images and MRI segmentation technology of cardiac aorta based on XR-MSF-U-Net model. The purpose of this method is to better analyze the patient's condition, reduce the misdiagnosis and mortality rate of cardiovascular disease in inhabitants, and effectively avoid the subjectivity and unrepeatability of manual segmentation of heart aorta, and reduce the workload of doctors.
Method
We implement the X ResNet (XR) convolution module to replace the different convolution kernels of each branch of two-layer convolution XR of common model U-Net, which can make the model extract more useful features more efficiently. Meanwhile, a plug and play attention module integrating multi-scale features Multi-scale features fusion module (MSF) is proposed, which integrates global local and spatial features of different receptive fields to enhance network details to achieve the goal of efficient segmentation of cardiac aorta through CT images and MRI.
Results
The model is trained on common cardiac CT images and MRI data sets and tested on our collected data sets to verify the generalization ability of the model. The results show that the proposed XR-MSF-U-Net model achieves a good segmentation effect on CT images and MRI. In the CT data set, the XR-MSF-U-Net model improves 7.99% in key index DSC and reduces 11.01 mm in HD compared with the benchmark model U-Net, respectively. In the MRI data set, XR-MSF-U-Net model improves 10.19% and reduces 6.86 mm error in key index DSC and HD compared with benchmark model U-Net, respectively. And it is superior to similar models in segmentation effect, proving that this model has significant advantages.
Conclusion
This study provides new possibilities for the segmentation of aortic CT images and MRI, improves the accuracy and efficiency of diagnosis, and hopes to provide substantial help for the segmentation of aortic CT images and MRI.}
}
@article{TRUDEAU2023100009,
title = {“Breaking the fourth wall”: The effects of cinematic virtual reality film-viewing on adolescent students’ empathic responses},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100009},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100009},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300003X},
author = {Andrea Trudeau and Ying Xie and Olha Ketsman and Fatih Demir},
keywords = {Virtual reality, Film-viewing, Experiential learning, Empathy, Adolescence},
abstract = {This research study investigated the use of cinematic virtual reality (CVR) in a seventh-grade social studies classroom and its effects on adolescents' empathic responses. In this quantitative research study, participants (n ​= ​60) completed the Adolescent Measure of Empathy and Sympathy (AMES, Vossen et al., 2015) as a pretest a week before viewing The Displaced, a film about the lives of three refugee children, in either CVR or two-dimensional (2D), 360-degree format. Promptly after viewing the film, participants repeated the AMES as a posttest. Paired t-tests were conducted to explore the changes in mean scores for the AMES subscale scores between participants viewing the film in CVR and 2D formats as well as the changes in mean subscales between male and female participants viewing the film in CVR. Gain scores were also calculated and analyzed through a two-way MANOVA to examine the possible interaction effect between film format and gender on AMES subscale scores. The results of this study indicated that while the 2D, 360-degree film format affected adolescent students' affective empathy, there was a greater increase in both cognitive and affective empathy scores for those viewing the film in CVR with male adolescent students’ scores demonstrating the most remarkable increase.}
}
@article{SCOROLLI2023107910,
title = {Would you rather come to a tango concert in theater or in VR? Aesthetic emotions & social presence in musical experiences, either live, 2D or 3D},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107910},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107910},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002613},
author = {Claudia Scorolli and Eduardo {Naddei Grasso} and Lorenzo Stacchio and Vincenzo Armandi and Giovanni Matteucci and Gustavo Marfia},
keywords = {Musical aesthetic experience, Immersive concert, Virtual reality, Aesthetic emotions, Social presence, Embodied experience, Psychology of art, Empirical aesthetics},
abstract = {This work shows the preliminary results of a pioneering project aimed at comparing the aesthetic experience of a musical concert experienced in different contexts for which the audience's perceived presence is modulated in a continuum ranging from a live concert to a music video, passing through immersive artificial environments. In contrast to previous qualitative investigations of various immersive contexts, our study is unique in both the use of validated scales and the structured comparison of four experimental conditions: 1.live concert (LC), 2.the same concert through a traditional non-immersive music video (MV, analogous to fruition on YouTube), and finally in a virtual reality environment (VR), provided by two different devices, 3. a google cardboard (CVR) and 4. an HTC vive (HVR), allowing respectively for a basic and easily accessible experience, or for a less affordable but more immersive one. Through these manipulations we presumably affected not just the subjective aesthetic experience, but also the perceived presence of the Other/s. Consistently we measured both through the administration of the Aesthetic Emotions Scale (Aesthemos) and the Networked Minds Measure of Social Presence (NMMSP). The NMMSP showed no notable differences between conditions, which instead emerged from the analyses on the Aesthemos. The most liked experience was the Live one. Results also showed that LC experience had a stronger emotional impact only when compared to MV and CVR, but not to HTC since this last manipulation was the one eliciting the greatest interest. Theoretical implications are critically discussed, suggesting novel applications of the proposed approach.}
}
@article{ZHAO2023107779,
title = {Psychodynamic-based virtual reality cognitive training system with personalized emotional arousal elements for mild cognitive impairment patients},
journal = {Computer Methods and Programs in Biomedicine},
volume = {241},
pages = {107779},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107779},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723004455},
author = {Yanfeng Zhao and Liang Li and Xu He and Shuluo Yin and Yuxuan Zhou and Cesar Marquez-Chin and Wenjie Yang and Jiang Rao and Wentao Xiang and Bin Liu and Jianqing Li},
keywords = {Mild cognitive impairment, Psychodynamic-based cognitive training, Virtual reality, Personalized emotional arousal elements, Assessment},
abstract = {Background and objective
Mild cognitive impairment (MCI) is a serious threat to the physical health and quality of life of the elderly, as well as a heavy burden on families and society. The current computer-based rehabilitation training ignores the role of emotions in cognitive impairment rehabilitation, making it difficult to improve patient engagement and efficiency. To address this, a psychodynamics-based cognitive rehabilitation training method with personalized emotional arousal elements was proposed using virtual reality technology.
Methods
Our proposed method contains four training tasks, which cover (audiovisual memory, attention & processing, working memory, abstract & Logic, spatial pathfinding) and six positive emotional arousal elements (sensory feedback, achievement system, multiplayer interaction, score comparison, relaxation scenarios, and peaceful videos) to motivate participants to persist during cognitive training continuously and maintain a positive mental attitude toward training. The six emotional arousal elements were divided into two personalized combinations—full combination and half combination—based on the results of the pre-assessment and were dynamically distributed throughout both the training tasks and post-training.
Results
Fifteen participants with MCI were recruited to complete the proposed experiment and validate the effectiveness of the system. They were first asked to complete two assessments (e.g., the big five scale and the positive and negative affect scale) to investigate their personalities. Based on the results of the assessments, they were provided with a full or half combination of arousal elements in the training tasks and post-training. Finally, the acceptability of the system and task experience were assessed using questionnaires. Notably, there was a significant increase in training scores for participants who completed a six-week training period (66.7%, 33.4%, and 25.0% for attention and processing, working memory, and abstraction and logic, respectively). The results show that positive emotional arousal had a positive effect on the MCI participants. The training tasks and arousal elements can improve cognitive function and enhance the confidence and engagement of participants. There were no significant differences in cognitive domain training scores between the two groups.
Conclusions
This personalized cognitive training system has the potential to serve as a convenient solution for complementary treatment of MCI.}
}
@article{BALL2021101728,
title = {Virtual reality adoption during the COVID-19 pandemic: A uses and gratifications perspective},
journal = {Telematics and Informatics},
volume = {65},
pages = {101728},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101728},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321001672},
author = {Christopher Ball and Kuo-Ting Huang and Jess Francis},
keywords = {Virtual reality, VR, Uses and gratifications theory, COVID-19, Pandemic},
abstract = {The coronavirus disease 2019 (COVID-19) pandemic has impacted all aspects of people's lives, including how we work, play, learn, exercise, and socialize. Virtual reality (VR) technology has the potential to mitigate many of the challenges brought about by the pandemic, which has spurred increased adoption. However, relatively low adoption overall and limited software still restrict the power of VR to address COVID-19 difficulties effectively. This study examines how the perceived impacts of COVID-19 might lead to different VR uses and gratifications and device ownership / variability. Furthermore, we investigate the importance of social interactivity within VR for increasing adoption intentions. We surveyed 298 Amazon Mechanical Turk users during the Fall of 2020. Results indicate that the pandemic's perceived impacts influenced the likelihood of acquiring VR for education, tourism, and work. For VR ownership and variability, those who purchased VR during the pandemic were more likely to report buying it for work. Those with access to high-end VR hardware were more likely to report a broader range of uses, including socializing, health, and telemedicine. Validating the importance of various applications during the pandemic, we found that the desire for social interactivity mediates the impacts of COVID-19 on future adoption intentions. Theoretically, we propose several gratifications sought via the use of VR during the pandemic. Practically, we discuss recommendations for future VR research, marketing, and software design.}
}
@article{ZHANG2021101432,
title = {Enhancing human indoor cognitive map development and wayfinding performance with immersive augmented reality-based navigation systems},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101432},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101432},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001841},
author = {Jinyue Zhang and Xiaolu Xia and Ruiqi Liu and Nan Li},
keywords = {Immersive augmented reality (IAR), Navigation, Wayfinding, Cognitive map, Workload},
abstract = {Augmented reality (AR) is an interactive experience where computer-generated perceptual information is superimposed into the real-world environment. Most existing research in AR-based wayfinding has focused on the technological aspects of developing AR-based software or devices to realize navigation. No previous investigations have focused on understanding the impact of immersive augmented reality (IAR)–based systems on human wayfinding performance from the cognitive perspective. Aimed at investigating the influence of IAR-based systems on people’s cognitive map development and their subsequent wayfinding performance as well as the effect of using three-dimensional (3D) layout models in IAR environments in addition to superimposed guideposts, an experiment was carried out in a building with a complex floor plan. A total of 54 university students were evenly divided into three groups: a control group with no IAR assistance, a second group using an IAR-based navigation system that includes only superimposed guideposts, and a third group using an IAR-based navigation system that includes both guideposts and a 3D layout model. Each participant was asked to conduct a spatial exploration task in the environment, sketch a floor map based on their spatial cognition, and perform a wayfinding task to find eight specific locations in the building. An analysis of the participants’ performance and responses to a number of self-evaluation questionnaires collected in the experiment indicates that IAR technology can help people develop their cognitive maps more effectively and can substantially improve their wayfinding performance with a much lower workload. A second finding is that adding a 3D layout model can enhance the effect of an IAR-based navigation system in terms of cognitive map development. The findings from this research extend the existing knowledge about IAR-based navigation and further verify that AR technology has the potential to reduce human workload for cognitive tasks. The results also could support its more effective application in various scenarios that require assisted wayfinding and cognitive map training, such as emergency evacuation drills.}
}
@article{HU201885,
title = {A VR simulation framework integrated with multisource CAE analysis data for mechanical equipment working process},
journal = {Computers in Industry},
volume = {97},
pages = {85-96},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518300289},
author = {Liang Hu and Zhenyu Liu and Jianrong Tan},
keywords = {Modeling and simulation, DEVS, VR simulation framework, Working process modeling, CAE analysis data integration},
abstract = {Nowadays, the virtual reality (VR) technology has been widely used in machinery to verify the working process and performance of equipment prototype in early design stage. To gain insight into the coupling of process verifying the timing and rhythm of equipment working and performance indicating the quality of equipment, a VR framework is proposed in this paper to integrate the modeling and simulation of working process with the visualization of multisource analysis data from various computer-aided engineering (CAE) software. The framework is equipped with an extended discrete event system specification (DEVS) method, named scripted DEVS, which divides the process modeling into two stages, namely, behavior modeling and data customization, and frees the mechanical designer from programming. Based on the method, a uniform procedure of data integration is designed to hide the specific structure and visualization implementation of diverse data from the modeler by a script format called PDTree (Process&Data Tree) and a series of visualization interfaces in model-view-controller (MVC) pattern. Three applications of different working processes are also presented finally to verify the flexibility and compatibility of the framework.}
}
@article{DEPAOLIS2022e00238,
title = {Virtual reality for the enhancement of cultural tangible and intangible heritage: The case study of the Castle of Corsano},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {27},
pages = {e00238},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00238},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000273},
author = {Lucio Tommaso {De Paolis} and Sofia Chiarello and Carola Gatto and Silvia Liaci and Valerio {De Luca}},
keywords = {Cultural heritage, Intangible heritage, 3D modelling, Virtual reality, User experience},
abstract = {This paper concerns the development of an immersive VR application for the enhancement of an inaccessible old Castle in Corsano, a small village in Salento - Italy. Starting from the 3D reconstruction of the building, the project allowed the development of an interaction system aimed at providing the user with the historical information about the Castle. These contents come from the analysis of the tangible cultural heritage (such as the architectural elements, the furniture, the decorative motifs of the castle, and their evolution through the centuries), but also from the collection of intangible heritage, such as reminiscences on folk customs and traditions related to the context of the castle. Moreover, in order to evaluate the user experience of the developed application, some tests were carried out on a heterogeneous sample of users, obtaining positive feedback on the degree of immersiveness and sense of presence of the application.}
}
@article{HASSENFELDT2020301011,
title = {Exploring the Learning Efficacy of Digital Forensics Concepts and Bagging & Tagging of Digital Devices in Immersive Virtual Reality},
journal = {Forensic Science International: Digital Investigation},
volume = {33},
pages = {301011},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2020.301011},
url = {https://www.sciencedirect.com/science/article/pii/S2666281720302602},
author = {Courtney Hassenfeldt and Jillian Jacques and Ibrahim Baggili},
keywords = {Virtual reality, Cyber forensics, Digital forensics, Education, Gamification},
abstract = {This work presents the first account of evaluating learning inside a VR experience created to teach Digital Forensics (DF) concepts, and a hands-on laboratory exercise in Bagging & Tagging a crime scene with digital devices. First, we designed and developed an immersive VR experience which included a lecture and a lab. Next, we tested it with (n = 57) participants in a controlled experiment where they were randomly assigned to a VR group or a physical group. Both groups were subjected to the same lecture and lab, but one was in VR and the other was in the real world. We collected pre- and post-test results to assess the participants’ knowledge in DF concepts learned. Our experimental results indicated no significant differences in scores between the immersive VR group and the physical group. However, our results showed faster completion times in VR by the participants, which hints at VR being more time efficient, as virtual environments can be spun programmatically with little downtime.}
}
@article{CHEN2020106418,
title = {Using augmented reality to experiment with elements in a chemistry course},
journal = {Computers in Human Behavior},
volume = {111},
pages = {106418},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106418},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220301710},
author = {Shih-Yeh Chen and Shiang-Yao Liu},
keywords = {Augmented reality, Hands-on, Chemistry, Interest, Mobile, Collaborative},
abstract = {The use of augmented reality (AR) in education is an emerging trend aimed at improving students' learning outcomes and affective factors. This study further investigated the effects of AR learning activities combined with different approaches, including teacher-centered demonstration and student-centered hands-on, on the conceptual understanding of chemistry and interest in science. Participants of this study included 104 ninth graders from four classes at a junior high school, who were taught by the same chemistry teacher. A quasi-experimental research design was conducted to compare students' pretest, immediate posttest, and delayed posttest scores. Statistical findings were further supplemented by student interviews toward the activities. Results showed that the hands-on learning group performed significantly better on the chemical reactions concept test and interest questionnaire of the immediate posttest than the demonstration learning group. Retention effects also revealed that the students’ conceptual understanding of chemical elements remained effective four months after completion of the learning activities. No significant decline of situational interest and a slightly upward trend of individual interest were found in both groups. According to the interview responses, students were satisfied with the hands-on activities and appreciated the opportunity to be active learners. From this study, hands-on AR could serve as a promising strategy to motivate students in learning chemistry not only for immediate effectiveness but longstanding influence.}
}
@article{ZHOU2023188,
title = {Web-based mixed reality video fusion with remote rendering},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {2},
pages = {188-199},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000274},
author = {Qiang Zhou and Zhong Zhou},
keywords = {Mixed reality, Video fusion, WebRTC, Remote rendering},
abstract = {Background
Mixed reality (MR) video fusion systems merge video imagery with 3D scenes to make the scene more realistic and help users understand the video content and temporal–spatial correlation between them, reducing the user′s cognitive load. MR video fusion are used in various applications; however, video fusion systems require powerful client machines because video streaming delivery, stitching, and rendering are computationally intensive. Moreover, huge bandwidth usage is another critical factor that affects the scalability of video-fusion systems.
Methods
Our framework proposes a fusion method for dynamically projecting video images into 3D models as textures.
Results
Several experiments on different metrics demonstrate the effectiveness of the proposed framework.
Conclusions
The framework proposed in this study can overcome client limitations by utilizing remote rendering. Furthermore, the framework we built is based on browsers. Therefore, the user can test the MR video fusion system with a laptop or tablet without installing any additional plug-ins or application programs.}
}
@article{JIANG2022101001,
title = {Adjacent surface trajectory planning of robot-assisted tooth preparation based on augmented reality},
journal = {Engineering Science and Technology, an International Journal},
volume = {27},
pages = {101001},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001130},
author = {Jingang Jiang and Yafeng Guo and Zhiyuan Huang and Yongde Zhang and Dianhao Wu and Yi Liu},
keywords = {Dental assisted robot, Adjacent surface tooth preparation, Trajectory planning, Augmented reality, NURBS curve},
abstract = {Dental caries is a common disease affecting the health of humans. Tooth preparation is an essential method for the treatment of dental caries, and its effect determines the quality of dental caries restoration directly. At present, the doctor-patient ratio of dentistry is incredibly unbalanced, and it is challenging to meet the increasing demand for tooth preparation. Besides, it is prone to produce visual deviation and positioning error if the tooth preparation is operated manually by doctors. This paper focuses on the adjacent surface tooth preparation of the full crown. By analyzing the preparation process, the robot body as well as the end-effector are selected. Further, the pose and position between the robot and the tooth are established. Through the inverse calculation and interpolation of the NURBS curve, the robot-assisted trajectory planning of adjacent surface tooth preparation is realized. On the basis of the traditional offline programming method, mobile App “Dental Preparation Assisting Software” is developed based on augmented reality, which can interactively display the planned adjacent preparation curves and preparation information. And the robot can be controlled by the mobile App, which improves the visualization and efficiency of the preparation process. Finally, the experimental system based on the Dobot Magician robot is established, and the robot experiments of adjacent surface tooth preparation are carried out. The maximum relative fixed-point error of each feature point in the XYZ axis is 0.24 mm, 0.29 mm and 0.37 mm, respectively. The confidence interval width of each feature point in the XYZ direction is stable at about 0.31 mm on average, and the correlation between the feature points in the three directional variables is not strong. The proposed method of robot-assisted trajectory planning based on augmented reality for adjacent surface tooth preparation can improve the efficiency of tooth preparation and relieve the pressure of manual tooth preparation within the margin of error, and its feasibility and validity are verified.}
}
@article{NIKITIN2020622,
title = {VR Training for Railway Wagons Maintenance: architecture and implementation},
journal = {Procedia Computer Science},
volume = {176},
pages = {622-631},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318883},
author = {Alexandr Nikitin and Nina Reshetnikova and Ivan Sitnikov and Olesya Karelova},
keywords = {VR Training, railway maintenances, digital railway, virtual reality, software architecture, implementation, HTC Vive, Unity},
abstract = {The problems of supporting the required professional skills of railway wagons maintenance workers are regarded in this article. It is proposed to design a simulator based on virtual reality to test the skills of a car inspector in accordance with the instructions. The requirements for the simulator, its architecture and the architecture of the software simulator, platforms and implementation tools are considered. The paper provides features of the implemented prototype of the simulator based on HTC Vive and Unity platform for training skills of car inspection by the inspector and their further verification by the instructor in accordance with the regulations illustrated by a 3D model of the wagon with the fault and the tools of the inspector. Analysis of the simulator prototype’s feasibility showed its sufficient ergonomic adequacy to the actual process of wagons maintenance and identified a number of implementation problems that will be taken into account in further research and development.}
}
@article{KLEPACZKO2016293,
title = {Simulation of MR angiography imaging for validation of cerebral arteries segmentation algorithms},
journal = {Computer Methods and Programs in Biomedicine},
volume = {137},
pages = {293-309},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304016},
author = {Artur Klepaczko and Piotr Szczypiński and Andreas Deistung and Jürgen R. Reichenbach and Andrzej Materka},
keywords = {MR angiography, Vessel segmentation, Cerebral vasculature modeling, MRI simulation, Quantitative validation},
abstract = {Background and objective
Accurate vessel segmentation of magnetic resonance angiography (MRA) images is essential for computer-aided diagnosis of cerebrovascular diseases such as stenosis or aneurysm. The ability of a segmentation algorithm to correctly reproduce the geometry of the arterial system should be expressed quantitatively and observer-independently to ensure objectivism of the evaluation.
Methods
This paper introduces a methodology for validating vessel segmentation algorithms using a custom-designed MRA simulation framework. For this purpose, a realistic reference model of an intracranial arterial tree was developed based on a real Time-of-Flight (TOF) MRA data set. With this specific geometry blood flow was simulated and a series of TOF images was synthesized using various acquisition protocol parameters and signal-to-noise ratios. The synthesized arterial tree was then reconstructed using a level-set segmentation algorithm available in the Vascular Modeling Toolkit (VMTK). Moreover, to present versatile application of the proposed methodology, validation was also performed for two alternative techniques: a multi-scale vessel enhancement filter and the Chan–Vese variant of the level-set-based approach, as implemented in the Insight Segmentation and Registration Toolkit (ITK). The segmentation results were compared against the reference model.
Results
The accuracy in determining the vessels centerline courses was very high for each tested segmentation algorithm (mean error rate = 5.6% if using VMTK). However, the estimated radii exhibited deviations from ground truth values with mean error rates ranging from 7% up to 79%, depending on the vessel size, image acquisition and segmentation method.
Conclusions
We demonstrated the practical application of the designed MRA simulator as a reliable tool for quantitative validation of MRA image processing algorithms that provides objective, reproducible results and is observer independent.}
}
@article{FERRAGUTI2019158,
title = {Augmented reality based approach for on-line quality assessment of polished surfaces},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {59},
pages = {158-167},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518305131},
author = {Federica Ferraguti and Fabio Pini and Thomas Gale and Franck Messmer and Chiara Storchi and Francesco Leali and Cesare Fantuzzi},
keywords = {Robotic polishing, Augmented reality, Industrial robotic solutions},
abstract = {Augmented reality is considered one of the enabling technologies of the fourth industrial revolution, within the Industry 4.0 program and beyond. Indeed, augmented reality solutions can increase the working quality and the productivity and allow a better use of the human resources. This technology can help the operator in the industrial applications during the crucial phases of the processes. Since the quality assessment of the surfaces is recognized to be a key phase in the polishing process, in this paper we propose a novel method that exploits augmented reality to support the operators during this phase. The metrology data measured by a surface measurement system are directly projected on the polished component through an augmented reality headset worn by the operators and used to assess the quality of the worked surfaces. Rather than imagine how a certain parameter change can affect the result achieved, the information is directly there on the component’s surface. Users can see from the data where refinements are required and make better and faster decisions, which is compelling for its potential beyond industrial polishing. The proposed method is implemented and validated on an industrial cell, where the robot automatically perform the polishing task and move the head of the surface measurement system along the surface to measure the metrology parameters. Thanks to the proposed approach, the end-user and the operator can directly see on the component if the quality reached satisfies the specifications or if some parts of the surface require further refinements through additional polishing steps.}
}
@article{SAIDANINEFFATI2021104030,
title = {An educational tool for enhanced mobile e-Learning for technical higher education using mobile devices for augmented reality},
journal = {Microprocessors and Microsystems},
volume = {83},
pages = {104030},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104030},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121002027},
author = {Omnia {Saidani Neffati} and Roy Setiawan and P Jayanthi and S Vanithamani and D K Sharma and R Regin and Devi Mani and Sudhakar Sengan},
keywords = {Mel, Mobile devices, Technical higher education, e-Learner, Smartphone},
abstract = {In all dimensions of education and all subjects, Smartphones have turned out to be broadly acknowledged technology. It plays an essential task in advanced online education systems. Because of smart devices' effortlessness and extension property, it is getting to be mandatory for portable applications. This paper analyses the research on Smart Devices (SD) to incorporate visual simulation into e-learning. The researchers created an Augmented Reality (AR) platform for e-learners to expand the coursebook with graphics and virtual multimedia applications. This paper recommends a Mobile e-Learning (MeL) application termed “MeL app". The advanced MeL app methods have been tested using Mann-Whitney ‘U' Test in the lecture hall using real-time learners. The proposed MeL app planned to create the learning practice easier, focusing on e-learner's requirements by encouraging e-learners and instructor relationships to maintain communicative development-based e-learning for Technical Higher Education (THE). Software engineering learners assess this proposed framework in THE. Future work in this investigation incorporates new highlights, testing the device in extreme situations, evaluating the instructive perspectives utilizing more significant and increasingly various understudy and beginner inhabitants, and at last, extending the application space.}
}
@article{SHI2020101153,
title = {A neurophysiological approach to assess training outcome under stress: A virtual reality experiment of industrial shutdown maintenance using Functional Near-Infrared Spectroscopy (fNIRS)},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101153},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101153},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301245},
author = {Yangming Shi and Yibo Zhu and Ranjana K. Mehta and Jing Du},
keywords = {Shutdown maintenance training, Virtual reality, Eye-tracking, fNIRS},
abstract = {Shutdown maintenance, i.e., turning off a facility for a short period for renewal or replacement operations is a highly stressful task. With the limited time and complex operation procedures, human stress is a leading risk. Especially shutdown maintenance workers often need to go through excessive and stressful on-site trainings to digest complex operation information in limited time. The challenge is that workers’ stress status and task performance are hard to predict, as most trainings are only assessed after the shutdown maintenance operation is finished. A proactive assessment or intervention is needed to evaluate workers’ stress status and task performance during the training to enable early warning and interventions. This study proposes a neurophysiological approach to assess workers’ stress status and task performance under different virtual training scenarios. A Virtual Reality (VR) system integrated with the eye-tracking function was developed to simulate the power plant shutdown maintenance operations of replacing a heat exchanger in both normal and stressful scenarios. Meanwhile, a portable neuroimaging device – Functional Near-Infrared Spectroscopy (fNIRS) was also utilized to collect user’s brain activities by measuring hemodynamic responses associated with neuron behavior. A human–subject experiment (n = 16) was conducted to evaluate participants’ neural activity patterns and physiological metrics (gaze movement) related to their stress status and final task performance. Each participant was required to review the operational instructions for a pipe maintenance task for a short period and then perform the task based on their memory in both normal and stressful scenarios. Our experiment results indicated that stressful training had a strong impact on participants’ neural connectivity patterns and final performance, suggesting the use of stressors during training to be an important and useful control factors. We further found significant correlations between gaze movement patterns in review phase and final task performance, and between the neural features and final task performance. In summary, we proposed a variety of supervised machine learning classification models that use the fNIRS data in the review session to estimate individual’s task performance. The classification models were validated with the k-fold (k = 10) cross-validation method. The Random Forest classification model achieved the best average classification accuracy (80.38%) in classifying participants’ task performance compared to other classification models. The contribution of our study is to help establish the knowledge and methodological basis for an early warning and estimating system of the final task performance based on the neurophysiological measures during the training for industrial operations. These findings are expected to provide more evidence about an early performance warning and prediction system based on a hybrid neurophysiological measure method, inspiring the design of a cognition-driven personalized training system for industrial workers.}
}
@incollection{GOMEZTONE20237,
title = {Chapter 2 - Augmented and immersive virtual reality to train spatial skills in STEAM university students},
editor = {Surbhi {Bhatia Khan} and Suyel Namasudra and Swati Chandna and Arwa Mashat and Fatos Xhafa},
booktitle = {Innovations in Artificial Intelligence and Human-Computer Interaction in the Digital Era},
publisher = {Academic Press},
pages = {7-30},
year = {2023},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-99891-8},
doi = {https://doi.org/10.1016/B978-0-323-99891-8.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998918000024},
author = {Hugo C. Gomez-Tone and Jorge Martin-Gutierrez and Betty K. Valencia-Anci},
keywords = {Augmented reality, Immersive virtual reality, Spatial skills, STEAM, Training},
abstract = {A mandatory requirement for all people who want to go into the STEAM field (science, technology, engineering, arts and math) is to have developed levels of spatial skills because these skills allow the adequate performance of students, especially in the first year of university studies. One hundred years of studies of these skills have shown that it is possible to train them, and countless forms of training have been developed. However, new technologies and human–computer interaction are proving to be more effective. We will show in this chapter the effectiveness of two short training courses using augmented reality and immersive virtual reality in engineering and architecture students from two universities in different countries. For the measurement of these skills, we considered three subcomponents measured with the Mental Rotation Test (MRT) for the mental rotations component, the Differential Aptitude Test (DAT5-SR) for the spatial visualization component, and Spatial Orientation Test (SOT) for the spatial orientation component. The data obtained from experimental and control groups were analyzed using parametric and non-parametric tests: one-factor ANOVA, Student’s t test, and Kruskal–Wallis. We conclude that augmented reality and immersive virtual reality are very effective as short spatial skills training in engineering and architecture students.}
}
@article{DANG2023105810,
title = {A 3D-Panoramic fusion flood enhanced visualization method for VR},
journal = {Environmental Modelling & Software},
volume = {169},
pages = {105810},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105810},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223001962},
author = {Pei Dang and Jun Zhu and Yuxuan Zhou and Yuting Rao and Jigang You and Jianlin Wu and Mengting Zhang and Weilian Li},
keywords = {Flood simulation, Panoramas, Fusion visualization, CA-PBD, Immersive virtual reality},
abstract = {Flood visualization plays a pivotal role in understanding and predicting flood disasters, as it aids in the development of effective disaster mitigation measures and emergency response strategies. However, traditional methods of flood computation and visualization often suffer from poor timeliness and subpar visualization effects, particularly in virtual reality (VR) scenarios. Panoramic data, inherently suitable for VR visualization, offers a low data volume attribute that significantly reduces the time cost of data processing. This allows for the real-time overlay of flood simulation results with the actual environment. We proposed a multi-scale flood simulation method which is subsequently integrated with panoramic images or videos through a dimensionality reduction process, enhancing visualization effects. We developed a prototype system to validate the proposed methodology, and the results demonstrated improved visualization outcomes, rendering efficiency, and user experience in VR panoramic flood simulations.}
}
@article{MENDOZA20155,
title = {Augmented Reality as a Tool of Training for Data Collection on Torque Auditing},
journal = {Procedia Computer Science},
volume = {75},
pages = {5-11},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036479},
author = {Manuel Mendoza and Miguel Mendoza and Eloy Mendoza and Eduardo González},
keywords = {Augmented Reality, Residual Torque, Auditing torque, Variability, Validating Torque},
abstract = {The objective of this research is to implement an augmented reality system focused on the training of audit torque, where a person without any knowledge of the theories and application of torque, will be able to understand, implement and audit the pair residual strength in the necessary processes in order to improve the quality of a product that has bolted joints. Today manufacturing companies and especially those that focus on the automotive, aerospace and consumer need to apply precise torque and validated to the large number of bolted connections they have in their products. For the application of torque there are several very complete theories because it is a very critical process for the final quality product. Theories of torque can become complex because they need to have a preliminary knowledge of engineering, as well as a company training to implement this. Each torque required to enter certain applied force specifications so to see if the applied torque is within these, you need to audit the residual torque. The residual torque refers to know the force which is at the junction after being screwed tight. It was observed as a result of this experiment that the inherent difference between the performance of the torque wrench and screw connection performance in delivering the force required for efficient clamping force decreases their variability with the help of augmented reality.}
}
@article{BARRY20231151,
title = {Creative teaching using hybrid e-learning and virtual reality},
journal = {Procedia Computer Science},
volume = {225},
pages = {1151-1160},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.103},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012619},
author = {Dana M. Barry and Hideyuki Kanematsu and Toshihiro Tanaka},
keywords = {Creative teaching, STEM, active student learning, hybrid e-learning, virtual reality, communication and problem solving skills},
abstract = {This paper describes three creative teaching activities that motivate and engage students into active learning. The teaching methods used include hybrid e-learning and virtual reality. The hybrid e-learning style is a combination of e-learning and a hands-on activity in a laboratory setting. For one hybrid e-learning project, students are asked to design, build, and test seawalls to help protect Japan from future tsunamis. In the other hybrid e-learning project, student teams are challenged to invent the best tasting nutritious fruit juice by using various fruits, blenders, water, plasticware, and other items. For the third project, participants use virtual reality (VR) headsets to learn about rollercoasters and to experience the thrill of riding on one.}
}
@article{FUKUDA2019179,
title = {An indoor thermal environment design system for renovation using augmented reality},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {2},
pages = {179-188},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S228843001830068X},
author = {Tomohiro Fukuda and Kazuki Yokoi and Nobuyoshi Yabuki and Ali Motamedi},
keywords = {Environmental design, Indoor thermal environment, Intuitive visualization, Interactive environment, Computational Fluid Dynamics (CFD), Augmented Reality (AR)},
abstract = {The renovation projects of buildings and living spaces, which aim to improve the thermal environment, are gaining importance because of energy saving effects and occupants' health considerations. However, the indoor thermal design is not usually performed in a very efficient manner by stakeholders, due to the limitations of a sequential waterfall design process model, and due to the difficulty in comprehending the CFD simulation results for stakeholders. On the other hand, indoor greenery has been introduced to buildings as a method for adjusting the thermal condition. Creating a VR environment, which can realistically and intuitively visualize a thermal simulation model is very time consuming and the resulting VR environment created by 3D computer graphics objects is disconnected from the reality and does not allow design stakeholders to experience the feelings of the real world. Therefore, the objective of this research is to develop a new AR-based methodology for intuitively visualizing indoor thermal environment for building renovation projects. In our proposed system, easy-to-comprehend visualization of CFD results augment the real scenes to provide users with information about thermal effects of their renovation design alternatives interactively. Case studies to assess the effect of indoor greenery alternatives on the thermal environment are performed. In conclusion, integrating CFD and AR provides users with a more natural feeling of the future thermal environment. The proposed method was evaluated feasible and effective.}
}
@article{SHAO2022105826,
title = {Augmented reality calibration using feature triangulation iteration-based registration for surgical navigation},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105826},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105826},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005856},
author = {Long Shao and Shuo Yang and Tianyu Fu and Yucong Lin and Haixiao Geng and Danni Ai and Jingfan Fan and Hong Song and Tao Zhang and Jian Yang},
keywords = {Augmented reality, Surgical navigation, Shape feature matching, Feature triangulation iteration registration},
abstract = {Background
Marker-based augmented reality (AR) calibration methods for surgical navigation often require a second computed tomography scan of the patient, and their clinical application is limited due to high manufacturing costs and low accuracy.
Methods
This work introduces a novel type of AR calibration framework that combines a Microsoft HoloLens device with a single camera registration module for surgical navigation. A camera is used to gather multi-view images of a patient for reconstruction in this framework. A shape feature matching-based search method is proposed to adjust the size of the reconstructed model. The double clustering-based 3D point cloud segmentation method and 3D line segment detection method are also proposed to extract the corner points of the image marker. The corner points are the registration data of the image marker. A feature triangulation iteration-based registration method is proposed to quickly and accurately calibrate the pose relationship between the image marker and the patient in the virtual and real space. The patient model after registration is wirelessly transmitted to the HoloLens device to display the AR scene.
Results
The proposed approach was used to conduct accuracy verification experiments on the phantoms and volunteers, which were compared with six advanced AR calibration methods. The proposed method obtained average fusion errors of 0.70 ± 0.16 and 0.91 ± 0.13 mm in phantom and volunteer experiments, respectively. The fusion accuracy of the proposed method is the highest among all comparison methods. A volunteer liver puncture clinical simulation experiment was also conducted to show the clinical feasibility.
Conclusions
Our experiments proved the effectiveness of the proposed AR calibration method, and revealed a considerable potential for improving surgical performance.}
}
@article{MAKRANSKY2017276,
title = {Development and validation of the Multimodal Presence Scale for virtual reality environments: A confirmatory factor analysis and item response theory approach},
journal = {Computers in Human Behavior},
volume = {72},
pages = {276-285},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.02.066},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217301474},
author = {Guido Makransky and Lau Lilleholt and Anders Aaby},
keywords = {Presence, Virtual reality, Confirmatory factor analysis, Item response theory, Virtual simulations},
abstract = {Presence is one of the most important psychological constructs for understanding human-computer interaction. However, different terminology and operationalizations of presence across fields have plagued the comparability and generalizability of results across studies. Lee's (2004) unified understanding of presence as a multidimensional construct made up of physical, social, and self-presence, has created a unified theory of presence; nevertheless, there are still no psychometrically valid measurement instruments based on the theory. Two studies were conducted that describe the development of a standardized multidimensional measure of presence (the MPS) for a VR learning context based on this theory, and its validation using confirmatory factor analysis and item response theory. The results from Study 1 which included 161 medical students from Denmark indicated that the items used in the MPS measure a three dimensional theoretical model of presence: physical, social, and self-presence. Furthermore, IRT analyses indicated that it was possible to limit the number of items in the MPS to 15 (five items per sub-dimension) while maintaining the construct validity and reliability of the measure. The results of Study 2, which included 118 biology students from Scotland, supported the validity and generalizability of the MPS in a new context.}
}
@article{SKULMOWSKI2023100023,
title = {Ethical issues of educational virtual reality},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100023},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100023},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300017X},
author = {Alexander Skulmowski},
keywords = {Virtual reality, Education, Ethics, Realism, Autonomy, Privacy},
abstract = {In response to the high demand for digital learning as a surrogate for physical experiences, virtual reality (VR) is positioning itself as a tool for creating educational virtual experiences. VR technology faces a number of ethical issues, including a reduction of users’ autonomy, health problems, and privacy concerns. The use of VR and realism in education can turn out to be a double-edged sword. While realistic visualizations can promote learning for some content domains, they can hinder comprehension in others. Furthermore, the effects of realism on learning also depend on learners’ spatial abilities. Letting young children and teenagers engage in virtual educational experiences can expose them to manipulation, could lead to health issues, and may infringe on their privacy. In short, realism and virtual experiences may severely limit learners’ autonomy in a number of ways. Based on a review of the literature and considerations of emerging technologies such as generative artificial intelligence, this paper presents guidelines for the ethically sound utilization of VR and realism. By applying findings and conclusions established in the context of research on the ethics of VR to the educational utilization of this technology, I develop several suggestions that may help to avoid negative consequences of educational VR. These suggestions include the utilization of spatial ability testing, requiring virtual experiences to offer alternative paths to prevent manipulation, as well as using algorithms that deidentify the highly detailed developmental profiles that can be generated through educational VR use.}
}
@article{OH2024108098,
title = {Domain transformation learning for MR image reconstruction from dual domain input},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {108098},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108098},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524001823},
author = {Changheun Oh and Jun-Young Chung and Yeji Han},
keywords = {Deep learning, End to end, Magnetic resonance, Image reconstruction, Parallel imaging, Recurrent neural network},
abstract = {Medical images are acquired through diverse imaging systems, with each system employing specific image reconstruction techniques to transform sensor data into images. In MRI, sensor data (i.e., k-space data) is encoded in the frequency domain, and fully sampled k-space data is transformed into an image using the inverse Fourier Transform. However, in efforts to reduce acquisition time, k-space is often subsampled, necessitating a sophisticated image reconstruction method beyond a simple transform. The proposed approach addresses this challenge by training a model to learn domain transform, generating the final image directly from undersampled k-space input. Significantly, to improve the stability of reconstruction from randomly subsampled k-space data, folded images are incorporated as supplementary inputs in the dual-input ETER-net. Moreover, modifications are made to the formation of inputs for the bi-RNN stages to accommodate non-fixed k-space trajectories. Experimental validation, encompassing both regular and irregular sampling trajectories, validates the method's effectiveness. The results demonstrated superior performance, measured by PSNR, SSIM, and VIF, across acceleration factors of 4 and 8. In summary, the dual-input ETER-net emerges as an effective both regular and irregular sampling trajectories, and accommodating diverse acceleration factors.}
}
@article{HOWARD2021106808,
title = {A meta-analysis of virtual reality training programs},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106808},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106808},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100131X},
author = {Matt C. Howard and Melissa B. Gutworth and Rick R. Jacobs},
keywords = {Virtual reality, Head mounted display, Computer-based training, Training and development, Learning, Meta-analysis},
abstract = {Virtual reality (VR) is the three-dimensional digital representation of a real or imagined space with interactive capabilities. The application of VR for organizational training purposes has been surrounded by much fanfare; however, mixed results have been provided for the effectiveness of VR training programs, and the attributes of effective VR training programs are still unknown. To address these issues, we perform a meta-analysis of controlled experimental studies that tests the effectiveness of VR training programs. We obtain an estimate of the overall effectiveness of VR training programs, and we identify features of VR training programs that systematically produce improved results. Our meta-analytic findings support that VR training programs produce better outcomes than tested alternatives. The results also show that few moderating effects were significant. The applied display hardware, input hardware, and inclusion of game attributes had non-significant moderating effects; however, task-technology fit and aspects of the research design did influence results. We suggest that task-technology fit theory is an essential paradigm for understanding VR training programs, and no set of VR technologies is “best” across all contexts. Future research should continue studying all types of VR training programs, and authors should more strongly integrate research and theory on employee training and development.}
}
@article{SOOD2023706,
title = {Classification and Pathologic Diagnosis of Gliomas in MR Brain Images},
journal = {Procedia Computer Science},
volume = {218},
pages = {706-717},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000510},
author = {Meenakshi Sood and Shruti Jain and Jyotsna Dogra},
keywords = {Glioma, Accuracy, High Grade Glioma, Low Grade Glioma, MRI, Graph-Cut},
abstract = {Detection and classification of a brain tumour in medical images is always a challenging task, as the treatment may lead to Radiosurgery depending upon the exact shape, size, and position of a tumour. To provide a complete characterization of glioma and the degree of malignancy, classification and grading are vital. In some complicated cases, localization of the tumour, comparison of tumour tissues with adjacent regions, to make it clear for detection, and finally classification without human intervention is need of the hour. Out of the available numerous imaging techniques to detect and classify brain tumours, MRI is the most suitable non-invasive technique and has superior image quality. The edge over other techniques is its superior soft-tissue resolution and the ability to acquire different images employing contrast-enhanced agents. This research work classifies Low-Grade Glioma (LGG) and High-Grade Glioma(HGG) tumours on the basis of features obtained from extracted tumor region. The results of the classification of HGG and LGG tumours using the most prominent features are validated by five-fold cross-validation. Satisfactory and encouraging classification results are obtained using large and publicly available clinical datasets. Using the extracted features, a computer-assisted algorithm is developed that uses a machine-learning algorithm along with Gradient-Based Kernel Selection Graph Cut to provide binary classification with an accuracy of 94.6% with T1ce sequence of MRI. The proposed models can be employed to assist physicians and radiologists in the early pathological detection and classification of gliomas. The proposed framework also can be a model for validating brain tumours and their initial screening for their grading classification.}
}
@article{HOAR2021106375,
title = {Combined Transfer Learning and Test-Time Augmentation Improves Convolutional Neural Network-Based Semantic Segmentation of Prostate Cancer from Multi-Parametric MR Images},
journal = {Computer Methods and Programs in Biomedicine},
volume = {210},
pages = {106375},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106375},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721004491},
author = {David Hoar and Peter Q. Lee and Alessandro Guida and Steven Patterson and Chris V. Bowen and Jennifer Merrimen and Cheng Wang and Ricardo Rendon and Steven D. Beyea and Sharon E. Clarke},
keywords = {Convolutional neural network, Prostate cancer, Segmentation, MRI, Computer aided diagnosis, Machine learning},
abstract = {Purpose
Multiparametric MRI (mp-MRI) is a widely used tool for diagnosing and staging prostate cancer. The purpose of this study was to evaluate whether transfer learning, unsupervised pre-training and test-time augmentation significantly improved the performance of a convolutional neural network (CNN) for pixel-by-pixel prediction of cancer vs. non-cancer using mp-MRI datasets.
Methods
154 subjects undergoing mp-MRI were prospectively recruited, 16 of whom subsequently underwent radical prostatectomy. Logistic regression, random forest and CNN models were trained on mp-MRI data using histopathology as the gold standard. Transfer learning, unsupervised pre-training and test-time augmentation were used to boost CNN performance. Models were evaluated using Dice score and area under the receiver operating curve (AUROC) with leave-one-subject-out cross validation. Permutation feature importance testing was performed to evaluate the relative value of each MR contrast to CNN model performance. Statistical significance (p<0.05) was determined using the paired Wilcoxon signed rank test with Benjamini-Hochberg correction for multiple comparisons.
Results
Baseline CNN outperformed logistic regression and random forest models. Transfer learning and unsupervised pre-training did not significantly improve CNN performance over baseline; however, test-time augmentation resulted in significantly higher Dice scores over both baseline CNN and CNN plus either of transfer learning or unsupervised pre-training. The best performing model was CNN with transfer learning and test-time augmentation (Dice score of 0.59 and AUROC of 0.93). The most important contrast was apparent diffusion coefficient (ADC), followed by Ktrans and T2, although each contributed significantly to classifier performance.
Conclusions
The addition of transfer learning and test-time augmentation resulted in significant improvement in CNN segmentation performance in a small set of prostate cancer mp-MRI data. Results suggest that these techniques may be more broadly useful for the optimization of deep learning algorithms applied to the problem of semantic segmentation in biomedical image datasets. However, further work is needed to improve the generalizability of the specific model presented herein.}
}
@article{MATSAS2018168,
title = {Prototyping proactive and adaptive techniques for human-robot collaboration in manufacturing using virtual reality},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {50},
pages = {168-180},
year = {2018},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736584516303611},
author = {Elias Matsas and George-Christopher Vosniakos and Dimitris Batras},
keywords = {Human-robot collaboration, Virtual environments, Human-robot interaction, Safety, Proactive techniques, Adaptive trajectory, Collision Avoidance},
abstract = {Human-Robot Interaction (HRI) has emerged in recent years as a need for common collaborative execution of manufacturing tasks. This work examines two types of techniques of safe collaboration that do not interrupt the flow of collaboration as far as possible, namely proactive and adaptive. The former are materialised using audio and visual cognitive aids, which the user receives as dynamic stimuli in real time during collaboration, and are aimed at information enrichment of the latter. Adaptive techniques investigated refer to the robot; according to the first one of them the robot decelerates when a forthcoming contact with the user is traced, whilst according to the second one the robot retracts and moves to the final destination via a modified, safe trajectory, so as to avoid the human. The effectiveness as well as the activation criteria of the above techniques are investigated in order to avoid possible pointless or premature activation. Such investigation was implemented in a prototype highly interactive and immersive Virtual Environment (VE), in the framework of H-R collaborative hand lay-up process of carbon fabric in an industrial workcell. User tests were conducted, in which both subjective metrics of user satisfaction and performance metrics of the collaboration (task completion duration, robot mean velocity, number of detected human-robot collisions etc.) After statistical processing, results do verify the effectiveness of safe collaboration techniques as well as their acceptability by the user, showing that collaboration performance is affected to a different extent.}
}
@article{KHATIB2021102030,
title = {Human-robot contactless collaboration with mixed reality interface},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102030},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102030},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302416},
author = {Maram Khatib and Khaled {Al Khudir} and Alessandro {De Luca}},
keywords = {Control system, Robotics, Human-robot collaboration, Redundancy control, Collision avoidance, Mixed reality},
abstract = {A control system based on multiple sensors is proposed for the safe collaboration of a robot with a human. New constrained and contactless human-robot coordinated motion tasks are defined to control the robot end-effector so as to maintain a desired relative position to the human head while pointing at it. Simultaneously, the robot avoids any collision with the operator and with nearby static or dynamic obstacles, based on distance computations performed in the depth space of a RGB-D sensor. The various tasks are organized with priorities and executed under hard joint bounds using the Saturation in the Null Space (SNS) algorithm. A direct human-robot communication is integrated within a mixed reality interface using a stereo camera and an augmented reality system. The proposed system is significant for on-line, collaborative quality assessment phases in a manufacturing process. Various experimental validation scenarios using a 7-dof KUKA LWR4 robot are presented.}
}
@article{WANG2023102078,
title = {User-centric immersive virtual reality development framework for data visualization and decision-making in infrastructure remote inspections},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102078},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102078},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002069},
author = {Zhong Wang and Yulun Wu and Vicente A. González and Yang Zou and Enrique {del Rey Castillo} and Mehrdad Arashpour and Guillermo Cabrera-Guerrero},
keywords = {Immersive virtual reality, Remote infrastructure inspection, Usability, Agile user experience design, Human-computer interaction},
abstract = {Data visualization and decision-making are critical aspects of infrastructure inspections. Numerous research efforts have been made to develop practical remote inspection systems that overcome the limitations of conventional inspection methods. Although the integration of Immersive Virtual Reality (IVR) into remote inspection is currently being addressed, most IVR frameworks have focused on data accuracy and there has been a lack of research on developing human-centered IVR frameworks through usability evaluation. This paper addresses this limitation by proposing a new IVR development framework utilizing agile User Experience (UX) design techniques, which can be implemented in remote infrastructure inspections for data visualization and decision-making. Usability tests were conducted through a case study with 22 participants, using an IVR prototype developed in accordance with this framework. Following its implementation, the framework demonstrated its ability to identify damage via remote bridge inspection and achieved acceptable usability. Future research can build on the framework for semantic enrichment tasks.}
}
@article{BINGCHAN2018221,
title = {Maintenance and Management of Marine Communication and Navigation Equipment Based on Virtual Reality},
journal = {Procedia Computer Science},
volume = {139},
pages = {221-226},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.254},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319239},
author = {Li Bingchan and Bo Mao and Jie Cao},
keywords = {Marine Communication, Navigation, Maintenance, Management, Virtual Reality},
abstract = {According to the present situation and existing problems of marine communication and navigation equipment maintenance and management, this paper expounds the virtual reality technology in equipment maintenance and management. We design and implement a VR based maintenance and management system for marine communication and navigation equipment. The system consists of four parts: VR editing module, VR training module, user behavior analysis module, VR intelligent deduction module and user management module function module. The design and hardware structure of the virtual system is given, and the management task modules on this platform are designed, finally realized the function simulation. Functional verification proved the system can effectively improve the equipment management level of electro-technical officer (ETO).}
}
@article{PEREIRA2020103584,
title = {Application of AR and VR in hand rehabilitation: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {111},
pages = {103584},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103584},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302136},
author = {Margarida F. Pereira and Cosima Prahm and Jonas Kolbenschlag and Eva Oliveira and Nuno F. Rodrigues},
keywords = {Rehabilitation, Video games, Training systems, Augmented reality, Virtual reality},
abstract = {Background
The human hand is the part of the body most frequently injured in work related accidents, accounting for a third of all accidents at work and often involving surgery and long periods of rehabilitation. Several applications of Augmented Reality (AR) and Virtual Reality (VR) have been used to improve the rehabilitation process. However, there is no sound evidence about the effectiveness of such applications nor the main drivers of therapeutic success.
Objectives
The objective of this study was to review the efficacy of AR and VR interventions for hand rehabilitation.
Methods
A systematic search of publications was conducted in October 2019 in IEEE Xplore, Web of Science, Cochrane library, and PubMed databases. Search terms were: (1) video game or videogame, (2) hand, (3) rehabilitation or therapy and (4) VR or AR. Articles were included if (1) were written in English, (2) were about VR or AR applications, (3) were for hand rehabilitation, (4) the intervention had tests on at least ten patients with injuries or diseases which affected hand function and (5) the intervention had baseline or intergroup comparisons (AR or VR intervention group versus conventional physical therapy group). PRISMA protocol guidelines were followed to filter and assess the articles.
Results
From the eight selected works, six showed improvements in the intervention group, and two no statistical differences between groups. We were able to identify motivators of patients’ adherence, namely real-time feedback to the patients, challenge, and increased individualized difficulty. Automated tracking, easy integration in the home setting and the recording of accurate metrics may increase the scalability and facilitate healthcare professionals’ assessments.
Conclusions
This systematic review provided advantages and drivers for the success of AR/VR application for hand rehabilitation. The available evidence suggests that patients can benefit from the use of AR or VR interventions for hand rehabilitation.}
}
@article{PARVEAU2018263,
title = {3iVClass: a new classification method for Virtual, Augmented and Mixed Realities},
journal = {Procedia Computer Science},
volume = {141},
pages = {263-270},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.180},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318301},
author = {Marc Parveau and Mehdi Adda},
keywords = {MR, Mixed Reality, Augmented Reality, AR, VR, Definition, SLAM, 3iVClass},
abstract = {For several years, augmented and virtual reality technologies have attracted increasing interest in all areas. In the midst of this universe, the concept, already well known, of mixed reality has established itself as a distinct paradigm. However, and contrary to augmented and virtual realities, there is not a clear and one definition of what it is exactly and why it is different from the other concepts. In this article, we attempt to provide a new classification method to standardize the definition of virtual, augmented and mixed realities. First, a quick overview of existing taxonomies is made, then we present our classification which is based on three criteria we called 3iVClass (Immersion, Interaction, Information). Finally, in order to verify its reliability, we used this classification to propose a definition of mixed reality.}
}
@article{HENRIKSEN2023100019,
title = {Virtual reality and embodied learning for improving letter-sound knowledge and attentional control in preschool children: A study protocol},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100019},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000132},
author = {Anne Husted Henriksen and Marta Katarzyna Topor and Rasmus Ahmt Hansen and Linn Damsgaard and Anne-Mette {Veber Nielsen} and Andreas Wulff-Abramsson and Jacob Wienecke},
keywords = {VR, Education, Literacy, Embodied learning, Movement},
abstract = {Previous studies have shown that many children struggle with the acquisition of literacy skills and that these problems can be observed since the earliest stages of literacy learning. Embodied learning has been highlighted as a way to enhance the acquisition of early literacy skills. In addition, the use of technologies has been suggested as way of improving attentional control and motivation, which ultimately can improve learning outcomes. However, the combination of Virtual Reality (VR) and embodied learning in preschool children is yet to be explored wherefore the idea to the VR PLAYMORE study emerged. The project is designed as a three-armed randomized controlled trial with 6-7-year-old children in the Copenhagen area of Denmark. Children will be allocated to either a: 1) VR group, 2) mirror group or 3) control group throughout a 2-week intervention period. The VR group and mirror group will perform activities designed with accordance to the embodied learning theory. However, the VR group will perform the activities while wearing VR headsets whereas the mirror group will perform the activities in front of a whole-body mirror without VR headsets. The control group will continue regular teaching activities without the research group interruption. This study protocol follows the SPIRIT guidelines. Outcome measurements will include testing of literacy skills and attentional control. The study will add new knowledge to the research field of embodied learning and the use of VR technology in a school setting with focus on reading- and spelling-related skills and attentional control since this combination is yet to be explored.}
}
@article{KERTHYAYANAMANUABA2021289,
title = {Mobile based Augmented Reality Application Prototype for Remote Collaboration Scenario Using ARCore Cloud Anchor},
journal = {Procedia Computer Science},
volume = {179},
pages = {289-296},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000090},
author = {Ida Bagus {Kerthyayana Manuaba}},
keywords = {Augmented Reality, Mobile App, Remote Collaboration, ARCore, Cloud Anchor},
abstract = {This paper describes a development study for mobile based Augmented Reality (AR) application prototype for simple remote collaboration scenario. The AR technology is implemented by using ARCore with cloud anchor into two different mobile devices as the host and client. A host is a user device that able to create and share a session of a camera viewpoint that could be connected with another client device remotely through the internet. For testing purposes of remote collaboration scenario in this application prototype, both users are able to add and manipulate location of virtual objects that overlay on each screen from their own device. Based on the functionality test, before placing the virtual object on the screen, a calibration of cloud anchor is required with minimum success performance in 250 to 1200 lumens of lights. The result for cloud anchor is being hosted with average delay up to 700+ milliseconds. In this paper, the result of the experiment shows an early stage of the utilization of AR technology in simple remote collaboration scenario. However, it shows the potential of mobile based AR technology for future remote collaboration scenario development.}
}
@article{KARNCHANAPAYAP2023107796,
title = {Activities-based virtual reality experience for better audience engagement},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107796},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107796},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001474},
author = {Gomesh Karnchanapayap},
keywords = {Virtual reality, Virtual exhibition, Virtual activities, Audience engagement, Interaction design},
abstract = {The benefits of virtual reality have definitely expanded beyond the gaming industry to a variety of other businesses. From virtual conferences to virtual exhibitions, the MICE (Meetings, Incentives, Conferences, and Exhibitions) industry has several applications for virtual reality. Researchers and experts should pay more attention to the virtual reality applications in this market segment. This study's objectives were to 1) create a virtual reality experience for Thailand's Ministry of Digital Economy and Society to serve as an international exhibition showcase by incorporating virtual activities, 2) evaluate the efficiency of the virtual reality experience based on the activities and audience engagement, and 3) investigate the virtual reality experience's audience satisfaction. This virtual immersive experience is the outcome of using Tilt Brush to build a virtual spatial playground centered on three topics: the rural broadband internet project, the cybersecurity act 2019 & the personal data protection act 2019, and awareness of fake news. The virtual reality experience was on show at Digital Thailand Big bang 2019, which was held at BITEC International Exhibition in Bangkok, Thailand from October 28 to October 31, 2019. The investigation utilized mixed-method research, consisting of a quantitative technique based on observation and a qualitative technique based on questionnaires. A convenience sample of 126 cases was obtained from viewers aged seven and up who participated in the virtual reality experience. The virtual experience required the audience to engage in four sorts of physical activity: looking, walking, performing a specific physical action, and locating an object. Observation forms, questionnaires, and examinations were utilized to collect data. The findings indicate that the virtual reality experience, while entertaining the audience, encourages learning through physical activities. These activities contribute to audience engagement and, ultimately, comprehension of the subject. The majority of the audience was able to accurately answer test questions, indicating that not only did they engage the presentation through active participation, but they also retained its content. This project is a case study of how virtual reality can be used to increase audience participation during an exhibition.}
}
@article{HINCAPIE2021107281,
title = {Augmented reality mobile apps for cultural heritage reactivation},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107281},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107281},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002639},
author = {Mauricio Hincapié and Christian Díaz and Maria-Isabel Zapata-Cárdenas and Henry de Jesus Toro Rios and Dalia Valencia and David Güemes-Castorena},
keywords = {Educational innovation, Cultural heritage, Augmented reality, GPS-based applications, Mobile Apps, Higher education},
abstract = {The use of augmented reality and GPS-guided applications has proven successful for reviving cultural heritage. However, the impact of using these technologies on the learning process of cultural heritage has not been studied in depth. This article reports the findings of how using GPS and augmented reality with mobile apps impacted learning during a project to reactivate cultural heritage around Medellin's Cisnero Square and its surroundings (also called the "Park of Lights"). An experimental test comparing two groups was performed. The experimental design goal was to determine the differences in learning and user perception when the proposed Mobile App was and was not used in guiding the cultural heritage tour as part of Cisnero Square's reactivation. The results showed that the experimental group achieved better learning and cultural heritage learning outcomes. This study offers valuable insights into technological applications for reactivating cultural heritage, which can be applied almost anywhere.}
}
@article{ALHEJRI20221,
title = {Reconstructing real object appearance with virtual materials using mobile augmented reality},
journal = {Computers & Graphics},
volume = {108},
pages = {1-10},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322001480},
author = {Aisha Alhejri and Naizheng Bian and Entesar Alyafeai and Mousa Alsharabi},
keywords = {Augmented reality, Deep learning, Depth map, Light estimation, RGB-D images, Materials design},
abstract = {In augmented reality (AR) applications, detecting and tracking real-world objects remains a challenge. Another challenge is making the loaded virtual objects truly reflect the lighting and shadow conditions of the actual environment in which they are located. In this paper, we proposed an improved method for real-time light estimation of environmental lighting in AR applications, tracking the movement of physical objects, and making the corresponding virtual objects reflect the lighting and shadows of the actual environment. First, we proposed an effective method for estimating light in an AR scene by training a deep neural network once on different scenes from the input of RGB-D images to determine the direction of the brightest light. Second, we experimented with advanced methods of detecting and tracking different types of real objects (flat surfaces, simple geometries, and complex geometries) for visualizing virtual materials on the top of these surfaces, where virtual materials will acquire the conditions of the real place in terms of lighting, reflection, and shadows depending on our proposed method for light estimation. Our method of light estimation was tested via experiments on the three types of real objects mentioned above. Results indicate that our method achieves higher resolution and lower error rate compared with other work. Our methods have achieved very good results in terms of detection accuracy, tracking accuracy, realistic visualization of materials, matching real shade, and lighting conditions.}
}
@article{LI2021101905,
title = {Towards quantitative and intuitive percutaneous tumor puncture via augmented virtual reality},
journal = {Computerized Medical Imaging and Graphics},
volume = {90},
pages = {101905},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101905},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121000549},
author = {Ruotong Li and Yuqi Tong and Tianpei Yang and Jianxi Guo and Weixin Si and Yanfang Zhang and Reinhard Klein and Pheng-Ann Heng},
keywords = {Augmented virtual reality, Optimal path planning, Collaborative holographic navigation, Respiratory tumor puncture},
abstract = {In recent years, the radiofrequency ablation (RFA) therapy has become a widely accepted minimal invasive treatment for liver tumor patients. However, it is challenging for doctors to precisely and efficiently perform the percutaneous tumor punctures under free-breathing conditions. This is because the traditional RFA is based on the 2D CT Image information, the missing spatial and dynamic information is dependent on surgeons’ experience. This paper presents a novel quantitative and intuitive surgical navigation modality for percutaneous respiratory tumor puncture via augmented virtual reality, which is to achieve the augmented visualization of the pre-operative virtual planning information precisely being overlaid on intra-operative surgical scenario. In the pre-operation stage, we first combine the signed distance field of feasible structures (like liver and tumor) where the puncture path can go through and unfeasible structures (like large vessels and ribs) where the needle is not allowed to go through to quantitatively generate the 3D feasible region for percutaneous puncture. Then we design three constraints according to the RFA specialists consensus to automatically determine the optimal puncture trajectory. In the intra-operative stage, we first propose a virtual-real alignment method to precisely superimpose the virtual information on surgical scenario. Then, a user-friendly collaborative holographic interface is designed for real-time 3D respiratory tumor puncture navigation, which can effectively assist surgeons fast and accurately locating the target step-by step. The validation of our system is performed on static abdominal phantom and in vivo beagle dogs with artificial lesion. Experimental results demonstrate that the accuracy of the proposed planning strategy is better than the manual planning sketched by experienced doctors. Besides, the proposed holographic navigation modality can effectively reduce the needle adjustment for precise puncture as well. Our system shows its clinical feasibility to provide the quantitative planning of optimal needle path and intuitive in situ holographic navigation for percutaneous tumor ablation without surgeons’ experience-dependence and reduce the times of needle adjustment. The proposed augmented virtual reality navigation system can effectively improve the precision and reliability in percutaneous tumor ablation and has the potential to be used for other surgical navigation tasks.}
}
@article{MA2022106279,
title = {A dual-branch hybrid dilated CNN model for the AI-assisted segmentation of meningiomas in MR images},
journal = {Computers in Biology and Medicine},
volume = {151},
pages = {106279},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106279},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522009878},
author = {Xin Ma and Yajing Zhao and Yiping Lu and Peng Li and Xuanxuan Li and Nan Mei and Jiajun Wang and Daoying Geng and Lingxiao Zhao and Bo Yin},
keywords = {Meningioma, Image segmentation, Convolutional neural networks, Multi-scale receptive fields},
abstract = {Background and objective:
Treatment for meningiomas usually includes surgical removal, radiation therapy, and chemotherapy. Accurate segmentation of tumors significantly facilitates complete surgical resection and precise radiotherapy, thereby improving patient survival. In this paper, a deep learning model is constructed for magnetic resonance T1-weighted Contrast Enhancement (T1CE) images to develop an automatic processing scheme for accurate tumor segmentation.
Methods:
In this paper, a novel Convolutional Neural Network (CNN) model is proposed for the accurate meningioma segmentation in MR images. It can extract fused features in multi-scale receptive fields of the same feature map based on MR image characteristics of meningiomas. The attention mechanism is added as a helpful addition to the model to optimize the feature information transmission.
Results and conclusions:
The results were evaluated on two internal testing sets and one external testing set. Mean Dice Similarity Coefficient (DSC) values of 0.886, 0.851, and 0.874 are demonstrated, respectively. In this paper, a deep learning approach is proposed to segment tumors in T1CE images. Multi-center testing sets validated the effectiveness and generalization of the method. The proposed model demonstrates state-of-the-art tumor segmentation performance.}
}
@article{KOUROUTHANASSIS201571,
title = {Tourists responses to mobile augmented reality travel guides: The role of emotions on adoption behavior},
journal = {Pervasive and Mobile Computing},
volume = {18},
pages = {71-87},
year = {2015},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2014.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1574119214001527},
author = {Panos Kourouthanassis and Costas Boletsis and Cleopatra Bardaki and Dimitra Chasanidou},
keywords = {Mobile augmented reality, Tourist guide, Personalization, Adoption study, Emotional design},
abstract = {This research presents a mobile augmented reality (MAR) travel guide, named CorfuAR, which supports personalized recommendations. We report the development process and devise a theoretical model that explores the adoption of MAR applications through their emotional impact. A field study on Corfu visitors (n=105) shows that the functional properties of CorfuAR evoke feelings of pleasure and arousal, which, in turn, influence the behavioral intention of using it. This is the first study that empirically validates the relation between functional system properties, user emotions, and adoption behavior. The paper discusses also the theoretical and managerial implications of our study.}
}
@article{BOFFI2023100014,
title = {An educational experience in ancient Rome to evaluate the impact of virtual reality on human learning processes},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100014},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000089},
author = {Paolo Boffi and Monica Clerici and Alberto Gallace and Pier Luca Lanzi},
keywords = {Virtual reality, Media in education, Adult learning, Human-computer interaction},
abstract = {Immersive Virtual Reality technology has recently gained significant attention and is expanding its applications to various fields. It also has many advantages in education, as it allows to both simplify the explanation of complex topics through their visualization, and explore lost or unreachable environments. To evaluate the impact of immersive experiences on learning outcomes we developed an educational experience that lets users visit an ancient Roman Domus and provides information about daily life in Roman times. We designed a between-subjects data collection to investigate learning ratio, user experience, and cybersickness of participants through anonymous questionnaires. We collected 76 responses of participants (18–35 y.o.) divided into three conditions: a Immersive Virtual Reality experience, a slide-based lecture and a 2D desktop-based experience. Our results show that the virtual reality experience is considered more engaging and as effective as more traditional 2D and slide-based experiences in terms of learning.}
}
@article{QIU2019597,
title = {Digital assembly technology based on augmented reality and digital twins: a review},
journal = {Virtual Reality & Intelligent Hardware},
volume = {1},
number = {6},
pages = {597-610},
year = {2019},
note = {VR/AR in industry},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579619300841},
author = {Chan Qiu and Shien Zhou and Zhenyu Liu and Qi Gao and Jianrong Tan},
keywords = {Augmented reality, Digital twin, Digital assembly, Virtual simulation, Assembly analysis},
abstract = {Product assembly simulation is considered as one of the key technologies in the process of complex product design and manufacturing. Virtual assembly realizes the assembly process design, verification, and optimization of complex products in the virtual environment, which plays an active and effective role in improving the assembly quality and efficiency of complex products. In recent years, augmented reality (AR) and digital twin (DT) technology have brought new opportunities and challenges to the digital assembly of complex products owing to their characteristics of virtual reality fusion and interactive control. This paper expounds the concept and connotation of AR, enumerates a typical AR assembly system structure, analyzes the key technologies and applications of AR in digital assembly, and notes that DT technology is the future development trend of intelligent assembly research.}
}
@article{XU20201,
title = {Co-axial depth sensor with an extended depth range for AR/VR applications},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {1},
pages = {1-11},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300012},
author = {Mohan Xu and Hong Hua},
keywords = {Depth map sensor, 3D camera, Controlled aberration},
abstract = {Background
Depth sensor is an essential element in virtual and augmented reality devices to digitalize users' environment in real time. The current popular technologies include the stereo, structured light, and Time-of-Flight (ToF). The stereo and structured light method require a baseline separation between multiple sensors for depth sensing, and both suffer from a limited measurement range. The ToF depth sensors have the largest depth range but the lowest depth map resolution. To overcome these problems, we propose a co-axial depth map sensor which is potentially more compact and cost-effective than conventional structured light depth cameras. Meanwhile, it can extend the depth range while maintaining a high depth map resolution. Also, it provides a high-resolution 2D image along with the 3D depth map.
Methods
This depth sensor is constructed with a projection path and an imaging path. Those two paths are combined by a beamsplitter for a co-axial design. In the projection path, a cylindrical lens is inserted to add extra power in one direction which creates an astigmatic pattern. For depth measurement, the astigmatic pattern is projected onto the test scene, and then the depth information can be calculated from the contrast change of the reflected pattern image in two orthogonal directions. To extend the depth measurement range, we use an electronically focus tunable lens at the system stop and tune the power to implement an extended depth range without compromising depth resolution.
Results
In the depth measurement simulation, we project a resolution target onto a white screen which is moving along the optical axis and then tune the focus tunable lens power for three depth measurement subranges, namely, near, middle and far. In each sub-range, as the test screen moves away from the depth sensor, the horizontal contrast keeps increasing while the vertical contrast keeps decreasing in the reflected image. Therefore, the depth information can be obtained by computing the contrast ratio between features in orthogonal directions.
Conclusions
The proposed depth map sensor could implement depth measurement for an extended depth range with a co-axial design.}
}