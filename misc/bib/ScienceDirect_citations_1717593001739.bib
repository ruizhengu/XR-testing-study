@article{ANDERIES2023582,
title = {The Application of Augmented Reality to Generate Realistic Interaction in the Property Sector},
journal = {Procedia Computer Science},
volume = {227},
pages = {582-590},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.561},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017283},
author = { Anderies and Rendy Adidarma and Maximillian Lemuel Chanyassen and Alexander Imanuel and Andry Chowanda},
keywords = {Augmented Reality, Property Industry, Vuforia Ground Plane, 3D Model, Android, Performance Testing},
abstract = {Augmented Reality is a technology that overlays computer-generated information and images in a natural environment to enhance or augment the contextual perception of the user's environment. The utilization of AR is currently one of the most important in developing several industrial sectors, such as architecture, engineering, and construction development. Nowadays, the property sector industry has always been an issue of sale for users that do not have direct access to the property. The pandemic and the economic problem in different countries have also reduced consumer interest in purchasing houses. Due to these issues, this paper aims to provide a platform for addressing these pain areas. The software can apply Augmented Reality to real estate in real-time using a mobile device. Not only can they see the outside of the property, but they can also see the interior design, furniture design, and property mapping in 3D. Our test results show that an application is needed to provide the property sector with high demands. Over 80% of our respondents confirm that our application's presence will help ease the sales problem. Because it can save the customer time, viewing property frequency, and cost saving. These increased demands, we are developing an AR-based application for the market to use and gain more traction in the property sector.}
}
@article{HU2021101436,
title = {Educational impact of an Augmented Reality (AR) application for teaching structural systems to non-engineering students},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101436},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101436},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001889},
author = {Xinping Hu and Yang Miang Goh and Alexander Lin},
keywords = {Augmented reality, Educational technology, Mobile application, Structural engineering},
abstract = {This study evaluates the effectiveness of Augmented Reality (AR) in improving undergraduate non-engineering students’ (n = 103) achievement of learning outcomes and their perception of AR as a learning tool. An AR application for teaching structural systems, Virtual and Augmented Reality for Structures (VARS), is developed and tested using a quasi-experiment and a questionnaire. The study shows that VARS AR significantly improves the posttest quiz score of the experimental group. In addition, a conceptual framework linking design considerations and achievement of learning outcomes is created. Based on the questionnaire survey, authentic context and user interface are important design considerations influencing students’ perceived achievement of learning outcomes. It was found that experience using AR and background knowledge did not influence the perceived achievement of learning outcomes and change in quiz score.}
}
@article{DEVI2023104177,
title = {Effect of situational and instrumental distortions on the classification of brain MR images},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104177},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104177},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006310},
author = {Swagatika Devi and Sambit Bakshi and Manmath Narayan Sahoo},
keywords = {Computer aided diagnosis, Convolutional neural network, Data augmentation, Out-of-focus blur, Low resolution images, Motion blur, MR image classification},
abstract = {Magnetic Resonance (MR) images of the brain play key role in exploiting pathological changes and non-invasive investigation of many neuro-degenerative diseases. Computer Aided Diagnosis (CAD) systems assist radiologists in interpreting MR images and classifying them into “normal” and “abnormal” categories. However, reduced strength of the used magnet in the machine or involuntary motions of the patients may lead to degraded MR images, which can negatively affect the performance of CAD system compromising the classification accuracy. This work aims at modeling these types of situations via out-of-focus blur, motion blur, effect of variation in resolution, and a combination of these on brain MR images for validating the impact of image quality on classification performance. To validate this, this article mathematically models the blurs (both individually and simultaneously) by varying the strength of image quality covariates and afterwards Deep Convolutional Neural Networks (DCNN) are employed to train and classify the distorted brain MR images. Besides, a single DCNN is experimented with a good mix of image quality and characteristics to test the reliability of the model for real-life scenario. The CNN models are validated through comprehensive evaluation on both original and degraded versions of brain MR images from two benchmark datasets DS-75 and DS-160 collected by Harvard Medical School as well as a self-collected dataset NITR-DHH. This study reveals that the models are able to classify distorted MR images and hence can be used for assisting the clinicians.}
}
@article{PAN2024100662,
title = {Sports game teaching and high precision sports training system based on virtual reality technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100662},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100662},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000302},
author = {Yang Pan},
keywords = {Posture recognition, Virtual reality, Digital filtering technology, Sports training, Teaching games},
abstract = {Sports can combine virtual reality technology with high-precision training to enhance the training effectiveness of users. In view of the current problems of insufficient physical quality and lack of exercise motivation faced by college students, this paper aims to explore a new type of college sports game teaching model through the integration of virtual reality technology. This is to improve students 'physical exercise efficiency and subjective initiative. This study constructed an experiential sports game model by applying posture recognition technology and fuzzy comprehensive evaluation method. The research indicated that the error accuracy with residual structure added will quickly decrease. When calculating the error, the overall error decreased by 6 mm. The proposed method can accurately distinguish human behavior in motion and trajectories in the motion area. The motion capture accuracy was high, and the deformation amount met the teaching needs of higher education institutions for sports game teaching. The contribution of this study is the significant improvement in the accuracy of motion capture, resulting in a total error reduction to 6 mm. This improvement ensured that the model can accurately distinguish between motion behavior and trajectory. The cumulative variance explanation rate of the scale reached 712.407 %, which was far more than that of the traditional model, and fully covered and explained the questionnaire information. Through the comparative test with four knowledge tracking models, the effectiveness and superiority of the sports game teaching mode integrated with virtual reality technology were verified.}
}
@article{FOKIDES2024100048,
title = {Development and testing of a model for explaining learning and learning-related factors in immersive virtual reality},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100048},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000429},
author = {Emmanuel Fokides and Panagiotis Antonopoulos},
keywords = {Immersive virtual reality, Learning, Model},
abstract = {Many believe that immersive virtual reality (IVR) possesses transformative potential for a plethora of human activities that are mediated via technology, including education. In light of this, it is critically important to understand the determinants that influence learning in IVR environments and the interrelations among these determinants. For that matter, a model was developed that encapsulated ten potential factors influencing learning outcomes. Three hundred and thirty-four university students interacted with a purpose-built application that presented ancient Greek inventions through the use of head-mounted displays. Data analysis, adhering to a structural equation modeling approach, indicated that a multitude of factors exerted a positive influence on learning outcomes. These included the perceived quality of graphics, the perceived quality of feedback and content, and the perceived degree of interaction. Moreover, intrinsic motivation and the immersive experience that IVR provides also demonstrated a positive impact. Conversely, the perceived cognitive load and symptoms of simulator sickness manifested a negative impact. Interestingly, these factors did not appear to inhibit learners' motivation or their positive feelings. Age did not have any effect, while gender seemed to have an impact only on immersion. The implications of the results are also discussed.}
}
@article{MOTAMEDI2017248,
title = {Signage visibility analysis and optimization system using BIM-enabled virtual reality (VR) environments},
journal = {Advanced Engineering Informatics},
volume = {32},
pages = {248-262},
year = {2017},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301203},
author = {Ali Motamedi and Zhe Wang and Nobuyoshi Yabuki and Tomohiro Fukuda and Takashi Michikawa},
keywords = {Signage visibility, Signage system optimization, Virtual reality, Building Information Modeling (BIM)},
abstract = {The proper placement of signage greatly influences pathfinding and information provision in public spaces. Clear visibility, easy comprehension, and efficient placement are all important for successful signage. We propose a signage visibility analysis and optimization system, utilizing an updated Building Information Model (BIM) and a game engine software application to simulate the movement of pedestrians. BIM can provide an up-to-date digital representation of a building and its assets, while computer simulation environments have the potential to simulate the movement of pedestrians. Combining these two technologies provides an opportunity to create a tool that analyzes the efficiency of installed signage and visualizes them in VR environments. The proposed tool contains algorithms, functions and predefined scenarios to calculate the coverage and the visibility of a building’s signage system. This system assists building managers to analyze (visually or by using statistics) the visibility of signboards, to assess their proper placement, and to optimize their placement. The applicability of the method has been validated in case studies performed in subway stations in Japan.}
}
@article{FORTUNA2024104057,
title = {A comparative study of Augmented Reality rendering techniques for industrial assembly inspection},
journal = {Computers in Industry},
volume = {155},
pages = {104057},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523002075},
author = {Santina Fortuna and Loris Barbieri and Emanuele Marino and Fabio Bruno},
keywords = {Augmented Reality, Industrial assembly inspection, Quality inspection, AR rendering techniques, Industry 4.0},
abstract = {In the manufacturing industry, Augmented Reality (AR) has shown significant potential in enhancing operators’ capabilities while performing inspection and assembly activities. However, the augmented visualization of virtual models on physical components can present challenges and potential misunderstandings, as the visualization mode greatly influences the perception of components and the amount of information received. This study investigates the impact of rendering techniques on user performance during industrial assembly inspection tasks. In particular, a set of rendering techniques, suitable for industrial AR applications, have been selected and implemented through a dedicated AR tool. The selected AR rendering techniques have been compared, in terms of qualitative and quantitative metrics, in order to test their efficiency and effectiveness for industrial assembly inspection activities. 17 domain experts and 33 representative users have been involved in the experimental study which outcomes reveal that the rendering techniques play a significant role in assisting operators for identifying design discrepancies in industrial products. Furthermore, a correlation has been observed between the two AR rendering techniques best suited to support industrial inspection activities and the types of assembly errors detected.}
}
@article{ROMERO2022103678,
title = {A user-centric computer-aided verification process in a virtuality-reality continuum},
journal = {Computers in Industry},
volume = {140},
pages = {103678},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103678},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000756},
author = {Victor Romero and Romain Pinquié and Frédéric Noël},
keywords = {Model-based systems engineering, Validation, Verification, User-centred design, Virtual reality, Human-in-the-loop simulation, Immersion, Realism},
abstract = {Although companies systematically strive for a full digitalisation of their products and their processes, the design phase shows that the quality of models is very unequal. Indeed, detailed design benefits from much more sophisticated methods and tools than the specification and architecture activities. Although, we should note the recent paradigm shift from document-based to model-based systems engineering, these models, which are mainly static 2D diagrams, remain poor to facilitate design verification early on. Thus, to detect most errors during the design phase, companies have no other alternative than to wait up to the testing phase which occurs after several years of development for complex systems. Thus, we propose a user-centric computer-aided verification process to ensure that the design meets the requirements under realistic operational conditions. The verification process provides a progressive immersion into the virtual system before seamlessly transitioning to the real system. Our work is built upon state-of-the-art MBSE methods such as the Property Model Methodology, which enables systems engineers to co-simulate specification models and design models. We improve such MBSE methods by increasing the level of realism that experiences the end-user during the verification of a design by the original combination of Model-In-the-Loop, Immersive Model-In-the-Loop, Human-In-the-Loop, and Hardware-In-the-Loop simulation strategies. A robot arm is used as a use case to illustrate the verification process.}
}
@article{GUALTIERI2023765,
title = {A human-centered conceptual model for integrating Augmented Reality and Dynamic Digital Models to reduce occupational risks in industrial contexts},
journal = {Procedia Computer Science},
volume = {217},
pages = {765-773},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.273},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023511},
author = {Luca Gualtieri and Andrea Revolti and Patrick Dallasega},
keywords = {Augmented reality, AR, safety, training, risk management},
abstract = {The article proposes a human-centered conceptual model to integrate Augmented Reality (AR) and Dynamic Digital Models (DDM) to improve training and reduce occupational risks in industrial contexts. A general model integrating DDM and AR to support immersive training and customized and real-time risk management is missing in the literature. The proposed conceptual model was preliminarily validated with company experts in a laboratory environment. According to the expert's feedback, the system can improve the efficacy of training by means of an immersive environment where users can better perceive hazards and safety-critical situations. Considering the capability of the conceptual model to support real-time and customized risk management, the participating experts argue that the technology seems not to be ready yet, even if it is very interesting and it would be very useful in practice to mitigate occupational risks. Future research activities will consist of the development of a prototypical system based on the presented conceptual model by considering specific user requirements and experts’ feedback.}
}
@article{SIIVOLA2024100058,
title = {Advantages of virtual reality childbirth education},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100058},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000084},
author = {Marjaana Siivola and Teemu Leinonen and Lauri Malmi},
keywords = {Virtual reality, Childbirth education, Learning effectiveness},
abstract = {Many expectant parents do not prepare enough for childbirth, and not getting a tour of the birthing hospital is causing them unnecessary stress. We enhanced childbirth education with an online virtual reality program for the users to experience what it might be like to give birth in a hospital. In this paper, we report a study that included observational user testing with a virtual reality headset and autonomic testing with the device of the user's choice. Data was collected with a pre-questionnaire, observations from the user tests, a semi-structured interview with the expecting parents, a post-questionnaire, and a follow-up questionnaire. The program improved learning outcomes and offered realistic and concrete birthing examples. Usability was good with the virtual reality headsets, while other devices need more research.}
}
@article{PEREIRA202440,
title = {Points of interest in the city of Barcelos in Portugal through augmented reality},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {4},
pages = {40-48},
year = {2024},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667345223000457},
author = {Miguel Pereira and João Carlos Silva and Marisa Pinheiro and Sandro Carvalho and Gilberto Santos},
keywords = {Mobile applications, Augmented reality, Tourism, Geolocation},
abstract = {Barcelos is a historic city in Portugal with many tourist attractions, attracting more and more visitors who come to the city with the aim of exploring it. The main objective of this article is to boost tourism in the city of Barcelos, specifically highlighting tourist, historical and leisure spots, based on the development of a mobile application using augmented reality technologies and geolocation. This application intends to allow the users to know historical points of interest in Barcelos, as well as interact with a certain point. The results of this investigation were evaluated by testing the application by end users, with the aim of identifying whether the application meets their needs, in particular the promotion of tourist and historical points.}
}
@article{WULFF20241991,
title = {Use of augmented reality for iterative robot program optimisation in robot-automated series production processes},
journal = {Procedia Computer Science},
volume = {232},
pages = {1991-2000},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001984},
author = {Lukas Antonio Wulff and Ole Schmedemann and Thorsten Schüppstuhl},
keywords = {industrial robot programming, augmented reality, mixed reality, automotive, simulation, sealing application},
abstract = {The inspection, analysis, and modification of robot programs in series production processes is a challenging recurring task in the automotive industry. Especially the holistic analysis of detected defects and the subsequent derivation and application of corrective measures to the underlying robot program is a complex task requiring extensive knowledge in multiple fields. Existing scientific literature has demonstrated that the utilisation of Augmented Reality (AR) assisted robot programming systems (ARRPS) improves programming efficiency as well as intuitiveness when compared to traditional programming methods. However, the accuracy especially of mobile AR devices limits the applicability of AR in industrial applications. We propose to leverage the iterative nature of the optimisation process common in series production and utilise the visual capabilities of the human worker to define corrective measures based on the inspected real process result. This mitigates the impact of limited tracking accuracy as the optimisation is based on the visual perception of the user and thus decoupled from the accuracy of the employed AR device. We implemented our system based on the requirements of a sealing workstation of the Mercedes-Benz Group in Germany. A conducted experiment validates the functionality and indicates that the presented strategy of iterative AR assisted robot programming enables optimisations with a similar accuracy as the Teach-In programming as well as a significantly reduced programming time. This suggests that utilising an ARRPS can be highly beneficial in the inspection, analysis, and modification of sealing applications common in automotive series production processes. As the presented iterative AR assisted robot programming is neither limited to sealing nor to the automotive sector it can be adapted for various robot-automated applications like path-welding, gluing, or painting and can extend its utility to related sectors such as aviation or the marine industry.}
}
@article{DEUSDADO20231112,
title = {Virtual Reality Haptic Device for Mental Illness Treatment},
journal = {Procedia Computer Science},
volume = {219},
pages = {1112-1119},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.391},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923004003},
author = {Leonel D. Deusdado and Alexandre F.J. Antunes},
keywords = {Schizophrenia, Rehabilitation, Health care, Virtual reality, Haptic device, Serious gamee},
abstract = {Schizophrenia is a mental disorder that alters mental functioning and can cause hallucinations, mental disorientation, and a variety of other symptoms, which results in a separation of schizophrenics from society. Despite the fact that the condition has been known about for a long time, there is still an urgent need for research and testing to develop better therapies. Virtual reality (VR) has had interesting outcomes when used to treat illnesses, and it is gradually emerging as a viable technical choice for the healthcare sector due to its immersion, which offers better and more intense user experiences. Once linked to a serious game, it can introduce the user to a number of situations that, when combined with medical assistance, aid in their rehabilitation and treatment. The use of haptic devices enhances VR immersion by enabling users to comprehend virtual environments with greater nuance, which increases their level of believing in the new environment they are in. When used in conjunction with VR, it can improve the efficacy of the treatment, making its use viable for the treatment of psychic diseases. The goal of this research is to develop haptic vest interactions using a three-dimensional virtual testing scenario to support and improve the use of VR for rehabilitation and therapy for schizophrenia. The established functionalities can be applied to a further serious game with the same focus.}
}
@article{BAEK2024106085,
title = {Validation of tremor measurements using quantified drawing analysis in patients with essential tremor or Parkinson’s disease treated with MR-guided focused ultrasound thalamotomy},
journal = {Biomedical Signal Processing and Control},
volume = {92},
pages = {106085},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106085},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424001435},
author = {Hongchae Baek and Daniel Lockwood and Emmanuel Obusez and Matthew Poturalski and Jacqueline Chen and Sean J. Nagel and Stephen E. Jones},
keywords = {MR-guided focused ultrasound, Thalamotomy, Essential tremor, Parkinson’s disease, Kinetic tremor, Drawing analysis, Long-term follow up},
abstract = {Background
During MR–guided focused ultrasound (MRgFUS) thalamotomy, patients’ tremors are qualitatively assessed with drawing tests to provide near-real-time feedback on the treatment effect. The objective of this study was to validate an automated measure of kinetic tremor in patients with essential tremor (ET) or Parkinson’s disease (PD) by computerized analysis of standardized drawings.
Methods
Drawings were performed in three settings: (1) patients supine in the MR scanner during the procedure, (2) patients seated immediately before and after treatment, and (3) months after discharge. Drawings comprised of a signature, the Archimedes spiral, and a straight line. Once the drawings were digitized, the automated analysis was used to calculate quantitative metrics. For validation, the composite scores were compared with wrist angle simultaneously measured using accelerometers during the patients’ drawings.
Results
Quantitative drawing scores for the spiral and line showed significant improvement immediately after thalamotomy in patients with ET (spiral: p = 0.0002; line: p = 0.017) and PD (spiral: p = 0.12; line: p = 0.016); no significant change occurred in the months after treatment. In addition, quantitative drawing scores for spirals and lines were correlated with accelerometer-measured wrist angle in ET (spiral: R2 = 0.51, p = 0.029; line: R2 = 0.63, p = 0.004) and PD (spiral: R2 = 0.85, p = 0.0008; line: R2 = 0.69, p = 0.017) groups.
Conclusions
Quantitative drawing scores for patients with ET or PD accurately reflect the degree of tremor and can be used for objective assessment of tremor improvement during and after treatment.}
}
@article{LU2022101520,
title = {Development and validation of a confined space rescue training prototype based on an immersive virtual reality serious game},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101520},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101520},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002688},
author = {Song Lu and Fei Wang and Xin Li and Qichuan Shen},
keywords = {Confined space rescue, Safety training, IVR SG, E-lecture},
abstract = {Due to the inherently hazardous nature of the confined space environment and a lack of effective rescue training on on-site personnel attempting the rescue of fellow workers in a confined space, accidents or even multiple fatalities may happen. For this reason, improving the skill set of rescuers is of vital importance. Immersive virtual reality serious games (IVR SG) have been shown to be effective and are now widely used in safety training. However, there is little research on its application in confined space rescue training. To bridge this gap, we have developed a confined space rescue training system (called Rescue Success) based on IVR SG, and then tested its effectiveness in a real environment. We carried out a single-blind, randomized comparative experiment to study and compare the learning effectiveness of using IVR SG compared to an E-lecture in terms of behavioral skills, knowledge acquisition, self-efficacy, and simulation-induced sickness. Our results show that both training methods significantly improve the participant’s rescue behavioral skills, knowledge, and self-efficacy. Overall, IVR SG performed better than an E-lecture in all aspects except simulation-induced sickness, where there is no difference between the two methods.}
}
@article{PARAMARTHA2023874,
title = {Multimedia Application based on Virtual Reality to Introduce College Majors in Universities},
journal = {Procedia Computer Science},
volume = {227},
pages = {874-883},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.594},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017611},
author = {Damar Harip Paramartha and M.Afdhal Arief Malik and Selvi Dian Pertiwi and Reinert Yosua Rumagit},
keywords = {Virtual Reality, College Major, Universities},
abstract = {The goal of this project is to create virtual reality-based multimedia programs that can introduce users to fresh university experiences and information. Many prospective students are currently selecting the incorrect major, which could have negative effects on those prospective students. The Game Development Life Cycle, often known as the GDLC approach, is the development process method employed in this application. The GDLC process begins with initiation and progresses through pre-production, testing, beta, and release. Several respondents have tested the application. According to the survey's findings, 74.5% of respondents said that this application may encourage students to select the majors they are passionate about. According to the testimonial results, 77.1% of respondents thought this application was simple to understand.}
}
@article{LAVIOLA2024104026,
title = {The minimal AR authoring approach: Validation in a real assembly scenario},
journal = {Computers in Industry},
volume = {154},
pages = {104026},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104026},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001768},
author = {Enricoandrea Laviola and Antonio Emmanuele Uva and Michele Gattullo},
keywords = {Industrial augmented reality, Authoring, Minimal information, Visual asset, Assembly, Work instruction},
abstract = {This work aims to validate the “minimal AR” authoring approach in a real industrial assembly scenario. It focuses on optimizing visual assets in Augmented Reality (AR) work instructions. The design of AR assembly documentation is influenced by three main variables: work instructions, affordance (dependent on equipment components and operator capabilities), and AR signifiers (combination of visual assets with their properties). In this study, we fixed the instruction complexity while exploring the relationship between affordance and AR signifiers. First, we set up a focus group of 10 experts in AR technical documentation to extract guidelines for the design of minimal AR signifiers for assembly instructions with a variable affordance. Then, we validated these guidelines through an industrial case study involving 34 participants in four assembly tasks. We verified if the candidate minimal AR signifier, obtained using the proposed guidelines, corresponded to the minimal AR signifier established by users. The results showed that in 33% of the cases, users exploited the candidate minimal AR signifier to accomplish the task successfully. Beyond the minimal AR signifier, an additional one conveying the notification about the task success must always be provided to ensure failure by those operators with reduced capabilities. We also found that, in 29% of the cases, users needed less information than the candidate minimal AR signifier due to their higher capabilities. However, as expected, this condition leads users to make more errors than with the candidate minimal AR signifier. Moreover, the study confirms that AR signifiers with redundant information or attractive appearance, such as animated product models, are unnecessary to improve task comprehension. Still, animations could be beneficial in reinforcing understanding when object properties are difficult to detect.}
}
@article{MORAES201497,
title = {Psychomotor skills assessment in medical training based on virtual reality using a Weighted Possibilistic approach},
journal = {Knowledge-Based Systems},
volume = {70},
pages = {97-102},
year = {2014},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705114001841},
author = {Ronei M. Moraes and Liliane S. Machado},
keywords = {Decision support system, Weighted possibility approach, Psychomotor skills assessment, Virtual reality, Medical training},
abstract = {Virtual reality has been used to provide training systems in several areas, particularly in medicine. In that area, user’s interactions in a virtual environment are modeled and compared with predefined classes of performance to know how much users are prepared to perform that procedure on human beings. In this paper is proposed a new approach for online Single User’s Assessment System (SUAS) using Weighted Possibility and Necessity measures. Those fuzzy measures provide an interval-based estimator to check the compatibility between interactions of an user and previous stored classes of performance. This approach integrates the kernel of a decision support system and it contributes to improve the user’s assessment taking into account the individual relevance of variables in a procedure. It was verified in performance tests, when this new SUAS approach achieved better results, according to Kappa Coefficient, when compared with other previous approaches.}
}
@article{ALAM2024100059,
title = {ASL champ!: a virtual reality game with deep-learning driven sign recognition},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100059},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000096},
author = {Md Shahinur Alam and Jason Lamberton and Jianye Wang and Carly Leannah and Sarah Miller and Joseph Palagano and Myles de Bastion and Heather L. Smith and Melissa Malzkuhn and Lorna C. Quandt},
keywords = {Virtual reality, Deep learning, American sign language, Interactive learning, VR learning, Avatar interaction},
abstract = {We developed an American Sign Language (ASL) learning platform in a Virtual Reality (VR) environment to facilitate immersive interaction and real-time feedback for ASL learners. We describe the first game to use an interactive teaching style in which users learn from a fluent signing avatar and the first implementation of ASL sign recognition using deep learning within the VR environment. Advanced motion-capture technology powers an expressive ASL teaching avatar within an immersive three-dimensional environment. The teacher demonstrates an ASL sign for an object, prompting the user to copy the sign. Upon the user’s signing, a third-party plugin executes the sign recognition process alongside a deep learning model. Depending on the accuracy of a user’s sign production, the avatar repeats the sign or introduces a new one. We gathered a 3D VR ASL dataset from fifteen diverse participants to power the sign recognition model. The proposed deep learning model’s training, validation, and test accuracy are 90.12%, 89.37%, and 86.66%, respectively. The functional prototype can teach sign language vocabulary and be successfully adapted as an interactive ASL learning platform in VR.}
}
@article{BUAYAI2023108194,
title = {Supporting table grape berry thinning with deep neural network and augmented reality technologies},
journal = {Computers and Electronics in Agriculture},
volume = {213},
pages = {108194},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108194},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923005823},
author = {Prawit Buayai and Kabin Yok-In and Daisuke Inoue and Hiromitsu Nishizaki and Koji Makino and Xiaoyang Mao},
keywords = {Smart agriculture, Grape detection, Berry thinning, Berry removing, Instance segmentation, Deep neural network},
abstract = {Berry thinning is a crucial process in table grape cultivation. Such visual features as bunch compactness, bunch form, and berry size are important factors affecting market value. Moreover, sufficient space for each berry to grow also largely influences the final product’s quality, such as the sugar concentration. Berry thinning requires professional skills, and it is usually accomplished only by experienced farmers. Furthermore, the appropriate period for berry thinning is limited to two weeks; hence, berry-thinning tasks have led to a bottleneck in terms of increasing the yield of table grape products. This paper addresses the aforementioned issue by proposing a system for empowering unskilled farmers to begin berry thinning without in-person coaching from expert farmers. The proposed system employs a deep neural network model to learn the knowledge required for identifying berries to be removed, and it uses augmented reality technology to display instructions based on this knowledge to naïve farmers through smart glasses. The proposed system was validated throughout the entire growing season in a real table grape field in Yamanashi Prefecture, Japan. It was confirmed that unskilled farmers can execute berry-thinning tasks immediately without training. Furthermore, they can become familiar with the proposed system quickly. The grape products from unskilled farmers who used the proposed system also had an 8.18 % higher average quality score than those from the skilled farmers.}
}
@article{REN2024113,
title = {Exploring the effect of fingertip aero-haptic feedforward cues in directing eyes-free target acquisition in VR},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {113-131},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000839},
author = {Xiaofei Ren and Jian He and Teng Han and Songxian Liu and Mengfei Lv and Rui Zhou},
keywords = {Haptic, Feedforward, Virtual reality, Aero-haptic},
abstract = {Background
The sense of touch plays a crucial role in interactive behavior within virtual spaces, particularly when visual attention is absent. Although haptic feedback has been widely used to compensate for the lack of visual cues, the use of tactile information as a predictive feedforward cue to guide hand movements remains unexplored and lacks theoretical understanding.
Methods
This study introduces a fingertip aero-haptic rendering method to investigate its effectiveness in directing hand movements during eyes-free spatial interactions. The wearable device incorporates a multichannel micro-airflow chamber to deliver adjustable tactile effects on the fingertips.
Results
The first study verified that tactile directional feedforward cues significantly improve user capabilities in eyes-free target acquisition and that users rely heavily on haptic indications rather than spatial memory to control their hands. A subsequent study examined the impact of enriched tactile feedforward cues on assisting users in determining precise target positions during eyes-free interactions, and assessed the required learning efforts.
Conclusions
The haptic feedforward effect holds great practical promise in eyeless design for virtual reality. We aim to integrate cognitive models and tactile feedforward cues in the future, and apply richer tactile feedforward information to alleviate users' perceptual deficiencies.}
}
@article{CUNHA2023634,
title = {Using Extended Reality in Experiential Learning for Hospitality Management Education},
journal = {Procedia Computer Science},
volume = {219},
pages = {634-641},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.333},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003423},
author = {Carlos R. Cunha and Catarina Martins and Vítor Mendonça},
keywords = {Extended Reality, Model, Hospitality, Experiential Learning, Training, Assessement},
abstract = {Training qualified human resources is a challenge in all areas of knowledge. In particular, in the areas where the training environment of the graduates is not easily laboratoryable and/or possible to emulate, internships are often used in the work context, where an On-the-Job Training is provided. Higher education all over the world has sought to innovate in the way it mixes education. Accordingly, the digital has populated teaching-learning strategies in an attempt to make teaching more universal, more ubiquitous and more personalized, without losing rigor or the necessary requirements. In this path of innovation, e-learning or b-learning mechanisms were and still are used. However, we believe that a more decisive step must be taken by incorporating Extended Reality tools to develop and evolve the way in which competences can be acquired and evaluated, both in the classroom and in home study - a new approach at the service of the teacher and of the student. In this context, this work analyzes the role of Extended Reality in Human Resources training in the hospitality sector. In addition, a conceptual model is proposed to implement the use of Extended Reality in the context of higher education in the field of hospitality, in a conceptualization approach. Finally, future work is referred concerning the validation of the model and drawn technological considerations for the model implementation.}
}
@article{HAN2021101398,
title = {A framework for semi-automatically identifying fully occluded objects in 3D models: Towards comprehensive construction design review in virtual reality},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101398},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101398},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001506},
author = {Bing Han and Jong Won Ma and Fernanda Leite},
keywords = {Virtual Reality, Occluded Objects, Construction Design Review, Automation, Visualization},
abstract = {Virtual Reality (VR)-based construction design review applications have shown potential to enhance user performance in many research projects and experiments. Currently, visualizing occluded objects in VR is a challenge, and this function is indispensable for construction design review and coordination. This paper proposes an occlusion detection framework that semi-automatically identifies occluded objects in 3D construction models. The framework determines the visibility status of an object by converting the object to a point cloud and comparing the point cloud to the virtual laser scanning result of the original model. It exports models that are interoperable with VR development software so that visualization effects can be easily employed to occluded objects. The authors validated the framework using two building information models. The algorithm achieved a recall rate of 90.30% and a precision rate of 75.05% in a gasoline refinery facility model. It reached a higher 98.06% recall rate and a 97.53% precision rate in an academic building model. This paper contributes to the body of knowledge by proposing a semi-automatic occlusion detection framework and validating that point cloud-based algorithms are appropriate for this classification task.}
}
@article{DAMMACCO2022103761,
title = {Designing complex manufacturing systems by virtual reality: A novel approach and its application to the virtual commissioning of a production line},
journal = {Computers in Industry},
volume = {143},
pages = {103761},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103761},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001580},
author = {Lucilla Dammacco and Raffaele Carli and Vito Lazazzera and Michele Fiorentino and Mariagrazia Dotoli},
keywords = {Industry 4.0, Complex manufacturing systems, System design, Virtual reality, Virtual commissioning, Human computer interaction},
abstract = {The design of complex manufacturing systems (CMSs) is challenging, because of the requirements of efficiency, safety, and ergonomics, and the need of optimizing resources, i.e., space, machines, operators, and data. Virtual reality (VR) – one of the promising technologies at the base of Industry 4.0 – is able to address the design issues of CMSs, and even decrease costs and time when employed from the initial conception to the final validation of production lines, since it facilitates their virtual commissioning, i.e., it enables the full verification of systems and related components by virtual inspection and tests. Despite the above advantages, VR is still rarely used in the design of CMSs, and there is no standard VR approach in industry yet. In addition, the related scientific literature is scarce and often limited to small or simplified cases. To fill this gap, this work presents a novel VR-based approach for designing CMSs, composed of four phases: Three-dimensional CAD Export, Model Import, Scene Creation, and VR Review. The proposed approach is applied to a real industrial use case related to the virtual commissioning of an electric axles production line and it is evaluated through a questionnaire from industry professionals. The case study shows that using the VR technology enhanced the technical communication between experts in the teamwork, and it was particularly effective in finding ergonomics flaws like issues in visibility, reach, and posture using a virtual golden zone. In addition, all users found the VR interaction enjoyable and easy to learn, and beginner users perceived a comparable workload as advanced users.}
}
@article{PIZZOLANTE2023107876,
title = {Awe in the metaverse: Designing and validating a novel online virtual-reality awe-inspiring training},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107876},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107876},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002273},
author = {Marta Pizzolante and Francesca Borghesi and Eleonora Sarcinella and Sabrina Bartolotta and Carola Salvi and Pietro Cipresso and Andrea Gaggioli and Alice Chirico},
keywords = {Creativity, Metaverse, Complex emotions, Virtual reality, Awe-inspiring training},
abstract = {An increasing number of studies have unveiled the nuanced nature of awe - a complex emotion stemming from stimuli so perceptually and conceptually vast to impact individuals’ current mental frames. Its positive impact on human wellbeing and health have been reported even after a single short exposure to awe-inspiring stimuli. Recently, Virtual Reality (VR) has emerged as a suitable technique for eliciting brief moments of intense awe. Moreover, nowadays, the Metaverse has increased the opportunity to access even complex experiences, such as awe, for a prolonged period. However, the impact of a prolonged exposure to an awe-inspiring simulated experience still must be investigated. Here, in the first study, we designed and tested usability, user experience and preliminary effectiveness of the first VR awe-inspiring training vs. an equivalent neutral training in VR. We relied on an immersive virtual reality online social platform- Altspace VR - for designing the training. In the second study, we investigated whether a prolonged exposure to awe could hold the same effect on creative thinking as it was demonstrated for brief exposure to awe in VR. Specifically, we tested the impact of a long-lasting exposure to awe on creative thinking in the short and on the long run (after the training and in a one-week follow-up) and vs. an equivalent neutral condition (the same as the first study). Creativity thinking, was assessed through Alternative Uses Task (AUT). Additionally, measures related to the disposition to feel positive emotions, social desirability and level of curiosity were collected. The first study supported the feasibility of the training together with the usability of the platform as well as its effectiveness in eliciting awe (vs. neutral condition). Moreover, for the second study, there was a main effect of time on some of the dimensions of creative thinking. Participants scored higher in fluency, originality, and flexibility one week after the training compared to the pre-training phase. These results suggested preliminary design guidelines for creating awe experiences in the Metaverse, unveiling the role of time exposure and duration of their effects on individuals.}
}
@article{PUJIASTUTI20241738,
title = {The Effectiveness of Using Augmented Reality on the Geometry Thinking Ability of Junior High School Students},
journal = {Procedia Computer Science},
volume = {234},
pages = {1738-1745},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.180},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924005398},
author = {Heni Pujiastuti and Rudi Haryadi},
keywords = {augmented reality, thinking ability, geometry},
abstract = {This study aimed to test the effectiveness of using augmented reality applications in learning geometry in junior high school students. The method used in this study is the quasi-experiment method. In this study, researchers used learning with augmented reality to find its effect on student learning outcomes in grade 8 junior high school, which has an average age of 13 years. The research design used in this study was a quasi-experimental nonequivalent control group design. This study, 2 sample classes were taken: an experimental class (learning with augmented reality) and a control class (learning without augmented reality). The experimental and control classes have the same characteristics; each has 20 male and 20 female students. The instrument used in this research is a geometric thinking ability test. This study uses a test instrument in the form of essay questions. The instrument was given twice, namely during the pretest and post-test. There are ten essay questions made. After obtaining the data from the pretest and post-test results, the normalized gain score formula (N-Gain) was used to know the increase in geometric thinking skills. The N-Gain result of the experimental class is 78% or 0.78, which is in the high category. Whereas in the control class, it was 50% or 0.58, this result is in the moderate category. The N-gain results show that there is a difference between the experimental class and the control class. In this case, the effectiveness of using augmented reality for learning geometry in junior high school students is higher than those who did not receive augmented reality treatment. From the N-gain value, it proves that learning using augmented reality learning media is effective in increasing the ability to think Geometry. Then the results of observations made during learning activities using augmented reality show that students look active and make students more enthusiastic when learning Geometry. The results of the responses showed that 39 students had become more proactive in learning Geometry by using augmented reality applications.}
}
@article{HOWARD2022107197,
title = {A meta-analysis and systematic literature review of mixed reality rehabilitation programs: Investigating design characteristics of augmented reality and augmented virtuality},
journal = {Computers in Human Behavior},
volume = {130},
pages = {107197},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107197},
url = {https://www.sciencedirect.com/science/article/pii/S074756322200019X},
author = {Matt C. Howard and Maggie M. Davis},
keywords = {Mixed reality, Augmented reality, Augmented virtuality, Rehabilitation, Kinect, Hololens},
abstract = {Researchers have shown great interest in the creation of rehabilitation programs using mixed reality (MR), which includes both augmented reality (inclusion of virtual elements in a real environment) and augmented virtuality (inclusion of real elements in a virtual environment). Due to its recent development and systematic limitations of associated studies (e.g., small sample sizes), many pivotal research questions for the study of MR rehabilitation (MRR) programs remain unanswered, and the current article seeks to answer these questions via a meta-analysis and systematic literature review. Our random-effects meta-analysis, including 29 studies, showed that MRR programs produce significant improvements in users’ outcomes, and they were significantly more effective than alternative rehabilitation programs when matched for duration. It also demonstrated that there was not a significant effect regarding all studied characteristics of MRR programs, including the incorporation of game elements, use of head-mounted displays, and task-technology fit. Our systematic literature review identified the most studied types of MRR programs, and it revealed that the most common research designs are the least methodologically robust (e.g., single group post-test only design). Our discussion suggests that MRR programs are still in the earliest stage of research, and the current results are encouraging for future development. Because none of our studied characteristics altered the effectiveness of MRR programs, we argue that characteristics that are generally consistent across all MRR programs are the drivers of their effectiveness, which can be categorized as fidelity and motivational characteristics.}
}
@article{TADEJA2024103919,
title = {Immersive presentations of real-world medical equipment through interactive VR environment populated with the high-fidelity 3D model of mobile MRI unit},
journal = {Computers & Graphics},
volume = {120},
pages = {103919},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103919},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000542},
author = {Sławomir Konrad Tadeja and Thomas Bohné and Kacper Godula and Artur Cybulski and Magdalena Maria Woźniak},
keywords = {Virtual reality, VR, Immersive interface, Magnetic resonance imaging, MRI, Virtual presentation},
abstract = {The primary goal behind the system presented in this paper is to investigate the efficacy of using virtual reality (VR) for showcasing sizable medical equipment. Specifically, we focused on a mobile magnetic resonance imaging (MRI) scanner mounted on a truck trailer. The latter is integral to the mobile MRI setup and must be presented as part of the immersive experience. Therefore, we not only have to depict the medical apparatus but also provide the means of understanding its surroundings. This is especially important to radiologists and other medical personnel to ascertain if a given mobile medical facility fulfills their needs and wants. Furthermore, despite such MRI devices being designed for mobility, their long-distance transportation can be time-consuming, troublesome and expensive. Therefore, we can observe the need for showcasing such mobile MRI units without additional cost and burden related to transportation. To achieve this, we designed an immersive environment in which the users can interact with the real-life scale 3D model of a mobile MRI. In addition, we also verified the usability and expressiveness of our system using established heuristical approaches.}
}
@article{WU2014869,
title = {Real-time advanced spinal surgery via visible patient model and augmented reality system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {113},
number = {3},
pages = {869-881},
year = {2014},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2013.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0169260713004112},
author = {Jing-Ren Wu and Min-Liang Wang and Kai-Che Liu and Ming-Hsien Hu and Pei-Yuan Lee},
keywords = {Visible patient, Augmented reality, Camera-projector system, Digital spinal surgery},
abstract = {This paper presents an advanced augmented reality system for spinal surgery assistance, and develops entry-point guidance prior to vertebroplasty spinal surgery. Based on image-based marker detection and tracking, the proposed camera-projector system superimposes pre-operative 3-D images onto patients. The patients’ preoperative 3-D image model is registered by projecting it onto the patient such that the synthetic 3-D model merges with the real patient image, enabling the surgeon to see through the patients’ anatomy. The proposed method is much simpler than heavy and computationally challenging navigation systems, and also reduces radiation exposure. The system is experimentally tested on a preoperative 3D model, dummy patient model and animal cadaver model. The feasibility and accuracy of the proposed system is verified on three patients undergoing spinal surgery in the operating theater. The results of these clinical trials are extremely promising, with surgeons reporting favorably on the reduced time of finding a suitable entry point and reduced radiation dose to patients.}
}
@article{RIVERAFLOR2019641,
title = {Evaluation of Task Workload and Intrinsic Motivation in a Virtual Reality Simulator of Electric-Powered Wheelchairs},
journal = {Procedia Computer Science},
volume = {160},
pages = {641-646},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931734X},
author = {Hamilton Rivera-Flor and Kevin A. Hernandez-Ossa and Berthil Longo and Teodiano Bastos},
keywords = {Electric-Powered Wheelchair, Driving Training, Virtual Reality Simulator},
abstract = {For some people with severe physical disabilities, the immediate driving of an Electric-Powered Wheelchair (EPW) appears as a safety problem, which can be solved by the use of a virtual reality (VR) simulator for safe-driving learning purposes. There are several VR environment approaches in the literature applied to EPW driving training, including several tests that were performed to validate these simulators. This work evaluates the influence of different display devices on task workload and intrinsic motivation of participants, using the Simcadrom EPW Simulator developed at UFES/Brazil. Results from two qualitative tests: Intrinsic Motivation Inventory (IMI) and NASA Task Load Index (NASA-TLX) were compared for three displays (Head Mounted Display – HMD, desktop screen, and video projector). The results show that the HMD provided the highest usefulness score. On the other hand, the desktop screen reported the lowest task workload.}
}
@article{MOHAMMADKHORASANI2023103936,
title = {Augmented reality-computer vision combination for automatic fatigue crack detection and localization},
journal = {Computers in Industry},
volume = {149},
pages = {103936},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103936},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000866},
author = {Ali Mohammadkhorasani and Kaveh Malek and Rushil Mojidra and Jian Li and Caroline Bennett and William Collins and Fernando Moreu},
keywords = {Augmented reality, Structural health monitoring, Crack detection algorithm, Fatigue crack, Computer vision, Holographic anchoring},
abstract = {Fatigue cracks in bridges are inspected visually by specialized bridge inspection professionals. Bridge inspectors conduct inspections in a limited amount of time, and at times small cracks may go unnoticed. Researchers have recently developed computer-based techniques to help overcome these issues. This study uses a computer vision algorithm combined with Augmented Reality (AR) to localize fatigue cracks during the visual inspection that otherwise may go unnoticed because of their size. The AR software utilizes a video processing algorithm for fatigue crack detection. Subsequently, the AR software generates a hologram using the algorithm’s detection result and anchors it over the crack’s location at the structure being inspected. The result of this new methodology is an automatic fatigue crack detection and localization AR software that provides holograms overlaid during the on-site visual inspection. This technique also provides the fatigue crack detection result in near-real-time. The method is verified using 2D and 3D benchmarks and a half-scale steel bridge girder specimen. The research team collected the feedback from bridge industry inspectors contextualized in real operations and their recommendations for integrating AR for inspections with the involvement of human resources and workforce development. This study is the first effort of holographic addressing the fatigue crack on the actual structure using AR headset.}
}
@article{GERAETS2021100432,
title = {Virtual reality facial emotion recognition in social environments: An eye-tracking study},
journal = {Internet Interventions},
volume = {25},
pages = {100432},
year = {2021},
issn = {2214-7829},
doi = {https://doi.org/10.1016/j.invent.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2214782921000725},
author = {C.N.W. Geraets and S. {Klein Tuente} and B.P. Lestestuiver and M. {van Beilen} and S.A. Nijman and J.B.C. Marsman and W. Veling},
keywords = {Virtual reality, Emotion recognition, Eye-tracking, Affect, Emotion, Avatars},
abstract = {Background
Virtual reality (VR) enables the administration of realistic and dynamic stimuli within a social context for the assessment and training of emotion recognition. We tested a novel VR emotion recognition task by comparing emotion recognition across a VR, video and photo task, investigating covariates of recognition and exploring visual attention in VR.
Methods
Healthy individuals (n = 100) completed three emotion recognition tasks; a photo, video and VR task. During the VR task, emotions of virtual characters (avatars) in a VR street environment were rated, and eye-tracking was recorded in VR.
Results
Recognition accuracy in VR (overall 75%) was comparable to the photo and video task. However, there were some differences; disgust and happiness had lower accuracy rates in VR, and better accuracy was achieved for surprise and anger in VR compared to the video task. Participants spent more time identifying disgust, fear and sadness than surprise and happiness. In general, attention was directed longer to the eye and nose areas than the mouth.
Discussion
Immersive VR tasks can be used for training and assessment of emotion recognition. VR enables easily controllable avatars within environments relevant for daily life. Validated emotional expressions and tasks will be of relevance for clinical applications.}
}
@article{PICANO2022109152,
title = {Human-in-the-loop virtual reality offloading scheme in wireless 6G Terahertz networks},
journal = {Computer Networks},
volume = {214},
pages = {109152},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S138912862200264X},
author = {Benedetta Picano and Romano Fantacci},
keywords = {Matching theory, Human-in-the-loop, Stochastic network calculus},
abstract = {Upcoming sixth-generation networks have to be carefully designed to offer support to a wide range of human-centric applications, typically computationally intensive and heavily dependent on human perception. This paper proposes a human-in-the-loop-based resource allocation approach found on the joint use of matching theory and stochastic network calculus principles to efficiently address tasks offloading issues within a sixth-generation network environment. The considered matching game approach is characterized by externalities, whose stability is properly discussed here. In particular, by focusing on virtual reality services, the paper aims at maximizing the average service reliability in terms of ability in guaranteeing a proper quality of experience target, i.e., keeping the end-to-end delay below a suitable threshold, by the proper exploitation of the human cognitive limitations. Finally, the effectiveness of the proposed human-in-the-loop-based resource allocation approach is verified by proving comparisons with simulation results and conventional brain-agnostic strategies.}
}
@article{DALLEL2023105655,
title = {Digital twin of an industrial workstation: A novel method of an auto-labeled data generator using virtual reality for human action recognition in the context of human–robot collaboration},
journal = {Engineering Applications of Artificial Intelligence},
volume = {118},
pages = {105655},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105655},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006455},
author = {Mejdi Dallel and Vincent Havard and Yohan Dupuis and David Baudry},
keywords = {Digital Twins (DTs), Industry 4.0, Auto-labeled data generation, Virtual Reality (VR), Human Action Recognition (HAR), Human–Robot Collaboration (HRC)},
abstract = {The recognition of human actions based on artificial intelligence methods to enable Human–Robot Collaboration (HRC) inside working environments remains a challenge, especially because of the necessary huge training datasets needed. Meanwhile, Digital Twins (DTs) of human centered productions are increasingly developed and used in the design and operation phases. As instance, DTs are already helping industries to design, visualize, monitor, manage, and maintain their assets more effectively. However, few works are dealing with using DTs as a dataset generator tool. Therefore, this paper explores the use of a DT of a real industrial workstation involving assembly tasks with a robotic arm interfaced with Virtual Reality (VR) to extract a digital human model. The DT simulates assembly operations performed by humans aiming to generate self-labeled data. Thereby, a Human Action Recognition dataset named InHARD-DT was created to validate a real use case in which we use the acquired auto-labeled DT data of the virtual representation of the InHARD dataset to train a Spatial–Temporal Graph Convolutional Neural Network with skeletal data on one hand. On the other hand, the Physical Twin (PT) data of the InHARD dataset was used for testing. Obtained results show the effectiveness of the proposed method.}
}
@article{XIA2024102664,
title = {Augmented reality and indoor positioning based mobile production monitoring system to support workers with human-in-the-loop},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {86},
pages = {102664},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102664},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001394},
author = {Luyao Xia and Jianfeng Lu and Yuqian Lu and Hao Zhang and Yuhang Fan and Zhishu Zhang},
keywords = {Augmented reality, Indoor positioning, Production monitoring system, Human factors, Human-in-the-loop},
abstract = {With the increasing demand for product customization, the exponential increase in large-scale and small-batch production orders has yielded new challenges for the capacity of traditional production lines. Due to the information redundancy and complex production process on the production site of the factory, the traditional monitoring system is incapable of meeting the information interaction requirements between the factory management level and the executive level. Meanwhile, the traditional augmented reality (AR) based on image recognition is not suitable in the complex industrial environments. To address the gap, we propose a novel mobile production monitoring system (AIMPMs) with human-in-the-loop control by leveraging the cutting-edge AR and indoor positioning technique. In the proposed system, Ultra-wideband (UWB) and Inertial Measurement Unit (IMU) fusion indoor positioning technology is proposed, which provides accurate indoor positioning information for the production factors in the factory. Subsequently, we build the lightweight indoor map for positioning that can serve as the location reference and path planning, and a nearest-neighbor decision algorithm with double rejection decision (NN-DRD) is proposed to match the positioning features to trigger virtual monitoring information. Finally, the AIMPMs is applied in a hydraulic cylinder factory to verify its enforceability and effectiveness, and the human factors evaluation model and index system are constructed to evaluate two systems, (1) the mobile monitoring system based on positioning information, and (2) the mobile monitoring system based on image recognition, respectively. The experimental results indicate that after the application of AIMPMs, the physiological and mental fatigue of the production personnel is immensely decreased. Therefore, the system realizes a smarter and highly humanized human-machine interaction mode with human-in-the-loop control.}
}
@article{FILLATREAU20131253,
title = {Using virtual reality and 3D industrial numerical models for immersive interactive checklists},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1253-1262},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513000717},
author = {P. Fillatreau and J.-Y. Fourquet and R. {Le Bolloc’h} and S. Cailhol and A. Datas and B. Puel},
keywords = {Virtual reality, 3D industrial inspection, Quality and control processes, 3D visualization, Sensorimotor interfaces},
abstract = {At the different stages of the PLM, companies develop numerous checklist-based procedures involving prototype inspection and testing. Besides, techniques from CAD, 3D imaging, animation and virtual reality now form a mature set of tools for industrial applications. The work presented in this article develops a unique framework for immersive checklist-based project reviews that applies to all steps of the PLM. It combines immersive navigation in the checklist, virtual experiments when needed and multimedia update of the checklist. It provides a generic tool, independent of the considered checklist, relies on the integration of various VR tools and concepts, in a modular way, and uses an original gesture recognition. Feasibility experiments are presented, validating the benefits of the approach.}
}
@article{RAHMAN2023753,
title = {The Effectiveness of Augmented Reality Using Flash Card in Education to Learn Simple English Words as a Secondary Language},
journal = {Procedia Computer Science},
volume = {227},
pages = {753-761},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.580},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017477},
author = {Muhammad Akbar Rahman and Rosyidan Rouf Faisal and Cuk Tho},
keywords = {Augmented Reality, Education, Flash Card, English, Effective, Technology},
abstract = {Considering the potential and benefits of augmented reality and the development of education efficiency is essential, it is necessary to utilize the benefits and potential suitable to be implemented in the education field. One of the benefits is the fun learning experience compared to the traditional method. Furthermore, this study aims to analyze the effectiveness of using augmented reality in learning simple English words. The technology method is marker-based tracking, where the marker is flashcards that will be uploaded into Vuforia SDK Kit. Also, this research method uses ADDIE because it is an instructional design that makes the researcher easily to keep track of the progress and result. Then, the method is tested and analyzed using paired t-tests to measure its effectiveness and the User Experience Questionnaire to measure the user experience. Based on the analysis, the result shows that the result is effective, and the questionnaire has a good overall score. In conclusion, the result corresponds to the contribution of this research that enables the education learning experience to be effective, fun, and interactive.}
}
@article{FAN2024108156,
title = {A hybrid robotic system for zygomatic implant placement based on mixed reality navigation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {249},
pages = {108156},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108156},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001524},
author = {Xingqi Fan and Yuan Feng and Baoxin Tao and Yihan Shen and Yiqun Wu and Xiaojun Chen},
keywords = {Zygomatic implant surgery, Mixed reality navigation, Hybrid robotic system},
abstract = {Backgrounds
Zygomatic implant (ZI) placement surgery is a viable surgical option for patients with severe maxillary atrophy and insufficient residual maxillary bone. Still, it is difficult and risky due to the long path of ZI placement and the narrow field of vision. Dynamic navigation is a superior solution, but it presents challenges such as requiring operators to have advanced skills and experience. Moreover, the precision and stability of manual implantation remain inadequate. These issues are anticipated to be addressed by implementing robot-assisted surgery and achieved by introducing a mixed reality (MR) navigation-guided hybrid robotic system for ZI placement surgery.
Methods
This study utilized a hybrid robotic system to perform the ZI placement surgery. Our first step was to reconstruct a virtual 3D model from preoperative cone-beam CT (CBCT) images. We proposed a series of algorithms based on coordinate transformation, which includes image-phantom registration, HoloLens-tracker registration, drill-phantom calibration, and robot-implant calibration, to unify all objects within the same coordinate system. These algorithms enable real-time tracking of the surgical drill's position and orientation relative to the patient phantom. Subsequently, the surgical drill is directed to the entry position, and the planned implantation paths are superimposed on the patient phantom using HoloLens 2 for visualization. Finally, the hybrid robot system performs the processed of drilling, expansion, and placement of ZIs under the guidance of the MR navigation system.
Results
Phantom experiments of ZI placement were conducted using 10 patient phantoms, with a total of 40 ZIs inserted. Out of these, 20 were manually implanted, and the remaining 20 were robotically implanted. Comparisons between the actual implanted ZI paths and the preoperatively planned ZI paths showed that our MR navigation-guided hybrid robotic system achieved a coronal deviation of 0.887 ± 0.213 mm, an apical deviation of 1.201 ± 0.318 mm, and an angular deviation of 3.468 ± 0.339° This demonstrates significantly better accuracy and stability than manual implantation.
Conclusion
Our proposed hybrid robotic system enables automated ZI placement surgery guided by MR navigation, achieving greater accuracy and stability compared to manual operations in phantom experiments. Furthermore, this system is expected to apply to animal and cadaveric experiments, to get a good ready for clinical studies.}
}
@article{ALSHAWABKEH2024e00325,
title = {Virtual reality as a tool to enhance the efficiency and reliability of the virtual reconstruction process for heritage/archaeological sites: The case of umm Al-Jimal in Jordan},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {33},
pages = {e00325},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00325},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000109},
author = {Rami {Al shawabkeh} and Mai Arar},
keywords = {Virtual reconstruction, Visualization, Virtual reality, Prediction, Photogrammetry, Archaeological site, Digital archaeology, Unreal engine},
abstract = {The growing attention towards reconstruction, driven by its potential to promote sustainable heritage preservation, is impacted by a shortage of the data, sources, and interpretive methods employed in virtual reconstruction. Consequently, the public (individuals and reviewers/Professionals) faces challenges in determining between the original elements and the added elements, in addition to identifying the origin of the new elements. Despite researchers' efforts to implement virtual reality as a solution, its effectiveness is constrained to the end outcome due to the lack of integration of reconstruction information. A set of factors contribute to the challenge of incorporating interactive information into virtual reality, including the researcher's expertise in virtual reality software and the deterministic nature of the C programming language. This necessitates the development of more user-friendly methods to support researchers in reducing the shortage and limitations. As a result, the study aims to develop a method for simplifying the process of transferring virtual reconstruction operations to virtual reality and test its efficiency through interviews with individuals and reviewers/Professionals. This is accomplished through the integration of virtual reconstruction steps with the Unreal Engine program, which serves as a virtual reality platform and is easier to program than Unity which is more prevalent in heritage research. The findings delivered a more efficient approach to collecting diverse sources and integrating them into one platform simply. This approach also facilitates direct communication between reviewers/Professionals and researchers, thereby enhancing the efficiency of the reconstruction peer review process. Furthermore, the utilization of virtual reality has played an important role in enhancing comprehension between what is the origin and added, as well as facilitating the validation of the rebuilding procedure among individuals and reviewers/Professionals.}
}
@article{RAMALHINHO2023102943,
title = {The value of Augmented Reality in surgery — A usability study on laparoscopic liver surgery},
journal = {Medical Image Analysis},
volume = {90},
pages = {102943},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102943},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002037},
author = {João Ramalhinho and Soojeong Yoo and Thomas Dowrick and Bongjin Koo and Murali Somasundaram and Kurinchi Gurusamy and David J. Hawkes and Brian Davidson and Ann Blandford and Matthew J. Clarkson},
keywords = {Augmented reality, Image-guided surgery, Usability, Laparoscopic surgery},
abstract = {Augmented Reality (AR) is considered to be a promising technology for the guidance of laparoscopic liver surgery. By overlaying pre-operative 3D information of the liver and internal blood vessels on the laparoscopic view, surgeons can better understand the location of critical structures. In an effort to enable AR, several authors have focused on the development of methods to obtain an accurate alignment between the laparoscopic video image and the pre-operative 3D data of the liver, without assessing the benefit that the resulting overlay can provide during surgery. In this paper, we present a study that aims to assess quantitatively and qualitatively the value of an AR overlay in laparoscopic surgery during a simulated surgical task on a phantom setup. We design a study where participants are asked to physically localise pre-operative tumours in a liver phantom using three image guidance conditions — a baseline condition without any image guidance, a condition where the 3D surfaces of the liver are aligned to the video and displayed on a black background, and a condition where video see-through AR is displayed on the laparoscopic video. Using data collected from a cohort of 24 participants which include 12 surgeons, we observe that compared to the baseline, AR decreases the median localisation error of surgeons on non-peripheral targets from 25.8 mm to 9.2 mm. Using subjective feedback, we also identify that AR introduces usability improvements in the surgical task and increases the perceived confidence of the users. Between the two tested displays, the majority of participants preferred to use the AR overlay instead of navigated view of the 3D surfaces on a separate screen. We conclude that AR has the potential to improve performance and decision making in laparoscopic surgery, and that improvements in overlay alignment accuracy and depth perception should be pursued in the future.}
}
@article{ZHAO2024102415,
title = {In-situ observation and calibration for structure safety diagnosis through finite element analysis and mixed reality},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102415},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102415},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000636},
author = {Xuefeng Zhao and Wangbing Li and Zhe Sun and Meng Zhang and Lingli Huang},
keywords = {Structure safety diagnosis, Mixed reality, Finite element analysis, In-situ},
abstract = {In-situ observation and calibration through non-intrusive approaches are necessary for diagnosing structural safety risks and predicting structure deterioration patterns during field inspections of critical civil infrastructures. Conventional visual inspections always subject to engineering knowledge and field experiences, which could hardly examine structure risks under as-is conditions. Finite Element Analysis (FEA) tools and model updating algorithms could help examine changes of stress and displacements under extreme environmental conditions according to the discovered structure defects and spatiotemporal changes. However, such tools usually require extensive computing resources for running simulations at a location that is usually far from the jobsite. Mixed Reality (MR) techniques could help in visualizing the virtual geometric composition of physical structural components. Unfortunately, existing MR device could hardly visualize tedious FEA and model updating processes of critical structural components. Integrated used of FEA tools and MR techniques could help support real-time safety diagnosis of critical structural components at remote locations. In this study, the authors established a framework that integrate sensing techniques, FEA tools, model updating algorithms, and MR for supporting structure safety diagnosis of civil infrastructures. The proposed method aims to 1) capture and analyze structural defects, 2) calibrate the virtual model with the real structure for visualizing FEA results of critical structural components through MR. The authors validate the proposed method through a case study. Results show that the proposed method provides fundamental support for effective on-site structure safety diagnosis.}
}
@article{ABBAS2024100065,
title = {Task difficulty impact on multitasking in mixed reality environments},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100065},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000151},
author = {Safanah Abbas and Heejin Jeong},
keywords = {Task difficulty, Digital-physical hybrid, Hand use, Priority, Multitasking, Learning environment},
abstract = {Mixed reality applications allow users to experience both physical and digital worlds simultaneously. However, limited research has compared multitasking performance in these two worlds with different difficulty levels. This study investigated the effect of task difficulty on mixed-reality multitasking performance. A block-matching task was used for the physical task experiment, and the N-back test was used for the virtual task experiment. Thirty-six participants completed eight experimental conditions and were tested on four measures: NASA-TLX, accuracy, priority, and hand use. Physical demand and effort subscales were significant in all the experimental manipulations: real-world tasks, virtual-world tasks, and multitasking. The priority variable was not found to be substantial. Most participants prioritized working on either real-world tasks or multitasking, leaving the virtual-world tasks to the least preferred. Such a study can empower instructional designers to tailor tasks that align with learners’ cognitive abilities for an optimal learning experience.}
}
@article{MEUSEL2019179,
title = {The importance of operator knowledge in evaluating virtual reality cue fidelity},
journal = {Computers and Electronics in Agriculture},
volume = {160},
pages = {179-187},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918309657},
author = {Chase Meusel and Chase Grimm and Jordan Starkey and Stephen B. Gilbert and Brian Gilmore and Greg Luecke and Don Kieu and Timothy Hunt},
keywords = {Virtual reality, Simulator, Fidelity, Operator knowledge, Automation},
abstract = {Research, development, testing, and operator training of large agricultural harvesting equipment has become increasingly expensive and complex. Operational simulators can offset these costs, but it is critical to evaluate the fidelity of the simulator experience to ensure that it will lead to operator behaviors that match those in the real world. This research describes a validation process for new visual cues within simulators and focuses on the presentation and fidelity of cues in a combine harvest simulator. The fully functional immersive combine simulator in this work provides a mixed-reality interface that combines a physically accurate operator interface with displays of virtual reality (VR) combine header and grain. Grain combine operators participated to assess how well they perceived the VR cues as representing actual field and crop activity. Results showed that operators successfully identified 85% of the visual cues in the combine simulator and recognized when these cues indicated machine settings adjustment needs but that operators' ability to choose the correct action to take correlated with their knowledge of the combine. Results also showed wide variation in the number of the grain combine’s head reel adjustments made by operators, likely reflecting strong individual preferences or habits around reel adjustment. Operators' number of adjustment interactions with the reel was also significantly correlated with the number of correct actions chosen, suggesting that careful attention to the reel is related to good farming performance. This research demonstrates the importance of taking operator knowledge and preferences into account when designing new agricultural products and offers a systematic method of validating cues within an agricultural simulator.}
}
@article{ESPINDOLA2013376,
title = {A model-based approach for data integration to improve maintenance management by mixed reality},
journal = {Computers in Industry},
volume = {64},
number = {4},
pages = {376-391},
year = {2013},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513000043},
author = {Danúbia Bueno Espíndola and Luca Fumagalli and Marco Garetti and Carlos E. Pereira and Silvia S.C. Botelho and Renato {Ventura Henriques}},
keywords = {Industrial maintenance, Mixed reality, Product data management, Data modeling/visualization},
abstract = {Facilitating interaction with maintenance systems through intuitive interfaces is a competitive advantage in terms of time and costs for industry. This work presents the CARMMI approach, which aims to integrate information coming from CAx tools, mixed/augmented reality tools and embedded intelligent maintenance systems. CARMMI aims to provide support to operators/technicians during maintenance tasks through mixed reality, providing an easier access, understanding and comprehension of information from different systems. Information about where, when and which data will be presented in interface are defined by CARMMI. The paper presents three test cases that were performed using the proposed concepts and infrastructure. The main benefit of the approach is to provide an extensive and generic model for the integration and management of maintenance data through the use of CARMMI.}
}
@article{KIM201628,
title = {Predicting the use of smartphone-based Augmented Reality (AR): Does telepresence really help?},
journal = {Computers in Human Behavior},
volume = {59},
pages = {28-38},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216300012},
author = {Hyeon-Cheol Kim and Martin Yongho Hyun},
keywords = {Revised TAM model, TMH model, Usefulness, Telepresence, Mediation effects},
abstract = {The purpose of this study was to determine what predicts the use of smartphone-based Augmented Reality (AR). The study proposes two models to determine whether telepresence can substitute for usefulness and whether both usefulness in the revised technology acceptance model and telepresence in the telepresence mediation hypothesis model can mediate the relationship among three types of AR quality and the intention to reuse AR. Two models were tested with 134 undergraduates who experienced the smartphone-based AR application OVJET. Two competing models demonstrate that all hypothesized paths in the revised technology acceptance model and the telepresence mediation hypothesis model are significant except for one path from service quality to telepresence. A path difference test shows that three pairs of paths (system quality → usefulness vs. system quality → telepresence; service quality → usefulness vs. service quality → telepresence; and usefulness → AR reuse intention vs. telepresence → AR reuse intention) are significantly different, while one pair of the paths (i.e., information system → elepresence vs. information quality → usefulness) is not. In addition, usefulness performs both partial and complete mediating roles, while telepresence has only a complete mediation effect.}
}
@article{LOPEZIBANEZ2021100404,
title = {Using gestural emotions recognised through a neural network as input for an adaptive music system in virtual reality},
journal = {Entertainment Computing},
volume = {38},
pages = {100404},
year = {2021},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100404},
url = {https://www.sciencedirect.com/science/article/pii/S187595212100001X},
author = {Manuel {López Ibáñez} and Maximiliano Miranda and Nahum Alvarez and Federico Peinado},
keywords = {Video games, Gesture recognition, Adaptive audio, Human-computer interaction, Machine learning},
abstract = {In this article, a head gesture recognition system is developed in order to identify emotional inputs and provide them to an adaptive music system (LitSens) in virtual reality applications, improving virtual presence in the process. Two iterations of this system, both founded on neural networks, are presented: the first one is based on a multi-layer perceptron, whereas the second one consists of a hybrid one-dimensional convolutional neural network. In both cases, the system is able to recognise fear by analysing head gestures. Whereas the first implementation is quicker when recognising this emotion, the second one is slower, but much more accurate, which makes it a better option overall for soundtrack adaptation. An experiment is then detailed, aimed towards validating the behaviour of a gestural recogniser when detecting fear in players. The results achieved through this validation are generally positive, but evince the need for an improvement in terms of system responsiveness.}
}
@article{YANG2023317,
title = {Design of a new kind of chemical experiment container with virtual reality fusion},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {4},
pages = {317-337},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000948},
author = {Lurong Yang and Zhiquan Feng and Junhong Meng},
keywords = {Multichannel intent fusion understanding, Virtual-real fusion, Natural interaction, Remove image highlights, Nonlinear curve fitting},
abstract = {Background
At present, the teaching of experiments in primary and secondary schools is affected by cost and security factors. Existing research on virtual experiment platforms has alleviated these problems. However, the lack of real experimental equipment and use of a single channel to understand user intentions weaken these platforms operationally and degrade the naturalness of interactions.
Methods
To solve these problems, we propose an intelligent experimental container structure and a situational awareness algorithm, both of which are verified and applied to a chemical experiment involving virtual-real fusion. First, the acquired images are denoised in the visual channel using the maximum diffuse reflection chroma to remove overexposure. Second, container situational awareness is realized by segmenting the image liquid level and establishing a relation-fitting model. Then, strategies for constructing complete behaviors and making priority comparisons among behaviors are adopted for information complementarity and independence, respectively. A multichannel intentional understanding model and an interactive paradigm that integrates vision, hearing, and touch are proposed.
Results
The experimental results show that the accuracy of the intelligent container situation awareness proposed in this paper reaches 99%, and the accuracy of the proposed intention understanding algorithm reaches 94.7%. The test shows that the intelligent experimental system based on the new interaction paradigm also has better performance and a more realistic sense of operation experience in terms of experimental efficiency.
Conclusion
The results indicate that the proposed experimental container and algorithm can achieve a natural level of human-computer interaction in a virtual chemical experiment platform, enhance the user′s sense of operation, and achieve high levels of user satisfaction.}
}
@article{SANCHO2023102893,
title = {SLIMBRAIN: Augmented reality real-time acquisition and processing system for hyperspectral classification mapping with depth information for in-vivo surgical procedures},
journal = {Journal of Systems Architecture},
volume = {140},
pages = {102893},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102893},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123000723},
author = {Jaime Sancho and Manuel Villa and Miguel Chavarrías and Eduardo Juarez and Alfonso Lagares and César Sanz},
keywords = {Brain tumor, AR, GPU, Machine Learning, Medicine, Depth image, Lidar, Classification, Machine learning, IA, Neurosurgery, Biomedical engineering},
abstract = {Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scene at the same time it is captured and processed, improving the visualization and hence effectiveness of the HS technology to delimit tumors. The whole system has been verified in real brain tumor resection operations.}
}
@article{YOO2024103881,
title = {Can engineers represent surgeons in usability studies? Comparison of results from evaluating augmented reality guidance for laparoscopic surgery},
journal = {Computers & Graphics},
volume = {119},
pages = {103881},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000086},
author = {Soojeong Yoo and João Ramalhinho and Thomas Dowrick and Murali Somasundaram and Kurinchi Gurusamy and Brian Davidson and Matthew J. Clarkson and Ann Blandford},
keywords = {Laparoscopic surgery, Augmented reality, Operating theatre, Usability study},
abstract = {Obtaining feedback from time-constrained end-users is a major challenge in evaluating novel systems for specialised applications. The performance and feedback of engineers and surgeons was evaluated through an experiment where participants were asked to identify tumour locations within an anatomically realistic silicon liver model across three different conditions of an Augmented Reality (AR) prototype system (Baseline, Split AR and Full AR). Our findings show that engineers and surgeons share some similarities in their performance, feedback and behaviour, particularly when reliance on the AR system is high for both groups. However, engineers typically focus more on accuracy of the image alignment and are more accurate in their responses when supported by AR. Senior surgeons typically perform faster and use AR as supplementary information, while the performance of junior surgeons is more closely aligned to the performance of engineers. We conclude that engineers could be involved in preliminary evaluations of a surgical system or in evaluations of systems which are aimed at training junior surgeons, but that it is essential to involve surgeons in later evaluations, where ecological validity is a more important consideration.}
}
@article{KIM2024104319,
title = {Development of a virtual reality system usability questionnaire (VRSUQ)},
journal = {Applied Ergonomics},
volume = {119},
pages = {104319},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2024.104319},
url = {https://www.sciencedirect.com/science/article/pii/S0003687024000966},
author = {Yong Min Kim and Ilsun Rhiu},
keywords = {Virtual reality, Usability evaluation, Perceived usability, Human-computer interaction},
abstract = {Virtual reality (VR) has gained significant attention as a technology that provides immersive experiences similar to the real world. In order for a VR system to be accepted, usability needs to be guaranteed. Accordingly, VR-related researchers are continuing their efforts to improve VR systems by conducting usability evaluations. However, existing studies have limitations in that they cannot comprehensively evaluate the detailed properties of VR systems by using questionnaires developed for general product usability evaluation or focusing only on some usability aspects of VR systems. This suggests it may be difficult to fully capture usability issues in a VR system, and that it is necessary to develop a usability evaluation tool that reflects the specific characteristics of the VR system. Therefore, this study develops and proposes the Virtual Reality System Usability Questionnaire (VRSUQ). In the development of the questionnaire, items were structured based on a literature review and discussions with experts. To account for the diverse characteristics of VR systems, the validity of the questionnaire was verified through both exploratory and confirmatory factor analysis, utilizing data obtained from three distinct experimental studies that employed different VR systems. In addition, by comparing the results of VRSUQ with the results from the System Usability Scale, which is widely used for perceived usability evaluation, alternative possibilities for using VRSUQ are presented. Further testing on various VR platforms is needed to ensure the reliability and validity of VRSUQ, and as results from using VRSUQ are accumulated, it is expected to be widely used as a more powerful and robust VR-specific perceived usability evaluation tool.}
}
@article{LUO2020105099,
title = {Augmented reality navigation for liver resection with a stereoscopic laparoscope},
journal = {Computer Methods and Programs in Biomedicine},
volume = {187},
pages = {105099},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.105099},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719300422},
author = {Huoling Luo and Dalong Yin and Shugeng Zhang and Deqiang Xiao and Baochun He and Fanzheng Meng and Yanfang Zhang and Wei Cai and Shenghao He and Wenyu Zhang and Qingmao Hu and Hongrui Guo and Shuhang Liang and Shuo Zhou and Shuxun Liu and Linmao Sun and Xiao Guo and Chihua Fang and Lianxin Liu and Fucang Jia},
keywords = {Augmented reality, Laparoscopic surgery, Liver resection, Surgical navigation},
abstract = {Objective
Understanding the three-dimensional (3D) spatial position and orientation of vessels and tumor(s) is vital in laparoscopic liver resection procedures. Augmented reality (AR) techniques can help surgeons see the patient's internal anatomy in conjunction with laparoscopic video images.
Method
In this paper, we present an AR-assisted navigation system for liver resection based on a rigid stereoscopic laparoscope. The stereo image pairs from the laparoscope are used by an unsupervised convolutional network (CNN) framework to estimate depth and generate an intraoperative 3D liver surface. Meanwhile, 3D models of the patient's surgical field are segmented from preoperative CT images using V-Net architecture for volumetric image data in an end-to-end predictive style. A globally optimal iterative closest point (Go-ICP) algorithm is adopted to register the pre- and intraoperative models into a unified coordinate space; then, the preoperative 3D models are superimposed on the live laparoscopic images to provide the surgeon with detailed information about the subsurface of the patient's anatomy, including tumors, their resection margins and vessels.
Results
The proposed navigation system is tested on four laboratory ex vivo porcine livers and five operating theatre in vivo porcine experiments to validate its accuracy. The ex vivo and in vivo reprojection errors (RPE) are 6.04 ± 1.85 mm and 8.73 ± 2.43 mm, respectively.
Conclusion and Significance
Both the qualitative and quantitative results indicate that our AR-assisted navigation system shows promise and has the potential to be highly useful in clinical practice.}
}
@article{FONSECA2014434,
title = {Relationship between student profile, tool use, participation, and academic performance with the use of Augmented Reality technology for visualized architecture models},
journal = {Computers in Human Behavior},
volume = {31},
pages = {434-445},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213000952},
author = {David Fonseca and Nuria Martí and Ernesto Redondo and Isidro Navarro and Albert Sánchez},
keywords = {Augmented Reality, Technology acceptance, Academic motivation, Architecture visualization, Mobile learning, User experience},
abstract = {In this study, we describe the implementation and evaluation of an experiment with Augmented Reality (AR) technology in the visualization of 3D models and the presentation of architectural projects by students of architecture and building engineering. The proposal is based on the premise that the technology used in AR, such as mobile devices, is familiar to the student. When used in a collaborative manner, the technology is able to achieve a greater level of direct engagement with the proposed content, thereby improving academic outcomes. The objective was to assess the feasibility of using AR on mobile devices in educational environments and to investigate the relationship between the usability of the tool, student participation, and the improvement in academic performance after using AR. The validation was performed through a case study in which students were able to experience a virtual construction process overlapped onto real environments. Results were obtained by students’ pre-tests and post-tests. In line with our assumptions, the use of mobile devices in the classroom is highly correlated with motivation, and there is a significant correlation with academic achievement. However, the difficulty of using and generating content is a complex factor that suggests difficulty when implementing more complicated models.}
}
@article{ASANO20221773,
title = {Cognitive Model of Understanding and Active Learning Environment to enhance Understanding with experiencing Moon Waxing and Waning by sharing AR Space},
journal = {Procedia Computer Science},
volume = {207},
pages = {1773-1782},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.235},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011206},
author = {Yudai Asano and Masato Soga},
keywords = {AR, group learning, Moon, waxing, waning},
abstract = {As a developmental task for learning phases of the Moon, students are sometimes given the opportunity to study the phases of the Earth from the perspective on the Moon as well as Moon phases from the perspective on the Earth. We proposed a cognitive model of understanding of Moon phases, and we developed an active learning environment in which two students work in pairs: one playing the role of the Moon and the other playing the role of the Earth. We used a technology by which two people can share the same AR space. We verified the effect of the learning environment by an evaluation test.}
}
@article{ALDOARISTA2023762,
title = {Preserving Indonesian Culture in the Digital Age: Implementing Augmented Reality to Address Cultural Appropriation Issue},
journal = {Procedia Computer Science},
volume = {227},
pages = {762-771},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.581},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017489},
author = {W.P. {Aldo Arista} and S. {Daniel Hendra} and P. {Felix Juwono} and H. {Kanz Abdillah} and Frihandhika Permana},
keywords = {Cultural appropriation, identity, Augmented Reality (AR), markers, traditional},
abstract = {Abstract—Cultural appropriation is a complex issue that can lead to the loss of a nation's cultural identity. With the widespread adoption of mobile phones in Indonesia, there is an opportunity to use technology, specifically augmented reality (AR). To address this issue, our user-friendly AR app allows users to interact with Indonesian culture in a virtual environment. Through AR technology, users can engage with Indonesian culture in an immersive and fun way. Our target market is the younger generation, and our app provides a user-friendly interface that allows users to scan AR markers to access different cultural settings. Users can choose from two cultural regions, Bali and Papua. Once a region is selected, a scene showcases the desired region's iconic places and objects accompanied by a traditional folk song. In developing the AR app, we used the agile software development method for approximately two months with four developers. At the end of the development phase, we conduct four testing aspects of the application—namely, angle testing, functionally testing, usability testing, and hardware testing. The testing revealed that the application could perform well under various testing conditions. Through our AR solution, we aim to raise awareness of the importance of Indonesian culture and provide an educational experience that can inspire users to take ownership of their cultural heritage.}
}
@article{KUNDU201727,
title = {Scanning Camera and Augmented Reality Based Localization of Omnidirectional Robot for Indoor Application},
journal = {Procedia Computer Science},
volume = {105},
pages = {27-33},
year = {2017},
note = {2016 IEEE International Symposium on Robotics and Intelligent Sensors, IRIS 2016, 17-20 December 2016, Tokyo, Japan},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.183},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917302016},
author = {Ananda Sankar Kundu and Oishee Mazumder and Ankit Dhar and Prasanna K. Lenka and Subhasis Bhaumik},
keywords = {Augmented reality, Fusion, Visual localization, Omni robot, Scanning Camera},
abstract = {The aim of this paper is to develop an absolute visual localization system of an Omni wheeled robot for indoor navigation. Omni wheeled based robots have omni directional drive capacity. Conventional localization technique like odometry is not suitable for such drives due to wheel slippage. An omni robot platform with 4 omni-directional wheels powered by dynamixel motor and a scanning platform has been developed. We have implemented a localization technique using camera as visual sensor and multiple markers based on ‘ArToolKiT’ an augmented reality application. Various camera related distortion were reduced using 2nd order surface fit of camera calibration data. To increase the accuracy of the system, fusion of results from multiple markers has been implemented. Performance of the proposed localization has been verified by studying different pattern based movement of the system in a test area of 5m X 5m. Novelty of this paper is in development of an omni wheeled robotic wheelchair and proposing a robust absolute visual based localization set up with single camera and multiple fused markers for indoor navigation.}
}
@article{POONJA2023104720,
title = {Engagement detection and enhancement for STEM education through computer vision, augmented reality, and haptics},
journal = {Image and Vision Computing},
volume = {136},
pages = {104720},
year = {2023},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2023.104720},
url = {https://www.sciencedirect.com/science/article/pii/S026288562300094X},
author = {Hasnain Ali Poonja and Muhammad Ayaz Shirazi and Muhammad Jawad Khan and Kashif Javed},
keywords = {Engagement detection, Engagement enhancement, Computer vision, Augmented reality, Haptics, Online learning},
abstract = {STEM (Science, technology, engineering, and mathematics) based education is being revolutionized with the rapid development of the applications of intelligent Haptics. Theoretical knowledge may now be put into practice, and complex ideas can now be presented using three-dimensional models that are enhanced in the actual environment via Augmented Reality (AR) and computer Haptics to enhance the student's interest in concepts and studies. This article presents a novel design and development of an AR and Haptics-based STEM product comprising of World Map with Haptic feedback using Vuforia, Unity 3D, and Open-Haptics to enhance student engagement. To detect students' engagement in online learning, a computer vision-based system is also designed and deployed in the real-time website that uses the features, such as facial emotions, pose estimation, and head rotation. Marker-based augmentation is employed, and a sense of touch is included for an immersive experience for the students with the use of the haptic Touch Omni device. Hence, the proposed approach can improve classroom learning activities by using Augmented reality and Haptics-based STEM product and by its evaluation using a computer vision system. To validate the improvement in engagement, a comprehensive user study is performed and analyzed on the proposed setup.}
}
@article{LI2024108142,
title = {Design and development of a personalized virtual reality-based training system for vascular intervention surgery},
journal = {Computer Methods and Programs in Biomedicine},
volume = {249},
pages = {108142},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108142},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400138X},
author = {Pan Li and Boxuan Xu and Xinxin Zhang and Delei Fang and Junxia Zhang},
keywords = {Surgical training, Vascular intervention surgery, Virtual reality, Deformation simulation, Collision detection, Unity3D},
abstract = {Background and objectives
Virtual training has emerged as an exceptionally effective approach for training healthcare practitioners in the field of vascular intervention surgery. By providing a simulated environment and blood vessel model that enables repeated practice, virtual training facilitates the acquisition of surgical skills in a safe and efficient manner for trainees. However, the current state of research in this area is characterized by limitations in the fidelity of blood vessel and guidewire models, which restricts the effectiveness of training. Additionally, existing approaches lack the necessary real-time responsiveness and precision, while the blood vessel models suffer from incompleteness and a lack of scientific rigor.
Methods
To address these challenges, this paper integrates position-based dynamics (PBD) and its extensions, shape matching, and Cosserat elastic rods. By combining these approaches within a unified particle framework, accurate and realistic deformation simulation of personalized blood vessel and guidewire models is achieved, thereby enhancing the training experience. Furthermore, a multi-level progressive continuous collision detection method, leveraging spatial hashing, is proposed to improve the accuracy and efficiency of collision detection.
Results
Our proposed blood vessel model demonstrated acceptable performance with the reduced deformation simulation response times of 7 ms, improving the real-time capability at least of 43.75 %. Experimental validation confirmed that the guidewire model proposed in this paper can dynamically adjust the density of its elastic rods to alter the degree of bending and torsion. It also exhibited a deformation process comparable to that of real guidewires, with an average response time of 6 ms. In the interaction of blood vessel and guidewire models, the simulator blood vessel model used for coronary vascular intervention training exhibited an average response time of 15.42 ms, with a frame rate of approximately 64 FPS.
Conclusions
The method presented in this paper achieves deformation simulation of both vascular and guidewire models, demonstrating sufficient real-time performance and accuracy. The interaction efficiency between vascular and guidewire models is enhanced through the unified simulation framework and collision detection. Furthermore, it can be integrated with virtual training scenarios within the system, making it suitable for developing more advanced vascular interventional surgery training systems.}
}
@article{POULIN2024104023,
title = {Investigating the effects of spatial augmented reality on user participation in co-design sessions: A case study},
journal = {Computers in Industry},
volume = {154},
pages = {104023},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104023},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001732},
author = {Maud Poulin and Cédric Masclet and Jean-François Boujut},
keywords = {Collaborative design, Design cognition, Multimodal analysis, Virtual reality, Prototypes},
abstract = {New technologies such as Spatial Augmented Reality (SAR) have created new opportunities for including end users in the early phases of the design process when prototypes are not always available for functional testing. This study investigates the impact of introducing SAR technology on designers and end users working together in co-design sessions. To this end, a multi-modal analysis focusing on cognitive activities and gestural behaviour was conducted. After comparing the traditional setting with the SAR setting (over 44 sessions), the session activity profiles proved to be quite similar. However, the study highlights specific correlations between gestures and cognitive activities, in both environments. Finally, contrary to expectations, free gestures continue to be made in the digitally mediated situation.}
}
@article{ENKLER202483,
title = {Towards engineering a portable platform for laparoscopic pre-training in virtual reality with haptic feedback},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {83-99},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S209657962300075X},
author = {Hans-Georg Enkler and Wolfgang Kunert and Stefan Pfeffer and Kai-Jonas Bock and Steffen Axt and Jonas Johannink and Christoph Reich},
keywords = {Laparoscopic surgery, Training, Virtual reality, Controller, Haptic feedback, Kinesthetic skills},
abstract = {Background
Laparoscopic surgery is a surgical technique in which special instruments are inserted through small incision holes inside the body. For some time, efforts have been made to improve surgical pre-training through practical exercises on abstracted and reduced models.
Methods
The authors strive for a portable, easy to use and cost-effective Virtual Reality-based (VR) laparoscopic pre-training platform and therefore address the question of how such a system has to be designed to achieve the quality of today's gold standard using real tissue specimens. Current VR controllers are limited regarding haptic feedback. Since haptic feedback is necessary or at least beneficial for laparoscopic surgery training, the platform to be developed consists of a newly designed prototype laparoscopic VR controller with haptic feedback, a commercially available head-mounted display, a VR environment for simulating a laparoscopic surgery, and a training concept.
Results
To take full advantage of benefits such as repeatability and cost-effectiveness of VR-based training, the system shall not require a tissue sample for haptic feedback. It is currently calculated and visually displayed to the user in the VR environment. On the prototype controller, a first axis was provided with perceptible feedback for test purposes. Two of the prototype VR controllers can be combined to simulate a typical both-handed use case, e.g., laparoscopic suturing. A Unity-based VR prototype allows the execution of simple standard pre-trainings.
Conclusions
The first prototype enables full operation of a virtual laparoscopic instrument in VR. In addition, the simulation can compute simple interaction forces. Major challenges lie in a realistic real-time tissue simulation and calculation of forces for the haptic feedback. Mechanical weaknesses were identified in the first hardware prototype, which will be improved in subsequent versions. All degrees of freedom of the controller are to be provided with haptic feedback. To make forces tangible in the simulation, characteristic values need to be determined using real tissue samples. The system has yet to be validated by cross-comparing real and VR haptics with surgeons.}
}
@article{HORVAT2022101760,
title = {Immersive virtual reality applications for design reviews: Systematic literature review and classification scheme for functionalities},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101760},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101760},
url = {https://www.sciencedirect.com/science/article/pii/S147403462200218X},
author = {Nikola Horvat and Steffen Kunnen and Mario Štorga and Arun Nagarajah and Stanko Škec},
keywords = {Immersive virtual reality, Design review, Classification scheme, Functionalities, Systematic literature review, PRISMA},
abstract = {The development of immersive virtual reality (IVR) applications for design reviews is a major trend in the design field. While many different applications have been developed, there is little consensus on the functionalities necessary for these applications. This paper proposes a classification scheme for IVR functionalities related to design reviews (DRs), combining conceptual-to-empirical and empirical-to-conceptual strategies. The classification scheme consists of eight class categories (Input, Representation, Navigation, Manipulation, Collaboration, Edit, Creation, and Output), 22 class subcategories, and 55 classes. The classification scheme has been validated by analysing several commercial IVR applications for DRs. As part of the classification scheme development, Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) was utilised to review 70 articles that develop IVR applications for DRs. The results from systematic literature reviews suggest the development of solutions that integrate several class categories, are better connected to current design workflows, include various design information, support a DR planning cycle, and support distributed work. The proposed classification scheme helps to orient the future development of IVR applications for DRs and provides a framework to systematically accumulate evidence on the effect of such applications on DRs.}
}
@article{NEKTARIOSBOLIERAKIS2023201,
title = {Training on LSA lifeboat operation using Mixed Reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {3},
pages = {201-212},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000128},
author = {Spyridon {Nektarios Bolierakis} and Margarita Kostovasili and Lazaros Karagiannidis and Angelos Amditis},
keywords = {Augmented Reality, Mixed Reality, Maritime training, Cruise industry, Lifeboat operation, Lifeboat maintenance, H2020 research project},
abstract = {Background
This work aims to provide an overview of the Mixed Reality (MR) technology's use in maritime industry for training purposes. Current training procedures cover a broad range of procedural operations for Life-Saving Appliances (LSA) lifeboats; however, several gaps and limitations have been identified related to the practical training that can be addressed through the use of MR. Augmented, Virtual and Mixed Reality applications are already used in various fields in maritime industry, but their full potential have not been yet exploited. SafePASS project aims to exploit MR advantages in the maritime training by introducing a relevant application focusing on use and maintenance of LSA lifeboats.
Methods
An MR Training application is proposed supporting the training of crew members in equipment usage and operation, as well as in maintenance activities and procedures. The application consists of the training tool that trains crew members on handling lifeboats, the training evaluation tool that allows trainers to assess the performance of trainees, and the maintenance tool that supports crew members to perform maintenance activities and procedures on lifeboats. For each tool, an indicative session and scenario workflow are implemented, along with the main supported interactions of the trainee with the equipment.
Results
The application has been tested and validated both in lab environment and using a real LSA lifeboat, resulting to improved experience for the users that provided feedback and recommendations for further development. The application has also been demonstrated onboard a cruise ship, showcasing the supported functionalities to relevant stakeholders that recognized the added value of the application and suggested potential future exploitation areas.
Conclusions
The MR Training application has been evaluated as very promising in providing a user-friendly training environment that can support crew members in LSA lifeboat operation and maintenance, while it is still subject to improvement and further expansion.}
}
@article{SANAGUANOMORENO2024200306,
title = {Real-time impulse response: a methodology based on Machine Learning approaches for a rapid impulse response generation for real-time Acoustic Virtual Reality systems},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200306},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200306},
url = {https://www.sciencedirect.com/science/article/pii/S266730532300131X},
author = {D.A. Sanaguano-Moreno and J.F. Lucio-Naranjo and R.A. Tenenbaum and G.B. Sampaio-Regattieri},
keywords = {Auralization, Acoustic Virtual Reality, Variational Auto-encoders, Binaural Room Impulse Responses, Generative models, Long Short-Term Memory},
abstract = {Simulation of high-definition binaural room impulse responses using conventional approaches involves a significant amount of computational resources, resulting in high computational time, making these approaches incapable of performing real-time high quality acoustic virtual reality. This research implemented a methodology for the rapid impulse response generation using the position of a moving listener inside a fixed sound field. The rapid generation of the impulse response is performed using its representative compressed dimension, with a smaller dimension than the original impulse response, learned by variational autoencoders and long short-term memory neural networks. First, the methodology selects a representative number of impulse responses covering the area of interest using a reliable room acoustic simulator. Second, it generates a dataset with sufficient impulse responses uniformly distributed through a data augmentation approach using a modified bilinear interpolation from the impulse responses previously simulated. Third, it applies an unsupervised model to positionally cluster the impulse responses to reduce the variability of the impulse responses in the given environment. Fourth, it splits the impulse response into time segments and generates a dataset per segment and cluster. Fifth, it trains a variational autocoder with a long short-term memory neural network model for each time segment cluster of impulse responses to infer the correspondent compressed impulse response part. In summary, the impulse response is generated by assigning the current listener position to the corresponding cluster and executing the decoders of the variational autoencoders with long short-term memory, trained previously. The findings are encouraging; the normalized mean absolute error of the impulse responses gathered by the interpolator and the impulse responses generated by the proposed model is less than 15% in the 88% of impulse responses reserved for testing. Moreover, the average of the absolute error of the interaural cross-correlation coefficient between the impulse responses obtained from the simulator and the proposed model is around 16%, implying that most acoustic characteristics of the real impulse are preserved. In addition, the computational time for generating the impulse response segment of 300 ms is approximately 65 ms, which is almost haft than the total system latency for a realistic auralization, 112 ms.}
}
@article{CAMPO2024100605,
title = {MC-AR — A software suite for comparative mocap analysis in an augmented reality environment},
journal = {Software Impacts},
volume = {19},
pages = {100605},
year = {2024},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2023.100605},
url = {https://www.sciencedirect.com/science/article/pii/S2665963823001422},
author = {Adriaan Campo and Bavo {Van Kerrebroeck} and Marc Leman},
keywords = {Motion capture, Bayesian statistics, Augmented reality, Music, Interaction, HoloLens},
abstract = {Three different software packages are presented here: 1) A Unity package for simulating a virtual violinist in augmented reality on a HoloLens. The virtual violinist plays a pre-recorded piece, either as a 2D or a 3D projection. The piece can be started, stopped, forwarded, or rewound using a dedicated user interface. During interaction, eye movements are tracked. 2) A MATLAB motion capture package for analyzing the kinematic data of a user while interacting with the virtual violinist. 3) An R package for power analysis and Bayesian statistical analysis of the kinematic data. These software packages can be easily adapted to test the kinematic behavior of music students interacting with virtual teachers.}
}
@article{OCONNOR2023107963,
title = {Exploring the impact of augmented reality on student academic self-efficacy in higher education},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107963},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107963},
url = {https://www.sciencedirect.com/science/article/pii/S074756322300314X},
author = {Yvonne O'Connor and Carolanne Mahony},
keywords = {Academic self-efficacy, Augmented reality, Conceptual model, Cognitive strategies, Learning space},
abstract = {This study develops and empirically tests a conceptual model comprising eight hypotheses that focus on the impact of Augmented Reality (AR) on student academic self-efficacy. A controlled experiment was conducted, followed by an online questionnaire with 65 students. Partial Least Square (PLS) Structural Equation Modelling (SEM) is used to assess and test the model. The results show that student cognitive strategies impact student perception and engagement with technology and learning tasks, with AR positively impacting student academic self-efficacy in higher education. Interestingly, neither the learning space nor the students' perception of task value was shown to impact their intention to use AR. Instead, it was the characteristics of the technology itself which encouraged adoption. The study's findings reinforce the need to (1) teach and facilitate effective cognitive strategies among third level students and (2) identify and overcome negative cognitive strategies that harm student self-efficacy and engagement.}
}
@article{GILL2024100070,
title = {Implementing Universal Design through augmented-reality game-based learning},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100070},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000205},
author = {Amarpreet Gill and Derek Irwin and Dave Towey and Yanhui Zhang and Pinzhuang Long and Linjing Sun and Wanling Yu and Yaxin Zheng},
keywords = {Augmented reality, Digital game-based learning, Microlearning, Collaborative learning, Engineering education, Educational technology},
abstract = {Technology integration in higher education (HE) provides educators with the opportunity to design stimulating learning environments, especially in design and engineering education (DEE) where it plays a unique role in nurturing creativity, problem-solving and innovation. This study investigates the integration of an augmented reality (AR) educational game in DEE, focusing on the teaching of Design for Manufacturing and Assembly (DfMA) principles. The primary aim of this research is to enhance traditional DfMA teaching and learning (T&L) practices by applying innovative T&L strategies. The resulting AR DfMA game, developed using digital game-based learning, microlearning and collaborative learning techniques, aligns with the Universal Design for Learning framework to create an inclusive learning environment that encourages participation from all students and supports several learning styles. The study tests the individual features designed for personal study opportunities to discover which elements are optimal in that environment, and then on the proposed in-class group components of the AR game, involving 68 participants from the appropriate program, though it is not currently implemented as part of a module. We utilize a mixed methods approach to examine the students’ experience and identify key design features that contribute to an inclusive educational experience. The findings highlight positive student experiences, and preferences for hands-on engagement, multimodal content, and collaborative activities. The AR DfMA game also has the potential to enhance intrinsic motivation and create an active and inclusive learning environment. Challenges and areas for improvement are also discussed.}
}
@article{SEGOVIA2015195,
title = {Machining and Dimensional Validation Training Using Augmented Reality for a Lean Process},
journal = {Procedia Computer Science},
volume = {75},
pages = {195-204},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036996},
author = {Daniel Segovia and Hector Ramírez and Miguel Mendoza and Manuel Mendoza and Eloy Mendoza and Eduardo González},
keywords = {Augmented Reality, FARO Gage, Workshop Training, Dimensional Validation, Mobile Device, HMD Plant Managers},
abstract = {Quality control does not concern only to a finished product, nowadays measuring technologies control all the fabrication process in an active way. If product quality does not meet the customer specifications exists the risk of even lose projects. When multiple or complex operations are required in the fabrication process of a part and its dimensional validation is primordial and also happens that the operator has doubts about which step to follow or that the expert supervisor is not at the workshop for some reason, that is why comes the opportunity of implementing Augmented Reality (A.R.) technology in education and in a lean manufacturing process, as support for technician operators of machine-tools and coordinate-measuring machines, with the objective of helping the technician to perform, in a proper and timely manner, the step sequence of the process. It was proved that having A.R. as ally in the technician's education, the mistakes and time required to fulfill the machining and dimensioning of a part can be considerably reduced, as well as eliminate operator's dependency to the experts. At the end of the implementation, savings were found in the three stages analyzed, generating 27.36% savings in lathe process, 26.54% in milling and 45.16% in dimensional validation.}
}
@article{ZHOU202364,
title = {Design and validation of a navigation system of multimodal medical images for neurosurgery based on mixed reality},
journal = {Visual Informatics},
volume = {7},
number = {2},
pages = {64-71},
year = {2023},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X23000177},
author = {Zeyang Zhou and Zhiyong Yang and Shan Jiang and Tao Zhu and Shixing Ma and Yuhua Li and Jie Zhuo},
keywords = {Augmented reality, Neurosurgery, Image-guided intervention, Multimodal images},
abstract = {Purpose:
This paper aims to develop a navigation system based on mixed reality, which can display multimodal medical images in an immersive environment and help surgeons locate the target area and surrounding important tissues precisely.
Methods:
To be displayed properly in mixed reality, medical images are processed in this system. High-quality cerebral vessels and nerve fibers with proper colors are reconstructed and exported to mixed reality environment. Multimodal images and models are registered and fused, extracting their key information. The multiple processed images are fused with the real patient in the same coordinate system to guide the surgery.
Results:
The multimodal image system is designed and validated properly. In phantom experiments, the average error of preoperative registration is 1.003 mm and the standard deviation is 0.096 mm. The average proportion of well-registered areas is 94.9%. In patient experiments, the surgeons who participated in the experiments generally indicated that the system had excellent performance and great application prospect for neurosurgery.
Conclusion:
This article proposes a navigation system of multimodal images for neurosurgery based on mixed reality. Compared with other navigation methods, this system can help surgeons locate the target area and surrounding important tissues more precisely and rapidly.}
}
@article{MUKHOPADHYAY202255,
title = {Virtual-reality-based digital twin of office spaces with social distance measurement feature},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {1},
pages = {55-75},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000043},
author = {Abhishek Mukhopadhyay and GS Rajshekar Reddy and KamalPreet Singh Saluja and Subhankar Ghosh and Anasol Peña-Rios and Gokul Gopal and Pradipta Biswas},
keywords = {Virtual environment, Digital twin, 3D visualization, Convolutional neural network, Object detection, Social distancing},
abstract = {Background
Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
Methods
In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
Results
We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
Conclusions
A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.}
}
@article{LI2024200100,
title = {Application of VR motion intelligent capture based on DLPMA algorithm in sports training},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200100},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200100},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000292},
author = {Xiaojie Li},
keywords = {DLPMA, VR, Intelligent capture of actions, Sports training},
abstract = {With the rapid development of Virtual Reality (VR) technology, its application in the field of sports training is also receiving increasing attention. This study applies the Distance Likelihood Based Probabilistic Model Averaging (DLPMA) algorithm to the VR motion intelligent capture system, aiming to provide an efficient and accurate motion data collection method to improve existing sports training methods. Introduced the design and implementation of a VR motion intelligent capture system based on DLPMA algorithm, and applied it to sports training. By conducting comparative experiments with traditional training methods, the advantages of the system in motion capture accuracy, real-time performance, and user experience are verified. The research results indicate that the system can accurately capture the movements of athletes and provide timely feedback to users, providing an effective auxiliary means for sports training. Although the system has shown good performance in sports training, there are still some limitations. Future research can further optimize algorithms, enhance system stability and flexibility, to meet a wider range of sports training needs.}
}
@article{ABDI2015242,
title = {In-Vehicle Augmented Reality Traffic Information System: A New Type of Communication Between Driver and Vehicle},
journal = {Procedia Computer Science},
volume = {73},
pages = {242-249},
year = {2015},
note = {International Conference on Advanced Wireless Information and Communication Technologies (AWICT 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034857},
author = {Lotfi Abdi and Faten Ben Abdallah and Aref Meddeb},
keywords = {Augmented Reality, Region Of Interest, Driving-Safety, Head-up Display, Camera Calibration, Traffic Information, OpenCV.},
abstract = {In order to improve driving safety and minimize driving workload, the information provided should be represented in such a way that it is more easily understood and imposing less cognitive load onto the driver. Augmented Reality Head-up Display (AR- HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance intelligent transportation systems by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper, we investigated the potential costs and benefits of using AR cues to improve driving safety as new form of dialog between the vehicle and the driver. We present a new approach for marker-less AR Traffics Signs Recognition system that superimposes augmented virtual objects onto a real scene under all types of driving situations, including unfavorable weather conditions. Our method uses two steps: hypothesis generation and hypothesis verification. In the first step, Region Of Interest (ROI) is extracted using a scanning window with Haar cascade detector and AdaBoost classifier to reduce the computational region in the hypothesis generation step. The second step verifies whether a given candidate and classified into vehicle and non-vehicle classes using edge information and symmetry measurement to verify them. We employ this approach to improve the accuracy of AR traffic information system to assist the driver in various driving situations, increase the driving comfort and reduce traffic accidents.}
}
@article{FLASH2024105458,
title = {Assessing the usability of an immersive virtual reality grocery store in healthy controls},
journal = {International Journal of Medical Informatics},
volume = {187},
pages = {105458},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105458},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001217},
author = {Sara Flash and Denise M. Goldsmith and Tanna L. Nelson and William Thompson and Patricia {Flatley Brennan}},
keywords = {Immersive Virtual Reality, User Experience Testing, Usability Testing, Virtual Grocery Store},
abstract = {Background
Immersive virtual reality (IVR) as a research platform to study human behaviors is an emerging field and may be useful for studying self-care management, especially in the gap between formal healthcare recommendations and day-to-day living. Self-care activities, such as grocery shopping, can be challenging for people with chronic illness. We developed an IVR environment that simulates a real-life grocery store and conducted a usability study to demonstrate the safety and acceptability of IVR as an experimental environment.
Methods
This study was a three-arm randomized control trial involving 24 participants, conducted as a usability study to evaluate aspects of the experimental condition including the effectiveness of a training exposure, the occurrence of undesirable effects associated with IVR, and participants’ experiences of realism, immersion, and spatial presence. The experiment, using a head mounted device and handheld controllers, included a 10-minute training exposure, followed by one of three unique 30-minute experimental conditions which exposed participants to different combinations of tasks and stimuli, and a post-experience interview. We measured controller errors, undesirable symptoms associated with IVR, and the perception of realism, immersion, and spatial presence.
Results
Participants used controllers effectively to interact within the IVR environment. Hand controller use errors were fewer during the experimental conditions compared to the training exposure. Minimal undesirable IVR symptoms were reported. Presence was rated in the middle range with no significant differences based on experimental condition. Overall, user experience feedback was positive.
Conclusions
We demonstrated that participants could engage in our IVR environment without excessive error or experiencing undesirable effects and confirmed that the virtual experience attained a level of presence necessary to effectively engage in the study. These findings give us confidence that this IVR intervention designed to explore instrumental activities of daily living is safe, effective and provides a credible, controlled simulated community-like setting.}
}
@article{BV20242693,
title = {ARIA: Augmented Reality and Artificial Intelligence enabled mobile application for Yield and grade prediction of tomato crops},
journal = {Procedia Computer Science},
volume = {235},
pages = {2693-2702},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.254},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400930X},
author = {Balaji Prabhu {B V} and Shashank R and Shreyas B and Omkar Subbaram {Jois Narsipura}},
keywords = {tomato detection, tomato grading, tomato yield prediction, augmented reality, deep learning},
abstract = {Agriculture is the most crucial sector of the Indian economy. Lately, there has been a surge in the usage of technologies like deep learning and computer vision to make the process of agriculture modern and consequently, lessening the mistakes related to conventional processes. This work delivers a user-friendly, accessible, and novel approach for the detection, counting, and grading of tomatoes found on a farm. An Augmented Reality (AR) based mobile application is developed to obtain the images efficiently from a tomato farm in the pre-harvest stage subjected to open situations. The proposed approach uses Faster RCNN, a convolutional neural network model for detection of tomatoes from the input image on a large scale. The proposed model is trained and tested using 2083 images. The results are then analyzed for the overall performance of detection, segmentation, and classification of tomatoes. The results have examined the efficiency of the proposed mobile application and demonstrate the robustness it exhibits for the detection, grading and yield prediction of Tomatoes.}
}
@article{TSEPAPADAKIS2023101797,
title = {Are you talking to me? An Audio Augmented Reality conversational guide for cultural heritage},
journal = {Pervasive and Mobile Computing},
volume = {92},
pages = {101797},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101797},
url = {https://www.sciencedirect.com/science/article/pii/S157411922300055X},
author = {Michalis Tsepapadakis and Damianos Gavalas},
keywords = {Audio Augmented Reality, Conversational audio guide, AI chatbot, Cultural heritage, Cultural user experience, IoT, Context-awareness, Spatial sound, User evaluation, IBM watson, Exhibot},
abstract = {Augmented Reality (AR) technologies are increasingly utilized as a means of stimulating immersive experiences to cultural site visitors, mainly through visual superimposition of interactive digital elements onto the physical world. Recent research has investigated the use of Audio AR (AAR) in heritage sites, wherein visitors listen to spatially registered sound which could be attributed to ‘talking’ physical artefacts. A parallel trend in the audience engagement programs of cultural institutions involves the employment of AI chatbots which are engaged in dialogues with followers or visitors to provide meaningful responses to a number of user questions. Herein, we present Exhibot, an intelligent audio guide system aiming at enhancing the user experience of cultural site visitors. Exhibot involves the combination of AAR and chatbot technologies to enable natural visitor-exhibit interaction, while also leveraging IoT devices to contextualize the delivered information. The key contribution of the proposed system lies in the interplay of AAR, chatbot and IoT technologies to create immersive learning experiences in the context of an integrated cultural guide system. Exhibot has undergone field trials to validate its usability and utility in realistic operational conditions. As a case study, we have chosen the statue of a prominent politician situated at a central square in Heraklion, Greece. The evaluation results indicated a very positive attitude of users, which is attributed both to the sense of immersion evoked by the AAR-powered storytelling and the natural human-like conversation enabled by the chatbot.}
}
@article{WANG2022100351,
title = {BIM Information Integration Based VR Modeling in Digital Twins in Industry 5.0},
journal = {Journal of Industrial Information Integration},
volume = {28},
pages = {100351},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100351},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200022X},
author = {Weixi Wang and Han Guo and Xiaoming Li and Shengjun Tang and You Li and Linfu Xie and Zhihan Lv},
keywords = {Digital Twins, Building Information Modeling (BIM), VR Modeling, Information Integration, Industry 5.0},
abstract = {This study aims to improve the efficiency of construction efficiency to ensure the infrastructure needs of urban development. Targeting at realizing the intelligent manufacturing of buildings, the digital twins are applied to all stages of building construction, and the three-dimensional (3D) modeling is implemented. A digital twins framework in the construction field is constructed based on the building information modeling (BIM). From physical construction site entity modeling (PCSE modeling) and digital twins virtual body modeling (DTVB modeling) to virtual and real interaction modeling (VRI modeling), the digital twins process is fully applied to the various stages of building construction. The performance of the method proposed is further verified through the analysis of specific data. It is found that the model proposed shows better performance on the same platform and node compared with other models, and it shows the best scalability in the iterative calculation test on the Flink platform through efficient memory management and incremental iteration mechanism. In addition, the model has a delay of less than 100 milliseconds: in the Sleep scenario, the system delay of the latest model is about 5 times that of on the Flink, and the system interruption probability will firstly decrease and then increase when the time division factor (TDF) is increased. This study provides important reference value for the intelligent development of the construction industry and the high-quality development of buildings.}
}
@article{ZHU2022105091,
title = {A neuroendoscopic navigation system based on dual-mode augmented reality for minimally invasive surgical treatment of hypertensive intracerebral hemorrhage},
journal = {Computers in Biology and Medicine},
volume = {140},
pages = {105091},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105091},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008854},
author = {Tao Zhu and Shan Jiang and Zhiyong Yang and Zeyang Zhou and Yuhua Li and Shixing Ma and Jie Zhuo},
keywords = {Augmented reality, Image-guided intervention, Computer assisted surgery, Neuroendoscopic surgery, Hypertensive intracerebral hemorrhage},
abstract = {Background and objective
Hypertensive intracerebral hemorrhage is characterized by a high rate of morbidity, mortality, disability and recurrence. Neuroendoscopy has been utilized for treatment as an advanced technology. However, traditional neuroendoscopy allows professionals to see only tissue surfaces, and the field of vision is limited, which cannot provide spatial guidance. In this study, an AR-based neuroendoscopic navigation system is proposed to assist surgeons in locating and clearing hematoma.
Methods
The neuroendoscope can be registered through the vector closed loop algorithm. The single-shot method is designed to register medical images with patients precisely. Real-time AR is realized based on video stream fusion. Dual-mode AR navigation is proposed to provide comprehensive guidance from catheter implantation to hematoma removal. A series of experiments is designed to validate the accuracy and significance of this system.
Results
The average root mean square error of the registration between medical images and patients is 0.784 mm, and the variance is 0.1426 mm. The pixel mismatching degrees are less than 1% in different AR modes. In catheter implantation experiments, the average error of distance is 1.28 mm, and the variance is 0.43 mm, while the average error of angles is 1.34°, and the variance is 0.45°. Comparative experiments are also conducted to evaluate the feasibility of this system.
Conclusion
This system can provide stereo images with depth information fused with patients to guide surgeons to locate targets and remove hematoma. It has been validated to have high accuracy and feasibility.}
}
@article{KROPIK20101605,
title = {Software for protection system of VR-1 training reactor},
journal = {Journal of Systems and Software},
volume = {83},
number = {9},
pages = {1605-1611},
year = {2010},
note = {Software Dependability},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S0164121209001629},
author = {Martin Kropik and Monika Jurickova},
keywords = {Nuclear safety, Protection system, Quality assurance of software, Verification and validation},
abstract = {The article deals with the software for a new protection system of the VR-1 training reactor which consists of the independent power protection and the operational power measuring systems. Both systems are computer-based, and they are diverse in sensors, hardware and software. They are responsible for nuclear safety of the reactor, so the quality requirements for their software are strict. The software was developed in accordance with nuclear standards. During the development, both software products were carefully tested, and after the integration of hardware/software, they were validated with the simulation of input signals and the checking of their responses.}
}
@article{HUA2024108218,
title = {EEG classification model for virtual reality motion sickness based on multi-scale CNN feature correlation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {251},
pages = {108218},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108218},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400213X},
author = {Chengcheng Hua and Jianlong Tao and Zhanfeng Zhou and Lining Chai and Ying Yan and Jia Liu and Rongrong Fu},
keywords = {Channel attention, Feature correlation matrix, Multi-scale feature fusion, Resting state EEG, Virtual reality motion sickness},
abstract = {Background
Virtual reality motion sickness (VRMS) is a key issue hindering the development of virtual reality technology, and accurate detection of its occurrence is the first prerequisite for solving the issue.
Objective
In this paper, a convolutional neural network (CNN) EEG detection model based on multi-scale feature correlation is proposed for detecting VRMS.
Methods
The model uses multi-scale 1D convolutional layers to extract multi-scale temporal features from the multi-lead EEG data, and then calculates the feature correlations of the extracted multi-scale features among all the leads to form the feature adjacent matrixes, which converts the time-domain features to correlation-based brain network features, thus strengthen the feature representation. Finally, the correlation features of each layer are fused. The fused features are then fed into the channel attention module to filter the channels and classify them using a fully connected network. Finally, we recruit subjects to experience 6 different modes of virtual roller coaster scenes, and collect resting EEG data before and after the task to verify the model. Results: The results show that the accuracy, precision, recall and F1-score of this model for the recognition of VRMS are 98.66 %, 98.65 %, 98.68 %, and 98.66 %, respectively. The proposed model outperforms the current classic and advanced EEG recognition models.
Significance
It shows that this model can be used for the recognition of VRMS based on the resting state EEG.}
}
@article{XIE2024106189,
title = {Electroencephalography-based recognition of six basic emotions in virtual reality environments},
journal = {Biomedical Signal Processing and Control},
volume = {93},
pages = {106189},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106189},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002477},
author = {Jialan Xie and Yutong Luo and Shiyuan Wang and Guangyuan Liu},
keywords = {Classification, Emotion recognition, Electroencephalography (EEG), Six basic emotions, Virtual reality},
abstract = {Emotion recognition (ER) in virtual reality (VR) environments has attracted increasing attention in the field of affective computing and human–computer interaction (HCI). Electroencephalography (EEG) is an effective tool for capturing human emotions and mental states, and the demand for collecting and analyzing EEG signals in VR environments has been on the increase. In this study, we built an EEG dataset for six basic emotions in VR environments—that is, the VR six discrete emotional EEG data (VRSDEED) dataset—and developed a machine-learning framework for classifying emotional states using it. Fifty participants were shown six VR emotion-inducing videos while their EEG signals were recorded. To determine the optimal features and classification methods, we extracted four frequency-domain features and applied five different machine-learning algorithms. To enhance prediction performance, we fused different features and used recursive feature elimination with cross-validation for feature selection. Classification performance was assessed using accuracy and F1-scores. Our findings demonstrated that (1) the light gradient-boosting machine algorithm achieved the highest average classification accuracy of 89.94%; (2) isolated features achieved the highest accuracy of 89.50% in the γ frequency band and feature fusion enhanced recognition performance; (3) the θ and γ frequency bands were more significant in emotion processing than other bands. The proposed dataset and framework should serve as valuable resources and provide insights for research on ER in VR environments.}
}
@article{MUNSINGER2023103368,
title = {Virtual reality for improving cyber situational awareness in security operations centers},
journal = {Computers & Security},
volume = {132},
pages = {103368},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103368},
url = {https://www.sciencedirect.com/science/article/pii/S016740482300278X},
author = {Brita Munsinger and Nicole Beebe and Turquoise Richardson},
keywords = {Cyber situational awareness, Virtual reality, Security operations centers, Network monitoring},
abstract = {Security operations centers (SOCs) are the 911 centers of many organizational networks, except they not only respond, but also monitor. SOC operators are charged with detection, response, and mitigation. This is a tall task when one considers the volume, velocity, and variety of both internal and external organizational network and system data. SOC operations are truly a big data problem. Security orchestration, incident event management, data fusion, and anomaly detection systems help, but more is needed. This study examines the impact virtual reality (VR) can have on SOC operator performance and perceived task load. We developed a VR based network monitoring tool and assigned human subjects to one of three conditions – VR only, traditional tool only, or both. Our results, though small in scale, provide very promising indication that VR based technology may be beneficial for improving cyber situational awareness (SA), particularly with overall data perception involving novice SOC operators. The results are promising, but the sample size is small, so future research should validate this pilot study. Given the workforce challenges in the cybersecurity space, and the need to perceive large quantities of data, VR may be a very good addition to SOCs.}
}
@article{CHOKWITTHAYA2023101903,
title = {Ontology for experimentation of human-building interactions using virtual reality},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101903},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101903},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000319},
author = {Chanachok Chokwitthaya and Yimin Zhu and Weizhuo Lu},
keywords = {Ontology, Virtual human-building interaction, Competency question, EXPO, DOGMA},
abstract = {Scientific experiments significantly enhance the understanding of human-building interactions in building and engineering research. Recently, conducting virtual reality (VR) experiments has gained acceptance and popularity as an approach to studying human-building interactions. However, little attention has been given to the standardization of the experimentations. Proper standardization can promote the reusability, replicability, and repeatability of VR experiments and accelerate the maturity of this emerging experimentation method. Responding to such needs, the authors proposed a virtual human-building interaction experimentation ontology (VHBIEO). It is an ontology at the domain level, extending the ontology of scientific experiments (EXPO) to standardize virtual human-building interaction experimentation. It was developed based on state-of-the-art ontology development approaches. Competency questions (CQs) were used to derive requirements and regulate the development. Semantic Web technologies were applied to make VHBIEO machine-readable, accessible, and processable. VHBIEO incorporates an application view (APV) to support the inclusion of unique information for particular applications. The authors performed taxonomy evaluations to assess the consistency, completeness, and redundancy, affirming no occurrence of errors in its structure. Application evaluations were applied for investigating its ability to standardize and support generating of machine-readable, accessible, and processable information. Application evaluations also verified the capability of APV to support the inclusion of unique information.}
}
@article{LI202373,
title = {X-Space: Interaction design of extending mixed reality space from Web2D visualization},
journal = {Visual Informatics},
volume = {7},
number = {4},
pages = {73-83},
year = {2023},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2023.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X23000463},
author = {Tiemeng Li and Songqian Wu and Yanning Jin and Haopai Shi and Shiran Liu},
keywords = {Mobile and ubiquitous visualization, Visualization system and toolkit design, Immersive analytic, Information data visualization, Human–computer interaction},
abstract = {Mixed reality offers a larger visualization space and more intuitive means of interaction for data exploration, and many works have been dedicated to combining 2D visualizations on screen with mixe reality. However, for each combination, we need to customize the implementation of the corresponding mixed reality 3D visualization. It is a challenge to simplify this development process and enable agile building of mixed reality 3D visualizations for 2D visualizations. In addition, many existing 2D visualizations do not provide interfaces oriented to immersive analytics, so how to extend the mixed reality 3D space from existing 2D visualizations is another challenge. This work presents an agile and flexible approach to interactively transfer visualizations from 2D screens to mixed reality 3D spaces. We designed an interactive process for spatial generation of mixed-reality 3D visualizations, defined a unified data transfer framework, integrated data deconstruction techniques for 2D visualizations, implemented interfaces to immersive visualization building tool-kits, and encapsulated these techniques into a tool named X-Space. We validated that the approach is feasible and effective through 2D visualization cases including scatter plots, stacked bar charts, and adjacency matrix. Finally, we conducted expert interviews to discuss the usability and value of the method.}
}
@article{CHRYSSOLOURIS2000267,
title = {A virtual reality-based experimentation environment for the verification of human-related factors in assembly processes},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {16},
number = {4},
pages = {267-276},
year = {2000},
issn = {0736-5845},
doi = {https://doi.org/10.1016/S0736-5845(00)00013-2},
url = {https://www.sciencedirect.com/science/article/pii/S0736584500000132},
author = {George Chryssolouris and Dimitris Mavrikios and Dimitris Fragos and Vassiliki Karabatsou},
keywords = {Virtual reality, Virtual assembly, Ergonomics, Process modelling},
abstract = {This paper investigates the use of virtual reality (VR)-based methods for the verification of performance factors related to manual assembly processes. An immersive and interactive virtual environment has been created to provide functionality for realistic process experimentation. Ergonomic models and functions have been embedded into the VR environment to support verification and constrain experimentation to ergonomically acceptable conditions. A specific assembly test case is presented, for which a semi-empirical time model is developed employing statistical design experimentation in the virtual environment. The virtual experimentation results enable the quantification and prediction of the influence of a number of process parameters and their combination at the process cycle time.}
}
@article{YIN2023102203,
title = {An empirical study of an MR-enhanced kinematic prototyping approach for articulated products},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102203},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102203},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003312},
author = {Yue Yin and Pai Zheng and Chengxi Li and Jingchen Cong and Yat Ming Pang},
keywords = {Kinematic prototyping, Articulated product, Mixed reality, Augmented reality, Conceptual design},
abstract = {In the conceptual design and early embodiment design stages, an effective kinematic prototype can assist in detecting design errors, minimizing unnecessary parameter modifications, and optimizing subsequent resource deployment. Existing methods for creating prototypes, such as 2D sketches, 3D modelling, or analysis software, have their advantages in precise modelling and simulation. However, there is a lack of an efficient approach that can achieve both rapid 3D kinematic idea generation and early on-site interactive verification. To address these challenges, this research proposes a Mixed Reality (MR)-enhanced kinematic prototyping approach for articulated products to enable the rapid generation of 3D kinematic concepts, movement preview, and early interactions with users or the actual workspace. The proposed approach facilitates the quick and rough construction of geometry and kinematic structures through intuitive hand gestures and interactive movement preview within the MR environment. The usability of the proposed approach was evaluated by a comparison experiment with traditional sketch and CAD tools involving eight participants. It is verified that our proposed MR kinematic prototyping system could accelerate the kinematic prototype generation process, improve the user’s kinematic understanding, and facilitate on-site interactive verification in advance.}
}
@article{LI20221516,
title = {Non-photorealistic Visualization of 3D City Models using Visual Variables in Virtual Reality Environments},
journal = {Procedia Computer Science},
volume = {214},
pages = {1516-1521},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.338},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020506},
author = {Bingchan Li and Zhangsong Luo and Bo Mao},
keywords = {visual variable, 3D city model, non-photorealistic visualization, virtual reality},
abstract = {Visual variables are important factors for visualization in 2D maps, and the study of non-photorealistic visualization of 3D city models in virtual environments using visual variables is also necessary to represent the attributes of city such as energy consumption. This study proposed a set of visual variable mapping model based on 3D geographical environment and conducted user cognitive test. The goal of this research is to verify the applicability of visual variables in 3D space and explore the impact of disturbing visual variables on user cognition. The results show that visualization of compound visual variables in 3D space is more effective than single variable mapping, and it makes up for the shortcomings of single mapping. It is effective to study the visualization effect in three-dimensional geographical environment using virtual reality technology. The results from this case study provide guidance for 3D user-centric visualization technology and make contribute to the development of smart city.}
}
@article{LENG2024107940,
title = {Development of a virtual reality-based zygomatic implant surgery training system with global collision detection and optimized finite element method model},
journal = {Computer Methods and Programs in Biomedicine},
volume = {243},
pages = {107940},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107940},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723006065},
author = {Ao Leng and Bolun Zeng and Yizhou Chen and Puxun Tu and Baoxin Tao and Xiaojun Chen},
keywords = {Computer-assisted surgery, Virtual reality, Zygomatic implant surgery, Surgical training},
abstract = {Background and objective
Zygomatic implant surgery is challenging due to the complex structure of the zygomatic bone, limited visual range during surgery, and lengthy implant path. Moreover, traditional training methods are costly, and experimental subjects are scarce.
Methods
To overcome these challenges, we propose a novel training system that integrates visual, haptic, and auditory feedback to create a more immersive surgical experience. The system uses dynamic bounding volume hierarchy (BVH) and Symplectic Euler to detect global collisions between surgical tools and models, while an optimized finite element method (FEM) model simulates soft tissue and detects collisions. Compared to previous works, our system achieves global rigid-body collisions between surgical tools and patient models, while also providing stable and realistic simulation and collisions of soft tissues. This advancement offers a more realistic simulation for zygomatic implant surgery.
Results
We conducted three experiments and evaluations. The first experiment measured the axial force generated during the zygomatic implant simulation process and compared it with actual surgery, demonstrating the realistic force rendering feedback of our system. The second evaluation involved 15 novice surgeons who experienced the system and completed a questionnaire survey focusing on five aspects. The results showed satisfactory evaluations. The third experiment involved six surgeons who underwent in-depth training for two hours daily and were tested on the first, third, and fifth days. We collected data and combined it with the doctors' feedback to prove that our system can improve surgeons' proficiency in zygomatic implant surgery and provide a novel training solution for this procedure.
Conclusion
We have innovatively integrated global collision detection and optimized soft tissue simulation into our system. Furthermore, we have conducted experimental validation to demonstrate the effectiveness of this implementation. We present a novel solution for zygomatic implant surgery training.}
}
@article{CHAN2023105042,
title = {A comparative analysis of digital health usage intentions towards the adoption of virtual reality in telerehabilitation},
journal = {International Journal of Medical Informatics},
volume = {174},
pages = {105042},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105042},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623000606},
author = {Yee Kiu Chan and Yuk Ming Tang and Long Teng},
keywords = {Telerehabilitation, Virtual reality, Behavioral intention, E-rehabilitation, Digital health},
abstract = {Background
With the rapid development of the metaverse and the problem of non-attendance in traditional rehabilitation, virtual reality in telerehabilitation has become increasingly vital in modern medicine. However, research on determining predictors that influence the public's behavioral intention to adopt VR-based telerehabilitation has not been extensively studied.
Objective
This study aims to propose a new research model with a comparative analysis on understanding factors affecting the public's behavioral intention to adopt VR in telerehabilitation for different user groups.
Methods
A total of 215 respondents from the general public completed an online questionnaire to validate the proposed research model. The collected data was analyzed using SPSS and AMOS. The proposed model was additionally validated using CFA and multiple linear regression.
Results
This study found that effort expectancy, threat appraisals, and trust had a positive significant influence on the public’s behavioral intention to adopt VR in telerehabilitation. However, performance expectancy and facilitating conditions had no significant relationship with behavioral intention. Notably, the average of the primary factors for older adults was generally higher than for younger adults.
Conclusions
The present study confirms the applicability of the proposed research model. Our findings contribute up-to-date insights for related stakeholders to minimize implementation failures and develop successful adoption strategies for the future expansion of telerehabilitation.}
}
@article{ANDERIES2023573,
title = {Implementation of Augmented Reality in Android-based Application to Promote Indonesian Tourism},
journal = {Procedia Computer Science},
volume = {227},
pages = {573-581},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.560},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017271},
author = { Anderies and Maevy Marvella and Nissa Adila Hakim and Priskilla Adriani Seciawanto and Andry Chowanda},
keywords = {Augmented Reality, Indonesian Tourism, Android, Usability Testing},
abstract = {Indonesia is known for its iconic tourism industry. However, there is a significant popularity difference between each destination. This imbalance has been a problem that the Indonesian Ministry of Tourism has been trying to solve through the 'Wonderful Indonesia" campaign. Unfortunately, some did not perceive it as a success. With the breakthrough of Augmented Reality (AR) that is seen as a great factor in enhancing user experience in the tourism sector, this research aims to design and develop an immersive android-based application that implements Augmented Reality and Audio Guidance to promote and create branding for Indonesian tourism spots. This research uses a methodology divided into 4 phases; Analysis Requirement, Research and Collect Data, System Design and Development, and Testing. A survey was conducted in the Analysis Requirement phase to find people's knowledge and preference for tourism spots in Indonesia. Another survey was conducted in the Testing phase to test the application's usability and whether the aim has been achieved. The main findings indicate that most local tourist respondents stated that the application developed is an effective way for them to explore and educate themselves about Indonesian tourism sites. Therefore, this application might be a sustainable answer and a new approach compared to what the Indonesian government has done in the past years.}
}
@article{VANKIPURAM2010661,
title = {A virtual reality simulator for orthopedic basic skills: A design and validation study},
journal = {Journal of Biomedical Informatics},
volume = {43},
number = {5},
pages = {661-668},
year = {2010},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410000857},
author = {Mithra Vankipuram and Kanav Kahol and Alex McLaren and Sethuraman Panchanathan},
keywords = {Simulator validation, Medical information system, Virtual reality, Orthopedic haptic simulators},
abstract = {Orthopedic drilling as a skill demands high levels of dexterity and expertise from the surgeon. It is a basic skill that is required in many orthopedic procedures. Inefficient drilling can be a source of avoidable medical errors that may lead to adverse events. It is hence important to train and evaluate residents in safe environments for this skill. This paper presents a virtual orthopedic drilling simulator that was designed to provide visiohaptic interaction with virtual bones. The simulation provides a realistic basic training environment for orthopedic surgeons. It contains modules to track and analyze movements of surgeons, in order to determine their surgical proficiency. The simulator was tested with senior surgeons, residents and medical students for validation purposes. Through the multi-tiered testing strategy it was shown that the simulator was able to produce a learning effect that transfers to real-world drilling. Further, objective measures of surgical performance were found to be able to differentiate between experts and novices.}
}
@article{MOON2021108026,
title = {AR and ARMA model order selection for time-series modeling with ImageNet classification},
journal = {Signal Processing},
volume = {183},
pages = {108026},
year = {2021},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2021.108026},
url = {https://www.sciencedirect.com/science/article/pii/S0165168421000657},
author = {Jihye Moon and Md Billal Hossain and Ki H. Chon},
keywords = {ImageNet classification, Convolutional neural network, Time series modeling, AR model identification, ARMA model identification, Deep neural network, Physical system modeling},
abstract = {We propose model order selection methods for autoregressive (AR) and autoregressive moving average (ARMA) time-series modeling based on ImageNet classifications with a 2-dimensional convolutional neural network (2-D CNN). We designed two models for two realistic scenarios: (1) a general model which emulates the scenario that validation and test datasets do not necessarily have the same dynamics as the training data, (2) a specific model which emulates the opposite scenario—the validation and test datasets share the dynamics of the training data. The results were compared to those of both Akaike Information criterion (AIC) and Bayesian Information criterion (BIC). Using simulation examples, we trained 2-D CNN-based Inception-v3 and ResNet50-v2 models for either AR or ARMA order selection for each of the two scenarios. The proposed ResNet50-v2 to use both time-frequency and the original time series data outperformed AIC and BIC for all scenarios. For the general model, the average of relative error reduction (ARER) when compared to the BIC method in the clean and three noisy environments was 19.07% (±14.22%) for the AR order for an AR process, and 5.67% (±2.83%) for the ARMA order for an ARMA process. The ARERs significantly improved to 73.92% (±30.95%) and 65.58% (±38.61%) for the AR and ARMA models, respectively, for the specific model scenario.}
}
@article{PORTALES2023125,
title = {Mixed Reality Annotation of Robotic-Assisted Surgery videos with real- time tracking and stereo matching},
journal = {Computers & Graphics},
volume = {110},
pages = {125-140},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322002291},
author = {Cristina Portalés and Jesús Gimeno and Antonio Salvador and Alfonso García-Fadrique and Sergio Casas-Yrurzum},
keywords = {Mixed Reality, Annotation, Robotic-Assisted Surgery, Tracking, Stereo matching},
abstract = {Robotic-Assisted Surgery (RAS) is beginning to unlock its potential. However, despite the latest advances in RAS, the steep learning curve of RAS devices remains a problem. A common teaching resource in surgery is the use of videos of previous procedures, which in RAS are almost always stereoscopic. It is important to be able to add virtual annotations onto these videos so that certain elements of the surgical process are tracked and highlighted during the teaching session. Including virtual annotations in stereoscopic videos turns them into Mixed Reality (MR) experiences, in which tissues, tools and procedures are better observed. However, an MR-based annotation of objects requires tracking and some kind of depth estimation. For this reason, this paper proposes a real-time hybrid tracking–matching method for performing virtual annotations on RAS videos. The proposed method is hybrid because it combines tracking and stereo matching, avoiding the need to calculate the real depth of the pixels. The method was tested with six different state-of-the-art trackers and assessed with videos of a sigmoidectomy of a sigma neoplasia, performed with a Da Vinci® X surgical system. Objective assessment metrics are proposed, presented and calculated for the different solutions. The results show that the method can successfully annotate RAS videos in real-time. Of all the trackers tested for the presented method, the CSRT (Channel and Spatial Reliability Tracking) tracker seems to be the most reliable and robust in terms of tracking capabilities. In addition, in the absence of an absolute ground truth, an assessment with a domain expert using a novel continuous-rating method with an Oculus Quest 2 Virtual Reality device was performed, showing that the depth perception of the virtual annotations is good, despite the fact that no absolute depth values are calculated.}
}
@article{MINISSI2024108194,
title = {Biosignal comparison for autism assessment using machine learning models and virtual reality},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108194},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108194},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002786},
author = {Maria Eleonora Minissi and Alberto Altozano and Javier Marín-Morales and Irene Alice {Chicchi Giglioli} and Fabrizia Mantovani and Mariano Alcañiz},
keywords = {Virtual reality, Statistical machine learning, Biosignal, Autism spectrum disorder, Eye movements, Motor skills},
abstract = {Clinical assessment procedures encounter challenges in terms of objectivity because they rely on subjective data. Computational psychiatry proposes overcoming this limitation by introducing biosignal-based assessments able to detect clinical biomarkers, while virtual reality (VR) can offer ecological settings for measurement. Autism spectrum disorder (ASD) is a neurodevelopmental disorder where many biosignals have been tested to improve assessment procedures. However, in ASD research there is a lack of studies systematically comparing biosignals for the automatic classification of ASD when recorded simultaneously in ecological settings, and comparisons among previous studies are challenging due to methodological inconsistencies. In this study, we examined a VR screening tool consisting of four virtual scenes, and we compared machine learning models based on implicit (motor skills and eye movements) and explicit (behavioral responses) biosignals. Machine learning models were developed for each biosignal within the virtual scenes and then combined into a final model per biosignal. A linear support vector classifier with recursive feature elimination was used and tested using nested cross-validation. The final model based on motor skills exhibited the highest robustness in identifying ASD, achieving an AUC of 0.89 (SD = 0.08). The best behavioral model showed an AUC of 0.80, while further research is needed for the eye-movement models due to limitations with the eye-tracking glasses. These findings highlight the potential of motor skills in enhancing objectivity and reliability in the early assessment of ASD compared to other biosignals.}
}
@article{GATTULLO2019276,
title = {Towards augmented reality manuals for industry 4.0: A methodology},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {56},
pages = {276-286},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518301236},
author = {Michele Gattullo and Giulia Wally Scurati and Michele Fiorentino and Antonio Emmanuele Uva and Francesco Ferrise and Monica Bordegoni},
keywords = {Augmented reality, Industry 4.0, Maintenance support, Technical documentation},
abstract = {Augmented Reality (AR), is one of the most promising technology for technical manuals in the context of Industry 4.0. However, the implementation of AR documentation in industry is still challenging because specific standards and guidelines are missing. In this work, we propose a novel methodology for the conversion of existing “traditional” documentation, and for the authoring of new manuals in AR in compliance to Industry 4.0 principles. The methodology is based on the optimization of text usage with the ASD Simplified Technical English, the conversion of text instructions into 2D graphic symbols, and the structuring of the content through the combination of Darwin Information Typing Architecture (DITA) and Information Mapping (IM). We tested the proposed approach with a case study of a maintenance manual of hydraulic breakers. We validated it with a user test collecting subjective feedbacks of 22 users. The results of this experiment confirm that the manual obtained using our methodology is clearer than other templates.}
}
@article{LI2020502,
title = {Who will use augmented reality? An integrated approach based on text analytics and field survey},
journal = {European Journal of Operational Research},
volume = {281},
number = {3},
pages = {502-516},
year = {2020},
note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718308725},
author = {Han Li and Ashish Gupta and Jie Zhang and Nick Flor},
keywords = {Text analytics, Multi-method approach, Technical features, Augmented reality, Post-adoption use intention},
abstract = {Next-generation technologies such as Augmented Reality and Virtual Reality are fast permeating many industry and society sectors. Their market is projected to reach $95 billion by 2025, representing a large portion of the economy within the next decade. With these technologies gaining wide popularity, it is critical to understand their usage in the context of the various benefits and perils that they offer. Even though top-rated mobile applications face an increasing challenge to retain users, few studies have attempted to decipher the dilemma in their continuance momentum. In this study, we focus on Pokémon GO, a top-rated Augmented Reality app, using it as a special case to investigate factors influencing user continuance and more use intention. We extend expectation confirmation theory by incorporating the effects of subjective norm, perceived risk, technical features and sense of direction. To increase the relevance and richness of our understanding of risks and benefits, we integrate the text analytics and survey-based theory-validating research methodology to build and test our research model. Our findings suggest that rational risk/benefit calculus and satisfaction are two primary inputs for continuance intention. Besides physical health benefits, users also value the benefits in mental health and relationship building. The risks in performance, time and safety are salient risk dimensions that negatively impact satisfaction. Furthermore, we find technical features play a strong role in influencing perceived benefits and user satisfaction. The findings also provide important practical implications for the designers of next-generation mobile apps enabled by Augmented Reality.}
}
@article{TAI2021274,
title = {Augmented reality-based visual-haptic modeling for thoracoscopic surgery training systems},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {4},
pages = {274-286},
year = {2021},
note = {Special Issue on Virtual reality and Augmented Reality in Medical Simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000528},
author = {Yonghang Tai and Junsheng Shi and Junjun Pan and Aimin Hao and Victor Chang},
keywords = {Augmented reality, VATS, Surgery training, XPBD},
abstract = {Background
Compared with traditional thoracotomy, video-assisted thoracoscopic surgery (VATS) has less minor trauma, faster recovery, higher patient compliance, but higher requirements for surgeons. Virtual surgery training simulation systems are important and have been widely used in Europe and America. Augmented reality (AR) in surgical training simulation systems significantly improve the training effect of virtual surgical training, although AR technology is still in its initial stage. Mixed reality has gained increased attention in technology-driven modern medicine but has yet to be used in everyday practice.
Methods
This study proposed an immersive AR lobectomy within a thoracoscope surgery training system, using visual and haptic modeling to study the potential benefits of this critical technology. The content included immersive AR visual rendering, based on the cluster-based extended position-based dynamics algorithm of soft tissue physical modeling. Furthermore, we designed an AR haptic rendering systems, whose model architecture consisted of multi-touch interaction points, including kinesthetic and pressure-sensitive points. Finally, based on the above theoretical research, we developed an AR interactive VATS surgical training platform.
Results
Twenty-four volunteers were recruited from the First People's Hospital of Yunnan Province to evaluate the VATS training system. Face, content, and construct validation methods were used to assess the tactile sense, visual sense, scene authenticity, and simulator performance.
Conclusions
The results of our construction validation demonstrate that the simulator is useful in improving novice and surgical skills that can be retained after a certain period of time. The video-assisted thoracoscopic system based on AR developed in this study is effective and can be used as a training device to assist in the development of thoracoscopic skills for novices.}
}
@article{TADEJA2023134,
title = {Exploring the repair process of a 3D printer using augmented reality-based guidance},
journal = {Computers & Graphics},
volume = {117},
pages = {134-144},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002546},
author = {Sławomir K. Tadeja and Luca O. {Solari Bozzi} and Kerr D.G. Samson and Sebastian W. Pattinson and Thomas Bohné},
keywords = {Augmented reality, AR, AR-guided repair, Immersive interface, 3D printing, 3D printer repair, Right to repair},
abstract = {In recent years, additive manufacturing (AM) techniques have transcended their typical rapid prototyping role and become viable methods to directly manufacture end products in a highly versatile manner. Due to its low cost and relative ease of use, fused deposition modeling (FDM) has become the most universally applied AM technology. Nonetheless, skilled operators are often still required to perform maintenance, diagnostic, and repair tasks. Such operators need to be adequately trained. Here, Augmented reality (AR) technology could be used to automate this training and help to promptly provide new operators with the necessary skills to perform specific tasks as required. However, the most effective approach to designing such AR-based assistance systems has not yet been fully explored. Consequently, we address this need by reporting on how to design such guiding systems using well-known design engineering methodologies. We then further assess the applicability of our approach through a user study with domain experts. In addition, we complete our assessment with heuristical verification of system expressiveness to reason about the influence of cognitively important components of the AR interface on the operators.}
}
@article{SHAO2024108108,
title = {Facial augmented reality based on hierarchical optimization of similarity aspect graph},
journal = {Computer Methods and Programs in Biomedicine},
volume = {248},
pages = {108108},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108108},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001044},
author = {Long Shao and Tianyu Fu and Yucong Lin and Deqiang Xiao and Danni Ai and Tao Zhang and Jingfan Fan and Hong Song and Jian Yang},
keywords = {Pose estimation, Aspect graph, Hierarchical optimization, Monte Carlo tree search, Facial augmented reality},
abstract = {Background
The existing face matching method requires a point cloud to be drawn on the real face for registration, which results in low registration accuracy due to the irregular deformation of the patient's skin that makes the point cloud have many outlier points.
Methods
This work proposes a non-contact pose estimation method based on similarity aspect graph hierarchical optimization. The proposed method constructs a distance-weighted and triangular-constrained similarity measure to describe the similarity between views by automatically identifying the 2D and 3D feature points of the face. A mutual similarity clustering method is proposed to construct a hierarchical aspect graph with 3D pose as nodes. A Monte Carlo tree search strategy is used to search the hierarchical aspect graph for determining the optimal pose of the facial 3D model, so as to realize the accurate registration of the facial 3D model and the real face.
Results
The proposed method was used to conduct accuracy verification experiments on the phantoms and volunteers, which were compared with four advanced pose calibration methods. The proposed method obtained average fusion errors of 1.13 ± 0.20 mm and 0.92 ± 0.08 mm in head phantom and volunteer experiments, respectively, which exhibits the best fusion performance among all comparison methods.
Conclusions
Our experiments proved the effectiveness of the proposed pose estimation method in facial augmented reality.}
}
@article{HAOMING2024100057,
title = {A systematic review on vocabulary learning in AR and VR gamification context},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100057},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000072},
author = {Lin Haoming and Wei Wei},
keywords = {Vocabulary learning, Augmented reality, Virtual reality, Gamification, Digital affordance},
abstract = {In recent scholarly literature, there has been a growing focus on the integration of augmented reality (AR), virtual reality (VR), and digital games to assist language learning. However, little has been written on reviewing the adoption of gamified AR or VR in facilitating the enhancement of specific language skills. Addressing this research gap, the present study conducted a systematic review of vocabulary learning within AR and VR gamification context. Following the predefined inclusion criteria and literature selection process outlined by the PRISMA framework, 23 articles were selected from a pool of 97 studies for final analysis. The major findings revealed that: (1) Form and meaning of vocabulary took precedence in gamified AR-assisted vocabulary learning or VR-assisted vocabulary learning, adopting intentional learning as the primary approach; (2) A significant majority of studies employed researcher-coined vocabulary tests as the measurements for assessing vocabulary knowledge, with translation, multiple-choice questions, and word-matching serving as the most common tasks; (3) The reviewed studies demonstrated that gamified AR and VR both prioritize interactivity as the core digital affordance in the context of vocabulary learning. Gamified AR owned unique features, including bridging reality and virtuality, promoting collaborative learning, and visualizing content, while gamified VR was more inclined toward immersion and facilitating self-regulated learning; (4) Three aspects of problems, namely technology-related, student-related, and teacher-related, emerged with the application of gamified AR or VR. Based on these results, the current study engaged in a comprehensive discussion and suggested a tripartite co-construction mechanism for learning materials, involving students, teachers, and researchers, integrating both gamified AR and VR.}
}
@incollection{SCHUBERT2009333,
title = {Development and Experimental Verification of Model-Based Process Control using Mixed-Reality Environments},
editor = {Jacek Jeżowski and Jan Thullie},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {26},
pages = {333-337},
year = {2009},
booktitle = {19th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70056-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609700562},
author = {Udo Schubert and Harvey Arellano-Garcia and Günter Wozny},
keywords = {advanced process control, model predictive control, estimation, uncertainty},
abstract = {In this paper, an extension of the PARSEX reactor introduced by Kershenbaum & Kittisupakorn, 1994 is presented. The proposed setup improves the capabilities of the original idea and constitutes a mixed-reality environment by replacing the reactor with a virtual counterpart. Furthermore, scenarios for the application of nonlinear model predictive control under the explicitly consideration of uncertainty on the mixed-reality plant are given. Simulation studies and experiments are used to validate the characteristic dynamic behavior in order to provide challenging aspects for control and estimation algorithms.}
}
@article{REYBECERRA2023104077,
title = {Improvement of short-term outcomes with VR-based safety training for work at heights},
journal = {Applied Ergonomics},
volume = {112},
pages = {104077},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104077},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023001151},
author = {Estefany Rey-Becerra and Lope H. Barrero and Rolf Ellegast and Annette Kluge},
keywords = {Serious games, Virtual environment, Construction industry},
abstract = {Serious games and virtual reality offer engaging learning opportunities and a cost-effective solution within an immersive and safe environment for safety training in construction. However, there have been few examples of safety training for work at heights developed using these technologies, especially commercial training. To fill this literature gap, a new VR-based safety training was developed and compared with lecture-based training across time. We conducted a quasi-experiment with a non-equivalent group design with 102 workers from six construction sites in Colombia. Learning objectives, observations from training centers, and national regulations were considered during the design of the training methods. Training outcomes were assessed using Kirkpatrick's model. We found that both training approaches were effective in improving knowledge test results and self-reported attitudes in the short-term; and risk-perception, self-reported behavior and safety climate in the long-term. In particular, participants of the VR-based training got significantly higher results in knowledge and reported higher attitudes (commitment and motivation) than participants of the lecture-based training. We suggest that safety managers and practitioners should invest in VR using serious games as an alternative to training programs based on short-term outcomes. Future work is needed to test VR for long-term outcomes.}
}
@article{KIRAN2024100674,
title = {Accelerating autonomous vehicle safety through Real-Time Immersive virtual reality gaming simulations},
journal = {Entertainment Computing},
volume = {50},
pages = {100674},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100674},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000429},
author = {Ajmeera Kiran and Satish S. Salunkhe and B. Srinivas and M. Vanitha and Mohammed {Altaf Ahmed} and Janjhyam {Venkata Naga Ramesh}},
keywords = {Autonomous Vehicles, Virtual Reality, Driving Simulation, Real-Time Rendering, Vehicle Safety, Immersive Environments, Gaming Engines},
abstract = {Although autonomous vehicles (AVs) have great potential for improving traffic safety, several questions about their practicality persist. Although it is crucial, doing tests on the road may be risky and costly. An easier, quicker, and less expensive option is to use virtual reality (VR) driving simulators. In this research, we investigate how virtual reality (VR) game simulations may speed up the process of validating the safety of AVs. We suggest building complex dynamic environments using consumer VR game engines to put AVs through their paces. Thanks to the physics modelling and real-time rendering features, you can test your car in realistic conditions. We review the advantages of conventional testing methods, including quickly iterating environments, assessing potentially dangerous edge situations, scaling effectively, and saving money. Additionally, our method enables programmers to collect information and opinions from big groups of real drivers via online games. We draw attention to unanswered questions concerning data-driven simulation adaptation, variability modelling, and fidelity. Achieving thorough, quick, and ethical safety validation has never been more feasible than using game-inspired VR simulations, significantly because the development of AVs is exceeding physical testing. This technique can potentially speed up the safe deployment of autonomous functions by making high-quality simulation more accessible to a broader audience.}
}