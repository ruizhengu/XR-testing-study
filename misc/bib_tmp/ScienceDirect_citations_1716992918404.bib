@article{ANDERIES2023582,
title = {The Application of Augmented Reality to Generate Realistic Interaction in the Property Sector},
journal = {Procedia Computer Science},
volume = {227},
pages = {582-590},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.561},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017283},
author = { Anderies and Rendy Adidarma and Maximillian Lemuel Chanyassen and Alexander Imanuel and Andry Chowanda},
keywords = {Augmented Reality, Property Industry, Vuforia Ground Plane, 3D Model, Android, Performance Testing},
abstract = {Augmented Reality is a technology that overlays computer-generated information and images in a natural environment to enhance or augment the contextual perception of the user's environment. The utilization of AR is currently one of the most important in developing several industrial sectors, such as architecture, engineering, and construction development. Nowadays, the property sector industry has always been an issue of sale for users that do not have direct access to the property. The pandemic and the economic problem in different countries have also reduced consumer interest in purchasing houses. Due to these issues, this paper aims to provide a platform for addressing these pain areas. The software can apply Augmented Reality to real estate in real-time using a mobile device. Not only can they see the outside of the property, but they can also see the interior design, furniture design, and property mapping in 3D. Our test results show that an application is needed to provide the property sector with high demands. Over 80% of our respondents confirm that our application's presence will help ease the sales problem. Because it can save the customer time, viewing property frequency, and cost saving. These increased demands, we are developing an AR-based application for the market to use and gain more traction in the property sector.}
}
@article{HU2021101436,
title = {Educational impact of an Augmented Reality (AR) application for teaching structural systems to non-engineering students},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101436},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101436},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001889},
author = {Xinping Hu and Yang Miang Goh and Alexander Lin},
keywords = {Augmented reality, Educational technology, Mobile application, Structural engineering},
abstract = {This study evaluates the effectiveness of Augmented Reality (AR) in improving undergraduate non-engineering students’ (n = 103) achievement of learning outcomes and their perception of AR as a learning tool. An AR application for teaching structural systems, Virtual and Augmented Reality for Structures (VARS), is developed and tested using a quasi-experiment and a questionnaire. The study shows that VARS AR significantly improves the posttest quiz score of the experimental group. In addition, a conceptual framework linking design considerations and achievement of learning outcomes is created. Based on the questionnaire survey, authentic context and user interface are important design considerations influencing students’ perceived achievement of learning outcomes. It was found that experience using AR and background knowledge did not influence the perceived achievement of learning outcomes and change in quiz score.}
}
@article{CIEZA2018352,
title = {Educational Mobile Application of Augmented Reality Based on Markers to Improve the Learning of Vowel Usage and Numbers for Children of a Kindergarten in Trujillo},
journal = {Procedia Computer Science},
volume = {130},
pages = {352-358},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304046},
author = {Edwin Cieza and David Lujan},
keywords = {Mobile app, Learning, Augmented reality, Markers, Development tools},
abstract = {The main objective of this research was to improve the level of understanding of the usage of vowels and numbers for children over 4 years of age in the Juana Alarco de Dammert Nursery School in Trujillo through an educational mobile application that is composed of the unit development platform, monodevelopment, Andriod Studio, Vuforia using the programming language C# that was made based on the development methodology of extreme software programming. The research design is a pre-experimental experiment grade which was composed of 10 children over the age of 4 of the nursery school and was used as a method of data analysis Student T test. In addition, with the implemented application it was possible to increase the level of academic performance of vowel usage by 27.60% and the use of numbers by 22.60%. It was concluded that with the implementation of the educational mobile application of augmented reality, the level of understanding of vowel usage and numbers had improved in the Juana Alarco de Dammart nursery school children.}
}
@article{BOEDIONO2023718,
title = {Markerless Augmented Reality Application for Indonesian Traditional House Education},
journal = {Procedia Computer Science},
volume = {227},
pages = {718-725},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.576},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301743X},
author = {Jonathan Apriliano Saputra Boediono and Muhammad Rizqi Aulia and Fairuz Iqbal Maulana},
keywords = {Augmented Reality, Traditional House, Rumah Adat, ARCore, Application},
abstract = {This paper proposes the development of a markerless augmented reality (AR) application for Indonesian traditional house education. The application aims to provide an immersive and interactive way of visualizing traditional houses from various regions of Indonesia, with the potential to raise public awareness about the importance of preserving traditional culture. The prototyping method was employed as the most appropriate approach to software development, and the ARFoundation platform was used to integrate plane detection into the AR application. The results showed that 80% of tested devices were compatible with ARCore, and the AR plane detection could detect up to 250cm from the device position. In addition, a feedback survey conducted on 17 testers showed that the application design and mechanism brought a good experience to users, with 53% stating that the app is easy to use. This research project contributes to the development of technology-based solutions to promote Indonesian culture and preserve traditional houses, especially for younger generations who may be less familiar with traditional culture.}
}
@article{KOUSI20191429,
title = {Enabling Human Robot Interaction in flexible robotic assembly lines: an Augmented Reality based software suite},
journal = {Procedia CIRP},
volume = {81},
pages = {1429-1434},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.328},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119309813},
author = {Niki Kousi and Christos Stoubos and Christos Gkournelos and George Michalos and Sotiris Makris},
keywords = {human robot interaction, augmented reality, mobile robots, remote guidance},
abstract = {This paper presents an Augmented Reality (AR) based software suite for supporting operators in production systems that employ mobile robots These robot workers may increase assembly system’s flexibility while supporting humans in assembly given their ability to navigate to different workstations and change tools for performing various assembly tasks. Driven by the need to immerse the human in the execution loop, the discussed AR based framework aims to enable their direct communication and interaction of human operations with a) the robot coworkers providing them online task instructions and b) the central execution system through natural means of communication such as virtual buttons. The developed tool has been tested in a case study inspired from the automotive industry showing that it may facilitate the communication between the humans and mobile robots increasing human operators’ job quality while supporting them during collaborative assembly.}
}
@article{GAO2023108086,
title = {Designing interactive augmented reality application for student's directed learning of continuous distillation process},
journal = {Computers & Chemical Engineering},
volume = {169},
pages = {108086},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108086},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422004197},
author = {Sitian Gao and Yunpeng Lu and Ching Hui Ooi and Yiyu Cai and Poernomo Gunawan},
keywords = {Augmented reality, Continuous distillation, Real-time simulation, Visualization, Virtual experiment},
abstract = {Continuous distillation is an important separation process in chemical engineering curriculum. Conventionally, it is often taught as black box diagrams that lack user visibility, thus preventing students from developing a deeper understanding and comprehension. This paper presents the technical development of the mobile AR application and its implementation in the curriculum of Chemical Engineering Unit Operations at Nanyang Technological University Singapore. The mobile AR application is to provide interactive visualization and real-time numerical simulation to promote students’ active learning in the classroom. The results of the pre-test and post-test indicated that the AR application helped students gain a better understanding of the principle and fundamental concepts of the distillation process. The evaluation survey with 6-point likert scale questions shows that students had a positive experience using the mobile AR application and it has enhanced their learning experience, as suggested by a mean evaluation score of 5.18 out of 6.0.}
}
@article{MOURTZIS2021103383,
title = {Integrated and adaptive AR maintenance and shop-floor rescheduling},
journal = {Computers in Industry},
volume = {125},
pages = {103383},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103383},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306175},
author = {Dimitris Mourtzis and John Angelopoulos and Vasilios Zogopoulos},
keywords = {Maintenance, Scheduling, Augmented reality, Machine monitoring, Industry 4.0},
abstract = {By virtue of the Digitalization of Manufacturing Systems, the importance of interconnecting multiple digital systems is becoming a necessity. By extension, the need for connecting the shop-floor technician to the rest of the production system is highlighted more than ever. Internet and Communication technologies (ICT) may connect the technician with data collected from the machine sensors, knowledge repositories and integrated production software. Mobile devices offer highly usable interfaces that can be exploited to involve the operator in this process, digitalize their inputs and also visualize machine information on top of the physical system through Augmented Reality (AR). Towards that direction, an application that allows the operator to monitor machine status and call event-driven AR remote machine maintenance and rescheduling based on maintenance time estimation is developed. The framework is tested and validated in a laboratory-base machine shop.}
}
@article{PEREIRA202440,
title = {Points of interest in the city of Barcelos in Portugal through augmented reality},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {4},
pages = {40-48},
year = {2024},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667345223000457},
author = {Miguel Pereira and João Carlos Silva and Marisa Pinheiro and Sandro Carvalho and Gilberto Santos},
keywords = {Mobile applications, Augmented reality, Tourism, Geolocation},
abstract = {Barcelos is a historic city in Portugal with many tourist attractions, attracting more and more visitors who come to the city with the aim of exploring it. The main objective of this article is to boost tourism in the city of Barcelos, specifically highlighting tourist, historical and leisure spots, based on the development of a mobile application using augmented reality technologies and geolocation. This application intends to allow the users to know historical points of interest in Barcelos, as well as interact with a certain point. The results of this investigation were evaluated by testing the application by end users, with the aim of identifying whether the application meets their needs, in particular the promotion of tourist and historical points.}
}
@article{LATINI2023106280,
title = {Development and application of an experimental framework for the use of virtual reality to assess building users’ productivity, comfort, and adaptive-behaviour},
journal = {Journal of Building Engineering},
volume = {70},
pages = {106280},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106280},
url = {https://www.sciencedirect.com/science/article/pii/S235271022300459X},
author = {Arianna Latini and Elisa {Di Giuseppe} and Marco D'Orazio},
keywords = {Virtual reality, Validity, Experimental framework, Human comfort and behaviour, Productivity},
abstract = {The use of Virtual Reality (VR) to assess comfort, productivity, and behaviour in built environments is currently emerging. However, an effective and standardised methodology to support researchers in performing VR tests to collect reliable data is lacking. Thus, the study aims to develop a novel and comprehensive experimental framework for organising the application of VR in this research domain, based on five validity types: content, internal, face, ecological, and criterion validity. To illustrate its suitability and applicability, the framework was applied to a single-domain case study of a virtual office room. 52 volunteers were recruited to perform cognitive tests (Stroop test, OSPAN test, Magnitude-Parity test) and answer questionnaires. Tests and surveys have been developed to support the content validity of the research. Each test was performed under different thermal stimuli: 24 °C and 16 °C, randomly assigned, to pursue internal validity. The first goal was to demonstrate that the framework allowed the creation of highly immersive scenarios. Findings confirmed the ecological validity of the model with an excellent sense of presence, graphical satisfaction, involvement and realism and non-significant levels of cybersickness. The second aim concerned the evaluation of the ability of the VR environment to capture the influence of the temperature set point on the dependent variables. As expected, a statistically significant influence was detected only on thermal comfort votes and adaptive behaviour, thus supporting the criterion validity. Results highlighted the values and potentialities of applying the present framework in the context of the emerging multi-domain research in support of user-centred design.}
}
@article{MOURTZIS201746,
title = {Augmented Reality Application to Support Remote Maintenance as a Service in the Robotics Industry},
journal = {Procedia CIRP},
volume = {63},
pages = {46-51},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117303360},
author = {D. Mourtzis and V. Zogopoulos and E. Vlachou},
keywords = {Augmented Reality, Remote Maintenance, Product Service System},
abstract = {Maintenance of manufactured products is among the most common services in industry and its cost often exceed 30% of the operating costs. Modern manufacturing companies are shifting their focus from products to combined ecosystem of Products- Service Systems (PSS). Towards that end, the main objective of this work is to develop a cloud-based service-oriented system that implements AR technology for remote maintenance by enabling cooperation between the on- spot technician and the manufacturer. The proposed system includes the methods for the record of the malfunction by the end user, the actions required by the expert so as to provide instructions in an Augmented Reality application for maintenance, as well as the cloud- based platform that will allow their communication and the exchange of information. In addition to the above, the proposed system consists of smart assembly/disassembly algorithms for automated generation of assembly sequences and AR scenes and improved interface, aiming to maximize existing knowledge usage while creating vivid AR service instructions. The proposed system is validated in a real-life case study following the requirements of a robotics SME.}
}
@article{KHANDELWAL2015698,
title = {Detection of Features to Track Objects and Segmentation Using GrabCut for Application in Marker-less Augmented Reality},
journal = {Procedia Computer Science},
volume = {58},
pages = {698-705},
year = {2015},
note = {Second International Symposium on Computer Vision and the Internet (VisionNet’15)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915022012},
author = {Pulkit Khandelwal and P. Swarnalatha and Neha Bisht and S. Prabu},
keywords = {Augmented Reality, Virtual Reality, Histogram of Oriented Gradients, GrabCut, Scale Invariant Feature Transform, Support Vector Machine, Interest Points Detectors, Image Descriptors, Segmentation, Human Body},
abstract = {Augmented Reality applications have hovered itself over various platforms such as desktop and most recently to handheld devices such as mobile phones and tablets. Augmented Reality (AR) systems have mostly been limited to Head Worn Displays with start-ups such as Magic Leap and Occulus Rift making tremendous advancement in such AR and VR research applications facing a stiff competition with Software giant Microsoft which has recently introduced Holo Lens. AR refers to the augmentation or the conglomeration of virtual objects in the real world scenario which has a distinct but close resemblance to Virtual Reality (VR) systems which are computer simulated environments which render physical presence in imaginary world. Developers and hackers round the globe have directed their research interests in the development of AR and VR based applications especially in the domain of advertisement and gaming. Many open source libraries, SDKs and proprietary software are available worldwide for developers to make such systems. This paper describes an algorithm for an AR prototype which uses a marker less approach to track and segment out real world objects and then overlay the same on another real world scene. The algorithm was tested on Desktop. The results are comparable with other existing algorithms and outperform some of them in terms of robustness, speed, and accuracy, precision and timing analysis.}
}
@article{LEE201541,
title = {BoreholeAR: A mobile tablet application for effective borehole database visualization using an augmented reality technology},
journal = {Computers & Geosciences},
volume = {76},
pages = {41-49},
year = {2015},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2014.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300414002830},
author = {Sangho Lee and Jangwon Suh and Hyeong-Dong Park},
keywords = {Borehole, Mobile tablet, Augmented reality, Mobile database, Geological visualization},
abstract = {Boring logs are widely used in geological field studies since the data describes various attributes of underground and surface environments. However, it is difficult to manage multiple boring logs in the field as the conventional management and visualization methods are not suitable for integrating and combining large data sets. We developed an iPad application to enable its user to search the boring log rapidly and visualize them using the augmented reality (AR) technique. For the development of the application, a standard borehole database appropriate for a mobile-based borehole database management system was designed. The application consists of three modules: an AR module, a map module, and a database module. The AR module superimposes borehole data on camera imagery as viewed by the user and provides intuitive visualization of borehole locations. The map module shows the locations of corresponding borehole data on a 2D map with additional map layers. The database module provides data management functions for large borehole databases for other modules. Field survey was also carried out using more than 100,000 borehole data.}
}
@article{KIMWANG2023111473,
title = {Auto-segmentation of the tibia and femur from knee MR images via deep learning and its application to cartilage strain and recovery},
journal = {Journal of Biomechanics},
volume = {149},
pages = {111473},
year = {2023},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111473},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023000428},
author = {Sophia Y. Kim-Wang and Patrick X. Bradley and Hattie C. Cutcliffe and Amber T. Collins and Bryan S. Crook and Chinmay S. Paranjape and Charles E. Spritzer and Louis E. DeFrate},
keywords = {Auto-segmentation, U-Net, Strain, MRI, Machine Learning},
abstract = {The ability to efficiently and reproducibly generate subject-specific 3D models of bone and soft tissue is important to many areas of musculoskeletal research. However, methodologies requiring such models have largely been limited by lengthy manual segmentation times. Recently, machine learning, and more specifically, convolutional neural networks, have shown potential to alleviate this bottleneck in research throughput. Thus, the purpose of this work was to develop a modified version of the convolutional neural network architecture U-Net to automate segmentation of the tibia and femur from double echo steady state knee magnetic resonance (MR) images. Our model was trained on a dataset of over 4,000 MR images from 34 subjects, segmented by three experienced researchers, and reviewed by a musculoskeletal radiologist. For our validation and testing sets, we achieved dice coefficients of 0.985 and 0.984, respectively. As further testing, we applied our trained model to a prior study of tibial cartilage strain and recovery. In this analysis, across all subjects, there were no statistically significant differences in cartilage strain between the machine learning and ground truth bone models, with a mean difference of 0.2 ± 0.7 % (mean ± 95 % confidence interval). This difference is within the measurement resolution of previous cartilage strain studies from our lab using manual segmentation. In summary, we successfully trained, validated, and tested a machine learning model capable of segmenting MR images of the knee, achieving results that are comparable to trained human segmenters.}
}
@article{VASILIS20221162,
title = {An Augmented Reality Framework for Visualization of Internet of Things Data for Process Supervision in Factory Shop-Floor},
journal = {Procedia CIRP},
volume = {107},
pages = {1162-1167},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.125},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004097},
author = {Siatras Vasilis and Nikolakis Nikos and Alexopoulos Kosmas},
keywords = {, , , , },
abstract = {Modern industrial environments provide a high level of digitalization, enabling great volumes of information to be used for shopfloor monitoring and control purposes. In cases where human operators participate in the production process, it is important to support operator’s intervention with Internet of Things (IoT) data and empower process supervision with constant information over Key Performance Indicators (KPIs). Towards this end, the proposed framework exploits Augmented Reality (AR) for the provision of an IoT data visualization solution to support KPIs based process supervision in the smart factory environment. The solution utilizes IoT-enabled components, in order to receive dynamic information over the status of sensors and resources spread across the factory shop-floor. The AR application facilitates the need for visualization of information available on the network, on the actual shopfloor components, which offer transparency and ease the supervision of production processes. The development of the AR and cloud API has been based on CSharp and Java, in a reusable framework that can be reused for similar industrial cases. The reconfigurability and reuse of the AR solution is supported by a template configuration, allowing industry practitioners to configure application features and external connections without any additional software development. The solution was validated on a thermoforming process supervision case of a white goods production system. The main goal was to use the AR framework to support operators during the process parameters design phase, and dynamic supervision of the shopfloor environment. It is demonstrated that the proposed system can be used for KPIs monitoring of the production cycles and for empowering process supervision and optimization.}
}
@article{WULFF20241991,
title = {Use of augmented reality for iterative robot program optimisation in robot-automated series production processes},
journal = {Procedia Computer Science},
volume = {232},
pages = {1991-2000},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001984},
author = {Lukas Antonio Wulff and Ole Schmedemann and Thorsten Schüppstuhl},
keywords = {industrial robot programming, augmented reality, mixed reality, automotive, simulation, sealing application},
abstract = {The inspection, analysis, and modification of robot programs in series production processes is a challenging recurring task in the automotive industry. Especially the holistic analysis of detected defects and the subsequent derivation and application of corrective measures to the underlying robot program is a complex task requiring extensive knowledge in multiple fields. Existing scientific literature has demonstrated that the utilisation of Augmented Reality (AR) assisted robot programming systems (ARRPS) improves programming efficiency as well as intuitiveness when compared to traditional programming methods. However, the accuracy especially of mobile AR devices limits the applicability of AR in industrial applications. We propose to leverage the iterative nature of the optimisation process common in series production and utilise the visual capabilities of the human worker to define corrective measures based on the inspected real process result. This mitigates the impact of limited tracking accuracy as the optimisation is based on the visual perception of the user and thus decoupled from the accuracy of the employed AR device. We implemented our system based on the requirements of a sealing workstation of the Mercedes-Benz Group in Germany. A conducted experiment validates the functionality and indicates that the presented strategy of iterative AR assisted robot programming enables optimisations with a similar accuracy as the Teach-In programming as well as a significantly reduced programming time. This suggests that utilising an ARRPS can be highly beneficial in the inspection, analysis, and modification of sealing applications common in automotive series production processes. As the presented iterative AR assisted robot programming is neither limited to sealing nor to the automotive sector it can be adapted for various robot-automated applications like path-welding, gluing, or painting and can extend its utility to related sectors such as aviation or the marine industry.}
}
@article{WELTIN2023750,
title = {Automatic Content Creation System for Augmented Reality Maintenance Applications for Legacy Machines},
journal = {Procedia CIRP},
volume = {120},
pages = {750-755},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.070},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123008028},
author = {Michael Weltin and Dominik Lucke and Johannes L. Jooste},
keywords = {Augmented Reality, Context-awareness, Automatic content creation},
abstract = {Augmented reality (AR) applications have great potential to assist maintenance workers in their operations. However, creating AR solutions is time-consuming and laborious, which limits its widespread adoption in the industry. It therefore often happens that even with the latest generation machines, instead of an AR solution, the user only receives an electronic manual for the equipment operation and maintenance. This is commonplace with legacy machines. For this reason, solutions are required that simplify the creation of such AR solutions. This paper presents an approach using an electronic manual as a basis to create fast and cost-effective AR solutions for maintenance. As part of the approach, an application was developed to automatically identify and subdivide the chapters of electronic manuals via the bookmarks in the table of contents. The contents are then automatically uploaded to a central server and indexed with a suitable marker to make the data retrievable. The prepared content can then be accessed for creating context-related AR instructions via the marker. The application is characterized by the fact that no developers or experts are required to prepare the information. In addition to complying with common design criteria, the clear presentation of the contents and the intuitive use of the system offer added value for the performance of maintenance tasks. Together, these two elements form a novel way to retrofit legacy machines with AR maintenance instructions. The practical validation of the system took place in a factory environment. For this purpose, the content was created for a filter change on a CNC milling machine. The results show that inexperienced users can extract appropriate content with the software application. Furthermore, it is shown that maintenance workers, can access the content with an AR application developed for the Microsoft HoloLens 2 and complete simple tasks provided in the manufacturer's electronic manual.}
}
@article{XIAO2020291,
title = {Multimodal interaction design and application in augmented reality for chemical experiment},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {291-304},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300589},
author = {Mengting Xiao and Zhiquan Feng and Xiaohui Yang and Tao Xu and Qingbei Guo},
keywords = {Augmented reality, Gesture recognition, Intelligent equipment, Multimodal Interaction, Augmented Reality Chemistry Lab},
abstract = {Background
Augmented reality classrooms have become an interesting research topic in the field of education, but there are some limitations. Firstly, most researchers use cards to operate experiments, and a large number of cards cause difficulty and inconvenience for users. Secondly, most users conduct experiments only in the visual modal, and such single-modal interaction greatly reduces the users' real sense of interaction. In order to solve these problems, we propose the Multimodal Interaction Algorithm based on Augmented Reality (ARGEV), which is based on visual and tactile feedback in Augmented Reality. In addition, we design a Virtual and Real Fusion Interactive Tool Suite (VRFITS) with gesture recognition and intelligent equipment.
Methods
The ARGVE method fuses gesture, intelligent equipment, and virtual models. We use a gesture recognition model trained by a convolutional neural network to recognize the gestures in AR, and to trigger a vibration feedback after a recognizing a fivefinger grasp gesture. We establish a coordinate mapping relationship between real hands and the virtual model to achieve the fusion of gestures and the virtual model.
Results
The average accuracy rate of gesture recognition was 99.04%. We verify and apply VRFITS in the Augmented Reality Chemistry Lab (ARCL), and the overall operation load of ARCL is thus reduced by 29.42%, in comparison to traditional simulation virtual experiments.
Conclusions
We achieve real-time fusion of the gesture, virtual model, and intelligent equipment in ARCL. Compared with the NOBOOK virtual simulation experiment, ARCL improves the users' real sense of operation and interaction efficiency.}
}
@article{LOUPESCANDE20141049,
title = {Needs’ elaboration between users, designers and project leaders: Analysis of a design process of a virtual reality-based software},
journal = {Information and Software Technology},
volume = {56},
number = {8},
pages = {1049-1061},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000895},
author = {Emilie Loup-Escande and Jean-Marie Burkhardt and Olivier Christmann and Simon Richir},
keywords = {Ergonomics, Design process, Emerging technologies, Needs},
abstract = {Context
The participation of users in the design process is recognized as a positive and a necessary element as artifacts suit their needs. Two complementary approaches of users’ involvement co-exist: the user-centered design and the participatory design. These approaches involve learning process from users to designers and vice versa. However, there has no research in design of virtual reality (VR)-based software dealing with how the elaboration of needs is actually distributed in time and among users, designers and project leaders, as well as how it is actually supported by tools and methods.
Objective
This paper aims to observe, in a real design project of a virtual reality-based software, how the various stakeholders (users, designers, project leaders) actually participate by sharing and pulling pieces of information from the process of needs elaboration, and how these contributions evolve throughout the decisions made in the course of the project.
Method
Our method, based on the observation of the practices in collective design, allows us to collect and analyze the relationship between each possible action (e.g., elicitation), each stakeholder who initiates these actions (e.g., users) and each phase of the design process (e.g., evaluation phase), and the dynamics of the construction of needs.
Results
Our results detail how the elicited needs are dealt with by designers, users and/or project leaders: (1) we show a strong contribution of users in the design, compared to others stakeholders, (2) among the needs elicited by users, most have been validated by the designers, (3) some elicited needs could have been firstly rejected and finally validated and implemented.
Conclusion
We identify the reasons which justify and explain our results confronting them to the literature. We underline the conditions have been satisfied in our study in order to involve effectively users in the design of emerging technologies.}
}
@article{HEINRICH2021210,
title = {Estimating depth information of vascular models: A comparative user study between a virtual reality and a desktop application},
journal = {Computers & Graphics},
volume = {98},
pages = {210-217},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001138},
author = {Florian Heinrich and Vikram Apilla and Kai Lawonn and Christian Hansen and Bernhard Preim and Monique Meuschke},
keywords = {Medical visualization, Virtual reality, Empirical user studies},
abstract = {Vascular structures are assessed, e.g., in tumor surgery to understand the influence of a planned resection on the vascular supply and venous drainage. The understanding of complex branching vascular structures may benefit from immersive virtual reality (VR) visualization. Therefore, the estimation of distance, depth and shape information is a crucial task to support diagnosis and therapy decisions. Depending on the visualization techniques used, perceptual issues can influence this process and may thus lead to false conclusions. Many studies were carried out to study depth perception for different variants of vessel visualization. However, these studies are restricted to desktop applications. Since VR exhibits specific perceptual problems, we aim at an understanding of the appropriateness of vessel visualization techniques in VR. Therefore, this paper presents a user study that investigates the effects of three commonly used visualization techniques on depth perception. The set of visualization techniques comprises Phong shading, pseudo-chromadepth and fog shading. An immersive VR setup of the study using a head-mounted display (HMD) was compared to a traditional desktop setup. Results suggest that depth judgments are less error-prone and more certain in VR than in desktop environments. Moreover, depth-enhancing visualization techniques had greater effects in the desktop study compared to the VR study.}
}
@article{SCORPIO2022103998,
title = {A calibration methodology for light sources aimed at using immersive virtual reality game engine as a tool for lighting design in buildings},
journal = {Journal of Building Engineering},
volume = {48},
pages = {103998},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.103998},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222000110},
author = {Michelangelo Scorpio and Roberta Laffi and Ainoor Teimoorzadeh and Giovanni Ciampi and Massimiliano Masullo and Sergio Sibilio},
keywords = {Immersive virtual reality, Innovative lighting design, Building performance, Unreal game engine, DIALux evo},
abstract = {Immersive virtual reality represents one of the most promising technologic aids in the development of a functional lighting design, especially when considering different points of view as users’ satisfaction. This paper presents a new methodology to use Unreal Engine 4.22 as a tool for lighting applications, allowing correct reproduction of artificial light distribution in the game engine by identifying and properly setting a restricted set of parameters. A real office was carefully reproduced, in both the Unreal Engine and DIALux evo software, as comparative case study. Then, the luminance distribution is calculated with both programs on the same surfaces in the virtual environment and compared with the real one. The results suggest a good reliability of the Unreal Engine in reproducing the light distribution, underlining slight differences under certain conditions. In light of these findings, the proposed method seems to validate the use of Unreal Engine as a light design tool.}
}
@article{CHIANG2022107125,
title = {Augmented reality in vocational training: A systematic review of research and applications},
journal = {Computers in Human Behavior},
volume = {129},
pages = {107125},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107125},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221004489},
author = {Feng-Kuang Chiang and Xiaojing Shang and Lu Qiao},
keywords = {Augmented reality (AR), Vocational training, On-the-job training, AR application, AR system},
abstract = {Augmented reality (AR) technology is widely used in various fields. However, there are few systematic reviews on the application of AR in vocational training. To fill this research gap, the current study reviewed the application of AR technology in the training of various industries over a 20 year period (2000–2021). Through cross-referencing and abstract reading, 80 relevant studies were selected for the final analysis from two perspectives: the improvement of vocational skills (including application area, target audience, training objectives, and effects) and AR training technology (including AR application, AR training system, and device). Furthermore, CiteSpaceV was employed to analyze the research hotspots and trends of AR vocational training. The results indicated that AR training is frequently applied in the industry, vocational education and medical fields. Among these industries, AR has been most frequently used in medical training, industrial maintenance, and assembly. Furthermore, AR glasses, simulators, the Unity3D game engine, 360° panorama, AR systems and apps are becoming widely used for training tasks. The benefits of these systems have been identified. We also explored the impact of AR on vocational training results. Based on 17 empirical studies, this study summarized the results and advantages of AR vocational training. This verified that AR has a high promotion effect on vocational training when the meta-analysis method is used. Future researchers can study how vocational skills education can be combined with these new intelligent technologies to design more mature teaching practice cases.}
}
@article{DAVID2023359,
title = {Deploying OWL ontologies for semantic mediation of mixed-reality interactions for human–robot collaborative assembly},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {359-381},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001450},
author = {Joe David and Eric Coatanéa and Andrei Lobov},
keywords = {Human–robot collaboration, Mixed reality, Multi-agent systems, OWL ontology, SHACL, Belief–desire–intent, Digital thread},
abstract = {For effective human–robot collaborative assembly, it is paramount to view both robots and humans as autonomous entities in that they can communicate, undertake different roles, and not be bound to pre-planned routines and task sequences. However, with very few exceptions, most of recent research assumes static pre-defined roles during collaboration with centralised architectures devoid of runtime communication that can influence task responsibility and execution. Furthermore, from an information system standpoint, they lack the self-organisation needed to cope with today’s manufacturing landscape that is characterised by product variants. Therefore, this study presents collaborative agents for manufacturing ontology (CAMO), which is an information model based on description logic that maintains a self-organising team network between collaborating human–robot multi-agent system (MAS). CAMO is implemented using the Web Ontology Language (OWL). It models popular notions of net systems and represents the agent, manufacturing, and interaction contexts that accommodate generalisability to different assemblies and agent capabilities. As a novel element, a dynamic consensus-driven collaboration based on parametric validation of semantic representations of agent capabilities via runtime dynamic communication is presented. CAMO is instantiated as agent beliefs in a framework that benefits from real-time dynamic communication with the assembly design environment and incorporates a mixed-reality environment for use by the operator. The employment of web technologies to project scalable notions of intentions via mixed reality is discussed for its novelty from a technology standpoint and as an intention projection mechanism. A case study with a real diesel engine assembly provides appreciable results and demonstrates the feasibility of CAMO and the framework.}
}
@article{THEOKTISTO2005704,
title = {Enhancing collaboration in virtual reality applications},
journal = {Computers & Graphics},
volume = {29},
number = {5},
pages = {704-718},
year = {2005},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2005.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0097849305001330},
author = {Víctor Theoktisto and Marta Fairén},
abstract = {We derive a complete component framework for transforming standalone virtual reality (VR) applications into full-fledged multithreaded collaborative virtual reality environments (CVREs), after characterizing existing implementations into a feature-rich superset. Our main contribution is placing over the existing VR tool a very concise and extensible class framework as an add-on component that provides emerging collaboration features. The enhancements include: a scalable arbitrated peer-to-peer topology for scene sharing; multi-threaded components for graphics rendering, user interaction and network communications; a streaming message protocol for client communications; a collaborative user interface model for session handling; and interchangeable user roles with multicamera perspectives, avatar awareness and shared 3D annotations. We validate the framework by converting the existing ALICE VR Navigator into complete CVRE, with experimental results showing good performance in the collaborative inspection and manipulation of complex models.}
}
@article{PARAMARTHA2023874,
title = {Multimedia Application based on Virtual Reality to Introduce College Majors in Universities},
journal = {Procedia Computer Science},
volume = {227},
pages = {874-883},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.594},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017611},
author = {Damar Harip Paramartha and M.Afdhal Arief Malik and Selvi Dian Pertiwi and Reinert Yosua Rumagit},
keywords = {Virtual Reality, College Major, Universities},
abstract = {The goal of this project is to create virtual reality-based multimedia programs that can introduce users to fresh university experiences and information. Many prospective students are currently selecting the incorrect major, which could have negative effects on those prospective students. The Game Development Life Cycle, often known as the GDLC approach, is the development process method employed in this application. The GDLC process begins with initiation and progresses through pre-production, testing, beta, and release. Several respondents have tested the application. According to the survey's findings, 74.5% of respondents said that this application may encourage students to select the majors they are passionate about. According to the testimonial results, 77.1% of respondents thought this application was simple to understand.}
}
@article{KIM2010373,
title = {AR interfacing with prototype 3D applications based on user-centered interactivity},
journal = {Computer-Aided Design},
volume = {42},
number = {5},
pages = {373-386},
year = {2010},
note = {Advanced and Emerging Virtual and Augmented Reality Technologies in Product Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S001044850800198X},
author = {Seungjun Kim and Anind K. Dey},
keywords = {Augmented reality, 3D application prototyping, Human–computer interaction, Ubiquitous computing},
abstract = {Augmented Reality (AR) has been acclaimed as one of the promising technologies for advancing future UbiComp (Ubiquitous Computing) environments. Despite a myriad of AR applications and its influence on the human–computer interaction field, end products that integrate AR are less commonplace than expected. This is due to a lack of being able to forecast in which direction mainstream standardization of AR-oriented platform components will be framed. It is mainly due to a focus on technology issues in implementing reasonable AR solutions, which also results in difficulty in initiating AR research and creating prototype test-beds. Thus, this paper provides a comprehensive review of AR prototyping trends and AR research activities. Through the technical review of a de facto AR prototyping method, we remark upon the key elements of AR techniques, and then present an alternative view of the AR environment centered on end-user advantages in interaction, which is characterized by three features: intuitive observation, informative visualization, and immersive interaction. In addition, we believe that these features can be used to motivate the integration of AR technology into custom-built 3D applications. In this paper, we propose a conceptual schema and an architectural framework for generic AR prototyping. On the basis of these, a video see-through AR interface is integrated into three prototype 3D applications in 3 different domains: engineering systems, geospace, and multimedia. As the case studies for validating our integration, we present three sample AR applications; (a) an AR-interfaced 3D CAE (Computer-Aided Engineering) simulation test-bed, (b) a two-stage distributed Traveler Guidance Service (TGS) test-bed based on a GIS database and AR, and (c) a haptically-enhanced broadcasting test-bed for AR-based 3D media production. For each application, a description of the test-bed implementation, demonstration of the feasibility and usefulness and AR-specific technical challenges are included in this paper.}
}
@article{AJORLOO2020365,
title = {MinDFul: Using double links for stabilizing mmWave wireless channels for application to autonomous vehicles and augmented reality},
journal = {Procedia Computer Science},
volume = {175},
pages = {365-372},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317336},
author = {Hossein Ajorloo and Cormac J. Sreenan and Roberto Bomfin and Martin Danneberg and Gerhard Fettweis},
keywords = {Millimetre wave (mmWave), Scheduling algorithm, Virtual reality (VR) games, Autonomous vehicles, Wireless communication, Beamsteering protocol, Steerable antenna array, Real-time system, Experimental testbed, 5G},
abstract = {Applications that require short-range ultra-high bitrate communication, such as cable removal in virtual reality games and communication between autonomous vehicles, are examining solutions such as millimetre wave wireless (mmWave). When using mmWave, steerable directional antennas are used to mitigate the severe signal power attenuation common with high frequencies. Nonetheless, even small movements in the user device can cause a sudden drop in data-rate down (even to 0 bits/s) making mmWave channels unstable and unusable. To make the channel more stable for the aforementioned applications, which are vulnerable due to frequent blockages and fast movement, we designed and developed a robust solution based on a double link mmWave system. We duplicate the radio transceivers (RT) of a user device (UD) to increase the probability of finding line of sight to an access point (AP) representing the other side of the communication channel. The AP selects one RT of the UD for communication, based on continuous measurement of quality compared to the channel of the other RT. This concept was implemented in a laboratory environment and evaluated using a series of controlled experiments. The experiments serve to validate that using double links is feasible, and is considerably more robust and it can double the link utilization, compared to only using one mmWave link. These results show great promise for the concept, by demonstrating that using multiple mmWave links yields ultra-high bit-rate wireless communication with no disruption, even in the presence of blockages and mobility.}
}
@article{LOPEZCHAVEZ2020104226,
title = {A comparative case study of 2D, 3D and immersive-virtual-reality applications for healthcare education},
journal = {International Journal of Medical Informatics},
volume = {141},
pages = {104226},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104226},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620304330},
author = {Omar {López Chávez} and Luis-Felipe Rodríguez and J. Octavio Gutierrez-Garcia},
keywords = {Healthcare education, Virtual reality, Comparative case study},
abstract = {Background and objective
The workings of medical educational tools are implemented using a myriad of approaches ranging from presenting static content to immersing students in gamified virtual-reality environments. The objective of this paper is to explore whether and how different approaches for designing medical educational tools affect students’ learning performance.
Materials and methods
Four versions of an educational tool for the study of clinical cases were implemented: a 2D version, a gamified 2D version, a gamified 3D version, and a gamified immersive-virtual-reality version. All complying with the same functional requirements. Each version was used and evaluated by an independent group of students. The participants (n = 78) evaluated the applications regarding usefulness, usability, and gamification. Afterward, the students took an exam to assess the retention of information on the clinical cases presented.
Results
One-sample Wilcoxon signed-rank tests confirmed that the participants perceived that it was at least quite likely that gamification helped improved their learning. In addition, based on the participants’ perception, the gamification of the immersive-virtual-reality version helped the most to improve their learning performance in comparison with the gamified 2D and 3D versions.
Conclusions
Regardless of whether different versions of a medical educational tool (complying with the same functional requirements) are perceived as equally useful and usable, the design approach (either 2D, 3D, or immersive-virtual-reality with or without gamification) affects students’ retention of information on clinical cases.}
}
@article{GOMEZJAUREGUI2019124,
title = {Quantitative evaluation of overlaying discrepancies in mobile augmented reality applications for AEC/FM},
journal = {Advances in Engineering Software},
volume = {127},
pages = {124-140},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818305532},
author = {Valentin GOMEZ-JAUREGUI and Cristina MANCHADO and Jesús DEL-CASTILLO-IGAREDA and Cesar OTERO},
keywords = {Augmented Reality, Mobile Augmented Reality, Error estimation, Tracking, CAD, BIM, Civil Engineering, AEC/FM, Geo-Location, Sensors},
abstract = {Augmented Reality (AR) is a trending technology that provides a live view of the real and physical environment augmented by virtual elements, enhancing the information of the scene with digital information (sound, video, graphics, text or geo-location). Its application to architecture, engineering and construction, and facility management (AEC/FM) is straightforward and can be very useful to improve the on-site work at different stages of the projects. However, one of the most important limitations of Mobile Augmented Reality (MAR) is the lack of accuracy when the screen overlays the virtual models on the real images captured by the camera. The main reasons are errors related to tracking (positioning and orientation of the mobile device) and image capture and processing (projection and distortion issues). This paper shows a new methodology to mathematically perform a quantitative evaluation, in world coordinates, of those overlaying discrepancies on the screen, obtaining the real-scale distances from any real point to the sightlines of its virtual projections for any AR application. Additionally, a new utility for filtering built-in sensor signals in mobile devices is presented: the Drift-Vibration-Threshold function (DVT), a straightforward tool to filter the drift suffered by most sensor-based tracking systems.}
}
@article{DAMMACCO2022103761,
title = {Designing complex manufacturing systems by virtual reality: A novel approach and its application to the virtual commissioning of a production line},
journal = {Computers in Industry},
volume = {143},
pages = {103761},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103761},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001580},
author = {Lucilla Dammacco and Raffaele Carli and Vito Lazazzera and Michele Fiorentino and Mariagrazia Dotoli},
keywords = {Industry 4.0, Complex manufacturing systems, System design, Virtual reality, Virtual commissioning, Human computer interaction},
abstract = {The design of complex manufacturing systems (CMSs) is challenging, because of the requirements of efficiency, safety, and ergonomics, and the need of optimizing resources, i.e., space, machines, operators, and data. Virtual reality (VR) – one of the promising technologies at the base of Industry 4.0 – is able to address the design issues of CMSs, and even decrease costs and time when employed from the initial conception to the final validation of production lines, since it facilitates their virtual commissioning, i.e., it enables the full verification of systems and related components by virtual inspection and tests. Despite the above advantages, VR is still rarely used in the design of CMSs, and there is no standard VR approach in industry yet. In addition, the related scientific literature is scarce and often limited to small or simplified cases. To fill this gap, this work presents a novel VR-based approach for designing CMSs, composed of four phases: Three-dimensional CAD Export, Model Import, Scene Creation, and VR Review. The proposed approach is applied to a real industrial use case related to the virtual commissioning of an electric axles production line and it is evaluated through a questionnaire from industry professionals. The case study shows that using the VR technology enhanced the technical communication between experts in the teamwork, and it was particularly effective in finding ergonomics flaws like issues in visibility, reach, and posture using a virtual golden zone. In addition, all users found the VR interaction enjoyable and easy to learn, and beginner users perceived a comparable workload as advanced users.}
}
@article{GUIMARAES20151373,
title = {Graphical High Level Analysis of Communication in Distributed Virtual Reality Applications},
journal = {Procedia Computer Science},
volume = {51},
pages = {1373-1382},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011515},
author = {Marcelo de Paiva Guimarães and Bruno Barberi Gnecco and Diego Roberto Colombo Dias and Jośe Remo Ferreira Brega and Luis Carlos Trevelin},
keywords = {Analysis, distributed computing, Virtual Reality, message passing, bug, defects},
abstract = {Analysing distributed virtual reality applications communicating through message-passing is challenging. Their development is complex, and knowing if something is wrong depends on the states of each process, defects (bugs) cause software crashes, hangs, and generation of incorrect results. To address this daunting problem we specify functional behavior models (for example, using synchronization barriers and shared variables) for these applications that ensures correctness. We also developed the GTracer tool, which compares the functional behavior models developed with the messages transmitted among processes. GTracer checks for violations of these models automatically and displays the message traffic graphically. It is a tool made for libGlass, a message library for distributed computing. We have been able to find several non-trivial defects during the tests of this tool.}
}
@article{HUANG2023107815,
title = {Effects of virtual reality on creative performance and emotions: A study of brainwaves},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107815},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107815},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001668},
author = {Huai-Te Huang and Yu-Shan Chang},
keywords = {Virtual reality, Creative performance, Emotions, Brainwaves},
abstract = {Creativity is the basis for innovation and a source of national and industrial competitiveness. Thus, pedagogical enhancement of creativity is desirable, particularly via application of new technologies such as virtual reality (VR). The purpose of this study was to examine the effects of VR on creativity (creative outcomes and the creative process), emotions, and brainwaves using a nonequivalent-group pre-post-test quasi-experimental design involving 76 tenth-grade students. An experimental group received training that incorporated VR elements, and a comparison group was taught via lectures combined with multimedia presentations. The main conclusions are as follows: VR exerted large positive effects on the elaboration, vividness, and novelty creative outcomes; VR exerted above-moderate positive effects on feasibility but not novelty; VR exhibited small effect sizes in terms of enjoyment, anger, and anxiety, but a large effect size for boredom, and these emotions positively influenced creativity; and beta and gamma brainwave activity during the ideation and elaboration stages, and delta brainwave activity during the latter stage, were significantly lower in the experimental than comparison group.}
}
@article{BEHZADAN200890,
title = {General-purpose modular hardware and software framework for mobile outdoor augmented reality applications in engineering},
journal = {Advanced Engineering Informatics},
volume = {22},
number = {1},
pages = {90-105},
year = {2008},
note = {Intelligent computing in engineering and architecture},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2007.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034607000481},
author = {Amir H. Behzadan and Brian W. Timm and Vineet R. Kamat},
abstract = {This paper presents a reusable, general-purpose, mobile augmented reality (AR) framework developed to address the critical and repetitive challenges specific to visualization in outdoor AR. In all engineering applications of AR developed thus far, basic functionality that supports accurate user registration, maximizes the range of user motion, and enables data input and output has had to be repeatedly re-implemented. This is primarily due to the fact that designed methods have been traditionally custom created for their respective applications and are not generic enough to be readily shared and reused by others. The objective of this research was to remedy this situation by designing and implementing a reusable and pluggable hardware and software framework that can be used in any AR application without the need to re-implement low-level communication interfaces with selected hardware. The underlying methods of hardware communication as well as the object-oriented design (OOD) of the reusable interface are presented. Details on the validation of framework reusability and pluggability are also described.}
}
@article{JETTER201818,
title = {Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits?},
journal = {Computers in Human Behavior},
volume = {87},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.054},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830222X},
author = {Jérȏme Jetter and Jörgen Eimecke and Alexandra Rese},
keywords = {Augmented reality, Industrial applications, Key performance indicator, Ready for market, Technology acceptance, Technology acceptance model},
abstract = {Augmented reality (AR) tools are poised to have great potential for organizations when it comes to complex processes in the field of industrial applications – like construction or maintenance in the automotive industry. The human-centred technology displays context-specific 3-D information in a real environment related to a specific targeted object. Immersive experiences are expected to boost task efficiency, the quality of training and maintenance purposes. However, ready-for-market AR tools are still rarely used and benefits seldom demonstrated. This paper focuses on the key performance indicators (KPIs) that are able to benchmark the impact of using ready-for-market AR tools on automotive maintenance performance. After a comprehensive literature review on the benefits of AR for several industrial applications in design, education and training, KPIs were extracted and evaluated by experts from the automotive industry. They were used in an empirical study – based on the technology acceptance model (TAM) – with users evaluating the KPIs before and after training situations. In addition, ‘Perceived Usefulness’ and ‘Intention to Use’ were investigated. Significant enhancements of all KPIs were observed and novice users were identified as a potential target group.}
}
@article{KYAW2023104912,
title = {Augmented Reality for high precision fabrication of Glued Laminated Timber beams},
journal = {Automation in Construction},
volume = {152},
pages = {104912},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104912},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523001723},
author = {Alexander Htet Kyaw and Arvin HaoCheng Xu and Gwyllim Jahn and Nick {van den Berg} and Cameron Newnham and Sasa Zivkovic},
keywords = {Augmented reality, Fabrication accuracy, Glulam manufacturing, QR-code marker, Drift error, Quality control, Fabrication, Precision},
abstract = {In recent years, the utilization of Augmented-Reality (AR) within the construction industry has increased. However, high-precision applications such as glulam beam fabrication requires tolerances of less than 2 mm, which is smaller than what current AR workflows can offer. This research investigates the use of QR-Code Markers on glulam beams to encode additional positional data in the environment to better interpolate between the physical space, the glulam beam, and the headset. The objective is to understand the effects of Marker placement, size, and frequency on the accuracy of AR projection for glulam fabrication. This paper describes the AR workflow, the effects of Markers, and the framework the Twinbuild software for drift correction in large-scale AR applications. The method can achieve an average tolerance as low as 0.97 mm when Markers are placed in 1.25 ft. increments along the beam edge. The research demonstrates the viability of AR for high-precision fabrication applications.}
}
@article{ROSALES2021618,
title = {IIoT based Augmented Reality for Factory Data Collection and Visualization},
journal = {Procedia Manufacturing},
volume = {53},
pages = {618-627},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S235197892100072X},
author = {Jonathan Rosales and Sourabh Deshpande and Sam Anand},
keywords = {IIoT, Augmented Reality, Factory Data, Industry 4.0, Mixed Reality, Smart Manufacturing},
abstract = {In Industry 4.0, machine data acquisition plays an important role in improving overall performance on the factory floor by monitoring processes, providing feedback, and integrating the information with other machines. Leveraging these hidden data instantaneously to reduce machine downtime could also minimize efforts in determining the causes of maintenance issues. This paper explores a novel method of bringing the factory floor data into an Industrial Internet of Things (IIoT) environment and visualizing real-time machine analytics through Augmented Reality (AR) and Mixed Reality (MR) based smart devices. The factory floor data from selective machines at a Volvo Group plant was integrated with IIoT and visualized in real-time through the use of AR-enabled devices such as an iPad and a HoloLens headset. The configuration process for the server and IIoT environment was automated through a software solution, and the performance and feasibility of spatial anchors were evaluated by testing accuracy, triggering times, and latency of data. The results displayed promising applications of AR/MR on factory floors for real-time machine data visualization and associated troubleshooting.}
}
@article{MOHAMMADKHORASANI2023103936,
title = {Augmented reality-computer vision combination for automatic fatigue crack detection and localization},
journal = {Computers in Industry},
volume = {149},
pages = {103936},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103936},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000866},
author = {Ali Mohammadkhorasani and Kaveh Malek and Rushil Mojidra and Jian Li and Caroline Bennett and William Collins and Fernando Moreu},
keywords = {Augmented reality, Structural health monitoring, Crack detection algorithm, Fatigue crack, Computer vision, Holographic anchoring},
abstract = {Fatigue cracks in bridges are inspected visually by specialized bridge inspection professionals. Bridge inspectors conduct inspections in a limited amount of time, and at times small cracks may go unnoticed. Researchers have recently developed computer-based techniques to help overcome these issues. This study uses a computer vision algorithm combined with Augmented Reality (AR) to localize fatigue cracks during the visual inspection that otherwise may go unnoticed because of their size. The AR software utilizes a video processing algorithm for fatigue crack detection. Subsequently, the AR software generates a hologram using the algorithm’s detection result and anchors it over the crack’s location at the structure being inspected. The result of this new methodology is an automatic fatigue crack detection and localization AR software that provides holograms overlaid during the on-site visual inspection. This technique also provides the fatigue crack detection result in near-real-time. The method is verified using 2D and 3D benchmarks and a half-scale steel bridge girder specimen. The research team collected the feedback from bridge industry inspectors contextualized in real operations and their recommendations for integrating AR for inspections with the involvement of human resources and workforce development. This study is the first effort of holographic addressing the fatigue crack on the actual structure using AR headset.}
}
@article{DINI201514,
title = {Application of Augmented Reality Techniques in Through-life Engineering Services},
journal = {Procedia CIRP},
volume = {38},
pages = {14-23},
year = {2015},
note = {Proceedings of the 4th International Conference on Through-life Engineering Services},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115008033},
author = {G. Dini and M. Dalle Mura},
keywords = {Augmented Reality, Through-life Engineering},
abstract = {Augmented Reality (AR) is an innovative human-machine interaction that overlays virtual components on a real world environment with many potential applications in different fields, ranging from training activities to everyday life (entertainment, head-up display in car windscreens, etc.). The capability to provide the user of the needed information about a process or a procedure directly on the work environment, is the key factor for considering AR as an effective tool to be also used in Through-life Engineering Services (TES). Many experimental implementations have been made by industries and academic institutions in this research area: applications in remote maintenance, diagnostics, non-destructive testing, repairing and setup activities represent the most meaningful examples carried out in the last few years. These applications have concerned different working environments such as aerospace, railway, industrial plants, machine tools, military equipment, underground pipes, civil constructions, etc. The keynote paper will provide a comprehensive survey by reviewing some recent applications in these areas, emphasizing potential advantages, limits and drawbacks, as well as open issues which could represent new challenges for the future.}
}
@article{RAMIREZ2013189,
title = {Authoring Software for Augmented Reality Applications for the Use of Maintenance and Training Process},
journal = {Procedia Computer Science},
volume = {25},
pages = {189-193},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012283},
author = {Hector Ramirez and Eduardo Gonzalez Mendivil and Pablo Ramirez Flores and Manuel Contero Gonzalez},
keywords = {Augmented Reality, PLM, Authoring, Maintenance},
abstract = {Augmented Reality (AR) in the last decade has increased the popularity on various areas, such as education, advertising, maintenance, marketing and entertainment. On the area of maintenance and education specially we have been researching the benefits of the use of augmented reality bring us, and we have discover that the transfer of knowledge is faster than the traditional methods, and help to companies to train their employees faster and better. Many of the AR applications are custom made to the client needs, and to make an application of AR involves different types of skills such as programming, designing, modeling, animating, texturing. Given the high cost of these or the lack of some of these abilities, the need of programs of “authoring” has increase that permit to users create AR processes using just the GUI without having learn how to program. This papers describes the program developed “ManAR” an authoring program that permits the user to create an AR process for maintenance and training. The application permit to companies to create process assisted by augmented reality to train and use on the field. The application links tridimensional models to a mark, and make use of pictures, texts and videos, to enhance the experience, and finally visualize the final product on tablets. Also other benefit is to access relevant information such as times, errors of the employees to improve AR process or to know how the users are progressing with their training.}
}
@article{XU2023100568,
title = {Design and application of VR-based college English game teaching},
journal = {Entertainment Computing},
volume = {46},
pages = {100568},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100568},
url = {https://www.sciencedirect.com/science/article/pii/S187595212300023X},
author = {Yuecheng Xu and Gawa Bao and Xiaokai Duan},
keywords = {DNN, English teaching, Game, HMM, VR},
abstract = {With the deepening of international exchanges, colleges and universities have higher and higher requirements for the quality of English teaching. In order to improve the quality of English teaching in colleges and universities, the research proposes to design and construct an English game education and teaching system based on virtual reality technology. First, the modern VR technology is used to build the initial English game teaching system, and then the HMM-DNN algorithm is introduced to improve the virtual system. In the process, the CMUShinx speech recognition platform is used to build a robot control command speech recognition system, in order to obtain a good voice acoustic model and achieve correct speech recognition in the virtual environment. Finally, the performance analysis of the built system is carried out to verify the feasibility of the system model. The results show that the step response coefficient of the model built by the research institute can be kept within 1 M for a long time; When the speech type is White and Cafe1, the correct recognition rate of the constructed model for English speech is as high as 98 % and 95 %, and is always higher than other algorithms; In the comparison of the accuracy of students' oral speech recognition scores by different models, the lowest error of the research model is 2.23 %, which is significantly smaller than the other models. The above results indicate that the overall teaching effect of the system built by the research institute is stable, and it has high feasibility for college students' English teaching, which helps to further improve students' interest and ability in English learning, and is of great significance for college English talent training.}
}
@article{WU2023104802,
title = {Cognitive ergonomics-based Augmented Reality application for construction performance},
journal = {Automation in Construction},
volume = {149},
pages = {104802},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104802},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000626},
author = {Shaoze Wu and Lei Hou and Haosen Chen and Guomin (Kevin) Zhang and Yang Zou and Quddus Tushar},
keywords = {Augmented reality, Cognitive ergonomics, Construction assembly task, Kinaesthetic performance},
abstract = {There is a growing interest in exploring the use of wearable Augmented Reality (AR) devices to improve the task performance of construction workers. However, user interaction with AR has not been well understood in the current literature, which may result in poor usability, occupational hazards, and low acceptance. To bridge this gap, this study introduced cognitive ergonomics theory to design and develop an AR application for improving the kinaesthetic performance of construction workers conducting onsite assembly tasks. The methodology of this study is three-fold. First, articles in relation to cognitive ergonomics were reviewed to propose a unique cognitive model that reveals the cognitive mechanisms of construction workers, including human information processing, selective attention, and attention resources. Second, the characteristics of existing AR application functions were synthesised to develop a customised and user-friendly wearable AR application that aligns with the identified cognitive mechanisms. Third, a rebar-tying experiment was conducted to validate the developed AR application. The results indicate that the experimenters instructed by the application can complete the task independently without the need to seek after expert assistance; the application has a potential to foster the skill development of construction workers and enhance their kinaesthetic performance; and the proposed cognitive model and the AR development principles are well aligned from the perspective of cognitive ergonomics, which could promote the uptake of wearable AR in the construction industry.}
}
@article{ALSHAWABKEH2024e00325,
title = {Virtual reality as a tool to enhance the efficiency and reliability of the virtual reconstruction process for heritage/archaeological sites: The case of umm Al-Jimal in Jordan},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {33},
pages = {e00325},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00325},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000109},
author = {Rami {Al shawabkeh} and Mai Arar},
keywords = {Virtual reconstruction, Visualization, Virtual reality, Prediction, Photogrammetry, Archaeological site, Digital archaeology, Unreal engine},
abstract = {The growing attention towards reconstruction, driven by its potential to promote sustainable heritage preservation, is impacted by a shortage of the data, sources, and interpretive methods employed in virtual reconstruction. Consequently, the public (individuals and reviewers/Professionals) faces challenges in determining between the original elements and the added elements, in addition to identifying the origin of the new elements. Despite researchers' efforts to implement virtual reality as a solution, its effectiveness is constrained to the end outcome due to the lack of integration of reconstruction information. A set of factors contribute to the challenge of incorporating interactive information into virtual reality, including the researcher's expertise in virtual reality software and the deterministic nature of the C programming language. This necessitates the development of more user-friendly methods to support researchers in reducing the shortage and limitations. As a result, the study aims to develop a method for simplifying the process of transferring virtual reconstruction operations to virtual reality and test its efficiency through interviews with individuals and reviewers/Professionals. This is accomplished through the integration of virtual reconstruction steps with the Unreal Engine program, which serves as a virtual reality platform and is easier to program than Unity which is more prevalent in heritage research. The findings delivered a more efficient approach to collecting diverse sources and integrating them into one platform simply. This approach also facilitates direct communication between reviewers/Professionals and researchers, thereby enhancing the efficiency of the reconstruction peer review process. Furthermore, the utilization of virtual reality has played an important role in enhancing comprehension between what is the origin and added, as well as facilitating the validation of the rebuilding procedure among individuals and reviewers/Professionals.}
}
@article{FEDORKO2021105615,
title = {Application possibilities of virtual reality in failure analysis of conveyor belts},
journal = {Engineering Failure Analysis},
volume = {128},
pages = {105615},
year = {2021},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2021.105615},
url = {https://www.sciencedirect.com/science/article/pii/S1350630721004763},
author = {Gabriel Fedorko},
keywords = {Failure analysis, Virtual reality, Conveyor belts},
abstract = {Failure analysis of conveyor belts is realised using a wide range of the scientific methods. Visualisation of damage plays a very important role in performing of the given failure analysis. Development of the exponential technologies offer a possibility to apply the damage visualisation for analysis of failures in the area of rubber-textile conveyor belts. Virtual reality enables to create an immersive display, what can be very beneficial in the process of failure analysis. The basic idea of this article is to verify whether application of the virtual reality is suitable for purposes of the failure analysis concerning the rubber-textile conveyor belts. Application possibility of the virtual reality will be investigated and presented on example of the failure analysis for a test specimen selected from the damaged conveyor belt. The main task of this analysis is to discover a damage in the conveyor belt specimen, to evaluate this damage and to analyse the damage status of the individual layers creating internal structure of the conveyor belt. At the same time, there is presented in this article a methodology how to realise the failure analysis using a support of the virtual reality. Till now, there is not known any scientific-research work focused on the failure analysis of the conveyor belts, which would be based on the virtual reality. This article emphasizes application possibilities of the virtual reality within the given research area.}
}
@article{ROULLIER2007239,
title = {Fuzzy algorithms: Application to adipose tissue quantification on MR images},
journal = {Biomedical Signal Processing and Control},
volume = {2},
number = {3},
pages = {239-247},
year = {2007},
note = {IFAC Symposia on Biomedical Systems Modelling & Control},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2007.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1746809407000481},
author = {Vincent Roullier and Christine Cavaro-Ménard and Guillaume Calmon and Christophe Aubé},
keywords = {Fuzzy methods, FGcM, MRI, Medical image analysis},
abstract = {Metabolic syndrome, which is related to abdominal obesity, is a fast growing disease in our western countries. Its presence greatly increases the risk of developing cardiovascular diseases. The accumulation of visceral adipose tissue plays a key role in the development of the metabolic syndrome. The increase of waist circumference is one of the five criteria of the metabolic syndrome diagnosis. But this increase can be due to visceral or subcutaneous adipose tissues. And these adipose tissues do not play the same rule in metabolic syndrome. The purpose of this study was to develop software for automatic and reliable quantification of visceral and subcutaneous adipose tissues, to detect patient with high risk to develop metabolic syndrome and to follow the evolution of adipose tissue repartition after treatment. A gradient echo magnetic resonance (MR) technique is used, with a TE such that fat and water are opposed in phase. The developed process is based on two fuzzy algorithms. First, we fuzzy generalized clustering algorithms allow to merge pixels according to their intensities. Then, fuzzy connectedness algorithm allows to merge pixels according to cost function related to distance, gradient distance and intensities. A validation is performed with a comparison between expert results made by manual drawing and purpose-made software results. Our software provides an automatic and reliable method to segment visceral and subcutaneous adipose tissue and additionally avoids in some case the problem of inhomogeneity of signal intensity.}
}
@article{BEHZADAN2015252,
title = {Augmented reality visualization: A review of civil infrastructure system applications},
journal = {Advanced Engineering Informatics},
volume = {29},
number = {2},
pages = {252-267},
year = {2015},
note = {Infrastructure Computer Vision},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2015.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034615000324},
author = {Amir H. Behzadan and Suyang Dong and Vineet R. Kamat},
keywords = {Engineering visualization, Augmented reality, Excavation safety, Building damage reconnaissance, Visual simulation, Education},
abstract = {In Civil Infrastructure System (CIS) applications, the requirement of blending synthetic and physical objects distinguishes Augmented Reality (AR) from other visualization technologies in three aspects: (1) it reinforces the connections between people and objects, and promotes engineers’ appreciation about their working context; (2) it allows engineers to perform field tasks with the awareness of both the physical and synthetic environment; and (3) it offsets the significant cost of 3D Model Engineering by including the real world background. This paper reviews critical problems in AR and investigates technical approaches to address the fundamental challenges that prevent the technology from being usefully deployed in CIS applications, such as the alignment of virtual objects with the real environment continuously across time and space; blending of virtual entities with their real background faithfully to create a sustained illusion of co-existence; and the integration of these methods to a scalable and extensible computing AR framework that is openly accessible to the teaching and research community. The research findings have been evaluated in several challenging CIS applications where the potential of having a significant economic and social impact is high. Examples of validation test beds implemented include an AR visual excavator-utility collision avoidance system that enables workers to “see” buried utilities hidden under the ground surface, thus helping prevent accidental utility strikes; an AR post-disaster reconnaissance framework that enables building inspectors to rapidly evaluate and quantify structural damage sustained by buildings in seismic events such as earthquakes or blasts; and a tabletop collaborative AR visualization framework that allows multiple users to observe and interact with visual simulations of engineering processes.}
}
@article{PRANOTO2023757,
title = {Augmented reality navigation application to promote tourism to local state attraction “Lawang Sewu”},
journal = {Procedia Computer Science},
volume = {216},
pages = {757-764},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022712},
author = {Hady Pranoto and Peter Pranata Saputra and Melki Sadekh and Herru Darmadi and Yanfi Yanfi},
keywords = {Attraction, Augmented Reality, Mobile Application},
abstract = {The writing of this paper and the creation of this application is a means to improve and add on facilities on the local state-run attraction. This subject matter was picked in conjunction with the decline of tourism in the said historical site due to a pandemic that rankles Indonesia's tourism industry and drives it to the ground. Augmented Reality as a method of deliverance was picked due to its popularity and freshness in the tourism market. The research method was conducted starting with analysis, development, implementation, then evaluation. Analysis was done using sourcing available to the public and a questioner. Implementation was done using Unity, Vuforia, and Maya. And with the positive responses of 66.6 percent of the sample market finding enjoyment in the use of this application in their exploration during User Acceptance testing with the existing prototype. This shows that the resulting use of this application improves user enjoyment in experiencing the state attraction.}
}
@article{HE2023107347,
title = {N- and p-type doping of transition-metal dichalcogenides by Ar plasma treatment and its application in CMOS},
journal = {Materials Science in Semiconductor Processing},
volume = {158},
pages = {107347},
year = {2023},
issn = {1369-8001},
doi = {https://doi.org/10.1016/j.mssp.2023.107347},
url = {https://www.sciencedirect.com/science/article/pii/S1369800123000409},
author = {Jiaoyan He and Yuanbo Wen and Dongshuang Han and Peiyu Zeng and Peng Zheng and Liang Zheng and Weitao Su and Zhangting Wu and Yang Zhang},
keywords = {TMDs, Ar plasma, n-type doping, p-type doping, CMOS inverter},
abstract = {Doping of two-dimensional (2D) semiconductors is necessary to achieve high performance and low power consumption in optoelectronic and logic devices. Herein, we report controllable p- and n-type doping of the transition-metal dichalcogenides (TMDs), tungsten diselenide (WSe2), and molybdenum ditelluride (MoTe2) via argon (Ar) plasma treatment. The doping of TMDs was tuned by controlling Ar plasma treatment conditions. The desired n-type doping in WSe2 and MoTe2 was obtained by applying a short treatment time, resulting in increased electron current and an upshift in binding energy. Owing to prolonged plasma treatment, abnormal p-type doping was achieved in WSe2 and MoTe2. Increased hole current, downshift in binding energy, appearance of oxygen bonds (O–W, Mo, Se, and Te), and redshift in Raman peaks revealed that the abnormal p-type doping behavior for WSe2 and MoTe2 was owing to considerable vacancies and edge defects induced in the top layer of TMDs by Ar plasma treatment. These were immediately occupied by oxygen atoms or oxidized to introduce oxygen doping when TMDs were exposed to air. The formation of a homogeneous oxide film on the top layer of TMDs was further demonstrated by the smooth surface and higher thickness of TMDs with heavy p-type doping compared to those of the pristine sample. A complementary metal-oxide-semiconductor (CMOS) inverter based on p- and n-WSe2 was demonstrated herein. Our study proposes a controllable way to modulate the doping type of TMDs, which has potential applications in the performance modulation of devices, design of novel device structures, and realization of 2D material-based logic circuits.}
}
@article{MURHIJ2019203,
title = {An application to simulate and control industrial robot in virtual reality environment integrated with IR stereo camera sensor⁎⁎The reported study was funded by RFBR according to the research project 19-01-00767.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {203-207},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.473},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319323821},
author = {Youshaa Murhij and Vladimir Serebrenny},
keywords = {Virtual Reality, Robot, KUKA, Unity3D, HTC VIVE, Leapmotion},
abstract = {The main goal of this research is to test for potential ways to control KUKA KR10 industrial arm manipulator using Virtual Reality technology and check for the advantages of applying this control methods. The final version of this application aims to achieve this goal by establishing an interaction between the user and the manipulator inside a virtual environment developed using the game engine Unity3D and the HTC VIVE Pro headset for the virtual visualization. By applying this control method, the user does not have to operate on site and instead he can work remotely. In addition to the ability to use off-line programming of the manipulator. The application is designed to simplify the controlling ways by displaying a complete virtual environment where the tridimensional model of the robotic arm can be visualized and programmed according to the real manipulator’s parameters and specifications. All the movements and parameters in the virtual environment can be synchronized with the real robot in an on-line or off-line path planning depending on the application or the task. The system integrates a set of virtual reality controllers and Leapmotion sensor as options to allow the user to control and see the robot and its parameters in the virtual environment. As a result of this research, the manipulator moves on the planned trajectory in a smooth way after applying some filtering techniques without losing its accuracy.}
}
@article{SANCHO2023102893,
title = {SLIMBRAIN: Augmented reality real-time acquisition and processing system for hyperspectral classification mapping with depth information for in-vivo surgical procedures},
journal = {Journal of Systems Architecture},
volume = {140},
pages = {102893},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102893},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123000723},
author = {Jaime Sancho and Manuel Villa and Miguel Chavarrías and Eduardo Juarez and Alfonso Lagares and César Sanz},
keywords = {Brain tumor, AR, GPU, Machine Learning, Medicine, Depth image, Lidar, Classification, Machine learning, IA, Neurosurgery, Biomedical engineering},
abstract = {Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scene at the same time it is captured and processed, improving the visualization and hence effectiveness of the HS technology to delimit tumors. The whole system has been verified in real brain tumor resection operations.}
}
@article{KHALEGHI2022100504,
title = {Toward using effective elements in adults’ amblyopia treatment in a virtual reality-based gamified binocular application},
journal = {Entertainment Computing},
volume = {43},
pages = {100504},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100504},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000283},
author = {Ali Khaleghi and Zahra Aghaei and Fateme Hosseinnia},
keywords = {Amblyopia, Gamification, Adult lazy eye treatment, Binocular game, Virtual reality, Smartphones},
abstract = {The common method for treating amblyopia (lazy eye) in adults is binocular therapy implemented as simple computer games. However, it lacks the maximum therapeutic elements effective in treating amblyopia and uses expensive stereoscopes and virtual reality (VR) glasses, limiting widespread use. Also, gamification can be incorporated into these games to keep patients’ motivation over a stable period. The features that have the potential to be included in binocular games are identified and included in a VR-based gamified binocular application with a more straightforward implementation than the games presented so far. The game is implemented on smartphones to increase accessibility. Color and size of game components, CAM stimulator, and binocular therapy using VR were identified and incorporated in a VR zombie first-person shooter game. Eight experts evaluated the game with a questionnaire designed based on Nielsen's heuristics evaluation and therapeutic aspects. The initial tests with experts indicated the game’s potential application as a therapeutic tool for improving visual acuity and vision’s contrast and depth. Also, the game has the potential application in increasing motivation and interaction. After rigorous evaluations with patients, the game can be used outside clinics by adjusting it to the patients’ eyes in advance and remotely.}
}
@article{MOURTZIS2020654,
title = {An augmented reality application for robotic cell customization},
journal = {Procedia CIRP},
volume = {90},
pages = {654-659},
year = {2020},
note = {27th CIRP Life Cycle Engineering Conference (LCE2020) Advancing Life Cycle Engineering : from technological eco-efficiency to technology that supports a world that meets the development goals and the absolute sustainability},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.135},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303000},
author = {D. Mourtzis and G. Synodinos and J. Angelopoulos and N. Panopoulos},
keywords = {Augmented reality, Industry 4.0, Mass customization, Robotic cell},
abstract = {Integrating the Internet of Things in Industry 4.0 demands the combination of existing practises with new technologies. Augmented Reality (AR) is a cutting-edge technology of the new manufacturing era. The fourth industrial revolution challenges and AR technology advances, promise to improve productiveness, working quality, user experience and allow better use of resources. AR combined with mass customization could fulfil rising market demands and customer functional requirements. This work presents the development of an AR application to integrate customers in the designing process with product customization. The application is to be validated and used in the industry of Robotic Cell Manufacturing.}
}
@article{ALDOARISTA2023762,
title = {Preserving Indonesian Culture in the Digital Age: Implementing Augmented Reality to Address Cultural Appropriation Issue},
journal = {Procedia Computer Science},
volume = {227},
pages = {762-771},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.581},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017489},
author = {W.P. {Aldo Arista} and S. {Daniel Hendra} and P. {Felix Juwono} and H. {Kanz Abdillah} and Frihandhika Permana},
keywords = {Cultural appropriation, identity, Augmented Reality (AR), markers, traditional},
abstract = {Abstract—Cultural appropriation is a complex issue that can lead to the loss of a nation's cultural identity. With the widespread adoption of mobile phones in Indonesia, there is an opportunity to use technology, specifically augmented reality (AR). To address this issue, our user-friendly AR app allows users to interact with Indonesian culture in a virtual environment. Through AR technology, users can engage with Indonesian culture in an immersive and fun way. Our target market is the younger generation, and our app provides a user-friendly interface that allows users to scan AR markers to access different cultural settings. Users can choose from two cultural regions, Bali and Papua. Once a region is selected, a scene showcases the desired region's iconic places and objects accompanied by a traditional folk song. In developing the AR app, we used the agile software development method for approximately two months with four developers. At the end of the development phase, we conduct four testing aspects of the application—namely, angle testing, functionally testing, usability testing, and hardware testing. The testing revealed that the application could perform well under various testing conditions. Through our AR solution, we aim to raise awareness of the importance of Indonesian culture and provide an educational experience that can inspire users to take ownership of their cultural heritage.}
}
@article{KUNDU201727,
title = {Scanning Camera and Augmented Reality Based Localization of Omnidirectional Robot for Indoor Application},
journal = {Procedia Computer Science},
volume = {105},
pages = {27-33},
year = {2017},
note = {2016 IEEE International Symposium on Robotics and Intelligent Sensors, IRIS 2016, 17-20 December 2016, Tokyo, Japan},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.183},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917302016},
author = {Ananda Sankar Kundu and Oishee Mazumder and Ankit Dhar and Prasanna K. Lenka and Subhasis Bhaumik},
keywords = {Augmented reality, Fusion, Visual localization, Omni robot, Scanning Camera},
abstract = {The aim of this paper is to develop an absolute visual localization system of an Omni wheeled robot for indoor navigation. Omni wheeled based robots have omni directional drive capacity. Conventional localization technique like odometry is not suitable for such drives due to wheel slippage. An omni robot platform with 4 omni-directional wheels powered by dynamixel motor and a scanning platform has been developed. We have implemented a localization technique using camera as visual sensor and multiple markers based on ‘ArToolKiT’ an augmented reality application. Various camera related distortion were reduced using 2nd order surface fit of camera calibration data. To increase the accuracy of the system, fusion of results from multiple markers has been implemented. Performance of the proposed localization has been verified by studying different pattern based movement of the system in a test area of 5m X 5m. Novelty of this paper is in development of an omni wheeled robotic wheelchair and proposing a robust absolute visual based localization set up with single camera and multiple fused markers for indoor navigation.}
}
@article{CAI201431,
title = {A case study of Augmented Reality simulation system application in a chemistry course},
journal = {Computers in Human Behavior},
volume = {37},
pages = {31-40},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214002271},
author = {Su Cai and Xu Wang and Feng-Kuang Chiang},
keywords = {Augmented Reality, Chemistry learning, Inquiry-based learning},
abstract = {The comprehension of micro-worlds has always been the focus and the challenge of chemistry learning. Junior high school students’ imaginative abilities are not yet mature. As a result, they are not able to visualize microstructures correctly during the beginning stage of chemistry learning. This study targeted “the composition of substances” segment of junior high school chemistry classes and, furthermore, involved the design and development of a set of inquiry-based Augmented Reality learning tools. Students could control, combine and interact with a 3D model of micro-particles using markers and conduct a series of inquiry-based experiments. The AR tool was tested in practice at a junior high school in Shenzhen, China. Through data analysis and discussion, we conclude that (a) the AR tool has a significant supplemental learning effect as a computer-assisted learning tool; (b) the AR tool is more effective for low-achieving students than high-achieving ones; (c) students generally have positive attitudes toward this software; and (d) students’ learning attitudes are positively correlated with their evaluation of the software.}
}
@article{CHOI2023103132,
title = {LUVI: Lightweight UWB-VIO based relative positioning for AR-IoT applications},
journal = {Ad Hoc Networks},
volume = {145},
pages = {103132},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103132},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523000525},
author = {Hong-Beom Choi and Keun-Woo Lim and Young-Bae Ko},
keywords = {Location-based service, Augmented reality, Ultra-wideband, Indoor positioning system, Internet of Things},
abstract = {In this paper, we propose LUVI, Lightweight UWB-VIO relative positioning method for indoor localization. Recent designs of handheld and embedded devices feature various technologies which have the means to enhance localization performance in indoor environments. These include visual odometry based on cameras and augmented reality, and communication hardware such as UWB. Integration of such technologies to exploit their advantages allows us to compensate for each other's errors in measurement. This improves the overall function of future services, such as visual representation of sensing information from sensors in areas that are not physically visible. However, existing work cannot fully exploit these technologies to high extent, often inducing high errors or wasted resources. LUVI is a novel localization method which estimates the location of a target object using relative coordinates of estimator devices without the aid of definitive coordinates. LUVI focuses on utilization of lightweight management of virtual anchors for localization, with functions that reduce the computing and communication complexity while maintaining the accuracy and improving energy efficiency of the localization. Our work has been fully implemented and tested in several indoor environments, showing robustness to NLOS while significantly reducing computational complexity, and up to 30% lower average error.}
}
@article{TAN2024105318,
title = {Building defect inspection and data management using computer vision, augmented reality, and BIM technology},
journal = {Automation in Construction},
volume = {160},
pages = {105318},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105318},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000542},
author = {Yi Tan and Wenyu Xu and Penglu Chen and Shuyan Zhang},
keywords = {Augmented reality, BIM, Building defects, Computer vision},
abstract = {Regular inspection and management of building defects are vital for the structural integrity of buildings, but traditional manual methods often lead to inefficiencies and misjudgments in assessing defect severity. This paper presents a framework integrating computer vision, augmented reality (AR), and building information modeling (BIM) to enhance defect inspection and management. The framework includes an AR-based defect inspection (ARDI) application with interactive user interfaces and a BIM-based defect data management (DDM) platform. The ARDI application utilizes YOLOv5 + DeepSORT algorithms for effective defect detection and tracking from real-time video streams, precisely mapping defect dimensions from 2D images to 3D coordinates and synchronizing this data with BIM for uploading to the DDM platform. Two experiments confirm that the AR technology achieves centimeter-level precision, and the framework overall enhances inspection efficiency by 78.63% compared to manual methods. This advanced framework not only improves inspection efficiency but also enables managers to comprehensively manage the entire inspection process.}
}
@article{CICCARELLI2022540,
title = {Interface and interaction design principles for Mixed Reality applications: the case of operator training in wire harness activities},
journal = {Procedia Computer Science},
volume = {204},
pages = {540-547},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008043},
author = {Marianna Ciccarelli and Agnese Brunzini and Alessandra Papetti and Michele Germani},
keywords = {Mixed Reality, Augmented Reality, Human-Computer Interaction, Operator training, Industry 4.0, Wire harness, User Interface},
abstract = {Operator 4.0 has to deal with a vast amount of product variants and production data especially within the mass customization paradigm, high mental demanding tasks, and smart production systems. Technologies capable of supporting his training and his work become fundamental, such as the extended reality (XR). Its increasing use in industrial applications, however, opens up new challenges related to interface and interaction design, which can determine the success of both the use and development experience. The lack of guidelines for designing interfaces for mixed reality (MR) applications is what this paper aims to address. Design requirements for MR interfaces are presented and applied in the context of operator training in wire harness activities. Different interaction modes and user interfaces have been developed to evaluate the most suitable and user-friendly one for the operator. A pilot test was conducted to assess the applications’ usability and potentialities with satisfactory results.}
}
@article{HORVAT2022101760,
title = {Immersive virtual reality applications for design reviews: Systematic literature review and classification scheme for functionalities},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101760},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101760},
url = {https://www.sciencedirect.com/science/article/pii/S147403462200218X},
author = {Nikola Horvat and Steffen Kunnen and Mario Štorga and Arun Nagarajah and Stanko Škec},
keywords = {Immersive virtual reality, Design review, Classification scheme, Functionalities, Systematic literature review, PRISMA},
abstract = {The development of immersive virtual reality (IVR) applications for design reviews is a major trend in the design field. While many different applications have been developed, there is little consensus on the functionalities necessary for these applications. This paper proposes a classification scheme for IVR functionalities related to design reviews (DRs), combining conceptual-to-empirical and empirical-to-conceptual strategies. The classification scheme consists of eight class categories (Input, Representation, Navigation, Manipulation, Collaboration, Edit, Creation, and Output), 22 class subcategories, and 55 classes. The classification scheme has been validated by analysing several commercial IVR applications for DRs. As part of the classification scheme development, Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) was utilised to review 70 articles that develop IVR applications for DRs. The results from systematic literature reviews suggest the development of solutions that integrate several class categories, are better connected to current design workflows, include various design information, support a DR planning cycle, and support distributed work. The proposed classification scheme helps to orient the future development of IVR applications for DRs and provides a framework to systematically accumulate evidence on the effect of such applications on DRs.}
}
@article{EDER20207,
title = {On the application of Augmented Reality in a learning factory working environment},
journal = {Procedia Manufacturing},
volume = {45},
pages = {7-12},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310684},
author = {Matthias Eder and Maria Hulla and Felizian Mast and Christian Ramsauer},
keywords = {Augmented Reality, AR, Digitalization, Assistance systems, Mixed Reality, Digital training, Learning Factories},
abstract = {The manufacturing industry is currently facing numerous challenges such as shortening innovation cycles, the demographic challenge and cost pressure. Augmented Reality (AR) offers tremendous potential for enhancing manufacturing processes in terms of productivity, education and safety and is therefore being intensively researched by scientists at present. One of the advantages is that it facilitates support for employees and optimizes the working environment. Thus, operators can exploit their innovation potential and creativity. The aim of this paper is to discuss the potentials of AR to face current challenges in production, show state-of-the art applications and present an own developed AR application to support the employee with their work tasks. With the application introduced, employees are provided with real-time data from their machinery, which is displayed in their field of vision. In addition, the application enables the processing of more complex and non-repetitive tasks that would otherwise require additional know-how or personnel. By using the application, instructions for maintenance work are embedded in the environment, allowing the operator to focus on his or her tasks. In the event of a problem, the operator can share the environment so an expert can perform remote assistance in real-time. In order to increase the acceptance of AR technologies, the application has been integrated into the LEAD factory at Graz University of Technology and is actively used in training courses. In this paper this AR application is tested and the test persons are asked with a questionnaire regarding their experience.}
}
@article{HAN2022104329,
title = {Generic extended reality and integrated development for visualization applications in architecture, engineering, and construction},
journal = {Automation in Construction},
volume = {140},
pages = {104329},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104329},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002023},
author = {Bing Han and Fernanda Leite},
keywords = {Extended reality, Building information modeling, Generic model, Model transfer, Quantitative measurement},
abstract = {Building Information Modeling (BIM)-enabled Extended Reality (XR) applications have already benefited the Architecture, Engineering, and Construction (AEC) industry in many tasks. However, the lack of knowledge on a generic XR (GenXR) model has obstructed efficient BIM-to-XR model transfer. This paper proposes a GenXR model that supports XR development in a project's lifecycle. The model bypasses repetitive activities in traditional BIM-to-XR. The authors developed six XR prototypes in two case studies to validate the GenXR model. Results show that the GenXR model supports typical model-related XR functions. Developing the GenXR model could save 63.8% to 66.7% in development time. Thus, a GenXR model could enable project engineers to leverage interactive visualization in a variety of use cases, where more appropriate technologies can be implemented with less technical or financial constraints. This paper contributes to the body of knowledge by creating a GenXR model with validations on information requirements, workflow, and performance.}
}
@article{JOHANSSON2024105233,
title = {Real-world applications of BIM and immersive VR in construction},
journal = {Automation in Construction},
volume = {158},
pages = {105233},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105233},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004934},
author = {Mikael Johansson and Mattias Roupé},
keywords = {Building information modeling, BIM, Virtual reality, VR, openBIM, IFC, Real-time rendering},
abstract = {The integration of immersive Virtual Reality (VR) and Building Information Modeling (BIM) has many applications within the Architecture, Engineering, and Construction (AEC) industries. In this context VR is often highlighted for its ability to convey scale and details, especially when compared to non-immersive visualizations. However, despite being an active area of research, there is currently a lack of real-world studies exploring immersive VR in a construction-oriented context. In addition, there are still technical challenges and barriers for an efficient integration, such as rendering performance and interoperability issues. This paper addresses these issues by investigating the use of immersive, single- and multi-user VR within the openBIM ecosystem. The contribution is two-fold: (a) an in-depth presentation of algorithms and technical details of a multi-user VR application for immersive visualization of large and complex BIMs and (b) an evaluation of this VR system on several real-world construction projects. In all cases the VR visualization has been directly realized from the design teams IFC-models and the multi-user sessions has been performed both co-located as well as fully remote. The results show that multi-user VR improves communication, understanding, and collaboration, and by letting staff with knowledge and experience from construction production review the project in VR, design errors and constructability issues can be identified and resolved before reaching the actual production stage. Moreover, the use of VR is helpful regarding sequencing and planning, and to identify alternative design solutions.}
}
@article{GRODOTZKI20231246,
title = {Enhancing manufacturing education based on controller-free augmented reality learning},
journal = {Manufacturing Letters},
volume = {35},
pages = {1246-1254},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.068},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001256},
author = {Joshua Grodotzki and Benedikt Tobias Müller and A. Erman Tekkaya},
keywords = {Augmented reality, Manufacturing education, Engineering education 4.0, Forming technology},
abstract = {Based on several exemplary use-cases, the technological limits as well as the educational impact of controller-free Augmented Reality learning, using the Microsoft HoloLens 2, were investigated. The field of application was manufacturing engineering, and here in particular, the aluminum extrusion process. Compared to traditional learning environments, such as lectures and exercises, the developed test environments included the additional use of learning setups at the actual forming machine in the laboratory, enhanced by Augmented Reality. A total of 12 students from various academic years participated in the impact analysis study. The study comprised a general survey regarding various aspects of the controller-free learning, including visual and acoustic appearance of the developed scenarios as well as details regarding the control of the augmented objects. It was found that, even though none of the students had used such a device before, interacting with the holograms by hand, and not through controllers, was intuitive for the majority of students. In contrast, the interaction mode for long distances, the hand ray control mode, was assessed as unintuitive and frustrating at times. The visual and acoustic quality of the developed applications was rated high and sufficient for the use in education and production environments. Specific features developed for the extrusion press application, such as the display of Finite-Element-Simulation results, kinematic process animations, live sensor data through webservers and virtual safety walls were regarded as fundamental for the beneficial use of Augmented Reality in education and production. Based on the findings, future Augmented Reality applications in the field of manufacturing education can improve in usability and acceptance by engineering students without prior testing.}
}
@article{FERRATI2019803,
title = {Developing an Augmented Reality Based Training Demonstrator for Manufacturing Cherry Pickers},
journal = {Procedia CIRP},
volume = {81},
pages = {803-808},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.203},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305086},
author = {Francesca Ferrati and John Ahmet Erkoyuncu and Samuel Court},
keywords = {Augmented reality, training, manufacturing},
abstract = {This paper presents an Augmented Reality (AR) demonstrator to test its feasibility with enhancing the training process, improving learning time and error rate. The application environment was a manufacturer of cherry pickers. The demonstrator focused on covering the assembly of hydraulic hoses to the relative valve; the choice was driven by Company needs. Requirements led to the choice of Microsoft HoloLens as hardware, while Unity and Vuforia were used as software. The demonstrator provides sequential instructions through texts, images and animations. Results showed improvements when introducing AR for error rates and for the average assembly times.}
}
@article{RIZZUTO20191,
title = {Evaluation of a virtual reality head mounted display as a tool for posture assessment in digital human modelling software},
journal = {Applied Ergonomics},
volume = {79},
pages = {1-8},
year = {2019},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687019300638},
author = {Michael A. Rizzuto and Michael W.L. Sonne and Nicolas Vignais and Peter J. Keir},
keywords = {Virtual reality, Simulation, Training, Posture, Joint angles, Digital human model},
abstract = {The purpose of this work was to assess the feasibility of using a head mounted display with a motion capture system to simulate real world occupational tasks. Participants performed a pointing task under 3 conditions: (1) real environment (REA), (2) virtual environment with auditory stimulus (VEA) and (3) virtual environment with visual stimulus (VEV). End point error, movement time and peak fingertip velocity were calculated for each discrete point event. Upper extremity joint angles were calculated at the end-state for each point and did not significantly differ between real and virtual conditions. There was significantly greater target error in virtual conditions, compared to the real condition. Peak pointing velocity was slower and movement time was longer during virtual conditions. The similarity of joint angles between real and virtual conditions suggests future use of posture-based ergonomic assessments for use with virtual reality task simulations using Oculus Rift and Siemens Jack.}
}
@article{SAIDANINEFFATI2021104030,
title = {An educational tool for enhanced mobile e-Learning for technical higher education using mobile devices for augmented reality},
journal = {Microprocessors and Microsystems},
volume = {83},
pages = {104030},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104030},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121002027},
author = {Omnia {Saidani Neffati} and Roy Setiawan and P Jayanthi and S Vanithamani and D K Sharma and R Regin and Devi Mani and Sudhakar Sengan},
keywords = {Mel, Mobile devices, Technical higher education, e-Learner, Smartphone},
abstract = {In all dimensions of education and all subjects, Smartphones have turned out to be broadly acknowledged technology. It plays an essential task in advanced online education systems. Because of smart devices' effortlessness and extension property, it is getting to be mandatory for portable applications. This paper analyses the research on Smart Devices (SD) to incorporate visual simulation into e-learning. The researchers created an Augmented Reality (AR) platform for e-learners to expand the coursebook with graphics and virtual multimedia applications. This paper recommends a Mobile e-Learning (MeL) application termed “MeL app". The advanced MeL app methods have been tested using Mann-Whitney ‘U' Test in the lecture hall using real-time learners. The proposed MeL app planned to create the learning practice easier, focusing on e-learner's requirements by encouraging e-learners and instructor relationships to maintain communicative development-based e-learning for Technical Higher Education (THE). Software engineering learners assess this proposed framework in THE. Future work in this investigation incorporates new highlights, testing the device in extreme situations, evaluating the instructive perspectives utilizing more significant and increasingly various understudy and beginner inhabitants, and at last, extending the application space.}
}
@article{CAMPO2024100605,
title = {MC-AR — A software suite for comparative mocap analysis in an augmented reality environment},
journal = {Software Impacts},
volume = {19},
pages = {100605},
year = {2024},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2023.100605},
url = {https://www.sciencedirect.com/science/article/pii/S2665963823001422},
author = {Adriaan Campo and Bavo {Van Kerrebroeck} and Marc Leman},
keywords = {Motion capture, Bayesian statistics, Augmented reality, Music, Interaction, HoloLens},
abstract = {Three different software packages are presented here: 1) A Unity package for simulating a virtual violinist in augmented reality on a HoloLens. The virtual violinist plays a pre-recorded piece, either as a 2D or a 3D projection. The piece can be started, stopped, forwarded, or rewound using a dedicated user interface. During interaction, eye movements are tracked. 2) A MATLAB motion capture package for analyzing the kinematic data of a user while interacting with the virtual violinist. 3) An R package for power analysis and Bayesian statistical analysis of the kinematic data. These software packages can be easily adapted to test the kinematic behavior of music students interacting with virtual teachers.}
}
@article{GIANNUZZI2020145,
title = {IC.IDO as a tool for displaying machining processes. The logic interface between Computer-Aided-Manufacturing and Virtual Reality},
journal = {Procedia CIRP},
volume = {88},
pages = {145-150},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303413},
author = {Michele Giannuzzi and Gabriele Papadia and Claudio Pascarelli},
keywords = {CAM solver, NC file verification, Virtual Reality},
abstract = {This scientific communication investigates the logic interface of a CAM solver, i.e., MasterCAM, into a Virtual Reality (VR) environment. This integration helps in displaying machining operations in virtual reality. Currently, to partially visualize the results of a simulation in an immersive environment, an import/export procedure must be done manually. Here, a software plugin integrated into IC.IDO (by ESI Group) has been realized and fully described. This application allows the complete integration of CAM solver into the VR environment. In particular, the VERICUT solver has been integrated into VR. This kind of integration has never been done yet.}
}
@article{HU2023170274,
title = {Research on the application of virtual reality technology in 3D animation creation},
journal = {Optik},
volume = {272},
pages = {170274},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170274},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622015327},
author = {Zhiyi Hu and Liangfang Liu},
keywords = {3D animation creation, Virtual reality, Application, Model},
abstract = {In order to improve the effect of three-dimensional animation creation, this paper combines the process of three-dimensional animation creation with virtual reality technology, and performs simulation research through the digitization of character movements. The animated image model controls the skin of the model through the controlled vertices. In order to achieve the control of the controlled vertices, we introduce a data structure that saves the information of the controlled vertices. In addition, this paper combines the needs of animation creation to construct a three-dimensional animation creation system, and uses case design to verify the effect. Through experimental research, it is concluded that the virtual reality creation technology in the three-dimensional animation creation proposed in this paper has a good effect, and it has a certain effect on promoting the development of virtual technology and the progress of animation technology.}
}
@article{SUTCLIFFE2004831,
title = {Heuristic evaluation of virtual reality applications},
journal = {Interacting with Computers},
volume = {16},
number = {4},
pages = {831-849},
year = {2004},
note = {Human Computer Interaction in Latin America},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2004.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0953543804000475},
author = {Alistair Sutcliffe and Brian Gault},
keywords = {CAVE, Virtual environment, Heuristic evaluation, Usability},
abstract = {This paper presents a heuristic method for evaluating virtual environment (VE) user interfaces. The method is based on Nielsen's [Usability Inspection Methods, 1994] usability heuristics, extended by VE-specific principles proposed by Sutcliffe and Kaur [Behaviour and Information Technology 19 (2000) 415–426]. Twelve heuristics are presented which address usability and presence issues. An inspection-based evaluation method is described and illustrated with three usability case study assessments, the last of which rates the applicability and validity of the heuristics by several evaluators. Use of the method uncovered several usability problems and trapped the most serious errors. Finally, VE applications integrating measures of usability and presence are discussed.}
}
@article{LANG2019118,
title = {Mixed reality in production and logistics: Discussing the application potentials of Microsoft HoloLensTM},
journal = {Procedia Computer Science},
volume = {149},
pages = {118-129},
year = {2019},
note = {ICTE in Transportation and Logistics 2018 (ICTE 2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930122X},
author = {Sebastian Lang and Mohammed Saif Sheikh {Dastagir Kota} and David Weigert and Fabian Behrendt},
keywords = {Mixed Reality, Microsoft HoloLens, Production, Logistics, Manufacturing},
abstract = {In 2016, Microsoft released the mixed reality (MR) head-mounted display (HMD) HoloLensTM. As augmented reality (AR) devices, the HoloLensTM can enrich the user’s perceived environment with virtual information. However, Microsoft does not consider the HoloLensTM as AR device, but as the first MR device, since it has some additional features compared to competing AR devices. Especially to mention is the possibility to interact with virtual objects and vice versa. This paper shall provide an insight about the application potentials of MR in the field of production and logistics. For this purpose, we present the findings of a literature review about the current state of research concerning the application of Microsoft HoloLensTM in the field of production and logistics. Furthermore, we present a small HoloLensTM application, which we have developed to evaluate the capabilities of the device. Several persons tested our application and gave us feedback concerning the utility and usability of the HoloLensTM. We provide an evaluation of the user experiences at the end of the paper.}
}
@article{GINTERS20133,
title = {Low Cost Augmented Reality and RFID Application for Logistics Items Visualization},
journal = {Procedia Computer Science},
volume = {26},
pages = {3-13},
year = {2013},
note = {ICTE in Regional Development, December 2013, Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S187705091301274X},
author = {Egils Ginters and Jorge Martin-Gutierrez},
keywords = {Virtual and augmented reality (VR/AR), Augmented reality platforms, Radio frequency identification (RFID), Logistics, Identification},
abstract = {One important component of the gross domestic product (GDP) is logistics services the quality and added value of which is growing due to the application of modern information and communication technologies and electronics. RFID use increases the performance of logistics items identification, however some errors, which could cause substantial damage and losses, remain. The amount of potential errors could be diminished by the additional checking of items using 3D visualisation. The authors researched the use of augmented reality for item visualisation in a warehouse combining AR and RFID solutions.}
}
@article{LI2024200100,
title = {Application of VR motion intelligent capture based on DLPMA algorithm in sports training},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200100},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200100},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000292},
author = {Xiaojie Li},
keywords = {DLPMA, VR, Intelligent capture of actions, Sports training},
abstract = {With the rapid development of Virtual Reality (VR) technology, its application in the field of sports training is also receiving increasing attention. This study applies the Distance Likelihood Based Probabilistic Model Averaging (DLPMA) algorithm to the VR motion intelligent capture system, aiming to provide an efficient and accurate motion data collection method to improve existing sports training methods. Introduced the design and implementation of a VR motion intelligent capture system based on DLPMA algorithm, and applied it to sports training. By conducting comparative experiments with traditional training methods, the advantages of the system in motion capture accuracy, real-time performance, and user experience are verified. The research results indicate that the system can accurately capture the movements of athletes and provide timely feedback to users, providing an effective auxiliary means for sports training. Although the system has shown good performance in sports training, there are still some limitations. Future research can further optimize algorithms, enhance system stability and flexibility, to meet a wider range of sports training needs.}
}
@article{NEZHAD2020209,
title = {Development of An Augmented Reality Equipped Composites Bonded Assembly and Repair for Aerospace Applications},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {209-215},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301804},
author = {H. Yazdani Nezhad and X. Wang and S.D. Court and B. Thapa and J.A. Erkoyuncu},
keywords = {Composite bonded assembly, Scarf repair, Aerospace, Augmented reality, Digital mock-up},
abstract = {The prosperity of aircraft transportation together with revolutionary promotion of composite components used in commercial aircraft pose enormous challenges to the aircraft composite assembly and repair (especially uncertainties associated with polymer process parameters control, barely visible defects and non-destructive inspection capability to inspect zero-thickness defects). Therefore, an industry solution owning merits on reliability and repeatability of assembly process is at high demand. Augmented Reality (AR), a human computer interaction technology, possesses its exclusive superiority on its capability of inflicting digital mock-up into physical environment. The above property of AR provides colossal opportunities to be utilised into industrial applications to contribute the realisation of automated, efficient, streamlined and reliable process and assembly. The current ongoing research aims at developing an AR System integrated into aircraft composite bonded assembly and repair as a guidance tool to instruct technicians’ repairing operation, mitigate human errors, and reduce duration of repair and assembly. Upon the accomplishment of the System, the researchers would aim to investigate the incorporation of machine learning and deep network algorithms to enable and significantly improve the interactions between the multitude of process parameters involved in the composites assembly control procedures, solely relying upon the AR geometric data. This will ultimately lead to dramatic reduction of sensors in aircraft assembly, mitigation of in-process analysis time, reduction of process and post-process inspection time, and a higher quality assembly. Stepped scarf composite repair embedded with soft composite patches was selected as the archetype to be brought into effect though hard patches were partially examined as well. The AR System has focused on composite patches assembly and vacuum bagging process to address the predicament of miscellaneous steps and fibre directions.}
}
@article{MANNI2021116,
title = {Snap2cad: 3D indoor environment reconstruction for AR/VR applications using a smartphone device},
journal = {Computers & Graphics},
volume = {100},
pages = {116-124},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100145X},
author = {Alessandro Manni and Damiano Oriti and Andrea Sanna and Francesco {De Pace} and Federico Manuri},
keywords = {Machine learning, Scene reconstruction, 3D Object retrieval, 3D Object pose estimation, Augmented reality, Virtual reality},
abstract = {Indoor environment reconstruction is a challenging task in Computer Vision and Computer Graphics, especially when Extended Reality (XR) technologies are considered. Current solutions that employ dedicated depth sensors require scanning of the environment and tend to suffer from low resolution and noise, whereas solutions that rely on a single photo of a scene cannot predict the actual position and scale of objects due to scale ambiguity. The proposed system addresses these limitations by allowing the user to capture single views of objects using an Android smartphone equipped with a single RGB camera and supported by Google ARCore. The system includes 1) an Android app tracking the smartphone’s position relative to the world, capturing a single RGB image for each object and estimating depth information of the scene, 2) a program running on a server that classifies the framed objects, retrieves the corresponding 3D models from a database and estimates their position, vertical rotation, and scale factor without deforming the shape. The system has been assessed measuring the translational, rotational and scaling errors of the considered objects with respect to the physical ones acting as a ground truth. The main outcomes show that the proposed solution obtains a maximum error of 18% for the scaling factor, less than nine centimeters for the position and less than 18∘ for the rotation. These results suggest that the proposed system can be employed for XR applications, thus bridging the gap between the real and virtual worlds.}
}
@article{PURZEL201335,
title = {Applications of a Modular Interaction Framework for Virtual Reality Testing in a Smart Environment},
journal = {Procedia CIRP},
volume = {9},
pages = {35-39},
year = {2013},
note = {2nd CIRP Global Web Conference - Beyond modern manufacturing: Technology for the factories of the future (CIRPe2013)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2013.06.164},
url = {https://www.sciencedirect.com/science/article/pii/S2212827113004496},
author = {Franziska Pürzel and Mario Lorenz and Eckhart Wittstock and Volker Wittstock and Reimund Neugebauer},
keywords = {interactive virtual testing, immersive testing environment},
abstract = {In the future mobile devices will be used for many different purposes in the Factory of the Future, like displaying user-specific location-aware information about production processes, energy flow and resource consumption. The planning of these challenging future factories requires intuitive interactive virtual testing in an immersive environment prior to the building of the plant. In the EU-funded project uTRUSTit we have developed a Virtual Reality interaction framework designed for testing target group specific intuitive user interfaces on mobile devices in the Internet of Things. This developed framework also enables us now to carry out interactive VR application scenarios in the context of the Factory of the Future. In this paper we will describe the requirements and developed functionalities of our framework. We will outline how the framework design and implementation can be adapted, using two exemplary Factory of the Future scenarios; first as an interaction device in an immersive environment for pre-building verification in industrial plant construction it supports extended toolsets; second as an immersive usability test environment that allows testing the reaction of user groups to interface elements depending on location and reaction.}
}
@article{KROPIK20101605,
title = {Software for protection system of VR-1 training reactor},
journal = {Journal of Systems and Software},
volume = {83},
number = {9},
pages = {1605-1611},
year = {2010},
note = {Software Dependability},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2009.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S0164121209001629},
author = {Martin Kropik and Monika Jurickova},
keywords = {Nuclear safety, Protection system, Quality assurance of software, Verification and validation},
abstract = {The article deals with the software for a new protection system of the VR-1 training reactor which consists of the independent power protection and the operational power measuring systems. Both systems are computer-based, and they are diverse in sensors, hardware and software. They are responsible for nuclear safety of the reactor, so the quality requirements for their software are strict. The software was developed in accordance with nuclear standards. During the development, both software products were carefully tested, and after the integration of hardware/software, they were validated with the simulation of input signals and the checking of their responses.}
}
@article{PALMARINI201723,
title = {An Innovative Process to Select Augmented Reality (AR) Technology for Maintenance},
journal = {Procedia CIRP},
volume = {59},
pages = {23-28},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116309830},
author = {Riccardo Palmarini and John Ahmet Erkoyuncu and Rajkumar Roy},
keywords = {Augmented Reality, Maintenance, Process.},
abstract = {Augmented Reality (AR) technology for maintenance aims to improve human performances by providing relevant information regarding both corrective and preventive maintenance. The development of an AR system involves the choice of a hardware, a development software and a visualisation method. These selections are challenging due to the wide choice of services and options available which result in fragmentation: different development processes and different user experiences. In order to ease the selection of an AR system for supporting maintenance operations, this paper proposes an innovative process. It guides the reader to identify the requirements and the constraints for any specific application through a number of questions developed in this study to help with the selection. This results in suggestions for the selection of the hardware, the development software and the visualisation method. The process is built based on a literature study, grey documents and experts interviews. Future works includes the validation of the selection process proposed in this project. It could be done by comparing the choices made using the proposed process with the choices made by experts for the same case study. Moreover, the decisional process could be extended to face the economical and ergonomics aspects related with the selection of an AR system. It could be done expanding the literature research including studies which investigate into the economical and ergonomics consequences of the application or AR for maintenance.}
}
@article{SEERS2022105006,
title = {Virtual outcrop geology comes of age: The application of consumer-grade virtual reality hardware and software to digital outcrop data analysis},
journal = {Computers & Geosciences},
volume = {159},
pages = {105006},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105006},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002892},
author = {Thomas D. Seers and Ali Sheharyar and Stefano Tavani and Amerigo Corradetti},
keywords = {Digital outcrop, Virtual reality, Fracture analysis, MATLAB language, Remote sensing},
abstract = {In this work, the application of consumer-grade virtual reality (VR) hardware and software to the analysis of digital outcrop data is explored. Here, we utilize a widely available VR hardware platform (HTC Vive) and VR based freeform 3D visual arts software package (Google Tilt Brush), to digitize fault traces from photo-textured digital outcrop models of exposures of the Penrith Sandstone Formation (Lacy's Caves), in northwest England. Using MATLAB routines provided herein, triangular meshes output from Tilt Brush are used as the basis for 3D fracture trace map extraction, which in turn, are used to generate fracture properties (trace length, orientation, areal fracture intensity). We compare the results of this analysis to two equivalent datasets obtained from the Lacy's Caves model using digital outcrop analysis deployed via a conventional flat panel display: namely (1) a 3D trace map extracted using optical ray tracing from manually interpreted calibrated images and (2) 3D traces fitted directly the Lacy's Caves textured mesh using manual polyline interpretation within an established digital outcrop analysis software platform (OpenPlot). Fault statistics obtained using VR based analysis are broadly equivalent to those acquired from 3D trace maps extracted using the flat panel display deployed analyses presented herein. In this case study, it was found that VR based digital outcrop analysis provided faster data acquisition than the comparative pixel-based approach, which requires linkage and merging of traces mapped from multiple contiguous images. Manual raster analysis and optical ray tracing did however provide 3D trace maps with significantly higher areal fault intensity, with VR analysis incurring censoring of finer fault traces, due to the limited resolution of the outcrop model textured mesh. Whilst data acquisition times and resultant fault intensities proved similar between the VR and OpenPlot workflows, it was noted anecdotally, that the VR analysis holds some advantages for the operator when interpreting models exhibiting complex geometries, such as mine workings and caves systems, with the clip point implemented within the viewport of conventional digital outcrop analysis software tools obstructing the user from obtaining an optimum view of the outcrop surface. VR based digital outcrop analysis techniques, such as those presented here, provide an immersive analytical environment to the operator. This allows users to fuse powerful 3D visualizations of photo-realistic outcrop models with geological interpretation and data collection, fulfilling the early promise of ‘virtual outcrops’ as an analytical medium that can emulate traditional fieldwork. It is hoped that this study and its associated code library will facilitate the evaluation of emerging VR technologies for digital outcrop applications, by provided access to VR analytical tools for non-specialists in virtual reality systems. Finally, prospects for the use of VR technology within the field of digital outcrop geology, as well as within the wider geosciences, are also discussed.}
}
@article{GRAJEWSKI2013289,
title = {Application of Virtual Reality Techniques in Design of Ergonomic Manufacturing Workplaces},
journal = {Procedia Computer Science},
volume = {25},
pages = {289-301},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012404},
author = {Damian Grajewski and Filip Górski and Przemysław Zawadzki and Adam Hamrol},
keywords = {virtual reality, haptic devices, workplace design, ergonomics},
abstract = {This paper presents possibilities of application of the immersive VR and the haptic technologies during the complex process of design and virtual prototyping of the manufacturing workplaces characterized with a high level of ergonomic quality. Two case studies are presented: a workplace for stud welding and a set of two workplaces, for hole drilling and manual assembly. In the first case study, haptic device with force feedback effect was used to improve ergonomics of main operator activities. In the second case study, immersive approach was used, namely Head-Mounted Device, tracking and gesture recognition systems, to test and improve ergonomics of the whole workplace. Application of VR techniques allows to present the virtual prototype of the workplace in its real operation environment, limiting the need for use of real mock-ups. Therefore, Virtual Reality allows to conduct a number of analyses related to designed prototypes, such as: dimensions of devices and possibilities of adjustment to height of the human operator, and arrangement of control and signaling devices according to the rules of ergonomic design. To conduct these analyses, full interaction between user and workplace must be programmed, including collision detection, kinematics of the devices and possibilities of activating their various functions in relation with other objects in the virtual scene.}
}
@article{MEZA20141,
title = {Component based engineering of a mobile BIM-based augmented reality system},
journal = {Automation in Construction},
volume = {42},
pages = {1-12},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514000363},
author = {Sebastjan Meža and Žiga Turk and Matevž Dolenc},
keywords = {Augmented reality, Mobile computing, Construction site, BIM},
abstract = {Over the last decade, building information models (BIM) have become increasingly popular. Yet their use on construction sites where the digital materializes, is limited. A technology that can bridge the gap between the digital and the real world is augmented reality (AR). We analyze this gap and the AR potential and present how the component based software engineering method can be used to efficiently implement a BIM-based AR system for construction. An architecture of the software system is proposed and verified by a prototype which was tested in a real construction project. We found out that the use of AR can significantly narrow the semantic gap between the digital model and the real world; that components for creating a BIM-based AR systems exist but currently do not scale well to large models; and that the use in AR applications creates additional requirements the for BIM models and tools, particularly related to the BIM's 4th dimension.}
}
@article{YIN2023102203,
title = {An empirical study of an MR-enhanced kinematic prototyping approach for articulated products},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102203},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102203},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003312},
author = {Yue Yin and Pai Zheng and Chengxi Li and Jingchen Cong and Yat Ming Pang},
keywords = {Kinematic prototyping, Articulated product, Mixed reality, Augmented reality, Conceptual design},
abstract = {In the conceptual design and early embodiment design stages, an effective kinematic prototype can assist in detecting design errors, minimizing unnecessary parameter modifications, and optimizing subsequent resource deployment. Existing methods for creating prototypes, such as 2D sketches, 3D modelling, or analysis software, have their advantages in precise modelling and simulation. However, there is a lack of an efficient approach that can achieve both rapid 3D kinematic idea generation and early on-site interactive verification. To address these challenges, this research proposes a Mixed Reality (MR)-enhanced kinematic prototyping approach for articulated products to enable the rapid generation of 3D kinematic concepts, movement preview, and early interactions with users or the actual workspace. The proposed approach facilitates the quick and rough construction of geometry and kinematic structures through intuitive hand gestures and interactive movement preview within the MR environment. The usability of the proposed approach was evaluated by a comparison experiment with traditional sketch and CAD tools involving eight participants. It is verified that our proposed MR kinematic prototyping system could accelerate the kinematic prototype generation process, improve the user’s kinematic understanding, and facilitate on-site interactive verification in advance.}
}
@article{GODETBAR2010492,
title = {HCI and business practices in a collaborative method for augmented reality systems},
journal = {Information and Software Technology},
volume = {52},
number = {5},
pages = {492-505},
year = {2010},
note = {TAIC-PART 2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909002079},
author = {Guillaume Godet-Bar and Dominique Rieu and Sophie Dupuy-Chessa},
keywords = {Information systems, Software engineering, Human–computer interaction, Augmented reality, Collaborative design, Evolution},
abstract = {Context
Every interactive system is composed of a functional core and a user interface. However, the software engineering (SE) and human–computer interaction (HCI) communities do not share the same methods, models or tools. This usually induces a large work overhead when specialists from the two domains try to connect their applicative studies, especially when developing augmented reality systems that feature complex interaction cores.
Objective
We present in this paper the essential activities and concepts of a development method integrating the SE and HCI development practices, from the specifications down to the design, as well as their application on a case study.
Method
The efficiency of the method was tested in a qualitative study involving four pairs of SE and HCI experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system. The effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment (based on the same analysis model), using software engineering metrics.
Results
The first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process, but raised concerns on the handling of conflicting collaborative activities. The second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software (in terms of coupling, stability and complexity).
Conclusion
It is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems. Areas of improvement are also described.}
}
@article{PILIA20151589,
title = {Application of virtual reality tools for assembly of WEST components: Comparison between simulations and physical mockups},
journal = {Fusion Engineering and Design},
volume = {98-99},
pages = {1589-1592},
year = {2015},
note = {Proceedings of the 28th Symposium On Fusion Technology (SOFT-28)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2015.06.168},
url = {https://www.sciencedirect.com/science/article/pii/S0920379615302180},
author = {Arnaud Pilia and Cyril Brun and Louis Doceul and Laurent Gargiulo and Jean-Claude Hatchressian and Delphine Keller and Roland Le and Serge Poli and Bertrand Zago},
keywords = {Virtual reality, Simulation, Integration, Assembly, Maintenance, Tore Supra, WEST},
abstract = {The WEST project (Tungsten (W) Environment in Steady state Tokamak) is an upgrade of the existing fusion machine, Tore Supra. The goal is to equip the tokamak with a fully cooled tungsten divertor and to transform the machine in a test platform open to all ITER partners. The main assembly challenge of this project consists of an implementation of two magnet systems, called divertors, with an accuracy of 1mm. Indeed, each divertor has about 4m as diameter and has a heavy weight of 10 tons; also it introduces piece by piece in the original vessel through tight ports then assembled inside. To ensure a perfect fitting between these new components and a very constrained environment, it is necessary to use the latest CAD technologies available. Beyond conventional CAD tools, the virtual reality (VR) room of the institute provides several useful tools. Thanks to the 185″ stereoscopic 3D screen and a force feedback arm linked to clash detection software developed by the CEA LIST, a new way to carry out design and assembly studies was performed. In order to improve VR results, metrology data (3D scan) enhance simulations. Therefore, it becomes possible to be aware of the real size of a component and future difficulties in assembling it. At last, performance of such simulations is evaluated and compared to physical mockup in order to bring enhancement to the VR tools, before to be compared to the real operations on Tore Supra. The aim is to build a design tool that helps the designer since early stage of the design of complex systems, taking into consideration integration, assembly and maintenance aspects while reducing costs and schedule of a project.}
}
@article{GARZA2013154,
title = {Augmented Reality Application for the Maintenance of a Flapper Valve of a Fuller-kynion Type M Pump},
journal = {Procedia Computer Science},
volume = {25},
pages = {154-160},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012246},
author = {Luis Eduardo Garza and Gabriel Pantoja and Pablo Ramírez and Hector Ramírez and Nestor Rodríguez and Eduardo González and Raúl Quintal and Juan A. Pérez},
keywords = {Augmented Reality, Virtual Reality, CAD models, maintenance process, mobile devices},
abstract = {The overall purpose of this project is to test the impact and potential benefits of Virtual and Augmented Reality technologies (AR&VR) to improve maintenance operation in industrial equipment. The main function for a Flapper valve of a Fuller-Kinyon type M pump is to prevent that the air generated to convey the bulk material through a conveying pipe flows inside of the material chute through the rotating screw. If this will occur, the material flow will decrease or even stop, causing a reduction of the pump capacity. Thus, it is necessary to maintain calibrated each flapper valve in the plant. This process of maintenance is done once a month, or when necessary, and it needs the pump to be shut down, taking up to four hours to finish the complete process. For this reason, an augmented reality application is being developed, aiming to reduce the consumed time by the maintenance process. Using this application, it is expected to dedicate less time training the new personnel responsible for the maintaining process, displaying tridimensional models, animations, images and text information that would simplify the instructions shown in a printed manual and adding an interactive environment between the users and the information displayed. A mobile device either a tablet or a smartphone is to be used as the hardware that will run the application, allowing the user to take it right to the working area, either in a workshop or directly in field. The information displayed includes CAD models of the pump and its components as well as animations illustrating the instructions to follow in each step of the process. Also, the right tool to use in each step will be indicated following by security warnings when needed. This project was developed according to the collaboration cathedra between CEMEX and ITESM, following the LEAD methodology developed by CEMEX to support the project administration, the general process development was constructed from knowledge and experiences gathered among the different previous AR projects developed at ITESM. This information has been studied and “best practices” has been noted, learned and established to develop and implement AR.}
}
@article{LINDNER201966,
title = {Augmented Reality applications as digital experiments for education – An example in the Earth-Moon System},
journal = {Acta Astronautica},
volume = {161},
pages = {66-74},
year = {2019},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2019.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0094576519303698},
author = {Claudia Lindner and Andreas Rienow and Carsten Jürgens},
keywords = {Education, Augmented Reality, Digital experiments, Gravitation, Moon},
abstract = {“You realise that the Earth is nothing but accumulated cosmic dust having formed a rock that is encompassed by a flimsy, fragile atmosphere. To grasp this, I needed the view out of the window.” German ESA Astronaut Alexander Gerst's perspective on Earth was changed sustainably by the view from the International Space Station (ISS) onto our home planet. It is possible to give pupils a very similar perspective within the means of public education due to the availability of Earth Observation data from the ISS. However, the data can be put to more educational use than providing a taste of the overview effect. Applying common remote sensing methods and modern teaching concepts to EO (Earth Observation) data from the National Aeronautics and Space Administration (NASA) High Definition Earth Viewing experiment, teaching modules for several STEM (Science, Technology, Engineering, and Mathematics) subjects could be implemented successfully. Building on this success, more ISS EO sensors are being implemented in teaching materials and new media techniques are explored. The more recent addition to the material pool are smartphone apps using Augmented Reality (AR) with which the pupils can experiment on their own. These apps are developed in a partial What You See Is What You Get (WYSIWYG) application development system called Unity with the Vuforia extension, the latter allowing the use of printed images as reference markers for AR. Complex theoretical topics can be visualised in 3-dimensional (3D) animations or turned into inexpensive, easy digital experiments. The app “The Earth-Moon System” applies this with experiments on the effects of changes in the distance between Earth and Moon and a 3D animation on the barycentre between two celestial bodies. Development of such apps is feasible for researchers to visualise their data even with no prior app development knowledge.}
}
@article{QIN2020111906,
title = {Application of virtual reality technology in nuclear device design and research},
journal = {Fusion Engineering and Design},
volume = {161},
pages = {111906},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111906},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620304543},
author = {Shijun Qin and Qingfeng Wang and Xianfeng Chen},
keywords = {CFETR, Virtual reality, Nuclear plant, Computer},
abstract = {The design, fabrication, assembly, operation, and decommissioning of nuclear devices are very complex processes, and virtual reality (VR) technology can be used at all stages to save time and reduce costs. This study introduces VR technology and its characteristics in detail. The China Fusion Engineering Test Reactor (CFETR) is in its engineering design phase and considerable analysis still needs to be conducted to ensure that it is safe. This includes investigating plasma geometry, stability, the scrape-off layer, and the discharge process and conducting an engineering analysis of its electromagnetic, thermodynamic, and structural characteristics. At the same time, many software applications are used, which generate a large amount of data; therefore, a system capable of managing such large quantities of data is required. VR technology certainly meets this requirement. In this paper, the design and development of a VR technology in a nuclear fission device is described, including the display interface of the melting pit and containment parameters, virtual roaming of the nuclear plant, and the operational flow of the main loop of the nuclear power plant, all in three dimensions. The CFETR VR platform setup is investigated (in terms of both hardware and software), and the future development of this platform is discussed.}
}
@article{PEREIRA2020103584,
title = {Application of AR and VR in hand rehabilitation: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {111},
pages = {103584},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103584},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302136},
author = {Margarida F. Pereira and Cosima Prahm and Jonas Kolbenschlag and Eva Oliveira and Nuno F. Rodrigues},
keywords = {Rehabilitation, Video games, Training systems, Augmented reality, Virtual reality},
abstract = {Background
The human hand is the part of the body most frequently injured in work related accidents, accounting for a third of all accidents at work and often involving surgery and long periods of rehabilitation. Several applications of Augmented Reality (AR) and Virtual Reality (VR) have been used to improve the rehabilitation process. However, there is no sound evidence about the effectiveness of such applications nor the main drivers of therapeutic success.
Objectives
The objective of this study was to review the efficacy of AR and VR interventions for hand rehabilitation.
Methods
A systematic search of publications was conducted in October 2019 in IEEE Xplore, Web of Science, Cochrane library, and PubMed databases. Search terms were: (1) video game or videogame, (2) hand, (3) rehabilitation or therapy and (4) VR or AR. Articles were included if (1) were written in English, (2) were about VR or AR applications, (3) were for hand rehabilitation, (4) the intervention had tests on at least ten patients with injuries or diseases which affected hand function and (5) the intervention had baseline or intergroup comparisons (AR or VR intervention group versus conventional physical therapy group). PRISMA protocol guidelines were followed to filter and assess the articles.
Results
From the eight selected works, six showed improvements in the intervention group, and two no statistical differences between groups. We were able to identify motivators of patients’ adherence, namely real-time feedback to the patients, challenge, and increased individualized difficulty. Automated tracking, easy integration in the home setting and the recording of accurate metrics may increase the scalability and facilitate healthcare professionals’ assessments.
Conclusions
This systematic review provided advantages and drivers for the success of AR/VR application for hand rehabilitation. The available evidence suggests that patients can benefit from the use of AR or VR interventions for hand rehabilitation.}
}
@article{ERTAS2008284,
title = {An interactive dynamic analysis and decision support software for MR mammography},
journal = {Computerized Medical Imaging and Graphics},
volume = {32},
number = {4},
pages = {284-293},
year = {2008},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2008.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0895611108000062},
author = {Gökhan Ertaş and H. Özcan Gülçür and Mehtap Tunacı},
keywords = {MR mammography, Lesion localization, Malignancy detection, Normalized maximum intensity–time ratio, Normalized complexity},
abstract = {A fully automated software is introduced to facilitate MR mammography (MRM) examinations and overcome subjectiveness in diagnosis using normalized maximum intensity–time ratio (nMITR) maps. These maps inherently suppress enhancements due to normal parenchyma and blood vessels that surround lesions and have natural tolerance to small field inhomogeneities and motion artifacts. The classifier embedded within the software is trained with normalized complexity and maximum nMITR of 22 lesions and tested with the features of remaining 22 lesions. Achieved diagnostic performances are 92% sensitivity, 90% specificity, 91% accuracy, 92% positive predictive value and 90% negative predictive value. DynaMammoAnalyst shortens evaluation time considerably and reduces inter and intra-observer variability by providing decision support.}
}
@incollection{SHANG2009435,
title = {How a process simulator and a rule-based system contribute to virtual reality applications for process safety},
editor = {Jacek Jeżowski and Jan Thullie},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {26},
pages = {435-439},
year = {2009},
booktitle = {19th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70073-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609700732},
author = {Xiaolei Shang and Paul Chung and Jeff Fry and Luca Vezzadini and Mauro Colombo},
keywords = {virtual reality, plant safety, process simulation, rule-based system},
abstract = {The VIRTHUALIS project aims to develop a number of virtual reality applications for improving safety in the process industries. The applications allow human factors experts to study how operators interact with plant, and provide a safe environment in which new safety actions can be tried and tested. Safety applications are built on the SafeVR technology platform, a distributed clientserver virtual reality system. This paper describes how two external modules - a process simulator and a rule-based system - are interfaced to the platform and the benefits they provide both separately and together. The two modules communicate with the platform's server by exchanging messages, conforming to a simple syntax. pSimProxy provides a generic interface to an external process simulator, which in turn delivers the realistic plant behaviour. It handles bidirectional data exchange with and control of the external simulator. It can be configured at run time to use whichever available mechanisms are supported by the actual process simulator that models the plant being simulated. ClipsClient is an expert or rule-based system, based on NASA's CLIPS expert system software that can make inferences about the information contained in the messages. It consists of a set of facts, a number of rules and an inference engine. It can be provided with a number of rules that monitor how operators are running the plant, and react in useful ways to these events. The simulator notifies the server of changes in process parameters through a message. The values may be displayed, for example as gauge readings, in the virtual environment. As operators control the plant, their actions, say opening a valve, are also reported by messages via the server to the process simulation. Messages can also be read by the rule-based system, allowing it to maintain its own representation of the plant. This in turn permits automated expert reasoning on the state of the plant and the actions of its operators which can cause further message to be sent to the server. The rule-based system is therefore, a powerful mechanism for rapidly reconfiguring the application and general rules can be written that only require new facts at run-time to change the behaviour of the entire virtual environment. The message syntaxes, the system architecture and the interfacing of the external modules are described along with examples showing their individual and joint benefits.}
}
@article{KERTHYAYANAMANUABA2021289,
title = {Mobile based Augmented Reality Application Prototype for Remote Collaboration Scenario Using ARCore Cloud Anchor},
journal = {Procedia Computer Science},
volume = {179},
pages = {289-296},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000090},
author = {Ida Bagus {Kerthyayana Manuaba}},
keywords = {Augmented Reality, Mobile App, Remote Collaboration, ARCore, Cloud Anchor},
abstract = {This paper describes a development study for mobile based Augmented Reality (AR) application prototype for simple remote collaboration scenario. The AR technology is implemented by using ARCore with cloud anchor into two different mobile devices as the host and client. A host is a user device that able to create and share a session of a camera viewpoint that could be connected with another client device remotely through the internet. For testing purposes of remote collaboration scenario in this application prototype, both users are able to add and manipulate location of virtual objects that overlay on each screen from their own device. Based on the functionality test, before placing the virtual object on the screen, a calibration of cloud anchor is required with minimum success performance in 250 to 1200 lumens of lights. The result for cloud anchor is being hosted with average delay up to 700+ milliseconds. In this paper, the result of the experiment shows an early stage of the utilization of AR technology in simple remote collaboration scenario. However, it shows the potential of mobile based AR technology for future remote collaboration scenario development.}
}
@article{CRESPO2015107,
title = {Virtual Reality Application for Simulation and Off-line Programming of the Mitsubishi Movemaster RV-M1 Robot Integrated with the Oculus Rift to Improve Students Training},
journal = {Procedia Computer Science},
volume = {75},
pages = {107-112},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.226},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503687X},
author = {Raúl Crespo and René García and Samuel Quiroz},
keywords = {Virtual Reality, Robotics, Off-Line Programming, Unity, Oculus Rift, A* algorithm},
abstract = {The overall purpose of this project is to test the impact and potential benefits of Virtual Reality technology by improving the current training methods for the operation of the Mitsubishi Movemaster RV-M1 Robot. The final application aims to achieve this goal by increasing the user's interactivity with the robot's features inside a virtual environment developed using the game engine Unity and the Oculus Rift headset for the virtual visualization. By using this application, on site operation time is decreased mainly by allowing off-line programming of the equipment. It also allows universities without or with limited industrial machinery to provide their students a way to learn and practice on industrial automation and robotics simulation topics without inconvenience. The application is designed to decrease the student's learning curve by displaying a complete virtual environment where the tridimensional model of the robotic arm can be visualized and programmed according to the real model's parameters and specifications. Joint type moving sequences are compiled into a file which afterwards can be transferred to the robot for real testing and execution. The system integrates a set of joysticks that allows the user to program each of the robot's joints as well as display several features of it in the virtual environment such as animations, images and text information that simplify the instructions shown in a printed manual. In order to find an optimized, collision-free movement sequence between two points an A* shortest path algorithm is implemented using some of the built-in tools and plugins available in Unity and its Asset store. As a result of this last feature, the application can create a comparison between the user's input sequence and a computer generated sequence based on the A* algorithm.}
}
@article{ANDERIES2023573,
title = {Implementation of Augmented Reality in Android-based Application to Promote Indonesian Tourism},
journal = {Procedia Computer Science},
volume = {227},
pages = {573-581},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.560},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017271},
author = { Anderies and Maevy Marvella and Nissa Adila Hakim and Priskilla Adriani Seciawanto and Andry Chowanda},
keywords = {Augmented Reality, Indonesian Tourism, Android, Usability Testing},
abstract = {Indonesia is known for its iconic tourism industry. However, there is a significant popularity difference between each destination. This imbalance has been a problem that the Indonesian Ministry of Tourism has been trying to solve through the 'Wonderful Indonesia" campaign. Unfortunately, some did not perceive it as a success. With the breakthrough of Augmented Reality (AR) that is seen as a great factor in enhancing user experience in the tourism sector, this research aims to design and develop an immersive android-based application that implements Augmented Reality and Audio Guidance to promote and create branding for Indonesian tourism spots. This research uses a methodology divided into 4 phases; Analysis Requirement, Research and Collect Data, System Design and Development, and Testing. A survey was conducted in the Analysis Requirement phase to find people's knowledge and preference for tourism spots in Indonesia. Another survey was conducted in the Testing phase to test the application's usability and whether the aim has been achieved. The main findings indicate that most local tourist respondents stated that the application developed is an effective way for them to explore and educate themselves about Indonesian tourism sites. Therefore, this application might be a sustainable answer and a new approach compared to what the Indonesian government has done in the past years.}
}
@article{ASGARI20171130,
title = {Advanced Virtual Reality Applications and Intelligent Agents for Construction Process Optimisation and Defect Prevention},
journal = {Procedia Engineering},
volume = {196},
pages = {1130-1137},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817331971},
author = {Zeynab Asgari and Farzad Pour Rahimian},
keywords = {Intelligent agent, Virtual Reality, Process Innovation, ICT, Sensors, Optimisation},
abstract = {Defects and errors in new or recently completed construction work continually pervade the industry. Whilst inspection and monitoring processes are established vehicles for their ‘control’, the procedures involved are often process driven, time consuming, and resource intensive. Paradoxically therefore, they can impinge upon the broader aspects of project time, cost and quality outcomes. Acknowledging this means appreciating concatenation effects such as the potential for litigation, impact on other processes and influence on stakeholders’ perceptions—that in turn, can impede progress and stifle opportunities for process optimisation or innovation. That is, opportunities relating to for example, logistics, carbon reduction, health and safety, efficiency, asset underutilisation and efficient labour distribution. This study evaluates these kinds of challenge from a time, cost and quality perspective, with a focus on identifying opportunities for process innovation and optimisation. It reviews—within the construction domain—state of the art technologies that support optimal use of artificial intelligence, cybernetics and complex adaptive systems. From this, conceptual framework is proposed for development of real-time intelligent observational platform supported by advanced intelligent agents, presented for discussion. This platform actively, autonomously and seamlessly manages intelligent agents (Virtual Reality cameras, Radio-Frequency Identification RFID scanners, remote sensors, etc.) in order to identify, report and document ‘high risk’ defects. Findings underpin a new ontological model that supports ongoing development of a dynamic, self-organised sensor (agent) network, for capturing and reporting real-time construction site data. The model is a ‘stepping stone’ for advancement of independent intelligent agents, embracing sensory and computational support, able to perform complicated (previously manual) tasks that provide optimal, dynamic, and autonomous management functions.}
}
@article{IWENDI2023266,
title = {Innovative augmented and virtual reality applications for disease diagnosis based on integrated genetic algorithms},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {4},
pages = {266-276},
year = {2023},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2023.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S266630742300027X},
author = {Celestine Iwendi},
keywords = {Genetic algorithm (GA), Ant colony optimization (ACO), Particle swarm optimization (PSO), Virtual reality, Audio reality},
abstract = {In this comprehensive paper the method of detecting various diseases within short period of time using Audio Reality/ Virtual Reality (AR/VR) techniques is proposed. For proper functioning of AR/VR models in medical applications three distinct algorithms such Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO) is integrated where the entire operation is performed with respect to search space. Moreover, the detection process in AR/VR models depends on several factors where minimum error functions must be ensured. Hence in the integrated technique both Absolute Errors (AE) and Time Errors (TE) are measured and compared with existing methods. As the performance of detection is greatly improved with search space the fitness function of each algorithm is observed and it is considered as maximization objective in the proposed method. Furthermore, the complexity of AR/VR models in real time detection process is detected and it is realistic that high complex detections are converted to simple detections. In the comparative analysis of three algorithms ACO proves to be much better as errors are minimized with maximization of fitness function.}
}
@article{ONG2020101820,
title = {Augmented reality-assisted robot programming system for industrial applications},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {61},
pages = {101820},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101820},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300250},
author = {S.K. Ong and A.W.W. Yew and N.K. Thanigaivel and A.Y.C. Nee},
keywords = {Augmented reality, Robot programming, Human-robot interaction},
abstract = {Robots are important in high-mix low-volume manufacturing because of their versatility and repeatability in performing manufacturing tasks. However, robots have not been widely used due to cumbersome programming effort and lack of operator skill. One significant factor prohibiting the widespread application of robots by small and medium enterprises (SMEs) is the high cost and necessary skill of programming and re-programming robots to perform diverse tasks. This paper discusses an Augmented Reality (AR) assisted robot programming system (ARRPS) that provides faster and more intuitive robot programming than conventional techniques. ARRPS is designed to allow users with little robot programming knowledge to program tasks for a serial robot. The system transforms the work cell of a serial industrial robot into an AR environment. With an AR user interface and a handheld pointer for interaction, users are free to move around the work cell to define 3D points and paths for the real robot to follow. Sensor data and algorithms are used for robot motion planning, collision detection and plan validation. The proposed approach enables fast and intuitive robotic path and task programming, and allows users to focus only on the definition of tasks. The implementation of this AR-assisted robot system is presented, and specific methods to enhance the performance of the users in carrying out robot programming using this system are highlighted.}
}
@article{MICHALOS2016370,
title = {Augmented Reality (AR) Applications for Supporting Human-robot Interactive Cooperation},
journal = {Procedia CIRP},
volume = {41},
pages = {370-375},
year = {2016},
note = {Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115010847},
author = {George Michalos and Panagiotis Karagiannis and Sotiris Makris and Önder Tokçalar and George Chryssolouris},
keywords = {Augmented Reality, Hybrid Assembly System, Human Aspect, Robot.},
abstract = {The paper presents an Augmented Reality (AR) tool for supporting operators where humans and robots coexist in a shared industrial workplace. The system provides AR visualization of the assembly process, video and text based instructions and production status updates. The tool also enhances the operator's safety and acceptance of hybrid assembly environments through the immersion capabilities of AR technology. A hardware landscape including the AR equipment and markers, the handheld devices for user input and the network infrastructure for interfacing the robot and the storage database is provided. The software architecture for coordinating the AR tool with the assembly process and the data retrieval from the robot controller are also presented. The tool has been tested on a pilot case in the automotive sector. The results indicate that the approach can significantly enhance the operator's working conditions and their integration in the assembly process.}
}
@article{DELAMO2022107954,
title = {Hybrid recommendations and dynamic authoring for AR knowledge capture and re-use in diagnosis applications},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107954},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010868},
author = {Iñigo Fernández {del Amo} and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Augmented Reality, Failure diagnosis, Authoring systems, Knowledge capture, Ontology-based reporting},
abstract = {In Industry 4.0, integrated data management is an important challenge due to heterogeneity and the lack of structure of numerous existing data sources. A relevant research gap involves human knowledge integration, especially in maintenance operations. Augmented Reality (AR) can bridge this gap, but it requires improved augmented content to enable effective and efficient knowledge capture. This paper proposes dynamic authoring and hybrid recommender methods for accurate AR-based reporting. These methods aim to provide maintainers with augmented data input formats and recommended datasets for enhancing the efficiency and effectiveness of their reporting tasks. The proposed contributions have been validated through experiments and surveys in two failure diagnosis reporting scenarios. Experimental results indicated that the proposed reporting solution can reduce reporting errors by 50% and reporting time by 20% compared to alternative recommender and AR tools. Besides, survey results suggested that testers perceived the proposed reporting solution as more effective and satisfactory for reporting tasks than alternative tools. Thus, proving that the proposed methods can improve the effectiveness and efficiency of diagnosis reporting applications. Finally, this paper proposes future works towards a framework for automatic adaptive authoring in AR knowledge transfer and capture applications for human knowledge integration in the context of Industry 4.0.}
}
@article{ARBELAEZESTRADA2013389,
title = {Augmented Reality Application for Product Concepts Evaluation},
journal = {Procedia Computer Science},
volume = {25},
pages = {389-398},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012532},
author = {Juan C. Arbeláez-Estrada and Gilberto Osorio-Gómez},
keywords = {Product evaluation, Augmented reality, Aesthetical response, Real-time feedback},
abstract = {The decision-making activities through all the design process are crucial for the final product success but currently there are limited computational tools available to provide better support to the designer especially at the earlier stages of the process. In addition the cost of fixing errors or making changes to a design escalates dramatically as the design advances in the product lifecycle. Besides, these activities, in a global design scenario, occur in different time and places, leading to a flexible and light solution that needs to be available for different users. Here is proposed an Augmented Reality (AR) application for Android mobile devices for getting feedback, via internet of a target user, in order to enhance the evaluation of aesthetical response in the conceptual design of discrete products}
}
@article{MIHCIN2017125,
title = {Methodology on quantification of sonication duration for safe application of MR guided focused ultrasound for liver tumour ablation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {152},
pages = {125-130},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2017.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S016926071730158X},
author = {Senay Mihcin and Ioannis Karakitsios and Nhan Le and Jan Strehlow and Daniel Demedts and Michael Schwenke and Sabrina Haase and Tobias Preusser and Andreas Melzer},
keywords = {MR guided FUS, Computer control of laboratory machines and device, Safety, Protocol development, Quality Management Systems, Experiment and measurement technics, medical device legislation, Sonication duration},
abstract = {Background and objective
Magnetic Resonance Guided Focused Ultrasound (MRgFUS) for liver tumour ablation is a challenging task due to motion caused by breathing and occlusion due the ribcage between the transducer and the tumour. To overcome these challenges, a novel system for liver tumour ablation during free breathing has been designed.
Methods
The novel TRANS-FUSIMO Treatment System (TTS, EUFP7) interacts with a Magnetic Resonance (MR) scanner and a focused ultrasound transducer to sonicate to a moving target in liver. To meet the requirements of ISO 13485; a quality management system for medical device design, the system needs to be tested for certain process parameters. The duration of sonication and, the delay after the sonication button is activated, are among the parameters that need to be quantified for efficient and safe ablation of tumour tissue. A novel methodology is developed to quantify these process parameters. A computerised scope is programmed in LabVIEW to collect data via hydrophone; where the coordinates of fiber-optic sensor assembly was fed into the TRANS-FUSIMO treatment software via Magnetic Resonance Imaging (MRI) to sonicate to the tip of the sensor, which is synchronised with the clock of the scope, embedded in a degassed water tank via sensor assembly holder. The sonications were executed for 50 W, 100 W, 150 W for 10 s to quantify the actual sonication duration and the delay after the emergency stop by two independent operators for thirty times. The deviation of the system from the predefined specs was calculated. Student's-T test was used to investigate the user dependency.
Results
The duration of sonication and the delay after the sonication were quantified successfully with the developed method. TTS can sonicate with a maximum deviation of 0.16 s (Std 0.32) from the planned duration and with a delay of 14 ms (Std 0.14) for the emergency stop. Student's T tests indicate that the results do not depend on operators (p > .05).
Conclusion
The evidence obtained via this protocol is crucial for translation- of-research into the clinics for safe application of MRgFUS. The developed protocol could be used for system maintenance in compliance with quality systems in clinics for daily quality assurance routines.}
}
@article{ZAVOLOVICH2021845,
title = {A Python software to evaluate geometric discrepancies between stereotactic CT and MR images in radiosurgery},
journal = {Procedia Computer Science},
volume = {190},
pages = {845-851},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013545},
author = {Bogdan D. Zavolovich and Irina I. Bannikova and Aleksandra V. Dalechina and Valery V. Kostjuchenko and Pavel N. Ryabov},
keywords = {MR imaging, CT, stereotactic radiosurgery, Elekta MR phantom, Python},
abstract = {Due to its excellent soft tissue contrast, magnetic resonance imaging (MRI) is widely used for target delineation in stereotactic radiosurgery. However, because of a number of factors (inhomogeneity of the constant magnetic field, nonlinearity of the gradient fields, etc.), MR images are more susceptible to spatial distortion as opposed to Computed tomography (CT) images, which could be considered as the reference studies in terms of the accuracy of target localization. It is crucial to assess the geometric errors in MR images as the accuracy of dose delivery strongly depends on MRI quality. One of the ways of MRI distortion evaluation is to scan a phantom with multiple marker points on both CT and MRI scanner and compare coordinates of the points defined on CT and MR images, respectively. Performing this test as a routine part of MRI quality assurance (QA) program for stereotactic radiosurgery is complicated by lack of a software for the automatic calculations of the discrepancies. The aim of our work was to develop a software to calculate discrepancies between stereotactic coordinates of the phantom’s plastic rods defined on the MR and CT images of the Elekta MR phantom. A special Python software was developed and the spatial accuracy of the stereotactic MR images was assessed. The calculated total differences between the coordinates of the CT and T1 weighted MR images, CT and T2 weighted MR images exceeded 1 mm in 3,5%, 0.1% of the points, respectively. Using the software in clinic routines could both speed up the time needed for the test performance and eliminate subjective visual assessment of the discrepancies.}
}
@article{BARTALUCCI2023102947,
title = {An original mechatronic design of a kinaesthetic hand exoskeleton for virtual reality-based applications},
journal = {Mechatronics},
volume = {90},
pages = {102947},
year = {2023},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2023.102947},
url = {https://www.sciencedirect.com/science/article/pii/S095741582300003X},
author = {Lorenzo Bartalucci and Nicola Secciani and Chiara Brogi and Alberto Topini and Andrea {Della Valle} and Alessandro Ridolfi and Benedetto Allotta},
keywords = {Hand exoskeleton, Kinaesthetics, Wearable robotics, Robotic rehabilitation, Virtual reality},
abstract = {Within the new industrial era, the interaction between humans and virtual reality is spreading across our lives. The development of exoskeleton designed to enhance the immersivity of virtual reality environments has a potentially considerable social impact and arises as a hot research topic. The presented work dwells well with the subject by describing the mechatronic design process of a kinaesthetic hand exoskeleton system meant to reproduce proprioceptive stimuli coming from the interaction with a virtual reality. The presented prototype is a modular device, equipped with force and pose sensors, and driven by a Bowden-cable-based remote actuation system. Unlike similar devices, the proposed exoskeleton is specifically thought for VR interaction and is designed to be reversible while exerting up to 15 N per finger. For a more accurate rendering of kinetostatic finger stimuli, a procedure for reconstructing HMI force as a function of measured force and position signals by employing a system’s kinematic and dynamic model is presented, detailed, and followed by some preliminary tests. The results showed that the model can trace forces back to the end-effector with a percentage error below 15%.}
}