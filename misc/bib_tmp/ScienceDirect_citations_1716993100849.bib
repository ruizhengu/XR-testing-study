@article{PFEIFER2023107816,
title = {More than meets the eye: In-store retail experiences with augmented reality smart glasses},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107816},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107816},
url = {https://www.sciencedirect.com/science/article/pii/S074756322300167X},
author = {Pauline Pfeifer and Tim Hilken and Jonas Heller and Saifeddin Alimamy and Roberta {Di Palma}},
keywords = {Augmented reality, Augmented reality smart glasses, Embodiment, Retail experience, Purchase intentions},
abstract = {Augmented reality smart glasses (ARSGs) promise to enhance consumer experiences and decision-making when deployed as in-store retail technologies. However, research to date has not studied in-store use cases; instead, it has focused primarily on consumers' potential adoption of these devices for everyday use. Nor have prior studies compared ARSG uses with the now-common use of AR on touchscreen devices. The current research addresses these knowledge gaps by examining whether ARSGs outperform AR on touchscreen devices in the context of in-store retail experiences. Testing with an actual retail application (n = 308) shows that ARSGs are superior to AR on touchscreen devices for evoking consumers’ perceptions of immersion and mental intangibility. Furthermore, this superiority leads consumers to evaluate their shopping experiences more positively in terms of their decision comfort, satisfaction, and ease of evaluation, with significantly positive effects on their purchase intentions. These results highlight the relevance of implementing ARSGs in-store and provide retailers with recommendations for effective ARSG strategies.}
}
@article{CEBECI202423,
title = {Gaze-directed and saliency-guided approaches of stereo camera control in interactive virtual reality},
journal = {Computers & Graphics},
volume = {118},
pages = {23-32},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002492},
author = {Berk Cebeci and Mehmet Bahadir Askin and Tolga K. Capin and Ufuk Celikcan},
keywords = {Virtual reality, Head mounted displays, Stereoscopic rendering, Depth perception, Visual comfort},
abstract = {Despite remarkable advances in virtual reality (VR) technologies, serious challenges remain in making extended VR sessions with head-mounted displays (HMDs) thoroughly comfortable. 3D stereo imagery can cause discomfort and eye fatigue due to poor stereo camera settings that result in extreme disparities and vergence-accommodation conflicts. The default stereoscopic parameters of consumer HMDs produce images with shallow depth to circumvent these issues. In this work, we propose a methodology to utilize the gaze-directed and visual saliency-guided paradigms for automatic stereo camera control in real-time interactive VR by employing the basics of stereo grading. We evaluate these two approaches at different levels of interaction, first through a user study and then through a performance benchmark. The results show that the gaze-directed approach outperforms the saliency-guided approach in the VEs tested and both methods are able to convey a better overall depth feeling than the default HMD setting without hindering visual comfort. It is also shown that both approaches lead to a significant overall enhancement of the VR experience in the more interactive VE.}
}
@article{DUAN2019101562,
title = {Adversarial learning for deformable registration of brain MR image using a multi-scale fully convolutional network},
journal = {Biomedical Signal Processing and Control},
volume = {53},
pages = {101562},
year = {2019},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2019.101562},
url = {https://www.sciencedirect.com/science/article/pii/S1746809419301363},
author = {Luwen Duan and Gang Yuan and Lun Gong and Tianxiao Fu and Xiaodong Yang and Xinjian Chen and Jian Zheng},
keywords = {Deformable registration, Deep network, Unsupervised model, Multi-scale, Adversarial training},
abstract = {Background and objective
Deformable registration is very significant for various clinical image applications. Unfortunately, existing conventional medical image registration approaches, which involve time-consuming iterative optimization, have not reached the level of routine clinical practice in terms of registration time and robustness. The aim of this study is to propose a tuning-free 3D image registration model based on adversarial deep network, and to achieve rapid and high-accurate registration.
Methods
We propose a fully convolutional network (FCN) to regress the 3D dense deformation field in one shot from the to-be-registered image pair. To precisely regress the complex deformation and produce optimal registration, we design the FCN as a novel multi-scale frame to capture the complementary multi-scale image features and effectively characterize the spatial correspondence between the image pair. Moreover, we learn a discriminator network simultaneously to discriminate the registered two images, where the discrimination loss helps further update the FCN. Thus by the adversarial training strategy, the registration network is urged to produce well-registered two images that are indistinguishable for the discriminator.
Results
We perform registration experiments on four different brain MR datasets using the model trained by ANDI database. Compared with some state-of-the-art registration algorithms including other newest deeplearning-based methods, the proposed method provides a considerable increase of large than 4% in terms of Dice similarity coefficient (DSC). Moreover, our model also obtains comparable distance errors. More significantly, our model can achieve a high-accurate 3D registration result in average 0.74 s, with roughly hundred speed-up over conventional registration methods.
Conclusions
The proposed model shows consistent high performance for various registration tasks under a second without any additional parameter tuning, which proves its potential for real-time clinical applications.}
}
@article{HANSON2017570,
title = {Augmented reality as a means of conveying picking information in kit preparation for mixed-model assembly},
journal = {Computers & Industrial Engineering},
volume = {113},
pages = {570-575},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217304655},
author = {Robin Hanson and William Falkenström and Mikael Miettinen},
keywords = {Kit preparation, Mixed-model assembly, Picking information, Augmented reality},
abstract = {Kitting is a materials feeding principle that is increasingly common in mixed-model assembly. Currently, there is no consensus within industry regarding how picking information should best be conveyed to support kit preparation and research on the topic is scarce. The purpose of this paper is to determine whether information conveyance through augmented reality can be used to support time-efficient kit preparation, considering the two commonly applied approaches of single-kit preparation and batch preparation. The paper presents a novel application of augmented reality and tests it in a realistic laboratory experiment. As a basis for comparison, a traditional printed paper list is also tested. In the experiment, augmented reality is competitive both in terms of time-efficiency and picking accuracy, both for single kit and batch preparation, which indicates that augmented reality can constitute a viable option for conveying picking information in kit preparation. Especially for the batch preparation, where more information needs to be displayed, the augmented reality application is associated with considerably better performance than the paper list. The paper suggests that future research efforts should include studies on augmented reality applied in an actual industrial setting over a longer period.}
}
@article{SONG201613,
title = {A VR-based Interactive System to Improve Users’ Involvement in OAP Development},
journal = {Procedia CIRP},
volume = {56},
pages = {13-18},
year = {2016},
note = {The 9th International Conference on Digital Enterprise Technology – Intelligent Manufacturing in the Knowledge Economy Era},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116310009},
author = {Hao Song and Qingjin Peng and Jian Zhang and Peihua Gu},
keywords = {Open-architecture products, VR-based interactive system, User involvement.},
abstract = {Open-architecture products (OAPs) allow users to add personalized functional modules in an original product to meet changeable requirements for the product. The development of personalized products requires users’ involvement to meet the users’ needs effectively. The existing methods of Internet-based interactive platforms and direct market user surveys cannot provide users full experience of product features. This research proposes a virtual reality (VR)-based interactive system to improve users’ involvement in OAPs development. The system consists of a user interactive interface, a product model-processing module, a function execution module, a data recording and process analyzing module. Users operate the product model in the VR environment to experience the product. A soft-bag folding machine designed using the OAP concept is used in a case study to verify the proposed method.}
}
@article{PIEREN2024108767,
title = {Perception-based noise assessment of a future blended wing body aircraft concept using synthesized flyovers in an acoustic VR environment—The ARTEM study},
journal = {Aerospace Science and Technology},
volume = {144},
pages = {108767},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2023.108767},
url = {https://www.sciencedirect.com/science/article/pii/S1270963823006636},
author = {Reto Pieren and Ingrid {Le Griffon} and Lothar Bertsch and Axel Heusser and Francesco Centracchio and Daniel Weintraub and Catherine Lavandier and Beat Schäffer},
keywords = {Technology assessment, Aircraft noise, Simulation, Auralization, Noise annoyance},
abstract = {New aircraft concepts are currently being developed with the goal of less emissions of CO2 and noise. Remarkable noise reductions in long-range aircraft can only be expected from disruptive vehicle designs, new propulsion systems and specific low-noise technologies. In this paper, one such future vehicle design, a blended wing body (BWB) long-range aircraft, is described and studied with respect to sound levels on the ground, sound characteristics and noise annoyance. Virtual flyovers of different vehicle variants were synthesized and auralized in an acoustic VR environment, and investigated through psychoacoustic laboratory experiments. The applied methodology was successfully hierarchically validated by comparison with measurements of existing jet aircraft, assessing acoustical indices, time-frequency features, perceived plausibility, and induced noise annoyance. The perception-based evaluation of the BWB revealed that, while the BWB aircraft may initially be perceived as somewhat more unfamiliar, they are substantially less annoying than current tube-and-wing long-range aircraft of similar range and mission for take-offs as well as for landings. For the best BWB variant, noise annoyance was reduced by 4.3 units for departures and by 3.5 units for approaches on the 11-point scale. The main reason for these findings seems to be the acoustic shielding by the body of the extended fuselage, which was found to be an important factor in reducing sound levels in the order of 10–20 dB, and accordingly also to strongly reduce loudness. Additional low noise technologies and geared turbofan engines with a high bypass ratio further contributed to the reduction of noise annoyance of the BWB. A large part of the BWBs benefit could be explained by its lower sound levels, but additional benefits were found. The observed reduction in noise annoyance was found to be larger than what can be explained with conventional noise metrics. This benefit is probably due to more favorable sound characteristics compared to today's reference aircraft, such as less variation in time and less audible tones. The current study thus suggests that the studied BWB vehicle concept may substantially reduce noise annoyance on humans.}
}
@article{WANI2022113998,
title = {In-Plane measurements using a novel streamed digital image correlation for shake table test of steel structures controlled with MR dampers},
journal = {Engineering Structures},
volume = {256},
pages = {113998},
year = {2022},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.113998},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622001493},
author = {Zubair Rashid Wani and Manzoor Tantray and Ehsan {Noroozinejad Farsangi}},
keywords = {Digital image correlation, Shaking table, Matching algorithm, Streamed capture, Steel structure, MR damper, Structural response},
abstract = {The major problems related to structural control and monitoring systems are the challenges involved with the installation of traditional transducers, the number of transducers required to obtain the desired structural responses, and the computation effort and time required to analyze, process, and post-process the recorded signals. Non-contact measurements employing digital image correlation (DIC) techniques have become popular in control engineering in recent years. However, it can only be used on certain types of members with a narrow frequency range, minor deflections, and a slower image frame rate. This study aims at determining a in-plane measurement of a five-story steel frame controlled with magnetorheological dampers (MR) on a shaking table using the streamed DIC technique employing a template matching algorithm. The MR dampers (Passive-ON and Passive-OFF) are involved in varying the characteristic of the structural response and therefore assess the accuracy and flexibility of the proposed DIC technique. A high level of precision was achieved for all response quantities for each image. The measurements of acceleration, displacement, velocity, inter-story drift of the floors, and column base strains of the structure subjected to high-intensity bi-directional ground motions are obtained and compared with results obtained from traditional transducers. The shaking table results specified the efficiency and precision of the proposed DIC system in monitoring the structural responses compared to the noisy transducer responses, which emphasized the potential of using this technique for monitoring, control, and reconnaissance of a wide range of structures during and after extreme events.}
}
@article{SIIVOLA2024100058,
title = {Advantages of virtual reality childbirth education},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100058},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000084},
author = {Marjaana Siivola and Teemu Leinonen and Lauri Malmi},
keywords = {Virtual reality, Childbirth education, Learning effectiveness},
abstract = {Many expectant parents do not prepare enough for childbirth, and not getting a tour of the birthing hospital is causing them unnecessary stress. We enhanced childbirth education with an online virtual reality program for the users to experience what it might be like to give birth in a hospital. In this paper, we report a study that included observational user testing with a virtual reality headset and autonomic testing with the device of the user's choice. Data was collected with a pre-questionnaire, observations from the user tests, a semi-structured interview with the expecting parents, a post-questionnaire, and a follow-up questionnaire. The program improved learning outcomes and offered realistic and concrete birthing examples. Usability was good with the virtual reality headsets, while other devices need more research.}
}
@article{LIU20201049,
title = {Data-driven and AR assisted intelligent collaborative assembly system for large-scale complex products},
journal = {Procedia CIRP},
volume = {93},
pages = {1049-1054},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306041},
author = {Xinyu Liu and Lianyu Zheng and Jiazhou Shuai and Renjie Zhang and Yun Li},
keywords = {Large-scale complex products, Collaborative assembly, Intelligent assembly, Real-time data driven, Augmented Reality},
abstract = {With the continuous improvement of function diversity, quality and reliability, the complexity and scale of significant products such as aerospace equipment are increasing. This trend poses severe challenges to the assembly process. The large scale of products requires the fixed-position assembly with multiple roles of workers in parallel, and it increases the difficulty of multi-person assembly. The high complexity of products makes the assembly process tedious. It is time-consuming and laborious for workers to understand the 2D documents, and it easily leads to mis-operation. The assembly process of complex products can produce huge amounts of data, which have the potential for improving efficiency and quality but remain underutilized. The high reliability of products puts forward high requirements for real-time detection of the assembly process. With the development of information technology, data driven approach and Augmented Reality (AR) provide an efficient way to change the above situation. Therefore, this paper proposes an intelligent collaboration assembly system (ICAS) combining real-time data driven method with AR technology. In the ICAS, real-time data driven method is used to realize the human-to-human and human-to-machine collaboration under the fixed-position assembly, and AR is introduced to the system to assist the assembly of large-scale complex products. The intelligent detection method in the ICAS combines 3D reconstruction with sensors measurement data, and the detection result is displayed in an intuitive way by AR. The proposed system is verified in the assembly process of a large-scale space deployable mechanism, and it paves the way for the further development of intelligent assembly in the future.}
}
@article{MEI2023102160,
title = {Multi-stage rotors assembly of turbine-based combined cycle engine based on augmented reality},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102160},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102160},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002884},
author = {Yingjie Mei and Yongmeng Liu and Chuanzhi Sun and Xiaoming Wang and Dawei Wang and Lamei Yuan and Jiubin Tan},
keywords = {Turbine-based combined cycle engine, Multi-stage rotors assembly, Augmented reality, Assembly model, Virtual-real registration},
abstract = {To improve the assembly accuracy and efficiency of turbine-based combined cycle engines, we propose a multi-stage rotors assembly method based on augmented reality (AR) for turbine-based combined cycle engines. Combining the high fidelity interaction and edge computing capabilities of AR device, the real-time assembly guidance of multi-stage rotors of turbine-based combined cycle engine is realized. A new multi-stage rotors assembly mathematical model is proposed by introducing clearance errors to achieve prediction of coaxiality and perpendicularity in multi-stage rotors assembly. A point cloud registration method with equal-area constraints is proposed based on the requirement of AR virtual-real registration, combined with the triangulated graph. The experiment selected the multi-stage rotors of a simulated combination cycle engine for assembly. The experimental results showed that our AR system has an average delay of 11 ms and an average frame rate of 45fps. The accuracy of virtual-real registration is superior to current mainstream registration methods. The coaxiality prediction error and the perpendicularity prediction error for multi-stage rotors assembly is 0.5 μm and 0.4 μm, respectively. This system provides an efficient and accurate assembly plan for the rotors assembly of turbine-based combined cycle engines, meeting the actual assembly requirements on site.}
}
@article{YANG2013434,
title = {A new simple non-linear hysteretic model for MR damper and verification of seismic response reduction experiment},
journal = {Engineering Structures},
volume = {52},
pages = {434-445},
year = {2013},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0141029613001223},
author = {Meng-Gang Yang and Chun-Yang Li and Zheng-Qing Chen},
keywords = {MR damper, Non-linear hysteretic model, Seismic response reduction experiment},
abstract = {Non-linear hysteresis is a complicated mechanical characteristic for magneto-rheological (MR) damper. In this paper, a new simple non-linear hysteretic model for MR damper is proposed to represent the hysteretic behavior. First, the force–displacement and force–velocity loops under a range of currents, amplitudes and frequencies are obtained by mechanical behavior test of a RD1097 type MR damper. Then the model’s parameters are identified by the non-linear least square method from test data and fitted by the polynomials as functions of the supplied current. Finally, the accuracy and the effectiveness of the model are demonstrated by the RMS errors comparison between the reconstructed hysteretic curves and the experimental ones, and further are verified by seismic response reduction experiment under three excitations including the sinusoidal wave, the Pingsheng Bridge earthquake wave and the El-Centro wave. The results show that the proposed model has higher accuracy than some of existing models with explicit functions and is easier to be identified than those models with non-linear differential equations. Therefore, the proposed model can be effectively applied to simulation analysis in engineering control subjected to frequency-fixed or random excitations.}
}
@article{XU2008673,
title = {Real-time camera tracking for marker-less and unprepared augmented reality environments},
journal = {Image and Vision Computing},
volume = {26},
number = {5},
pages = {673-689},
year = {2008},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2007.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0262885607001266},
author = {Ke Xu and Kar Wee Chia and Adrian David Cheok},
keywords = {Vision based tracking, Optical flow, Fundamental matrix, Homography, Augmented reality},
abstract = {For three-dimensional video-based augmented reality applications, accurate measurements of the 6DOF camera pose relative to the real world are required for proper registration of the virtual objects. This paper presents an accurate and robust system for real-time 6DOF camera pose tracking based on natural features in an arbitrary scene. Crucially, the calculation is based on pre-captured reference images. This prevents a gradual increase in the camera position error. Point features in the current image frame are first matched to two spatially separated reference images. This wide baseline correspondence problem is overcome by constructing (1) a global homography between current and previous image frame and (2) local affine transforms derived from known matches between previous frame and reference images. Chaining these two mappings constrains the search for potential matches in the reference images and allows the warping of corner intensity neighborhoods so that a viewpoint invariant similarity measure for assessing potential point matches can be defined. We then minimize deviations from the two-view and three-view constraints between the reference images and current frame as a function of the camera motion parameters to obtain an estimate of the current camera pose relative to the reference images. This calculation is stabilized using a recursive form of temporal regularization similar in spirit to the Kalman filter. We can track camera pose reliably over hundreds of image frames and realistically integrate three-dimensional virtual objects with only slight jitter. This paper also tries to simplify the above described algorithm and present a real-time, robust tracking system based on computing homographies. Homography can exactly describe the image motion between two frames when the camera motion is pure rotation, or it is viewing a planar scene. For outdoor registration applications, the system is robust under small translations as long as the majority of the scene contents are distant.}
}
@article{GHORBANI2022100458,
title = {Towards an intelligent assistive system based on augmented reality and serious games},
journal = {Entertainment Computing},
volume = {40},
pages = {100458},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100458},
url = {https://www.sciencedirect.com/science/article/pii/S1875952121000550},
author = {Fatemeh Ghorbani and Mahsa {Farshi Taghavi} and Mehdi Delrobaei},
keywords = {Augmented reality, Fuzzy decision-making, Intelligent assistive technology, Internet of Things, Serious game},
abstract = {Age-related cognitive impairment is generally characterized by gradual memory loss and decision-making difficulties. The aim of this study is to investigate multi-level support and suggest relevant helping means for the elderly with mild cognitive impairment as well as their caregivers as the primary end-users. This work reports preliminary results on an intelligent assistive system, achieved through the integration of Internet of Things, augmented reality, and adaptive fuzzy decision-making methods. The proposed system operates in different modes, including automated and semi-automated modes. The former helps the user complete their daily life activities by showing augmented reality messages or making automatic changes; while the latter allows manual changes after the real-time assessment of the user’s cognitive state based on the augmented reality serious game score. We have also evaluated the accuracy of the serious game score with 37 elderly participants and compared it with users’ paper-based cognitive test results. We further noted that there is an acceptable correlation between the paper-based test and users’ serious game scores. Moreover, we observed that the system response in the semi-automated mode causes less data loss compared with the automated mode, as the number of active devices decreases.}
}
@article{MEYER2020142,
title = {Mixed-reality assistive robotic power chair simulator for Parkinson’s tremor testing},
journal = {Medical Engineering & Physics},
volume = {83},
pages = {142-147},
year = {2020},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1350453320300692},
author = {Richard T. Meyer and Yuliia Sergeeva},
keywords = {Virtual reality, Power chair, Simulation, Parkinson’s tremor},
abstract = {This note describes the development of a mixed-reality assistive robotic wheel chair simulator for testing of Parkinson’s tremor mitigation and operator assistance. It consists of a power chair (PCh), roller dynamometer, head mounted virtual reality (VR) display, computer with VR game engine, and microcontroller to interface the PCh and computer. Unlike past VR PCh simulators, both a tremor notch filter and basic collision avoidance is implemented. Further, the simulator identifies the Parkinson’s tremor frequency. Operator performance is assessed using deviations from a given route and velocity profile. To demonstrate the simulator’s operation, an operator with Parkinson’s tremor drove the PCh down two 20 m long hallways connected by a 90∘ turn while operating data was collected. Results show less variation in velocity tracking with use of the notch filter than without it; route tracking was nearly the same. Advantages of the simulator compared to a wholly physical approach are low cost, improved safety, portability, small footprint, and environments and robotic features are virtual.}
}
@article{MAHMOOD2023201,
title = {Layout planning and analysis of a Flexible Manufacturing System based on 3D Simulation and Virtual Reality},
journal = {Procedia CIRP},
volume = {120},
pages = {201-206},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123007072},
author = {Kashif Mahmood and Tauno Otto and Arnab Chakraborty},
keywords = {Flexible Manufacturing System, Layout planning, Manufacturing digitalization, 3D simulation, Industrial Virtual Reality},
abstract = {The high competition in the global market where the agile product launch and variable demand of goods by customers persuaded manufacturing companies to adopt Flexible Manufacturing System (FMS) and many companies have already implemented FMS solutions. On the other hand, manufacturing digitalization such as digital twin development, Industrial Virtual Reality (IVR), virtual modeling and 3D simulation of manufacturing systems offer new possibilities for effective facility layout planning, quick and easy modification and validation of production processes, analysis, and optimize the workflow and activities conducted on a factory floor. These new technological developments in digitalization have changed the thinking of manufacturing companies and they are eager to use digitalization solutions in their factory operations. However, there is a lack of harmonized methods and procedures to implement virtual modeling, 3D simulation, and IVR for layout planning and analysis of FMS. This paper proposed an approach for performance analysis of a FMS that is based on the 3D layout creation and simulation in a virtual environment, monitoring of key performance indicators via a digital dashboard, and immersive visualization through virtual reality. The relevance and feasibility of the proposed performance analysis approach are demonstrated by a case study.}
}
@article{JEYARAJ2023117013,
title = {MR image restoration and segmentation via denoising deep adversarial network for blood vessels accurate diagnosis},
journal = {Signal Processing: Image Communication},
volume = {117},
pages = {117013},
year = {2023},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2023.117013},
url = {https://www.sciencedirect.com/science/article/pii/S0923596523000954},
author = {Pandia Rajan Jeyaraj and Edward Rajan Samuel Nadar},
keywords = {Deep adversarial learning, Multiphase MRI, Image restoration, Segmentation, Interclass decision making},
abstract = {Segmenting the hepatic and portal veins is a difficult task, since it has multiple distortions. For effective restoration and to minimize distortions, a multi-stage deep adversarial learning network was proposed. The proposed network provides high reliability in segmenting hepatic and portal veins from distorted Magnetic Resonance (MR) images. The Proposed network directly considers the variation in vascular flow for vessel segmentation. By a two-stage deep adversarial network, which consist of geometric distortion by regularization in the first stage and in the second stage image blurriness is adjusted by a sub-pixel mechanism. To detect the hepatic venous flow, shared clustering was used to correlate the hepatic flow and venous flow. To have better image segmentation by restoration, end-to-end training by image adaptive by denoising was employed. The proposed Denoising Deep Adversarial Network (DDAN) generalizability is shown by curating a distorted TCGA dataset. We perform experiments by segmenting the distorted MR image with estimating depth, semantic interclass decision, and classification. The proposed DDAN is evaluated on a multi-phase open MRI public dataset from a local clinic and Kaggle Br35H. The obtained quantitative and qualitative evaluation data shows the practical applicability of the proposed deep learning model. We observed that the image edges, portal, and hepatic vein scale texture are clearly segmented to assist real-time applications like tumour surgery.}
}
@article{FISCHER200739,
title = {A hybrid tracking method for surgical augmented reality},
journal = {Computers & Graphics},
volume = {31},
number = {1},
pages = {39-52},
year = {2007},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2006.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849306001841},
author = {Jan Fischer and Michael Eichler and Dirk Bartz and Wolfgang Straßer},
keywords = {Medical augmented reality, Pose estimation, Hybrid tracking},
abstract = {Camera pose estimation is one of the most important, but also one of the most challenging tasks in augmented reality. Without a highly accurate estimation of the position and orientation of the digital video camera, it is impossible to render a spatially correct overlay of graphical information. This requirement is even more crucial in medical applications, where virtual objects typically have to be correctly aligned with the patient. Many experimental AR systems use specialized tracking devices, which usually are not certified for medical settings. We have developed an AR framework for surgical applications based on existing medical equipment. A surgical navigation device delivers tracking information measured by a built-in infrared camera system, which is the basis for the pose estimation of the AR video camera. However, depending on the conditions in the environment, this infrared pose data can contain discernible tracking errors. One main drawback of the medical tracking device is the fact that, while it delivers a very high positional accuracy, the reported camera orientation can contain a relatively large error. In this article, we present a hybrid tracking scheme for medical augmented reality based on a certified medical tracking system. The final pose estimation takes the initial infrared tracking data as well as salient features in the camera image into account. The vision-based component of the tracking algorithm relies on a pre-defined graphical model of the observed scene. The infrared and vision-based tracking data are tightly integrated into a unified pose estimation algorithm. This algorithm is based on an iterative numerical optimization method. We describe an implementation of the algorithm and present experimental data showing that our new method is capable of delivering a more accurate pose estimation.}
}
@article{LU2022101520,
title = {Development and validation of a confined space rescue training prototype based on an immersive virtual reality serious game},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101520},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101520},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002688},
author = {Song Lu and Fei Wang and Xin Li and Qichuan Shen},
keywords = {Confined space rescue, Safety training, IVR SG, E-lecture},
abstract = {Due to the inherently hazardous nature of the confined space environment and a lack of effective rescue training on on-site personnel attempting the rescue of fellow workers in a confined space, accidents or even multiple fatalities may happen. For this reason, improving the skill set of rescuers is of vital importance. Immersive virtual reality serious games (IVR SG) have been shown to be effective and are now widely used in safety training. However, there is little research on its application in confined space rescue training. To bridge this gap, we have developed a confined space rescue training system (called Rescue Success) based on IVR SG, and then tested its effectiveness in a real environment. We carried out a single-blind, randomized comparative experiment to study and compare the learning effectiveness of using IVR SG compared to an E-lecture in terms of behavioral skills, knowledge acquisition, self-efficacy, and simulation-induced sickness. Our results show that both training methods significantly improve the participant’s rescue behavioral skills, knowledge, and self-efficacy. Overall, IVR SG performed better than an E-lecture in all aspects except simulation-induced sickness, where there is no difference between the two methods.}
}
@article{BOGES202012,
title = {Virtual reality framework for editing and exploring medial axis representations of nanometric scale neural structures},
journal = {Computers & Graphics},
volume = {91},
pages = {12-24},
year = {2020},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2020.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0097849320300789},
author = {Daniya Boges and Marco Agus and Ronell Sicat and Pierre J. Magistretti and Markus Hadwiger and Corrado Calì},
keywords = {Ultrastructural analysis, Medial axis representation, Immersive environments, Virtual reality in neuroscience},
abstract = {We present a novel virtual reality (VR) based framework for the exploratory analysis of nanoscale 3D reconstructions of cellular structures acquired from rodent brain samples through serial electron microscopy. The system is specifically targeted on medial axis representations (skeletons) of branched and tubular structures of cellular shapes, and it is designed for providing to domain scientists: i) effective and fast semi-automatic interfaces for tracing skeletons directly on surface-based representations of cells and structures, ii) fast tools for proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) natural methods for interactive exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. Neuroscientists currently use the system for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat. The framework runs in a standard PC and has been tested on two different display and interaction setups: PC-tethered stereoscopic head-mounted display (HMD) with 3D controllers and tracking sensors, and a large display wall with a standard gamepad controller. We report on a user study that we carried out for analyzing user performance on different tasks using these two setups.}
}
@article{HUSSAIN2024105315,
title = {Conversational AI-based VR system to improve construction safety training of migrant workers},
journal = {Automation in Construction},
volume = {160},
pages = {105315},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105315},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000517},
author = {Rahat Hussain and Aqsa Sabir and Do-Yeop Lee and Syed Farhan Alam Zaidi and Akeem Pedro and Muhammad Sibtain Abbas and Chansik Park},
keywords = {Safety training, Virtual reality, Conversational AI, Migrant workers, Knowledge transfer, User interaction and communication},
abstract = {Inadequate communication during construction safety training can hinder knowledge transfer to workers, particularly migrant workers who face linguistic and literacy barriers, resulting in higher rates of fatal injuries. Additionally, limited abilities of trainers can further exacerbate this problem. To overcome these gaps, this paper presents a virtual reality-based knowledge delivery approach by integrating artificial Intelligence (AI) agent using ChatGPT as a live instructor. The system's effectiveness is assessed with participants from five countries. Results show a significant increase of 23% in scores between pre- and post-tests, indicating improved knowledge. The system performs uniformly across diverse language and educational backgrounds particularly benefiting those with prior experience. This research significantly contributes to the global construction community by improving safety training, promotes safer practices, and ultimately reduces accidents in diversified construction sites. It also opens avenues for research in immersive technologies and AI techniques, advancing in training methodologies and enhancing safety outcomes.}
}
@article{LAVIOLA2024104026,
title = {The minimal AR authoring approach: Validation in a real assembly scenario},
journal = {Computers in Industry},
volume = {154},
pages = {104026},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104026},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001768},
author = {Enricoandrea Laviola and Antonio Emmanuele Uva and Michele Gattullo},
keywords = {Industrial augmented reality, Authoring, Minimal information, Visual asset, Assembly, Work instruction},
abstract = {This work aims to validate the “minimal AR” authoring approach in a real industrial assembly scenario. It focuses on optimizing visual assets in Augmented Reality (AR) work instructions. The design of AR assembly documentation is influenced by three main variables: work instructions, affordance (dependent on equipment components and operator capabilities), and AR signifiers (combination of visual assets with their properties). In this study, we fixed the instruction complexity while exploring the relationship between affordance and AR signifiers. First, we set up a focus group of 10 experts in AR technical documentation to extract guidelines for the design of minimal AR signifiers for assembly instructions with a variable affordance. Then, we validated these guidelines through an industrial case study involving 34 participants in four assembly tasks. We verified if the candidate minimal AR signifier, obtained using the proposed guidelines, corresponded to the minimal AR signifier established by users. The results showed that in 33% of the cases, users exploited the candidate minimal AR signifier to accomplish the task successfully. Beyond the minimal AR signifier, an additional one conveying the notification about the task success must always be provided to ensure failure by those operators with reduced capabilities. We also found that, in 29% of the cases, users needed less information than the candidate minimal AR signifier due to their higher capabilities. However, as expected, this condition leads users to make more errors than with the candidate minimal AR signifier. Moreover, the study confirms that AR signifiers with redundant information or attractive appearance, such as animated product models, are unnecessary to improve task comprehension. Still, animations could be beneficial in reinforcing understanding when object properties are difficult to detect.}
}
@article{VANDUREN201852,
title = {Augmented reality fluoroscopy simulation of the guide-wire insertion in DHS surgery: A proof of concept study},
journal = {Medical Engineering & Physics},
volume = {55},
pages = {52-59},
year = {2018},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350453318300341},
author = {B.H. {van Duren} and K. Sugand and R. Wescott and R. Carrington and A. Hart},
keywords = {Simulation, Fluoroscopy, DHS/SHS, Guide-wire, Augmented reality, Surgical training},
abstract = {Background
Hip fractures contribute to a significant clinical burden globally with over 1.6 million cases per annum and up to 30% mortality rate within the first year. Insertion of a dynamic hip screw (DHS) is a frequently performed procedure to treat extracapsular neck of femur fractures. Poorly performed DHS fixation of extracapsular neck of femur fractures can result in poor mobilisation, chronic pain, and increased cut-out rate requiring revision surgery. A realistic, affordable, and portable fluoroscopic simulation system can improve performance metrics in trainees, including the tip-apex distance (the only clinically validated outcome), and improve outcomes.
Method
We developed a digital fluoroscopic imaging simulator using orthogonal cameras to track coloured markers attached to the guide-wire which created a virtual overlay on fluoroscopic images of the hip. To test the accuracy with which the augmented reality system could track a guide-wire, a standard workshop femur was used to calibrate the system with a positional marker fixed to indicate the apex; this allowed for comparison between guide-wire tip-apex distance (TAD) calculated by the system to be compared to that physically measured. Tests were undertaken to determine: (1) how well the apex could be targeted; (2) the accuracy of the calculated TAD. (3) The number of iterations through the algorithm giving the optimal accuracy–time relationship.
Results
The calculated TAD was found to have an average root mean square error of 4.2 mm. The accuracy of the algorithm was shown to increase with the number of iterations up to 20 beyond which the error asymptotically converged to an error of 2 mm.
Conclusion
This work demonstrates a novel augmented reality simulation of guide-wire insertion in DHS surgery. To our knowledge this has not been previously achieved. In contrast to virtual reality, augmented reality is able to simulate fluoroscopy while allowing the trainee to interact with real instrumentation and performing the procedure on workshop bone models.}
}
@article{RAMBLI2013211,
title = {Fun Learning with AR Alphabet Book for Preschool Children},
journal = {Procedia Computer Science},
volume = {25},
pages = {211-219},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012313},
author = {Dayang Rohaya Awang Rambli and Wannisa Matcha and Suziah Sulaiman},
keywords = {Augmented Reality, Education, Alphbet Learning, Fun Learning},
abstract = {This paper describes the design and evaluation of an AR alphabet book, an Augmented Reality based book for teaching the alphabet to preschool children. Used together with camera and computer, children could view the superimposed virtual alphabet in a fun and interactive manner using the pattern markers as an interaction tool. Generally, teaching young children could be difficult this is due to the focus of young children is different from elderly since they could only focus on something for short period of time. Introducing the fun and interactive learning could grab the attention therefore enhance teaching and learning for young learner. Fun-learning emphasized interactive learning through play, songs; dance, drama and the use of information and communication technology (ICT). Researches showed that fun learning also increase the ability to memorize and understanding of the user. Flashcard is one of the learning approaches to teach children the alphabet. The tangible manner of flashcard introduce the interaction of learning therefore it could create the joyful learning. However, the potential of flash-card could be further enhance through the use of AR technology. Therefore, introducing the use of AR could encompass fun learning since AR offer rich media learning. Besides displaying each alphabet upon presentation of its corresponding pattern marker, children have the options to see 3D models of objects that begin with each alphabet character. Additional book features includes pattern markers for children to view animation of how each letter is drawn and jigsaw puzzle game for each letter to test children understanding. An informal study was conducted among 15 preschool children aged between 5-6 years old to examine user perception of the book. The preliminary results indicate the children reacted positively towards the books; most reported they like and enjoy using the AR book. Observation of the children behaviors during study corroborates this finding. Most students requested to use the AR book repeatedly. These results suggest the potential of AR book as a tool to create fun learning environment especially for preschool children. Furthermore, the brief interview with the teacher of the preschool also suggested that the system seem to grab the attention of the children.}
}
@article{PUYUELO2013171,
title = {Experiencing Augmented Reality as an Accessibility Resource in the UNESCO Heritage Site Called “La Lonja”, Valencia},
journal = {Procedia Computer Science},
volume = {25},
pages = {171-178},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S187705091301226X},
author = {Marina Puyuelo and José Luís Higón and Lola Merino and Manuel Contero},
keywords = {Accessibility, Heritage sites, Vitual Reality, Augmented Reality, Interactivity, Interface design},
abstract = {This paper presents the design of an augmented reality application for the Gothic Silk Market Building called the “Lonja de la Seda” in Valencia (UNESCO Monument Heritage Site, 1996) and the results of the experiments carried out “in-situ” to observe its usability as an accessibility resource. The objective of this project has been to use and validate augmented reality (AR) as a tool to increase the accessibility to the architectural elements of this monumental setting. The AR application aims to resolve the perception issues derived from poor lighting, the distance in relation to the multiple details, access, etc. 145 individuals of different ages and diverse origin, making the visit to the monument with or without guide used the AR application. When visitors had used the application as they wished, interviews were conducted individually in order to receive their feedback about the AR experience. Users enjoyed identifying and selecting the motifs they were going to visualize with the AR tool. In general, the experience was very positively evaluated by participants, promoting a favourable and renewed image of the monument}
}
@article{ZHANG202038,
title = {Collaborative robot and mixed reality assisted microgravity assembly for large space mechanism},
journal = {Procedia Manufacturing},
volume = {51},
pages = {38-45},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S235197892031862X},
author = {Renjie Zhang and Xinyu Liu and Jiazhou Shuai and Lianyu Zheng},
keywords = {Intelligent Assembly, Human-machine collaboration, Mixed reality, Condition monitoring, Large space mechanism},
abstract = {Gravity balance has a great impact on the assembly and deployment experiment of the large space mechanism to achieve the perfect performance in space. Traditional gravity balance methods have their advantages in the deployment experiment, but not suitable for the assembly process which has a more stringent location requirement. This paper proposed an advanced microgravity assembly method for a large space mechanism. The architecture of the system for microgravity assembly is proposed. The collaborative robot and mixed reality (MR) are integrated into this system, which can realize human-robot collaboration assembly, improve assembly efficiency and reduce errors. The microgravity assembly is realized by the six-dimensional force sensor which makes the robot work in force control and installed at the end of the collaborative robot. The prototype system is verified in the laboratory environment, which provides a new and feasible assembly mode for intelligent collaborative assembly for large complex space products.}
}
@article{DEVI2019235,
title = {Hiding medical information in brain MR images without affecting accuracy of classifying pathological brain},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {235-246},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18322647},
author = {Swagatika Devi and Manmath Narayan Sahoo and Khan Muhammad and Weiping Ding and Sambit Bakshi},
keywords = {Medical data hiding, Pathological brain detection, Steganography, Privacy preservation, MR image classification, Internet of Medical Things},
abstract = {This work aims at validating the effect of steganography on a Computer-Aided Diagnosis (CAD) system for segregating pathological brain from normal brain from seeing MR images. Usually MR images of the brain achieves significant accuracy in classifying “normal” and “abnormal” brains. However, the brain images labeled with patient information is a severe threat to privacy preservation of the medical data, especially when the medical data is a part of Internet of Medical Things (IoMT) and travels over the network. Therefore, our article investigates on a two-fold objective i.e. ensuring visual hiding of steganographic data containing identity of the patient and at the same time retaining almost similar classification accuracy towards detecting a diseased MR brain image. In this approach, the identity of the patient is hidden in the image itself by employing a spatial steganographic algorithm on the T2-weighted MR images, thereby ensuring minimal chance of misuse of the private medical data. Two MR image datasets collected from Harvard whole brain Atlas have been used to validate our framework comparing against some well performing classification techniques. The classification techniques are applied on normal as well as stego images. It is concluded that using a steganographic algorithm on the MR images does not much affect the classification rate and yield equally satisfactory performance.}
}
@article{SORKO2020367,
title = {Implementing AR/MR – Learning factories as protected learning space to rise the acceptance for Mixed and Augmented Reality devices in production},
journal = {Procedia Manufacturing},
volume = {45},
pages = {367-372},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310751},
author = {Sabrina Romina Sorko and Christian Trattner and Joachim Komar},
keywords = {Augmented Reality, Mixed Reality, Employee Acceptance, Learning Factory},
abstract = {When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.}
}
@article{CUNHA2023634,
title = {Using Extended Reality in Experiential Learning for Hospitality Management Education},
journal = {Procedia Computer Science},
volume = {219},
pages = {634-641},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.333},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003423},
author = {Carlos R. Cunha and Catarina Martins and Vítor Mendonça},
keywords = {Extended Reality, Model, Hospitality, Experiential Learning, Training, Assessement},
abstract = {Training qualified human resources is a challenge in all areas of knowledge. In particular, in the areas where the training environment of the graduates is not easily laboratoryable and/or possible to emulate, internships are often used in the work context, where an On-the-Job Training is provided. Higher education all over the world has sought to innovate in the way it mixes education. Accordingly, the digital has populated teaching-learning strategies in an attempt to make teaching more universal, more ubiquitous and more personalized, without losing rigor or the necessary requirements. In this path of innovation, e-learning or b-learning mechanisms were and still are used. However, we believe that a more decisive step must be taken by incorporating Extended Reality tools to develop and evolve the way in which competences can be acquired and evaluated, both in the classroom and in home study - a new approach at the service of the teacher and of the student. In this context, this work analyzes the role of Extended Reality in Human Resources training in the hospitality sector. In addition, a conceptual model is proposed to implement the use of Extended Reality in the context of higher education in the field of hospitality, in a conceptualization approach. Finally, future work is referred concerning the validation of the model and drawn technological considerations for the model implementation.}
}
@article{VINURAJKUMAR2022103093,
title = {An Enhanced Fuzzy Segmentation Framework for extracting white matter from T1-weighted MR images},
journal = {Biomedical Signal Processing and Control},
volume = {71},
pages = {103093},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.103093},
url = {https://www.sciencedirect.com/science/article/pii/S174680942100690X},
author = {S. Vinurajkumar and S. Anandhavelu},
keywords = {Fuzzy clustering, Magnetic resonance imaging, Segmentation, White matter},
abstract = {Background
White matter atrophy computed from Magnetic Resonance (MR) images is a clinical indication of a broad spectrum of neurological disorders. Accurate segmentation of white matter from MR images is necessary to estimate the white matter volume. Most of the techniques in literature used for the segmentation of white matter are computationally slow. The repeatability of segmentation results and consistency of performance on different input images are often poor.
Objectives
A computationally simple fuzzy clustering technique termed Enhanced Fuzzy Segmentation Framework (EFSF) for segmenting the white matter from the T1-Weighted MR images is proposed in this paper.
Methodology
IN EFSF, the fuzzy membership function and prototype value are derived from the generic objective function of FCM using the method of Lagrange’s multiplier. The membership and prototype values are updated iteratively. The clustered image is obtained by replacing each grey level in the input image with the prototype value of the cluster with the largest membership value in the corresponding row of the fuzzy partition matrix. The pixels in the clustered image whose values are equal to the largest prototype value belong to the white matter region.
Results
On 100 test images, the Dice Similarity Index (DSI) and the computational time (in sec) shown by EFSF are 0.8051 ± 0.0577 and 0.6522 ± 0.0502, respectively.
Conclusion
EFSF offers high segmentation accuracy and is computationally fast. Segmentation results offered by EFSF have good repeatability on the same MR slice and consistency on MR slices from various regions of the brain.}
}
@article{TRENTSIOS202025,
title = {Remote Lab meets Virtual Reality – Enabling immersive access to high tech laboratories from afar},
journal = {Procedia Manufacturing},
volume = {43},
pages = {25-31},
year = {2020},
note = {Sustainable Manufacturing - Hand in Hand to Sustainability on Globe: Proceedings of the 17th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306818},
author = {Pascalis Trentsios and Mario Wolf and Sulamith Frerich},
keywords = {Remote Lab, Virtual Reality, Engineering Education, Unity Engine, LabVIEW},
abstract = {Virtual Reality is currently one of the strongest trends in the consumer and gaming industry, while the typical Remote Lab in engineering education uses traditional 2D visualization options for desktop PCs. In this paper those two domains were combined by establishing state-of-the-art frontends for an existing engineering education Remote Lab. As the whole nature of this approach is experimental, the authors aimed at a variable degree of immersion and established two different approaches for the creation of a virtual environment. The first approach is to display the representation of the physical laboratory with 360°-images in a Street View type fashion. The second approach is a completely reconstructed virtual 3D-model of the existing physical laboratory. Both approaches were realized with the Unity engine and tested in the concerning laboratory.}
}
@article{MOURTZIS2020151,
title = {Augmented reality visualization of production scheduling and monitoring},
journal = {Procedia CIRP},
volume = {88},
pages = {151-156},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303425},
author = {Dimitris Mourtzis and Vasilis Siatras and Vasilios Zogopoulos},
keywords = {Augmented Reality, Production Monitoring, Scheduling},
abstract = {Scheduling and monitoring are two of the main components of production management that are crucial for the seamless function of production. Moreover, following the recent technological achievements, Augmented Reality (AR) is an upcoming way to display a variety of data in a more easily perceivable and intractable way. The proposed approach aims to couple production scheduling and monitoring along with augmented reality, in order to allow the user to interact with those components in a more realistic way. The proposed approach is validated in a case study of a real workshop.}
}
@article{LIN201926,
title = {Visualization of indoor thermal environment on mobile devices based on augmented reality and computational fluid dynamics},
journal = {Automation in Construction},
volume = {103},
pages = {26-40},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518305922},
author = {Jia-Rui Lin and Jun Cao and Jian-Ping Zhang and Christoph {van Treeck} and Jérôme Frisch},
keywords = {Augmented reality (AR), Computational fluid dynamics (CFD), Indoor thermal environment, Mobile device, Performance improvement, Client-server framework},
abstract = {Augmented reality (AR) based visualization of computational fluid dynamics (CFD) simulation on mobile devices is important for the understanding and discussion of indoor thermal environments in the design process. However, utilizing AR-based mobile device for indoor thermal environment understanding still encounters problems due to limited computational power and lack of efficient interaction methods. To improve the performance of AR-based CFD visualization on a mobile device and provide the users with intuitive interaction with indoor environment, an integrated approach based on client-server framework is established. Within the approach, a new mobile friendly data format (cfd4a) with low computational complexity is proposed for CFD simulation results representation. Server-side data pre-processing method is also introduced to reduce the computational power and time needed on the client side. Furthermore, interactive section view selection and time-step animation methods are proposed for intuitive interaction with the AR environment. Then, a prototype system is developed with Unity3D engine and Tango Tablet. Demonstration in some typical scenarios shows that the proposed method and prototype system provide an intuitive and fluent AR-based environment for interactive indoor thermal environment visualization. Comparing to the widely used vtk format, a data compression ratio of 63.4% and a loading time saving ratio of 89.3% are achieved in the performance test with the proposed method. The proposed approach also shows good flexibility and extensibility in supporting different AR devices through exposing standard web services and utilizing the widely used Unity3D engine. Source code of the developed prototype and relevant testing data are also shared through github, enabling other researchers to compare our work with theirs. It is also suggested that stability of AR-based mobile devices, new interaction methods and integration with cloud computing still need further investigation and improvement.}
}
@incollection{DROBNJAK2022165,
title = {Chapter 9 - Optimization of the MR imaging pipeline using simulation},
editor = {Ninon Burgos and David Svoboda},
booktitle = {Biomedical Image Synthesis and Simulation},
publisher = {Academic Press},
pages = {165-193},
year = {2022},
series = {The MICCAI Society book  Series},
isbn = {978-0-12-824349-7},
doi = {https://doi.org/10.1016/B978-0-12-824349-7.00016-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243497000165},
author = {Ivana Drobnjak and Mark Graham and Hui Zhang and Mark Jenkinson},
keywords = {Simulation, MRI, Optimization, POSSUM, Motion correction},
abstract = {Careful optimization of the full imaging pipeline, from data acquisition to post-processing, is a crucial part of any magnetic resonance imaging study. However, testing and optimization of pipelines in the “real world” is expensive and challenging, and hence simulations play a key role in the process. In this chapter, we discuss simulation frameworks and the ways they can be applied to optimize imaging pipelines. We cover examples of the use of simulation to evaluate post-processing algorithms, study imaging artifacts, design and test new acquisition techniques, and to produce data for training machine-learning based post-processing tools.}
}
@article{JIMENOMORENILLA20131371,
title = {Augmented and Virtual Reality techniques for footwear},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1371-1382},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S016636151300122X},
author = {Antonio Jimeno-Morenilla and Jose Luis Sánchez-Romero and Faustino Salas-Pérez},
keywords = {Augmented reality in footwear sector, Stereoscopic vision for design and display of footwear, 3D gloves for footwear design},
abstract = {The use of 3D imaging techniques has been early adopted in the footwear industry. In particular, 3D imaging could be used to aid commerce and improve the quality and sales of shoes. Footwear customization is an added value aimed not only to improve product quality, but also consumer comfort. Moreover, customisation implies a new business model that avoids the competition of mass production coming from new manufacturers settled mainly in Asian countries. However, footwear customisation implies a significant effort at different levels. In manufacturing, rapid and virtual prototyping is required; indeed the prototype is intended to become the final product. The whole design procedure must be validated using exclusively virtual techniques to ensure the feasibility of this process, since physical prototypes should be avoided. With regard to commerce, it would be desirable for the consumer to choose any model of shoes from a large 3D database and be able to try them on looking at a magic mirror. This would probably reduce costs and increase sales, since shops would not require storing every shoe model and the process of trying several models on would be easier and faster for the consumer. In this paper, new advances in 3D techniques coming from experience in cinema, TV and games are successfully applied to footwear. Firstly, the characteristics of a high-quality stereoscopic vision system for footwear are presented. Secondly, a system for the interaction with virtual footwear models based on 3D gloves is detailed. Finally, an augmented reality system (magic mirror) is presented, which is implemented with low-cost computational elements that allow a hypothetical customer to check in real time the goodness of a given virtual footwear model from an aesthetical point of view.}
}
@article{FARAHMANDAZAR2020110657,
title = {Control of the nonlinear building using an optimum inverse TSK model of MR damper based on modified grey wolf optimizer},
journal = {Engineering Structures},
volume = {214},
pages = {110657},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.110657},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619332390},
author = {Bahman {Farahmand Azar} and Hedayat Veladi and Farzad Raeesi and Siamak Talatahari},
keywords = {Inverse model, Optimum TSK model, Grey wolf optimizer, Nonlinear building},
abstract = {This paper proposes an inverse model of the magnetorheological (MR) damper, based on a new Takagi-Sugeno-Kang (TSK) model to estimate the required voltage for producing the MR force. Usually, a MATLAB toolbox function, adaptive neuro-fuzzy inference systems (ANFIS), is used to train the TSK models, which uses the gradient-based learning algorithms to tune the weights or membership functions parameters. The main drawback is that, these algorithms most often find themselves trapped in local minima, depending on the initial estimations. To overcome this issue, a grey wolf optimizer (GWO) is selected and modified to achieve the training task and called the model as an optimum modified grey wolf-TSK model (OMGT). Also, the superiority of the modified grey wolf optimizer over its standard one is investigated using some mathematical benchmark test functions. Moreover, the linear quadratic regulator (LQR) controller is designed to estimate the optimal control force of an MR damper. The effectiveness of this optimum inverse model in structural control is illustrated and verified using an eight-story nonlinear benchmark building. The performance of the designed OMGT model is compared with the different control algorithms such on (PON), clipped optimal control (COC), active control, and ANFIS under different earthquakes, which demonstrate an acceptable performance of the OMGT over these control algorithms.}
}
@article{WEI2022108420,
title = {A cascaded nested network for 3T brain MR image segmentation guided by 7T labeling},
journal = {Pattern Recognition},
volume = {124},
pages = {108420},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108420},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321005963},
author = {Jie Wei and Zhengwang Wu and Li Wang and Toan Duc Bui and Liangqiong Qu and Pew-Thian Yap and Yong Xia and Gang Li and Dinggang Shen},
keywords = {Brain segmentation, Cascaded nested network, Deep learning, Magnetic resonance imaging},
abstract = {Accurate segmentation of the brain into gray matter, white matter, and cerebrospinal fluid using magnetic resonance (MR) imaging is critical for visualization and quantification of brain anatomy. Compared to 3T MR images, 7T MR images exhibit higher tissue contrast that is contributive to accurate tissue delineation for training segmentation models. In this paper, we propose a cascaded nested network (CaNes-Net) for segmentation of 3T brain MR images, trained by tissue labels delineated from the corresponding 7T images. We first train a nested network (Nes-Net) for a rough segmentation. The second Nes-Net uses tissue-specific geodesic distance maps as contextual information to refine the segmentation. This process is iterated to build CaNes-Net with a cascade of Nes-Net modules to gradually refine the segmentation. To alleviate the misalignment between 3T and corresponding 7T MR images, we incorporate a correlation coefficient map to allow well-aligned voxels to play a more important role in supervising the training process. We compared CaNes-Net with SPM and FSL tools, as well as four deep learning models on 18 adult subjects and the ADNI dataset. Our results indicate that CaNes-Net reduces segmentation errors caused by the misalignment and improves segmentation accuracy substantially over the competing methods.}
}
@article{NEB202174,
title = {Usability study of a user-friendly AR assembly assistance},
journal = {Procedia CIRP},
volume = {104},
pages = {74-79},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009112},
author = {Alexander Neb and David Brandt and Ramez Awad and Silvana Heckelsmüller and Thomas Bauernhansl},
keywords = {Augmented Reality, Assembly Support, User Interface},
abstract = {The Augmented Reality (AR) technology is proven suitable to support manual assembly activities. However, existing applications are hardly adapted to the users. To improve AR assembly assistance, we developed a user-friendly and intuitive user interface that was designed for specific user groups and validated in a usability study. The validation took place in a facility for handicapped people and in a manufacturing company. It confirms the intuitive and easy use of the elaborated user interface and demonstrates the importance of its user group’s specific design.}
}
@article{GERAETS2021100432,
title = {Virtual reality facial emotion recognition in social environments: An eye-tracking study},
journal = {Internet Interventions},
volume = {25},
pages = {100432},
year = {2021},
issn = {2214-7829},
doi = {https://doi.org/10.1016/j.invent.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2214782921000725},
author = {C.N.W. Geraets and S. {Klein Tuente} and B.P. Lestestuiver and M. {van Beilen} and S.A. Nijman and J.B.C. Marsman and W. Veling},
keywords = {Virtual reality, Emotion recognition, Eye-tracking, Affect, Emotion, Avatars},
abstract = {Background
Virtual reality (VR) enables the administration of realistic and dynamic stimuli within a social context for the assessment and training of emotion recognition. We tested a novel VR emotion recognition task by comparing emotion recognition across a VR, video and photo task, investigating covariates of recognition and exploring visual attention in VR.
Methods
Healthy individuals (n = 100) completed three emotion recognition tasks; a photo, video and VR task. During the VR task, emotions of virtual characters (avatars) in a VR street environment were rated, and eye-tracking was recorded in VR.
Results
Recognition accuracy in VR (overall 75%) was comparable to the photo and video task. However, there were some differences; disgust and happiness had lower accuracy rates in VR, and better accuracy was achieved for surprise and anger in VR compared to the video task. Participants spent more time identifying disgust, fear and sadness than surprise and happiness. In general, attention was directed longer to the eye and nose areas than the mouth.
Discussion
Immersive VR tasks can be used for training and assessment of emotion recognition. VR enables easily controllable avatars within environments relevant for daily life. Validated emotional expressions and tasks will be of relevance for clinical applications.}
}
@article{RADANOVIC2023101960,
title = {Aligning the real and the virtual world: Mixed reality localisation using learning-based 3D–3D model registration},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101960},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101960},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000885},
author = {Marko Radanovic and Kourosh Khoshelham and Clive Fraser},
keywords = {Augmented reality (AR), Mixed reality (MR), Large-scale localisation, Indoor positioning, 3D building models, Deep learning},
abstract = {Existing camera localisation methods for indoor augmented and mixed reality (AR/MR) are almost exclusively image based. The main issue with image-based methods is that they do not scale well and, as a consequence, AR/MR applications are mostly limited to small-scale room experiences. To tackle the challenge of large-scale indoor AR/MR localisation, we propose a novel framework for AR/MR localisation based solely on 3D–3D model registration. The localisation is performed by an automated registration of a low-density model of the surroundings created by the device to the existing point cloud of the environment based on learning-based keypoint detection and description. Our solution takes advantage of recent significant improvements in automated coarse-to-fine 3D–3D model registration methods. Unlike the existing image-based AR/MR localisation methods, which are restricted to small room-sized environments, the proposed 3D registration-based approach is applicable to large environments and is robust to changes in colour and illumination of the scene. We perform extensive testing and analysis of the approach with real-world experiments and datasets using a prototype developed for the Microsoft HoloLens. Experimental results show high localisation reliability and accuracy, with a mean translation error of 2.8 cm and a mean rotation error of 0.30°. The method performs well in a large-scale environment (300 m2) and shows good robustness to changes in scene geometry.}
}
@article{ZAPPALA2024111913,
title = {In vivo strain measurements in the human buttock during sitting using MR-based digital volume correlation},
journal = {Journal of Biomechanics},
volume = {163},
pages = {111913},
year = {2024},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111913},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023004840},
author = {Stefano Zappalá and Bethany E. Keenan and David Marshall and Jing Wu and Sam L. Evans and Rami M.A. Al-Dirini},
keywords = {Digital volume correlation, Strain analysis, Deep tissue injury, Pressure ulcers, Soft tissues},
abstract = {Advancements in systems for prevention and management of pressure ulcers require a more detailed understanding of the complex response of soft tissues to compressive loads. This study aimed at quantifying the progressive deformation of the buttock based on 3D measurements of soft tissue displacements from MR scans of 10 healthy subjects in a semi-recumbent position. Measurements were obtained using digital volume correlation (DVC) and released as a public dataset. A first parametric optimisation of the global registration step aimed at aligning skeletal elements showed acceptable values of Dice coefficient (around 80%). A second parametric optimisation on the deformable registration method showed errors of 0.99mm and 1.78mm against two simulated fields with magnitude 7.30±3.15mm and 19.37±9.58mm, respectively, generated with a finite element model of the buttock under sitting loads. Measurements allowed the quantification of the slide of the gluteus maximus away from the ischial tuberosity (IT, average 13.74 mm) that was only qualitatively identified in the literature, highlighting the importance of the ischial bursa in allowing sliding. Spatial evolution of the maximus shear strain on a path from the IT to the seating interface showed a peak of compression in the fat, close to the interface with the muscle. Obtained peak values were above the proposed damage threshold in the literature. Results in the study showed the complexity of the deformation of the soft tissues in the buttock and the need for further investigations aimed at isolating factors such as tissue geometry, duration and extent of load, sitting posture and tissue properties.}
}
@article{FOROUGHISABZEVAR2023105017,
title = {AR-QR code for improving crew access to design and construction information},
journal = {Automation in Construction},
volume = {154},
pages = {105017},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105017},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002777},
author = {Mohsen {Foroughi Sabzevar} and Masoud Gheisari and L. James Lo},
keywords = {Augmented reality, AR-QR code, Design and construction information, Poor communication, Error},
abstract = {Traditional methods of delivering construction information, relying on paper-based shop drawings, specifications, and verbal instructions, often lead to increased mental workload, error rates, and difficulties in accessing information. This raises the question of how the traditional approach to accessing design and construction information on construction sites can be improved to address these issues. This paper describes a hybrid approach known as AR-QR code, which aims to enhance the conventional paper-based method of accessing such information using the power of Augmented Reality (AR). The research methodology involved: process mapping, prototype development, and evaluation. The evaluation results revealed that professionals expressed confidence in the potential of the AR-QR code approach to alleviate the limitations associated with traditional methods at construction sites. The outcomes of this study provide valuable insights into the need for a hybrid approach that preserves the advantages of traditional methods while avoiding their drawbacks.}
}
@article{PICANO2022109152,
title = {Human-in-the-loop virtual reality offloading scheme in wireless 6G Terahertz networks},
journal = {Computer Networks},
volume = {214},
pages = {109152},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S138912862200264X},
author = {Benedetta Picano and Romano Fantacci},
keywords = {Matching theory, Human-in-the-loop, Stochastic network calculus},
abstract = {Upcoming sixth-generation networks have to be carefully designed to offer support to a wide range of human-centric applications, typically computationally intensive and heavily dependent on human perception. This paper proposes a human-in-the-loop-based resource allocation approach found on the joint use of matching theory and stochastic network calculus principles to efficiently address tasks offloading issues within a sixth-generation network environment. The considered matching game approach is characterized by externalities, whose stability is properly discussed here. In particular, by focusing on virtual reality services, the paper aims at maximizing the average service reliability in terms of ability in guaranteeing a proper quality of experience target, i.e., keeping the end-to-end delay below a suitable threshold, by the proper exploitation of the human cognitive limitations. Finally, the effectiveness of the proposed human-in-the-loop-based resource allocation approach is verified by proving comparisons with simulation results and conventional brain-agnostic strategies.}
}
@article{XIA2024102664,
title = {Augmented reality and indoor positioning based mobile production monitoring system to support workers with human-in-the-loop},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {86},
pages = {102664},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102664},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001394},
author = {Luyao Xia and Jianfeng Lu and Yuqian Lu and Hao Zhang and Yuhang Fan and Zhishu Zhang},
keywords = {Augmented reality, Indoor positioning, Production monitoring system, Human factors, Human-in-the-loop},
abstract = {With the increasing demand for product customization, the exponential increase in large-scale and small-batch production orders has yielded new challenges for the capacity of traditional production lines. Due to the information redundancy and complex production process on the production site of the factory, the traditional monitoring system is incapable of meeting the information interaction requirements between the factory management level and the executive level. Meanwhile, the traditional augmented reality (AR) based on image recognition is not suitable in the complex industrial environments. To address the gap, we propose a novel mobile production monitoring system (AIMPMs) with human-in-the-loop control by leveraging the cutting-edge AR and indoor positioning technique. In the proposed system, Ultra-wideband (UWB) and Inertial Measurement Unit (IMU) fusion indoor positioning technology is proposed, which provides accurate indoor positioning information for the production factors in the factory. Subsequently, we build the lightweight indoor map for positioning that can serve as the location reference and path planning, and a nearest-neighbor decision algorithm with double rejection decision (NN-DRD) is proposed to match the positioning features to trigger virtual monitoring information. Finally, the AIMPMs is applied in a hydraulic cylinder factory to verify its enforceability and effectiveness, and the human factors evaluation model and index system are constructed to evaluate two systems, (1) the mobile monitoring system based on positioning information, and (2) the mobile monitoring system based on image recognition, respectively. The experimental results indicate that after the application of AIMPMs, the physiological and mental fatigue of the production personnel is immensely decreased. Therefore, the system realizes a smarter and highly humanized human-machine interaction mode with human-in-the-loop control.}
}
@article{SINGH201818,
title = {Influence of 100keV Ar+ implantation on electrical and optical properties of TiO2/Ag/TiO2 multilayer films},
journal = {Materials Science in Semiconductor Processing},
volume = {75},
pages = {18-25},
year = {2018},
issn = {1369-8001},
doi = {https://doi.org/10.1016/j.mssp.2017.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S1369800117321066},
author = {Satyavir Singh and Vikas Sharma and Dinesh Saini and Surbhi Shekhawat and K. Asokan and Kanupriya Sachdev},
keywords = {Transparent conducting oxide, Implantation, Multilayer films, Hall measurement, Transmittance},
abstract = {The objective of this study was to investigate the effect of 100keV Ar+ ion implantation with fluence ranging from 1 × 1014 to 1 × 1016ions/cm2 on structural, electrical and optical properties of TiO2/Ag/TiO2 films. The TiO2/Ag/TiO2 multilayer films with a total thickness of 100nm were prepared by sputtering at room temperature on glass and silicon substrates. The structural properties studied via X-ray diffraction showed amorphous nature of TiO2 and a peak centred at 38.15° assigned to Ag (111) diffraction plane. Morphological analysis carried by field emission scanning electron microscopy and atomic force microscopy exhibit uniform deposition of films. A decrease in surface roughness was observed from 1.71 to 0.87nm with increase in ion fluence from 0 to 1 × 1016ions/cm2. Measured resistivity of the pristine film was 2.04 × 10−4Ωcm, which minimized to 9.14 × 10−5Ωcm at the highest fluence of 1 × 1016ions/cm2. The transmittance decreases from 80% to 73% for higher fluences. Changes in electrical resistivity and transmittance can be attributed to the structural changes in the samples due to amorphization, defect creation and doping on ion implantation. Carrier concentration, mobility of charge carriers and sheet resistance for pristine and implanted films have been calculated as a function of ion fluence. X-ray photoelectron spectroscopy data revealed creation of Ti ions (Ti3+) after Ar+ ion implantation. The core level spectrum of O 1s was also studied and presence of some OH species is found on the surface. Optical and transport properties of implanted TAT films are comparable to that of indium tin oxide (ITO) and make them a good choice for the current and future transparent conducting electrode (TCE) applications.}
}
@article{ZHANG2024108157,
title = {Attention-ResNet-AR-LSTM: An intelligent method for PCCP deformation prediction via structure monitoring based on distributed fiber optics},
journal = {Engineering Failure Analysis},
volume = {160},
pages = {108157},
year = {2024},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2024.108157},
url = {https://www.sciencedirect.com/science/article/pii/S1350630724002036},
author = {Ye Zhang and Minghui Ma and Yanlong Li and Heng Zhou and Kangping Li and Lifeng Wen},
keywords = {PCCP deformation, Crack, AR, ResNet, LSTM, Attention},
abstract = {The prestressed concrete cylinder pipeline (PCCP) is extensively employed in long water diversion projects due to its commendable performance and cost-effectiveness. Nonetheless, the continuous application of load on the prestressed steel wire during the production phase can amplify the creep deformation in both the concrete and steel cylinder of the PCCP. This phenomenon may potentially lead to the development of cracks at the socket end of the PCCP, posing a significant risk to the safe operational integrity of the pipeline. In this study, the strain development prediction model of PCCP concrete and steel cylinder is constructed through the integration of ResNet, AR, and LSTM modules, and the attention mechanism is also applied. Based on the prototype test of PCCP structure monitoring, the strain values of steel cylinder and inner concrete were recorded by pre-set distributed fiber optic sensors on the 1st, 3rd, 5th, 7th, 9th, 11th, 13th, 15th, 17th, and 20th day after the production of PCCP. Then an Attention-ResNet-AR-LSTM model is constructed to predict the strain development of PCCP steel cylinder and concrete based on monitoring data on the 20th day using the data of 1st to 11th days. For steel cylinder and concrete structures, the R2 of the proposed models is above 98%, indicating that the proposed models can capture the strain changes of different structures well. Compared with other deep models and machine learning models, the proposed model also shows a significant predictive ability. This model introduces a novel approach to forecast strain evolution during the manufacturing of PCCP.}
}
@article{RAMALHINHO2023102943,
title = {The value of Augmented Reality in surgery — A usability study on laparoscopic liver surgery},
journal = {Medical Image Analysis},
volume = {90},
pages = {102943},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102943},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002037},
author = {João Ramalhinho and Soojeong Yoo and Thomas Dowrick and Bongjin Koo and Murali Somasundaram and Kurinchi Gurusamy and David J. Hawkes and Brian Davidson and Ann Blandford and Matthew J. Clarkson},
keywords = {Augmented reality, Image-guided surgery, Usability, Laparoscopic surgery},
abstract = {Augmented Reality (AR) is considered to be a promising technology for the guidance of laparoscopic liver surgery. By overlaying pre-operative 3D information of the liver and internal blood vessels on the laparoscopic view, surgeons can better understand the location of critical structures. In an effort to enable AR, several authors have focused on the development of methods to obtain an accurate alignment between the laparoscopic video image and the pre-operative 3D data of the liver, without assessing the benefit that the resulting overlay can provide during surgery. In this paper, we present a study that aims to assess quantitatively and qualitatively the value of an AR overlay in laparoscopic surgery during a simulated surgical task on a phantom setup. We design a study where participants are asked to physically localise pre-operative tumours in a liver phantom using three image guidance conditions — a baseline condition without any image guidance, a condition where the 3D surfaces of the liver are aligned to the video and displayed on a black background, and a condition where video see-through AR is displayed on the laparoscopic video. Using data collected from a cohort of 24 participants which include 12 surgeons, we observe that compared to the baseline, AR decreases the median localisation error of surgeons on non-peripheral targets from 25.8 mm to 9.2 mm. Using subjective feedback, we also identify that AR introduces usability improvements in the surgical task and increases the perceived confidence of the users. Between the two tested displays, the majority of participants preferred to use the AR overlay instead of navigated view of the 3D surfaces on a separate screen. We conclude that AR has the potential to improve performance and decision making in laparoscopic surgery, and that improvements in overlay alignment accuracy and depth perception should be pursued in the future.}
}
@article{KARAYEGEN2021102458,
title = {Brain tumor prediction on MR images with semantic segmentation by using deep learning network and 3D imaging of tumor region},
journal = {Biomedical Signal Processing and Control},
volume = {66},
pages = {102458},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102458},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421000550},
author = {Gökay Karayegen and Mehmet Feyzi Aksahin},
keywords = {Semantic segmentation, Deep learning network, 3D imaging},
abstract = {When it comes to medical image segmentation on brain MR images, using deep learning techniques has a significant impact to predict tumor existence. Manual segmentation of a brain tumor is a time-consuming task and depends on knowledge and experience of physicians. In this paper, we present a semantic segmentation method by utilizing convolutional neural network to automatically segment brain tumor on 3D Brain Tumor Segmentation (BraTS) image data sets that comprise four different imaging modalities (T1, T1C, T2 and Flair). In addition, our study includes 3D imaging of whole brain and comparison between ground truth and predicted labels in 3D. In order to obtain exact tumor region and dimensions such as height, width and depth, this method was successfully applied and images were displayed different planes including sagittal, coronal and axial. Evaluation results of semantic segmentation which was executed by a deep learning network are significantly promising in terms of tumor prediction. Mean prediction ratio was determined as 91.718. Mean IoU (Intersection over Union) and Mean BF score were calculated as 86.946 and 92.938, respectively. Finally, dice scores of the test images were showed significant similarity between ground truth and predicted labels. As a result, both semantic segmentation metrics and 3D imaging can be interpreted as meaningful for diagnosing brain tumor accurately.}
}
@article{MASAD2024107842,
title = {CT-based generation of synthetic-pseudo MR images with different weightings for human knee},
journal = {Computers in Biology and Medicine},
volume = {169},
pages = {107842},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107842},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523013070},
author = {Ihssan S. Masad and Isam F. Abu-Qasmieh and Hiam H. Al-Quran and Khaled Z. Alawneh and Khalid M. Abdalla and Ali M. Al-Qudah},
keywords = {Synthetic MRI, Computed tomography, Spin echo, Human knee},
abstract = {Synthetic MR images are generated for their high soft-tissue contrast avoiding the discomfort by the long acquisition time and placing claustrophobic patients in the MR scanner's confined space. The aim of this study is to generate synthetic pseudo-MR images from a real CT image for the knee region in vivo. 19 healthy subjects were scanned for model training, while 13 other healthy subjects were imaged for testing. The approach used in this work is novel such that the registration was performed between the MR and CT images, and the femur bone, patella, and the surrounding soft tissue were segmented on the CT image. The tissue type was mapped to its corresponding mean and standard deviation values of the CT# of a window moving on each pixel in the reconstructed CT images, which enabled the remapping of the tissue to its MRI intrinsic parameters: T1, T2, and proton density (ρ). To generate the synthetic MR image of a knee slice, a classic spin-echo sequence was simulated using proper intrinsic and contrast parameters. Results showed that the synthetic MR images were comparable to the real images acquired with the same TE and TR values, and the average slope between them (for all knee segments) was 0.98, while the average percentage root mean square difference (PRD) was 25.7%. In conclusion, this study has shown the feasibility and validity of accurately generating synthetic MR images of the knee region in vivo with different weightings from a single real CT image.}
}
@article{KOCH201418,
title = {Natural markers for augmented reality-based indoor navigation and facility maintenance},
journal = {Automation in Construction},
volume = {48},
pages = {18-30},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514001885},
author = {Christian Koch and Matthias Neges and Markus König and Michael Abramovici},
keywords = {Augmented reality, Facility maintenance, Natural markers, Performance study},
abstract = {The longest phase in a facility's lifecycle is its maintenance period, during which operators perform activities to provide a comfortable living and working environment as well as to upkeep equipment to prevent functional failures. In current practice operators need a considerable amount of time to manually process dispersed and unformatted facility information to perform an actual task. Existing research approaches rely on expensive hardware infrastructure or use artificial, thus unesthetic Augmented Reality (AR) markers. In this paper we present a natural marker based AR framework that can digitally support facility maintenance (FM) operators when navigating to the FM item of interest and when actually performing the maintenance and repair actions. Marker detection performance experiments and case studies on our university campus indicate the feasibility and potential of natural markers for AR-based maintenance support.}
}
@article{HEEMSKERK2023113778,
title = {Using modern virtual reality techniques to perform analysis of ITER ECH EL port cell maintenance},
journal = {Fusion Engineering and Design},
volume = {191},
pages = {113778},
year = {2023},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2023.113778},
url = {https://www.sciencedirect.com/science/article/pii/S0920379623003605},
author = {Cock Heemskerk and Jelle Hofland and Jonathan Peres and David Bult and Ken Kajiwara and Noriyuki Kobayahi and Satoru Yajima and Akihiro Ide and Toshimichi Omori},
keywords = {ITER, ECH, Port cell, Maintenance, Virtual reality, ALARA},
abstract = {In order to maintain an ITER Port Plug after failure, the Port Cell needs to be cleared to allow for a transport cask to access the Port Plug and transfer it for maintenance to the Hot Cell Complex. The Port Cell clearing process is a complex, manual and machine assisted procedure, taking out the equipment blocking cask access in several stages. For the Equatorial Launcher (EL) of the Electron Cyclotron Heating (ECH) system, which is located in Equatorial Port Cell #14 of the ITER tokamak, the Port Cell clearing operation includes the removal of 24 Ex-vessel Waveguides, a complex set of CCWS (component cooling) and PHTS (primary cooling) lines, several manifolds, and other ancillaries, like the Service Vacuum System (SVS) to monitor the torus vacuum tightness. This paper shows how modern Virtual Reality (VR) simulation techniques, in combination with a state of the art, high-end game-PC environment and Head Mounted Display, can be used as a tool to visualize and analyse the maintenance procedures, to predict the time to repair, and to assist in minimizing radiation exposure to take in to consideration the ALARA principles. Special emphasis is put on the verification and validation of manual and machine-assisted maintenance procedures and the identification of tooling requirements. The possibility to simulate logistics and repair actions in an early stage of the design process allows for the identification of those maintenance actions that require dedicated tests or the development of dedicated tools. The added value of using modern VR techniques to analyse the maintenance scenarios are:•Immersive & intuitive – very little training required to experience the process first-hand•Quick and accurate assessment of maintenance scenarios, based on the actual CATIA models•The ability to assess in detail geometrically very complex maintenance sequencing•Less need for expensive hardware mock ups}
}
@article{FIORENTINO2014270,
title = {Augmented reality on large screen for interactive maintenance instructions},
journal = {Computers in Industry},
volume = {65},
number = {2},
pages = {270-278},
year = {2014},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513002340},
author = {Michele Fiorentino and Antonio E. Uva and Michele Gattullo and Saverio Debernardis and Giuseppe Monno},
keywords = {Computer aided task guidance, Augmented reality, Large screen instruction, Maintenance},
abstract = {We present an empirical study that evaluates the effectiveness of technical maintenance assisted with interactive augmented reality instructions. Our approach consists in an augmented visualization on a large screen and a combination of multiple fixed and mobile cameras. We used commercially available solutions. In our test, 14 participants completed a set of 4 maintenance tasks based on manual inspections of a motorbike engine. Tool selection, removal of bolts, and part dis\assembly, are supported by visual labels, 3D virtual models and 3D animations. All participants executed similar operations in two modalities: paper manuals and augmented instructions. Statistical analyses proved that augmented instructions reduced significantly participants’ overall execution time and error rate.}
}
@article{EHEMANN2023828,
title = {Digital Integration-Twins using Mixed Reality for smart Product Integration in the context of System of Systems},
journal = {Procedia CIRP},
volume = {119},
pages = {828-833},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.03.128},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123005425},
author = {Tobias Ehemann and Sven Forte and Damun Mollahassani and Jens C. Göbel},
keywords = {Smart Systems Engineering, Mixed Reality, Digital Twin, System of Systems},
abstract = {This contribution presents a concept for the integration of multi-disciplinary smart products into a comprehensive smart product ecosystem in order to fulfill an additional product-overreaching function or service. One of the key elements of the proposed concept is the use of specific digital product twins from the existing smart product ecosystem in operation. These “digital integration-twins" of the smart products in the product ecosystem are systematically created. They are described by a subset of the respective digital product twins, with a specific focus on product integration. These digital integration-twins contain integration-relevant processes, methods, and product data (e.g. partial product models, and usage data in the information technology sense) which are managed and distributed by a comprehensive system of the system integration platform. The current use of the new smart product is virtually visualized, integrated, and implemented within the real product integration context and the existing (physically operating) smart product ecosystem supported by immersive mixed reality techniques. This enables fast, easy and goal-oriented development of cross-product functionality in a hybrid way (mixture of physical and virtual products) already in the virtual product development phase (e.g. virtual user-centered validation of functional interaction in interactive sprints) and iterative optimization. Finally, the presented concept is implemented for a real existing use case.}
}
@article{LIN2018130,
title = {Integrated BIM, game engine and VR technologies for healthcare design: A case study in cancer hospital},
journal = {Advanced Engineering Informatics},
volume = {36},
pages = {130-145},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617303373},
author = {Yu-Cheng Lin and Yen-Pei Chen and Huey-Wen Yien and Chao-Yung Huang and Yu-Chih Su},
keywords = {Building information modeling, BIM, Healthcare design, Virtual reality, Game engine, Design phase, Communication, Semi-immersed VR},
abstract = {The results of healthcare design should meet the requirements of design teams as well as healthcare stakeholders. However, misunderstandings that occur between the design teams and healthcare stakeholders when using 2D illustrations leads to the need for re-design and rework during the design phase. To overcome this problem, this study develops a Database-supported VR/BIM-based Communication and Simulation (DVBCS) system integrated with BIM, game engine and VR technologies for healthcare design special in the Semi-immersed VR environment. The DVBCS system is applied in a case study of a design project of a cancer center in Taiwan to verify the system and demonstrate its effectiveness in practice. The results demonstrate that a DVBCS system is an effective visual communication and simulation platform for healthcare design. The advantage of the DVBCS system lies not only in improving the communication efficiency between the design teams and healthcare stakeholders, but also in facilitating visual interactions and easing the decision-making process while communicating in the 3D VR/BIM environment. The effective use of the proposed DVBCS system will assist design teams and stakeholders significantly in systematically handling healthcare design work in future healthcare design.}
}
@article{CHEN2020103041,
title = {BIM-based augmented reality inspection and maintenance of fire safety equipment},
journal = {Automation in Construction},
volume = {110},
pages = {103041},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103041},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518309956},
author = {Yi-Jao Chen and Yong-Shan Lai and Yen-Han Lin},
keywords = {Augmented reality, Building information model, BIM, Fire safety equipment inspection and management, Mobil application},
abstract = {The purpose of fire safety equipment (FSE) inspection and maintenance is to ensure that this equipment is in good working condition during emergencies, so that fire damage could be kept to a minimum. At present, the maintenance and inspection of FSE must be performed according to the standards and methods specified by fire safety regulations, which makes it necessary to consult relevant files and drawings. However, the reading of maintenance and inspection information can be a very time-consuming endeavor as the majority of these files are in paper. To solve the aforementioned issues, building information modeling (BIM) was used in this study to construct the FSE elements so that the required information could be rapidly acquired by FSE inspectors. A cloud database for equipment inspection and maintenance was then generated by organizing the compiled information. This database was combined with the augmented reality (AR) technology to facilitate FSE inspection and maintenance using mobile devices, thus overcoming the constraints imposed by paper files on these tasks. The results of the demonstration and validation shown that the proposed BIM AR FSE system provides highly comprehensive, mobile, and effective access to FSE information. The combination of information and real-life objects via AR effectively facilitated the presentation of information in an immediate, visual, and convenient manner.}
}
@article{CHEN2024114991,
title = {An augmented reality surgical navigation system based on co-axial projection of surgical paths for open liver tumor surgery},
journal = {Measurement},
volume = {235},
pages = {114991},
year = {2024},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.114991},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124008765},
author = {Long Chen and Tong Qiu and Li Ma and Wei Zhan and Yu Zhang and Lining Sun},
keywords = {Augmented reality, Surgical navigation, Projection, Image fusion, Surgical paths, Liver tumors},
abstract = {The traditional surgical navigation system for open liver tumor resection has some problems, such as low tumor localization accuracy, high complexity of system operation, and heavy surgical burden on surgeons. To solve these problems, this paper proposes an augmented reality surgical navigation system based on coaxial projection of surgical paths. Firstly, the intraoperative tumor localization module is used to locate the tumor. An augmented reality coaxial projection module for surgical path projection has been developed. In addition, a unified view of the camera and projector in the module is designed. Secondly, we propose an algorithm for image registration and fusion. The actual scene image of the liver captured by the augmented reality coaxial projection module is registered and fused with the model image to determine the tumor’s location in the surgical image. Then, surgical path planning is carried out for the tumors in the surgical images. An augmented reality coaxial projection module projects the surgical path onto the surface of the liver to assist surgeons in tumor resection. Finally, we validate the performance of our proposed surgical navigation system through multiple sets of experiments on tumor resection. The experimental results show that the average error and time of tumor resection based on this system are 2.95 mm and 59 s, respectively. It can be seen that the system proposed in this study achieved better results under laboratory conditions. The development of this system lays the foundation for our future research in clinical surgical navigation systems.}
}
@article{YOO2024103881,
title = {Can engineers represent surgeons in usability studies? Comparison of results from evaluating augmented reality guidance for laparoscopic surgery},
journal = {Computers & Graphics},
volume = {119},
pages = {103881},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000086},
author = {Soojeong Yoo and João Ramalhinho and Thomas Dowrick and Murali Somasundaram and Kurinchi Gurusamy and Brian Davidson and Matthew J. Clarkson and Ann Blandford},
keywords = {Laparoscopic surgery, Augmented reality, Operating theatre, Usability study},
abstract = {Obtaining feedback from time-constrained end-users is a major challenge in evaluating novel systems for specialised applications. The performance and feedback of engineers and surgeons was evaluated through an experiment where participants were asked to identify tumour locations within an anatomically realistic silicon liver model across three different conditions of an Augmented Reality (AR) prototype system (Baseline, Split AR and Full AR). Our findings show that engineers and surgeons share some similarities in their performance, feedback and behaviour, particularly when reliance on the AR system is high for both groups. However, engineers typically focus more on accuracy of the image alignment and are more accurate in their responses when supported by AR. Senior surgeons typically perform faster and use AR as supplementary information, while the performance of junior surgeons is more closely aligned to the performance of engineers. We conclude that engineers could be involved in preliminary evaluations of a surgical system or in evaluations of systems which are aimed at training junior surgeons, but that it is essential to involve surgeons in later evaluations, where ecological validity is a more important consideration.}
}
@article{MOURTZIS2020977,
title = {A Framework for Automatic Generation of Augmented Reality Maintenance & Repair Instructions based on Convolutional Neural Networks},
journal = {Procedia CIRP},
volume = {93},
pages = {977-982},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.130},
url = {https://www.sciencedirect.com/science/article/pii/S221282712030771X},
author = {Dimitris Mourtzis and John Angelopoulos and Nikolaos Panopoulos},
keywords = {Augmented Reality, Convolutional Neural Networks, Maintenance, Repair Operations},
abstract = {In modern manufacturing world, MRO (Maintenance and Repair Operations) are the cornerstone for keeping industrial equipment in near-optimum condition. Successful completion of MRO has been benefited from Augmented Reality (AR), by considerably decreasing MTTR (Mean Time to Repair). To that end, AR delivers the digital tools that help on-the-field technicians to perform MRO easily and intuitively, however intense development is required for generating AR instructions. The latest advances in computer technologies, concretely in Convolutional Neural Networks, have enabled advanced computer vision. This research paper presents a framework for generating AR maintenance instructions, based on advanced computer vision and Convolutional Neural Networks (CNN). The applicability of the framework is tested in-vitro in a lab-based machine shop.}
}
@article{STOLTZ201712979,
title = {Augmented Reality in Warehouse Operations: Opportunities and Barriers},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {12979-12984},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1807},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317324291},
author = {Marie-Hélène Stoltz and Vaggelis Giannikas and Duncan McFarlane and James Strachan and Jumyung Um and Rengarajan Srinivasan},
keywords = {augmented reality, warehousing, cyber-physical systems, wearables},
abstract = {In today’s business environment, the efficiency of warehouses can be critical for the efficiency of the overall supply chains they belong to. As a result, new technologies are being tested and adopted in industry to improve the performance of warehouse operations. An example technology that has recently gained interest by both academia and industry is augmented reality. In this paper, we investigate the opportunities arising from the usage of augmented reality in warehouses as well as the barriers for its industrial adoption. This is done via a series of practitioners interviews and via an experiment designed using Google Glass. Our results indicate that even though the technology is not mature enough at the moment, the potential benefits it can offer make it promising for the near future.}
}
@article{M2024104,
title = {Augmented reality guided autonomous assembly system: A novel framework for assembly sequence input validations and creation of virtual content for AR instructions development},
journal = {Journal of Manufacturing Systems},
volume = {72},
pages = {104-121},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523002297},
author = {Eswaran M. and V.S.S. Vara Prasad and M. Hymavathi and M.V.A. Raju Bahubalendruni},
keywords = {Augmented reality, Assembly sequence, Virtual reality, Assembly input validation, Virtual scene creation, Manufacturing system, Assembly system},
abstract = {In recent years, industries are striving to adapt immersive technologies in the real-time assembly shop floor setting to achieve effective outcomes in terms of enabling the worker to handle the complex cognitive assembly task execution/training circumstances. The manufacturers are presently facing difficulties with the process of Augmented reality (AR) instruction creation for the required product assembly system. The existing AR instruction development process demands highly skilled experts and more time consumption. Hence, the manufacturers are presently relying on third parties to acquire the required AR instructions by providing the necessary assembly particulars as input. Where, the cost of developing AR instruction is another big challenging concern. Predominantly, the significance of providing exact assembly input is vital for the experts to develop the appropriate AR content without any errors. An automated approach is developed in this study to resolve the difficulties with assembly sequence input for the AR instructions development process. The automated approach affords four different provisions that include validation of an assembly sequence input prior to the AR instructions development stage, suggestions for assembly sequence input generation, simulation of validated assembly sequence in the Virtual reality (VR) environment for user perception, and generation of virtual content for AR visualizations. Subsequently, the generated digital contents from the automated approach are imparted into the AR Unity platform to develop the required AR instructions for the product assembly system. Notably, this study explores the AR customized marker technique to visualize the AR content in the user’s view of the physical context. Finally, the AR guidelines are built as apps for the selected 11-part assembly and deployed on the shop floor for the user to perceive the AR content through visualization devices for execution of assembly operations in a real-time context. In addition, the effectiveness of the automated approach is analyzed with the existing approach, and the consequences are explored through the process of receiving feedback from the experts about the error rate, cognitive level, and creation time.}
}
@article{FONSECA2014434,
title = {Relationship between student profile, tool use, participation, and academic performance with the use of Augmented Reality technology for visualized architecture models},
journal = {Computers in Human Behavior},
volume = {31},
pages = {434-445},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213000952},
author = {David Fonseca and Nuria Martí and Ernesto Redondo and Isidro Navarro and Albert Sánchez},
keywords = {Augmented Reality, Technology acceptance, Academic motivation, Architecture visualization, Mobile learning, User experience},
abstract = {In this study, we describe the implementation and evaluation of an experiment with Augmented Reality (AR) technology in the visualization of 3D models and the presentation of architectural projects by students of architecture and building engineering. The proposal is based on the premise that the technology used in AR, such as mobile devices, is familiar to the student. When used in a collaborative manner, the technology is able to achieve a greater level of direct engagement with the proposed content, thereby improving academic outcomes. The objective was to assess the feasibility of using AR on mobile devices in educational environments and to investigate the relationship between the usability of the tool, student participation, and the improvement in academic performance after using AR. The validation was performed through a case study in which students were able to experience a virtual construction process overlapped onto real environments. Results were obtained by students’ pre-tests and post-tests. In line with our assumptions, the use of mobile devices in the classroom is highly correlated with motivation, and there is a significant correlation with academic achievement. However, the difficulty of using and generating content is a complex factor that suggests difficulty when implementing more complicated models.}
}
@article{VITTORI2021102368,
title = {Subjective thermal response driving indoor comfort perception: A novel experimental analysis coupling building information modelling and virtual reality},
journal = {Journal of Building Engineering},
volume = {41},
pages = {102368},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.102368},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221002242},
author = {Filippo Vittori and Ilaria Pigliautile and Anna Laura Pisello},
keywords = {Max 6): indoor environmental quality, Energy efficiency, Virtual reality, Occupant behavior, Indoor comfort, Human-centric design},
abstract = {Building sector professionals, together with building physicists and human scientists, recognize the occupants' key role in determining final buildings energy needs. Occupants’ behavior is strongly influenced by their perception and represents a major variable affecting buildings' energy performance, but its impact is difficult to predict since the early design stage. This study proposes a new analysis framework and field test method to better comprehend and monitor people environmental perception and attitudes since the design stage, while stimulated through immersive environment modeled in a parametric platform, able to change every specific aspect of the future spaces. To this aim, a physical office room was re-designed through virtual reality (VR) and sense-of-presence feeling was registered, with a 76% of satisfaction rate. The VR designed environment was therefore used to reproduce key design variables without modifying the indoor thermal-energy environment during the experiment. Three triggers were simulated, i.e. glass filter, window aspect ratio, and artificial lighting color temperature. Both these two latter variables significantly produced placebo effects on most of the interviewed people, who declared to feel relatively hotter in high aspect ratio window conditions and low color temperature. As expected, non-thermal triggers may affect indoor thermal perception and, potentially, may induce better thermal satisfaction and energy saving behaviors, if properly considered. The proposed novel method can assist building design since the early stages, leading to low-cost and human-centric energy efficiency enhancements.}
}
@article{RATAVA2019497,
title = {Quality assurance and process control in virtual reality},
journal = {Procedia Manufacturing},
volume = {38},
pages = {497-504},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.063},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300640},
author = {J. Ratava and S. Penttilä and H. Lund and M. Lohtander and P. Kah and M. Ollikainen and J. Varis},
keywords = {virtual reality, cyber-physical systems, robotics, flexible manufacturing unit},
abstract = {In this study, quality assurance and automatic process control are combined with a Virtual Reality enabled man-in-the-loop system. Increasingly flexible manufacturing demands increasingly flexible process control. Autonomous smart manufacturing systems increase the amount of tolerable variation in manufacturing processes by reacting to minor changes in part positioning, preparation, and other issues by adaptive process parameters. Due to unforeseen circumstances or high variation, there may be workpieces requiring human interaction to continue manufacturing, repair, or be sent to recycling or even waste. Human-robot collaboration may be achieved by safety systems allowing the human operator within the working area of a robot, or then augmented or virtual reality may allow a human operator to assess the situation remotely. The AR and VR technologies may include information usually invisible to the operator. In this study, a 3D workpiece is measured, and defects are visualized in a VR environment, focusing on the geometrical distortion of the workpiece. Measurement had a standard deviation of +/- 0.067 mm for the root gap. Visualization facilitates rapid human feedback in case distortion is getting out of bounds. A man-in-the-loop system is used to provide adjustments to the robot parameters.}
}
@article{POP2012505,
title = {Construction of 3D MR image-based computer models of pathologic hearts, augmented with histology and optical fluorescence imaging to characterize action potential propagation},
journal = {Medical Image Analysis},
volume = {16},
number = {2},
pages = {505-523},
year = {2012},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1361841511001691},
author = {Mihaela Pop and Maxime Sermesant and Garry Liu and Jatin Relan and Tommaso Mansi and Alan Soong and Jean-Marc Peyrat and Michael V. Truong and Paul Fefer and Elliot R. McVeigh and Herve Delingette and Alexander J. Dick and Nicholas Ayache and Graham A. Wright},
keywords = {Cardiac computer models, MRI, Optical imaging, Electrophysiology},
abstract = {Cardiac computer models can help us understand and predict the propagation of excitation waves (i.e., action potential, AP) in healthy and pathologic hearts. Our broad aim is to develop accurate 3D MR image-based computer models of electrophysiology in large hearts (translatable to clinical applications) and to validate them experimentally. The specific goals of this paper were to match models with maps of the propagation of optical AP on the epicardial surface using large porcine hearts with scars, estimating several parameters relevant to macroscopic reaction–diffusion electrophysiological models. We used voltage-sensitive dyes to image AP in large porcine hearts with scars (three specimens had chronic myocardial infarct, and three had radiofrequency RF acute scars). We first analyzed the main AP waves’ characteristics: duration (APD) and propagation under controlled pacing locations and frequencies as recorded from 2D optical images. We further built 3D MR image-based computer models that have information derived from the optical measures, as well as morphologic MRI data (i.e., myocardial anatomy, fiber directions and scar definition). The scar morphology from MR images was validated against corresponding whole-mount histology. We also compared the measured 3D isochronal maps of depolarization to simulated isochrones (the latter replicating precisely the experimental conditions), performing model customization and 3D volumetric adjustments of the local conductivity. Our results demonstrated that mean APD in the border zone (BZ) of the infarct scars was reduced by ∼13% (compared to ∼318ms measured in normal zone, NZ), but APD did not change significantly in the thin BZ of the ablation scars. A generic value for velocity ratio (1:2.7) in healthy myocardial tissue was derived from measured values of transverse and longitudinal conduction velocities relative to fibers direction (22cm/s and 60cm/s, respectively). The model customization and 3D volumetric adjustment reduced the differences between measurements and simulations; for example, from one pacing location, the adjustment reduced the absolute error in local depolarization times by a factor of 5 (i.e., from 58ms to 11ms) in the infarcted heart, and by a factor of 6 (i.e., from 60ms to 9ms) in the heart with the RF scar. Moreover, the sensitivity of adjusted conductivity maps to different pacing locations was tested, and the errors in activation times were found to be of approximately 10–12ms independent of pacing location used to adjust model parameters, suggesting that any location can be used for model predictions.}
}
@article{TEJASHWINI2024106312,
title = {EBT Deep Net: Ensemble brain tumor Deep Net for multi-classification of brain tumor in MR images},
journal = {Biomedical Signal Processing and Control},
volume = {95},
pages = {106312},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106312},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424003707},
author = {P.S. Tejashwini and J. Thriveni and K.R. Venugopal},
keywords = {Brain Tumors, Deep learning, ML, CNN, ResNet50V2, DenseNet201, MobileNetV3, VGG19},
abstract = {The classification of brain tumors (BTs) is an expensive intricate challenge in the section of clinical image analysis. Radiologists were able to reliably detect tumors using machine learning (ML) algorithms without the need for extensive surgery. Hand-crafted features are necessary for the conventional ML classifiers, which take a lot of effort. Deep Learning (DL), on the other hand, has gained popularity recently for identification and categorization-based functionalities because of its effective feature extraction capabilities. Creating the most precise DL architecture for classifying BTs is a huge task. These challenges inspired us to develop a cutting-edge and highly accurate framework based on DL and computational methods that evolve to effectively create the hybrid framework automatically for categorizing three distinct types of brain cancers on a vast library of MR images. We present a unique EBT Deep Net DL model that uses a substantial convolutional neural network (CNN) technique for the classification of three distinct forms of brain cancers: gliomas, tumors in brain meningeal tissue, and tumors in pituitary glands. Pretrained deep CNN models like ResNet50V2, ResNet152, DenseNet201, MobileNetV3, and VGG19 were used to extract deep features. The efficacy of this paper is examined using classification accuracy. In comparison to the most recent classification findings from ResNet50V2, ResNet152, DenseNet201, MobileNetV3, and VGG19, the proposed methodology had the greatest accuracy. The models' respective accuracy on a classification test is as follows: VGG19 scored 97.46 %, ResNet152 scored 98 %, DenseNet201 scored 95.44 %, ResNet50V2 scored 90.04 %, MobileNetV3 scored 97.15 %, and the ensemble model outperformed all of them with an accuracy of 98.8 %. The suggested model demonstrated its advantage over the currently used approaches for classifying BTs from MR images.}
}
@article{YEOM2023113380,
title = {Analyzing the optimal visible light transmittance of thin-film photovoltaic using experiment with virtual reality and economic assessment},
journal = {Energy and Buildings},
volume = {296},
pages = {113380},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.113380},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823006102},
author = {Seungkeun Yeom and Jongbaek An and Hyuna Kang and Taehoon Hong},
keywords = {Thin-film Photovoltaic, Visible light transmittance, Economic performance, Psychological satisfaction, Cadmium-Telluride, Physiological response},
abstract = {Despite the recent growth in installing photovoltaic (PV) systems in buildings, densely populated cities with high-rise buildings have limited space for rooftop PV systems, leading to semi-transparent PV systems being considered an alternative. However, thin-film PV can reduce visibility from windows, potentially negatively impacting satisfaction and visual comfort of occupants. Therefore, this study aims to experimentally verify the efficiency of cadmium-telluride (CdTe) thin-film solar cells with different visible light transmittance (VLT) values and find the optimal VLT of thin-film PV that can be applied to building windows. To this end, this study measured the psychological and physiological responses of occupants through VR experiments and analyzed the energy performance and economic feasibility using energy simulation tools and life cycle cost analysis. Consequently, the optimal VLT varied between 15.8% and 22.6% based on factors such as psychological satisfaction and economic performance. Additionally, the physiological response demonstrated that the level of psychological satisfaction with the implemented changes was influenced by the VLT of thin-film PV. Hence, this study recommends guidelines for VLT when developing thin-film PV, which can contribute to determining the optimal VLT of thin-film PV, considering both psychological satisfaction of occupants and economic performance targeted by potential decision-makers.}
}
@article{VELUPPAL2022340,
title = {Detection of Mild Cognitive Impairment Using Kernel Density Estimation Based Texture Analysis of the Corpus Callosum in Brain MR Images},
journal = {IRBM},
volume = {43},
number = {5},
pages = {340-348},
year = {2022},
issn = {1959-0318},
doi = {https://doi.org/10.1016/j.irbm.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1959031821000968},
author = {A. Veluppal and D. Sadhukhan and V. Gopinath and R. Swaminathan},
keywords = {Mild cognitive impairment, Alzheimer's disease, Corpus callosum, Magnetic resonance imaging, Texture analysis, Kernel density estimation},
abstract = {Objectives
Mild Cognitive Impairment (MCI) is the prodromal stage of Alzheimer's disease (AD), which is a progressive and fatal neurodegenerative disorder. Detection of MCI condition can enable early diagnosis resulting in timely intervention to delay the disease progression. Onset of MCI causes tissue alterations in Corpus Callosum (CC) of the brain. Texture analysis of brain Magnetic Resonance (MR) images aids in characterising these imperceptible changes. In this study, Kernel Density Estimation (KDE) technique is used to analyse the textural variations in CC to detect MCI condition.
Materials and method
The pre-processed brain MR images are obtained from a public access database. Reaction Diffusion level set is employed to segment CC from sagittal slices of the images. Kernel density estimation method is applied to study the local intensity variations within the segmented CC. Statistical features quantifying these variations are extracted from the KDE values. These features are used to differentiate MCI condition using linear classifiers based on discriminant analysis and support vector machine. The results are compared with conventional Grey Level Co-occurrence Matrix (GLCM) features for validation.
Results
The KDE-based texture features extracted from CC show significant variation between normal and MCI classes. Results demonstrate that this approach can differentiate MCI condition with high accuracy and specificity of 81.3% and 82.7%, respectively. The KDE-based features perform better when compared with GLCM features for distinguishing MCI.
Conclusions
The KDE-based texture features are able to capture the subtle changes occurring in CC at the MCI stage. This technique achieves comparable performance to other state-of-the-art methods with reduced number of features. Efficiency of the KDE-based texture analysis confirms that the proposed computer assisted technique can be used for mass screening of MCI, which can aid in handling the disease severity.}
}
@article{PAYEDARARDAKANI2024111638,
title = {Daylight illuminance levels, user preferences, and cognitive performance in office environments: Exploring an optimal illuminance range using virtual reality},
journal = {Building and Environment},
volume = {258},
pages = {111638},
year = {2024},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.111638},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324004803},
author = {Pegah Payedar-Ardakani and Yousef Gorji-Mahlabani and Abdul Hamid Ghanbaran and Reza Ebrahimpour},
keywords = {Daylight illuminance, Office environment, Virtual Reality (VR), Lighting preferences, Cognitive performance, Stroop test},
abstract = {This study investigates the impact of varied daylight illuminance levels on user preferences and cognitive performance in offices, employing a virtual reality platform and HDRI 360-degree panorama images, whose illuminance level was validated using simulation. With 46 participants, a cognitive task known as the Stroop-test was conducted under nine illuminance levels, ranging up to 1500 lux. Additionally, participants were surveyed to determine their preferred horizontal illuminance level at desk height. The results uncovered distinct user preferences, with the majority of participants favoring illuminance levels above 700 lux, specifically 1100 and 790 lux, for reading and work-related tasks. Notably, none of the participants opted for illuminance levels below 300 lux, indicating these levels were deemed insufficient for their tasks. An analysis of cognitive task performance revealed significant differences between various illuminance levels. Generally, increasing the illuminance level in an office building will increase the office workers’ task performance. As illuminance levels exceeded 300 lux, participants exhibited enhanced performance in tasks such as reading words (RW), naming colors (NC), and total Stroop-test performance (TT). The 700–1500 lux can be considered suitable illuminance level for high-precision tasks, and 300–700 lux can be considered the medium illuminance in office environments. Based on these findings, an optimal illuminance range of 900–1100 lux is recommended for office environments, aligning with both user preferences and performance. This study offers valuable insights for architects and researchers in the development of daylighting design guidelines aimed at enhancing employees' cognitive capabilities and satisfaction.}
}
@article{DANGELMAIER2005371,
title = {Virtual and augmented reality support for discrete manufacturing system simulation},
journal = {Computers in Industry},
volume = {56},
number = {4},
pages = {371-383},
year = {2005},
note = {The Digital Factory: An Instrument of the Present and the Future},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2005.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361505000266},
author = {Wilhelm Dangelmaier and Matthias Fischer and Jürgen Gausemeier and Michael Grafe and Carsten Matysczok and Bengt Mueck},
keywords = {Virtual reality, Augmented reality, Production planning, Simulation},
abstract = {Nowadays companies operate in a difficult environment: the dynamics of innovations increase and product life cycles become shorter. Furthermore products and the corresponding manufacturing processes get more and more complex. Therefore, companies need new methods for the planning of manufacturing systems. One promising approach in this context is digital factory/virtual production—the modeling and analysis of computer models of the planned factory with the objective to reduce time and costs. For the modeling and analysis various simulation methods and programs have been developed. They are a highly valuable support for planning and visualizing the manufacturing system. But there is one major disadvantage: only experienced and long trained experts are able to operate with these programs. The graphical user interface is very complex and not intuitive to use. This results in an extensive and error-prone modeling of complex simulation models and a time-consuming interpretation of the simulation results. To overcome these weak points, intuitive and understandable man–machine interfaces like augmented and virtual reality can be used. This paper describes the architecture of a system which uses the technologies of augmented and virtual reality to support the planning process of complex manufacturing systems. The proposed system assists the user in modeling, the validation of the simulation model, and the subsequent optimization of the production system. A general application of the VR- and AR-technologies and of the simulation is realized by the development of appropriate linking and integration mechanisms. For the visualization of the arising 3D-data within the VR- and AR-environments, a dedicated 3D-rendering library is used.}
}
@article{JUNG2012131,
title = {Efficient mobile AR technology using scalable recognition and tracking based on server-client model},
journal = {Computers & Graphics},
volume = {36},
number = {3},
pages = {131-139},
year = {2012},
note = {Novel Applications of VR},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2012.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0097849312000052},
author = {Jinki Jung and Jaewon Ha and Sang-Wook Lee and Francisco A. Rojas and Hyun S. Yang},
keywords = {Augmented Reality, Mobile devices, Scalable recognition, Server-client model, Optimization},
abstract = {Advancements in mobile devices and vision technology have enabled mobile Augmented Reality (AR) to be serviced in real-time using natural features. However, in viewing AR while moving around in the real world, users often encounter new and diverse target objects. Whether the AR system is scalable to the number of target objects is a very crucial issue for mobile AR services in the real world. This scalability, however, has been severely limited because of the small internal storage capacity and memory of the mobile devices. In this paper, a new framework is proposed that achieves scalability for mobile AR. The scalability is achieved with a bag-of-visual-words based recognition module on the server side that is connected to the clients, which are mobile devices, through a conventional Wi-Fi network. On the client side, the coarse-to-fine tracking module enables robust tracking performance with natural features in real-time. In this study, we optimized modules in mobile devices for expediting pose-tracking processing and simultaneously enabled 3D rendering and animation in real-time. We also propose an efficient recognition method in which metadata are provided by the sensors of mobile devices. In the experiment, it takes approximately 0.2s for the cold start of an AR service initiated on a 10K object database with a recognition accuracy of 99.87%, which should be acceptable for a variety of real-world mobile AR applications.}
}
@article{LI2020110259,
title = {Building environment information and human perceptual feedback collected through a combined virtual reality (VR) and electroencephalogram (EEG) method},
journal = {Energy and Buildings},
volume = {224},
pages = {110259},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110259},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820303248},
author = {Junjie Li and Yichun Jin and Shuai Lu and Wei Wu and Pengfei Wang},
keywords = {EEG, VR, Human perceptual feedback, Building spatial environment},
abstract = {In order to accurately and quantitatively describe the influence of a building’s spatial environment on subjective human perception, this research establishes a relationship framework between the spatial environment and subjective feelings, with the goal of improving occupant satisfaction and work efficiency. In pursuit of this goal, this study presents actual scenes through virtual space and introduces a new research concept for analyzing human brain data. The researchers adopted virtual reality technology in a controlled laboratory environment to compose building simulation spaces and created immersive space perceptions in response to different scenarios. Neural signal electroencephalogram (EEG) data were obtained in the simulation space from participants wearing EEG signal acquisition caps. The experiment process was divided into two phases: scene cognition and task performance. Changes in human perception, as measured on physiological, psychological, and work efficiency indexes, were examined in three environments: open natural, semi-open library, and closed basement spaces. Based on a 32-point analysis of the EEGs of 30 subjects, researchers determined four points and one region with the most significant EEG changes after scene switching. Also, by examining the EEG rhythms in the scene cognition experiment phase, the authors identified a coupling relationship between β rhythms and total time to task completion, proving the mechanistic relationship between β rhythms and work efficiency. Finally, this research revealed a correlation between subjective perception and physiological signals by analyzing the relevance of the connection between subjective questionnaire responses and the β rhythms demonstrated in the EEG experiment, and then deducing the mechanism affecting work efficiency as influenced by different environments. The results obtained show that in the context of changes to elements of a building’s spatial environment, human work efficiency is most related to the β rhythms at several test points and the right temporal lobe region of the brain. Moreover, β rhythms are closely related to satisfaction with human spatial perception. Therefore, this research provides a more accurate set of reference information for building space design based on occupant satisfaction and physical and mental health. The methods and conclusions demonstrated here can be adopted as feedback for ways of obtaining more realistic information from human brain signals and using those data to optimize architectural space design.}
}
@article{TOBISKOVA2023734,
title = {Augmented Reality for AI-driven Inspection? – A Comparative Usability Study},
journal = {Procedia CIRP},
volume = {119},
pages = {734-739},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.03.122},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123005929},
author = {Nicole Tobisková and Erik Sanderson Gull and Swathanandan Janardhanan and Thomas Pederson and Lennart Malmsköld},
keywords = {Usability Evaluation, Augmented Reality, Human-Computer Interaction, Automated Visual Inspection, Aerospace, Collaborative Automation},
abstract = {Inspection in Aerospace industry can, as well as many other industrial applications, benefit from using Augmented Reality (AR) due to its ability to superimpose helpful digital information in 3D, leading to fewer errors and decreased mental demand. However, each AR device has advantages and disadvantages, and not all AR devices are suitable for use in industrial settings. We compare a tripod-fitted-adjustable-arm tablet-based AR solution (Apple iPad Pro) to head-mounted AR (Microsoft HoloLens 2) and a traditional, computer screen-based human-machine interface (HMI), all three designed to guide operators based on previously performed AI-based image analysis. Following an iterative design process with three formative evaluations, a final field test in a real industrial shop floor engaging 6 professional inspectors revealed an overall preference for the tripod-fitted iPad variant which receiving the best scores in most dimensions covered in both a usability-focused SUS questionnaire (score 71) and a NASA-RTLX form focused on perceived workload. More specifically, the tripod-fitted iPad was considered more usable (SUS) than the classic computer display HMI (M=5.83, SD=4.92, p=0.034, N=6); the temporal demand (NASA-RTLX) was considered lower using the iPad compared to both HoloLens 2 and the HMI (M=6.67, SD=4.08, p=0.010; M=10.83, SD=9.70, p=0.040, N=6), respectively.}
}
@article{YE200677,
title = {An investigation into the implementation of virtual reality technologies in support of conceptual design},
journal = {Design Studies},
volume = {27},
number = {1},
pages = {77-97},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000347},
author = {J. Ye and R.I. Campbell and T. Page and K.S. Badni},
keywords = {conceptual design, interface design, virtual reality, computer aided design},
abstract = {Virtual reality (VR) technologies provide novel and enhanced modes of human computer interaction that can be used to overcome communication drawbacks that prevail in conventional computer aided design (CAD) systems in the support of conceptual design, principally in concept generation and evaluation. The work reported here focuses on how VR technologies can proffer more natural and intuitive interaction between the designer and the CAD system thus enabling a rapid and more straightforward approach to concept design creation and evaluation. As the result of this research, an innovative conceptual design system called the LUCID system has been developed. LUCID provides designers with a new interaction paradigm that enables them to take fuller advantage of their visual, auditory and tactile sensorial channels in order to create, view, touch, manipulate and listen to CAD digital models more effectively. The LUCID system has shown particular human computer interface (HCI) benefits to designers during conceptual design stages based on the outcome from a user evaluation test.}
}
@article{BARRY20231151,
title = {Creative teaching using hybrid e-learning and virtual reality},
journal = {Procedia Computer Science},
volume = {225},
pages = {1151-1160},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.103},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012619},
author = {Dana M. Barry and Hideyuki Kanematsu and Toshihiro Tanaka},
keywords = {Creative teaching, STEM, active student learning, hybrid e-learning, virtual reality, communication and problem solving skills},
abstract = {This paper describes three creative teaching activities that motivate and engage students into active learning. The teaching methods used include hybrid e-learning and virtual reality. The hybrid e-learning style is a combination of e-learning and a hands-on activity in a laboratory setting. For one hybrid e-learning project, students are asked to design, build, and test seawalls to help protect Japan from future tsunamis. In the other hybrid e-learning project, student teams are challenged to invent the best tasting nutritious fruit juice by using various fruits, blenders, water, plasticware, and other items. For the third project, participants use virtual reality (VR) headsets to learn about rollercoasters and to experience the thrill of riding on one.}
}
@article{MEUNIER20181337,
title = {Virtual Reality: Lessons learned from WEST design and perspectives for nuclear environment},
journal = {Fusion Engineering and Design},
volume = {136},
pages = {1337-1341},
year = {2018},
note = {Special Issue: Proceedings of the 13th International Symposium on Fusion Nuclear Technology (ISFNT-13)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920379618304319},
author = {Lionel Meunier and Delphine Keller and Pierre Guédon},
keywords = {WEST, Virtual Reality, Maintenance, Nuclear, Accessibility},
abstract = {From 2012–2016, the Tore Supra tokamak was upgraded in an x-point divertor device within the frame of the WEST project. A huge design activity was made to modify the whole configuration of Tore Supra including the in-vessel components. Some of the most noticeable changes are the addition of inner divertor coils inside the vacuum vessel and the replacement of all plasma facing components. Fitting all the new components in a previously existing environment was probably one of the major challenges of the project. To improve the assembly plan and the design of toolings, Virtual Reality (VR) was introduced in the design process. For complex assembly operations, simulations were developed for VR headsets from CAD models. From the requirements expressed by the design team, the VR engineers developed dedicated tools to address the specific issues. Engineers, designers and operators were then able to experience the assembly conditions in a 3D 360° real size immersive environment and interact with the models thanks to a large set of dedicated tools which are developed in the VR lab. This approach showed impressive results for issues such as accessibility, complex kinematics of large components and design of handling tools. It allowed reducing the number of mock-ups and helped the WEST assembly team to reduce the number of unforeseen assembly issues. It was for example used to simulate the assembly of the divertor: insertion through the ports, toroidal displacement on rails, and assembly with gaps lower than 1 mm. Virtual Reality will also provide innovative solutions to help designers in the anticipation of challenges linked to operation of a nuclear facility: operator’s training for nuclear maintenance, virtual mock-ups for qualification of maintenance, maintenance scenario optimization with real time simulations… A first module plotting the dose rate is currently being tested. On the one hand, it would for example allow designers to improve the design by adapting maintenance operations to any complex 3D map of dose rate, and on the other hand, it would permit workers to train for operations in radioactive environments (showing no go areas, hot spots…).}
}
@article{CIRULIS201314,
title = {Augmented Reality in Logistics},
journal = {Procedia Computer Science},
volume = {26},
pages = {14-20},
year = {2013},
note = {ICTE in Regional Development, December 2013, Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012751},
author = {Arnis Cirulis and Egils Ginters},
keywords = {Augmented reality (AR), Logistics elements},
abstract = {This paper describes the basic elements of logistics and pays special attention to improvement possibilities in packaging, handling, storage and transportation phases where manpower and personnel also play an important role. To decrease the error rate of object selection and decision-making time it is necessary to simplify natural logistics element execution and make it more humane where human resources are involved. Modern technologies can improve those processes by taking care of stressful situations and depressing warehouse worker routines. Augmented reality (AR) offers a key technology to solve these problems by allowing to make decisions based on computer generated visualizations and 3D model projections. The successful use of AR technologies in various industries and some experimental approbation in warehouse environments confirms the potential and perspectives. The use of instructions in three dimensional space instead of text and image based guides is a general improvement which is also introduced in this paper.}
}
@incollection{HARTMANN2019323,
title = {16 - Virtual reality and immersive approaches to contextual food testing},
editor = {Herbert L. Meiselman},
booktitle = {Context},
publisher = {Woodhead Publishing},
pages = {323-338},
year = {2019},
isbn = {978-0-12-814495-4},
doi = {https://doi.org/10.1016/B978-0-12-814495-4.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144954000167},
author = {Christina Hartmann and Michael Siegrist},
keywords = {Virtual reality, Food behavior, Consumer, Immersive environment, Food},
abstract = {The goal of this chapter is to summarize existing studies that have used different forms of virtual reality (VR) in the domain of consumer research on foods. Throughout this chapter, we refer to articles that were identified via a systematic literature search. Some studies focused on the applicability and validity of VR in the food context, while others tested the products’ sensory appeal in different immersive environments. We describe two of our own studies in which we first tested the validity of a virtual food buffet, and then tested whether participants apply the same search strategies in VR as they do in reality food shopping. The applicability of VR to evoke food-related emotions is discussed, and the limitations of VR in food behavior research that must be overcome are highlighted. Research gaps are identified, and the potential for VR applications in food behavior research is highlighted.}
}
@article{CHU2023313,
title = {Augmented reality user interface design and experimental evaluation for human-robot collaborative assembly},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {313-324},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000717},
author = {Chih-Hsing Chu and Yu-Lun Liu},
keywords = {Augmented reality, Human-robot collaboration, User interface, Sensory cues, Manual assembly},
abstract = {Artificial intelligence (AI) has been applied to a wide spectrum of industrial sectors, but manual operations remain indispensable in most manufacturing systems due to high complexity or costs involving in automation. A more practical approach is enabling humans to collaborate with machines complementary to each other. One critical issue in human-robot collaboration (HRC) is to assure the operator's productivity, safety, and trust while interacting with the robot. This paper presents an experimental study on user interface design in augmented reality (AR) for human-robot collaborative assembly in a shared workspace. The experiment aims to verify and cross-compare the effectiveness of visual and haptic cues in various forms that convey the robot intent to human. Analysis of the work performance and gazing behavior of participants shows that both cues can reduce their visual attention on the moving robot during the collaboration. The interface that provides the proximity of robot using visual cues is considered most useful. It is not intuitive to recognize complex information by vibration on different parts of the human hand in the experiment. Finally, human trust to robot has a higher correlation on the usability of a user interface than the work performance assisted by the interface. These findings may work as design guidelines for AR assisted human-robot interaction in smart manufacturing.}
}
@article{LI2020112836,
title = {Experimental and numerical study on the thermal performance of ventilated roof composed with multiple phase change material (VR-MPCM)},
journal = {Energy Conversion and Management},
volume = {213},
pages = {112836},
year = {2020},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2020.112836},
url = {https://www.sciencedirect.com/science/article/pii/S0196890420303745},
author = {Han Li and Jinchao Li and Chang Xi and Wanhe Chen and Xiangfei Kong},
keywords = {Thermal storage system, Phase change material, Ventilated roof, Passive cooling storage, Building energy conservation},
abstract = {In the hot summer, roof is the prime location absorbed the most solar radiation for both residential and commercial buildings. Hence, enhancing the thermal performance of roofs is essential for reducing the heat gain thus building load. In this study, a novel ventilated roof composed with multiple phase change material (VR-MPCM) was proposed and its thermal performance was investigated experimentally and numerically. A contrast experiment was conducted in two full-scale chambers. The research results indicated that VR-MPCM can reduce the indoor peak temperature by 16.9–18.8%, and push back the peak temperature occurrence time by 30–50 min compared with the conventional ventilated roof (CVR). In addition, the cumulative heat flux during 4320 min demonstrated that the energy conservation rate of VR-MPCM can go up to 97.1% compared with the CVR. A verified three-dimensional transient heat transfer model was introduced for further research. Through the simulation research, intermittent opening of vents was found to be the best operating strategy for reducing heat gain. Regional comparison indicated that VR-MPCM in severe cold and cold climate zones had obvious energy saving potential. In conclusion, VR-MPCM has a potential of energy saving by reducing the diurnal heat gain and utilizing the nightly free cooling.}
}
@article{XIE2024106189,
title = {Electroencephalography-based recognition of six basic emotions in virtual reality environments},
journal = {Biomedical Signal Processing and Control},
volume = {93},
pages = {106189},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106189},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002477},
author = {Jialan Xie and Yutong Luo and Shiyuan Wang and Guangyuan Liu},
keywords = {Classification, Emotion recognition, Electroencephalography (EEG), Six basic emotions, Virtual reality},
abstract = {Emotion recognition (ER) in virtual reality (VR) environments has attracted increasing attention in the field of affective computing and human–computer interaction (HCI). Electroencephalography (EEG) is an effective tool for capturing human emotions and mental states, and the demand for collecting and analyzing EEG signals in VR environments has been on the increase. In this study, we built an EEG dataset for six basic emotions in VR environments—that is, the VR six discrete emotional EEG data (VRSDEED) dataset—and developed a machine-learning framework for classifying emotional states using it. Fifty participants were shown six VR emotion-inducing videos while their EEG signals were recorded. To determine the optimal features and classification methods, we extracted four frequency-domain features and applied five different machine-learning algorithms. To enhance prediction performance, we fused different features and used recursive feature elimination with cross-validation for feature selection. Classification performance was assessed using accuracy and F1-scores. Our findings demonstrated that (1) the light gradient-boosting machine algorithm achieved the highest average classification accuracy of 89.94%; (2) isolated features achieved the highest accuracy of 89.50% in the γ frequency band and feature fusion enhanced recognition performance; (3) the θ and γ frequency bands were more significant in emotion processing than other bands. The proposed dataset and framework should serve as valuable resources and provide insights for research on ER in VR environments.}
}
@article{TAO2018683,
title = {A customized garment collaborative design process by using virtual reality and sensory evaluation on garment fit},
journal = {Computers & Industrial Engineering},
volume = {115},
pages = {683-695},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217305089},
author = {Xuyuan Tao and Xiao Chen and Xianyi Zeng and Ludovic Koehl},
keywords = {Virtual garments, Fit preference, Collaborative design, Sensory evaluation, Computational model},
abstract = {As the successful implantation of CAD (computer-aided-design) technology in garment industry, the 3D virtual garment technology has attracted a great attention of textile/garment companies. However, wearer’s perception on virtual garment in terms of fitting and comfort has never been systematically studied. In this paper, we propose an original customized 3D garment collaborative design process by integrating interactions between the designer and the specific consumer. In this context, a normalized sensory evaluation on garment fitting effects will be organized in a virtual environment, in order to enhance communications of the concerned actors on perception of products. Also, by learning from the measured distances between the garment surfaces and the mannequin (input) and sensory descriptors (output), we model the relationship between the garment design parameters and the human perception on fit of the finished virtual products. Using this model, we can estimate the fit perception for a specific garment size on a specific body shape without any real try-on experience. In practice, the proposed collaborative design process will permit to develop an online recommendation system for garment size selection and fit estimation. Furthermore, it will permit to recursively modify the initial garment patterns according to the consumer’s fit preferences so that they can obtain a real personalized garment. In this way, the consumer will be directly involved in the product design process by performing a series of sensory evaluations on virtual garment fit in order to obtain a desired finished product. This process has been validated by creating a collection of T-shirts meeting requirements of various customers.}
}
@article{YEOM2021108134,
title = {Psychological and physiological effects of a green wall on occupants: A cross-over study in virtual reality},
journal = {Building and Environment},
volume = {204},
pages = {108134},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108134},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321005357},
author = {Seungkeun Yeom and Hakpyeong Kim and Taehoon Hong},
keywords = {, , , , , },
abstract = {Social change due to COVID-19 has had negative impacts on the mental health of people who live indoors. One of the possible solutions to these problems is to introduce biophilic design. Among the elements of this design, green walls are often favored due to their spatial efficiency. This study investigated the effectiveness of green walls in relieving stress indoors. First, the stress level of the occupants of an indoor space that has green walls was measured. Then their psychological and physiological responses to the green walls were analyzed. To this end, using virtual reality, an experimental condition was set up in which a green wall was installed, and the stress level of the subjects was analyzed based on their state-trait anxiety (STAI), heart rate variability, electrodermal activity (EDA), and electroencephalogram (EEG). Twenty-seven subjects were recruited and the changes in their psychological and physiological responses when their environment changed from a non-green wall condition to a small to large green wall condition were measured. The results showed statistically significant differences in STAI, EDA, the relative alpha power in the parietal and occipital lobes, mental stress, and mental fatigue. Ultimately, the small green wall had a more significant relaxing effect. On the other hand, in the indoor space with a large green wall, the stress level of the residents increased. Thus, when installing a green wall in an indoor space, a small green wall is proposed.}
}
@article{LELIEVELDT20001,
title = {Anatomical Modeling with Fuzzy Implicit Surface Templates: Application to Automated Localization of the Heart and Lungs in Thoracic MR Volumes},
journal = {Computer Vision and Image Understanding},
volume = {80},
number = {1},
pages = {1-20},
year = {2000},
issn = {1077-3142},
doi = {https://doi.org/10.1006/cviu.2000.0864},
url = {https://www.sciencedirect.com/science/article/pii/S1077314200908646},
author = {Boudewijn P.F. Lelieveldt and Milan Sonka and Lizann Bolinger and Thomas D. Scholz and Hein Kayser and Rob {van der Geest} and Johan H.C. Reiber},
abstract = {In this paper, a novel model-driven segmentation approach for thoracic MR-images is presented. The goal of this work is to coarsely, but fully automatically localize the boundary surfaces of the heart and lungs in thoracic MR sets. The major organs in the thorax are described in a three-dimensional analytical model template by combining a set of fuzzy implicit surfaces by means of constructive solid geometry and formulating model registration as an energy minimization. The method has been validated on 20 thoracic MR volumes from two centers (patients and normal subjects). On average 90% of the contour length of the heart and lung contours was localized with sufficient accuracy (average positional error 6 mm) to automatically provide the initial conditions for a subsequently applied locally accurate segmentation method.}
}
@article{ELSAADY2020105265,
title = {A one-way coupled numerical magnetic field and CFD simulation of viscoplastic compressible fluids in MR dampers},
journal = {International Journal of Mechanical Sciences},
volume = {167},
pages = {105265},
year = {2020},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2019.105265},
url = {https://www.sciencedirect.com/science/article/pii/S002074031931776X},
author = {Wael Elsaady and S. Olutunde Oyadiji and Adel Nasser},
keywords = {Viscoplastic fluid, Magnetic FE analysis, Computational fluid dynamics, Two-phase flow},
abstract = {Recent developments in employment of magnetorheological (MR) fluids in different applications heighten the need for better understanding of their non-linear flow characteristics. In this paper, the modelling of the dynamic hysteretic behaviour of a twin-tube MR damper via a novel one-way coupled numerical approach is presented. The approach couples the Finite Element Analysis (FEA) of the MR damper magnetic circuit with the Computational Fluid Dynamics (CFD) analysis of the fluid flow field. A User-Defined Function (UDF) predefines the fluid shear-rate-dependent apparent viscosity into the CFD solver according to the magnetic field density attained from the FE solver. Two transient CFD cases are presented, one of them studies the flow in the damper as an incompressible single-phase flow, while the other accounts for the effect of fluid compressibility in the liquid-gas domain as a two-phase flow. The validation of the proposed numerical approaches is achieved via the direct comparison with the published experimental measurements for the same MR damper. It has been found that the fluid compressibility affects the hysteretic behaviour of MR dampers greatly. Moreover, the flow field shows the distributions of pressure, velocity and viscosity contours. In particular, the results show higher non-Newtonian viscosity in the throttling area and lower Newtonian viscosity elsewhere. Furthermore, it is shown that the variation of some design parameters of the damper, such as the width of magnetic poles, input current and frequency of the piston motion, has significant effects on the damper behaviour.}
}
@article{CHEN2016230,
title = {Virtual-reality simulator system for double interventional cardiac catheterization using haptic force producer with visual feedback},
journal = {Computers & Electrical Engineering},
volume = {53},
pages = {230-243},
year = {2016},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2015.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0045790615003699},
author = {Guan-Chun Chen and Chia-Hung Lin and Chien-Ming Li and Kai-Sheng Hsieh and Yi-Chun Du and Tainsong Chen},
keywords = {Interventional cardiac catheterization (ICC), Atrial septal defect (ASD), Visual and haptic feedback, Double-catheter training system},
abstract = {Atrial septal defect is a serious congenital heart disease. Interventional cardiac catheterization has increasingly become a treatment for its correction, with reduced risks for infections and complications of anesthesia. However, the cardiac catheterization still poses a huge challenge for novice surgeons; therefore a double-catheter system is warrant for training surgeons. In this study, a novel mechanical training system is designed for interventional cardiac catheterization simulation. Visual and haptic feedback simulators were developed from medical records to simulate accidental bumps into the vessel walls. The results indicate that this proposed system could accurately simulate the resistance encountered during cardiac catheterization and achieve the desired training outcomes. This simulation module can be a potential of teaching platform for training surgeons in medical education.}
}
@article{DUCHOWSKI20221641,
title = {3D Gaze in Virtual Reality: Vergence, Calibration, Event Detection},
journal = {Procedia Computer Science},
volume = {207},
pages = {1641-1648},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.221},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201105X},
author = {Andrew T. Duchowski and Krzysztof Krejtz and Matias Volonte and Chris J. Hughes and Marta Brescia-Zapata and Pilar Orero},
keywords = {virtual reality, eye tracking},
abstract = {Eye movement analysis in modern 3D rendering systems is reviewed and three new techniques are derived inspired by work developed in early Virtual Reality so-called 2.5D implementations, namely (a) gaze depth (i.e., vergence) estimation, (b) vergence calibration, and (c) real-time 3D event detection that considers eye- and head-coupling. The new 3D calibration shows excellent error reduction in terms of Mean Squared Error (MSE).}
}
@article{PETROLO201086,
title = {Effect of a virtual reality interface on the learning curve and on the accuracy of a surgical planner for total hip replacement},
journal = {Computer Methods and Programs in Biomedicine},
volume = {97},
number = {1},
pages = {86-91},
year = {2010},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2009.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260709002922},
author = {Luca Petrolo and Debora Testi and Fulvia Taddei and Marco Viceconti},
keywords = {Virtual reality, Surgical planning, Total hip replacement},
abstract = {The aim of this study is to evaluate the performance of a non-conventional input and output device (virtual reality) in a total hip replacement surgical planner. A test was performed asking five users to position a cup in a defined position. Every user performed the task using three different hardware configurations: (I) conventional mouse and monitor, (II) mouse and auto-stereoscopic monitor, and (III) 12-DOF tracker (haptic device) and auto-stereoscopic monitor. The results were evaluated in terms of root mean square error of the obtained position with respect to the target one and in terms of learning curve. The results showed that the examined VR technology does not show a sufficient positioning accuracy to be considered for clinical assessment.}
}
@article{KARNCHANAPAYAP2023107796,
title = {Activities-based virtual reality experience for better audience engagement},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107796},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107796},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001474},
author = {Gomesh Karnchanapayap},
keywords = {Virtual reality, Virtual exhibition, Virtual activities, Audience engagement, Interaction design},
abstract = {The benefits of virtual reality have definitely expanded beyond the gaming industry to a variety of other businesses. From virtual conferences to virtual exhibitions, the MICE (Meetings, Incentives, Conferences, and Exhibitions) industry has several applications for virtual reality. Researchers and experts should pay more attention to the virtual reality applications in this market segment. This study's objectives were to 1) create a virtual reality experience for Thailand's Ministry of Digital Economy and Society to serve as an international exhibition showcase by incorporating virtual activities, 2) evaluate the efficiency of the virtual reality experience based on the activities and audience engagement, and 3) investigate the virtual reality experience's audience satisfaction. This virtual immersive experience is the outcome of using Tilt Brush to build a virtual spatial playground centered on three topics: the rural broadband internet project, the cybersecurity act 2019 & the personal data protection act 2019, and awareness of fake news. The virtual reality experience was on show at Digital Thailand Big bang 2019, which was held at BITEC International Exhibition in Bangkok, Thailand from October 28 to October 31, 2019. The investigation utilized mixed-method research, consisting of a quantitative technique based on observation and a qualitative technique based on questionnaires. A convenience sample of 126 cases was obtained from viewers aged seven and up who participated in the virtual reality experience. The virtual experience required the audience to engage in four sorts of physical activity: looking, walking, performing a specific physical action, and locating an object. Observation forms, questionnaires, and examinations were utilized to collect data. The findings indicate that the virtual reality experience, while entertaining the audience, encourages learning through physical activities. These activities contribute to audience engagement and, ultimately, comprehension of the subject. The majority of the audience was able to accurately answer test questions, indicating that not only did they engage the presentation through active participation, but they also retained its content. This project is a case study of how virtual reality can be used to increase audience participation during an exhibition.}
}
@article{DAVIS2023103890,
title = {Investigating movement through smoke in virtual reality},
journal = {Fire Safety Journal},
volume = {140},
pages = {103890},
year = {2023},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2023.103890},
url = {https://www.sciencedirect.com/science/article/pii/S0379711223001583},
author = {Cameron Davis and Chelsea Sole and Humayun Khan and Daniel Nilsson},
keywords = {Movement speed, Smoke, Reduced visibility, Virtual reality (VR), Evacuation, Pedestrian movement, Human behaviour},
abstract = {Accurately estimating movement through smoke is critical for fire safety design. However, physical laboratory experiments using artificial smoke can be very expensive. Virtual reality (VR) can potentially be an alternative method to cheaply investigate movement through smoke. The primary objective of this study is to determine the relationship between movement speed and visibility in VR. The secondary objective was to compare movement through smoke in VR to existing experimental data. Five scenarios were tested: real-world unimpeded movement, unimpeded movement in VR, and movement in VR with virtual smoke reducing the visibility to 3.5 m, 2.5 m, and 1.5 m. A wireless head mounted display was used to immerse participants in the virtual environment. For the study, 42 participants experienced the smoke scenarios in a random order. The results indicate that movement speed decreased with visibility, but to a lesser extent than in previous physical laboratory experiments with artificial smoke. Unimpeded movement in VR was shown to be significantly slower than real-world unimpeded movement. Prior experience with VR was not shown to have a significant impact on unimpeded VR movement speed. Increasing the realism of the virtual environment could potentially better align results from VR experiments with past physical laboratory experiments.}
}
@article{MARIE2007590,
title = {French RSE-M and RCC-MR code appendices for flaw analysis: Presentation of the fracture parameters calculation—Part I: General overview},
journal = {International Journal of Pressure Vessels and Piping},
volume = {84},
number = {10},
pages = {590-600},
year = {2007},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2007.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0308016107000890},
author = {S. Marie and S. Chapuliot and Y. Kayser and M.H. Lacire and B. Drubay and B. Barthelet and P. {Le Delliou} and V. Rougier and C. Naudin and P. Gilles and M. Triay},
abstract = {Two French nuclear codes include flaw assessment procedures: the RSE-M code “Rules for In-service Inspection of Nuclear Power Plant Components” and the RCC-MR code “Design and Construction rules for mechanical components of FBR nuclear islands and high temperature applications”. An important effort of development of these analytical methods has been made for the last 10 years in the frame of a collaboration between CEA, EDF and AREVA-NP, and in the frame of R&D actions involving CEA and IRSN. These activities have led to a unification of the common methods of the two codes. The calculation of fracture mechanics parameters, and in particular the stress intensity factor KI and the J integral, has been widely developed for industrial configurations. All developments have been integrated in the 2005 edition of RSE-M and in the 2007 edition of RCC-MR. This series of articles is composed of five parts: this first one presents an overview of the methods proposed in the RCC-MR and RSE-M codes. Parts II–IV provide compendia for specific components: plates (part II), pipes (part III) and elbows (part IV). Finally, part V presents the validation elements of the methods, with details on the process followed for their development and on evaluation of the accuracy of the proposed analytical methods. This first article of the series presents an overview of the calculation of KI and J in these two codes and describes briefly the defect assessment analyses. Specific details in the Appendix A16 of RCC-MR (LBB procedure and creep analyses) are also introduced in this article.}
}
@article{SIEDLER2021307,
title = {Engineering changes in manufacturing systems supported by AR/VR collaboration},
journal = {Procedia CIRP},
volume = {96},
pages = {307-312},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.092},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121001207},
author = {Carina Siedler and Moritz Glatt and Pavel Weber and Achim Ebert and Jan C. Aurich},
keywords = {Digital Manufacturing System, Augmented reality, Virtual reality, Digital technology, Collaboration},
abstract = {Various triggers in manufacturing operations lead to engineering changes, such as the conversion of a machine or changes in the factory layout. In companies, the priority is often to avoid engineering changes as they are time-intensive and lead to change costs. If changes cannot be avoided, the implementation must be simplified and accelerated as much as possible. This can be achieved through structured, efficient change processes, which are specifically supported by methods and tools. Augmented Reality (AR) and Virtual Reality (VR) show great potential to facilitate processes and to improve collaboration in the context of engineering changes. By combining these methods and tools, the implementation time of engineering changes can be shortened, and the likelihood of cost-intense planning errors can be reduced. Therefore, the objective of this paper is to present an approach which identifies whether AR and VR are suitable to support the engineering change process steps. The goal is to support the process of recording the current layout, identifying machines, determining the material flow, and visualizing this information and thereby simplify and accelerate the engineering change process. Furthermore, potential collaborations between AR and VR in each engineering change process are identified and described.}
}
@article{MARIE2007687,
title = {French RSE-M and RCC-MR code appendices for flaw analysis: Presentation of the fracture parameters calculation—Part V: Elements of validation},
journal = {International Journal of Pressure Vessels and Piping},
volume = {84},
number = {10},
pages = {687-696},
year = {2007},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2007.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0308016107000932},
author = {S. Marie and S. Chapuliot and Y. Kayser and M.H. Lacire and B. Drubay and B. Barthelet and P. {Le Delliou} and V. Rougier and C. Naudin and P. Gilles and M. Triay},
abstract = {French nuclear codes include flaw assessment procedures: the RSE-M Code “Rules for In-service Inspection of Nuclear Power Plant Components” and the RCC-MR code “Design and Construction Rules for Mechanical Components of FBR Nuclear Islands and High Temperature Applications”. Development of analytical methods has been made for the last 10 years in the framework of a collaboration between CEA, EDF and AREVA-NP, and by R&D actions involving CEA and IRSN. These activities have led to a unification of the common methods of the two codes. The calculation of fracture mechanics parameters, in particular the stress intensity factor KI and the J integral, has been widely developed for industrial configurations. All the developments have been integrated in the 2005 edition of RSE-M and in 2007 edition of RCC-MR. This series of articles consists of 5 parts: the first part presents an overview of the methods proposed in the RCC-MR and RSE-M codes. Parts II–IV provide the compendia for specific components. The geometries are plates (part II), pipes (part III) and elbows (part IV). This part presents validation of the methods, with details on the process followed for their development and of the evaluation accuracy of the proposed analytical methods.}
}
@article{TAI2021274,
title = {Augmented reality-based visual-haptic modeling for thoracoscopic surgery training systems},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {4},
pages = {274-286},
year = {2021},
note = {Special Issue on Virtual reality and Augmented Reality in Medical Simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000528},
author = {Yonghang Tai and Junsheng Shi and Junjun Pan and Aimin Hao and Victor Chang},
keywords = {Augmented reality, VATS, Surgery training, XPBD},
abstract = {Background
Compared with traditional thoracotomy, video-assisted thoracoscopic surgery (VATS) has less minor trauma, faster recovery, higher patient compliance, but higher requirements for surgeons. Virtual surgery training simulation systems are important and have been widely used in Europe and America. Augmented reality (AR) in surgical training simulation systems significantly improve the training effect of virtual surgical training, although AR technology is still in its initial stage. Mixed reality has gained increased attention in technology-driven modern medicine but has yet to be used in everyday practice.
Methods
This study proposed an immersive AR lobectomy within a thoracoscope surgery training system, using visual and haptic modeling to study the potential benefits of this critical technology. The content included immersive AR visual rendering, based on the cluster-based extended position-based dynamics algorithm of soft tissue physical modeling. Furthermore, we designed an AR haptic rendering systems, whose model architecture consisted of multi-touch interaction points, including kinesthetic and pressure-sensitive points. Finally, based on the above theoretical research, we developed an AR interactive VATS surgical training platform.
Results
Twenty-four volunteers were recruited from the First People's Hospital of Yunnan Province to evaluate the VATS training system. Face, content, and construct validation methods were used to assess the tactile sense, visual sense, scene authenticity, and simulator performance.
Conclusions
The results of our construction validation demonstrate that the simulator is useful in improving novice and surgical skills that can be retained after a certain period of time. The video-assisted thoracoscopic system based on AR developed in this study is effective and can be used as a training device to assist in the development of thoracoscopic skills for novices.}
}
@article{TONNIS2013997,
title = {Representing information – Classifying the Augmented Reality presentation space},
journal = {Computers & Graphics},
volume = {37},
number = {8},
pages = {997-1011},
year = {2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313001313},
author = {Marcus Tönnis and David A. Plecher and Gudrun Klinker},
keywords = {Augmented Reality, Taxonomy, Information Presentation},
abstract = {Augmented Reality has a wide-ranging presentation space. In addition to presenting virtual information in a 3D space, such information can also be placed in relation to physical objects, locations or events. Decomposing this presentation space – or more exactly, the principles of how information is represented in Augmented Reality – into unique and independent dimensions provides a fundamental spectrum of options. First, this decomposition facilitates a fine-grained analysis of effects on human understanding. Second, multiple factors, given by multiple differences between different presentation systems with respect to more than one such principle, can be determined and properly addressed. Third, this decomposition facilitates a determination of new fields of research by identifying not-yet-used concepts. Since the beginning of Augmented Reality research, a growing number of applications have emerged that exploit various ways to represent information. This paper resumes this development and presents a set of independent dimensions covering representation principles of virtual information related to a physical environment: the temporality of virtual information, dimensionality, the frame of reference, mounting/registration and the type of reference. The suitability of the devised dimensions is tested by categorizing a wide variety of AR applications. The categorized data is analyzed for the most-often and less-frequently used combinations of classes. In particular, the classes that have not yet been used exhibit the potential to allow future work that investigates new options for information presentation.}
}
@article{SHAO2024108108,
title = {Facial augmented reality based on hierarchical optimization of similarity aspect graph},
journal = {Computer Methods and Programs in Biomedicine},
volume = {248},
pages = {108108},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108108},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001044},
author = {Long Shao and Tianyu Fu and Yucong Lin and Deqiang Xiao and Danni Ai and Tao Zhang and Jingfan Fan and Hong Song and Jian Yang},
keywords = {Pose estimation, Aspect graph, Hierarchical optimization, Monte Carlo tree search, Facial augmented reality},
abstract = {Background
The existing face matching method requires a point cloud to be drawn on the real face for registration, which results in low registration accuracy due to the irregular deformation of the patient's skin that makes the point cloud have many outlier points.
Methods
This work proposes a non-contact pose estimation method based on similarity aspect graph hierarchical optimization. The proposed method constructs a distance-weighted and triangular-constrained similarity measure to describe the similarity between views by automatically identifying the 2D and 3D feature points of the face. A mutual similarity clustering method is proposed to construct a hierarchical aspect graph with 3D pose as nodes. A Monte Carlo tree search strategy is used to search the hierarchical aspect graph for determining the optimal pose of the facial 3D model, so as to realize the accurate registration of the facial 3D model and the real face.
Results
The proposed method was used to conduct accuracy verification experiments on the phantoms and volunteers, which were compared with four advanced pose calibration methods. The proposed method obtained average fusion errors of 1.13 ± 0.20 mm and 0.92 ± 0.08 mm in head phantom and volunteer experiments, respectively, which exhibits the best fusion performance among all comparison methods.
Conclusions
Our experiments proved the effectiveness of the proposed pose estimation method in facial augmented reality.}
}
@article{SEELIGER2023103985,
title = {Augmented reality for industrial quality inspection: An experiment assessing task performance and human factors},
journal = {Computers in Industry},
volume = {151},
pages = {103985},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103985},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001355},
author = {Arne Seeliger and Long Cheng and Torbjørn Netland},
keywords = {Augmented reality, Quality inspection, Manufacturing, Experiment, Head-mounted display, Cognitive load},
abstract = {Augmented reality (AR) technologies promise to increase the flexibility and productivity of the workforce by providing real-time information to workers right where it is needed. Following Cognitive Load Theory and Attention Theory, AR assistance can reduce the burden on workers’ mental capacity while performing a task, thereby improving task performance. The benefits of AR on industrial activities like assembly or maintenance have been investigated extensively, but the literature on AR-assisted quality control, specifically, quality inspection, is scarce. This stands in contrast to the importance of industrial quality inspection and highlights the need for research on the topic. In this work, we develop an AR-based system for head-mounted displays (HMDs) that visualizes product defects directly on physical products. We investigate its effect on task performance and human factors through an experiment. Participants performed multiple quality inspections using real manufacturing products and equipment with the help of an AR HMD, a screen, or no additional help. We find increased task performance using the AR HMD system. In comparison to screen-based assistance, a moderating effect of task difficulty was observed. Specifically, AR was more beneficial to task completion times given a difficult task. Furthermore, the AR HMD system reduced mental workload and received a positive rating regarding user experience. Our work contributes to the research in industrial AR usage through a novel AR HMD system for quality inspection and provides so far missing empirical and theoretically grounded evidence on its benefits with regard to task performance and human factors.}
}
@article{HAOMING2024100057,
title = {A systematic review on vocabulary learning in AR and VR gamification context},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100057},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000072},
author = {Lin Haoming and Wei Wei},
keywords = {Vocabulary learning, Augmented reality, Virtual reality, Gamification, Digital affordance},
abstract = {In recent scholarly literature, there has been a growing focus on the integration of augmented reality (AR), virtual reality (VR), and digital games to assist language learning. However, little has been written on reviewing the adoption of gamified AR or VR in facilitating the enhancement of specific language skills. Addressing this research gap, the present study conducted a systematic review of vocabulary learning within AR and VR gamification context. Following the predefined inclusion criteria and literature selection process outlined by the PRISMA framework, 23 articles were selected from a pool of 97 studies for final analysis. The major findings revealed that: (1) Form and meaning of vocabulary took precedence in gamified AR-assisted vocabulary learning or VR-assisted vocabulary learning, adopting intentional learning as the primary approach; (2) A significant majority of studies employed researcher-coined vocabulary tests as the measurements for assessing vocabulary knowledge, with translation, multiple-choice questions, and word-matching serving as the most common tasks; (3) The reviewed studies demonstrated that gamified AR and VR both prioritize interactivity as the core digital affordance in the context of vocabulary learning. Gamified AR owned unique features, including bridging reality and virtuality, promoting collaborative learning, and visualizing content, while gamified VR was more inclined toward immersion and facilitating self-regulated learning; (4) Three aspects of problems, namely technology-related, student-related, and teacher-related, emerged with the application of gamified AR or VR. Based on these results, the current study engaged in a comprehensive discussion and suggested a tripartite co-construction mechanism for learning materials, involving students, teachers, and researchers, integrating both gamified AR and VR.}
}
@article{MA2022106279,
title = {A dual-branch hybrid dilated CNN model for the AI-assisted segmentation of meningiomas in MR images},
journal = {Computers in Biology and Medicine},
volume = {151},
pages = {106279},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106279},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522009878},
author = {Xin Ma and Yajing Zhao and Yiping Lu and Peng Li and Xuanxuan Li and Nan Mei and Jiajun Wang and Daoying Geng and Lingxiao Zhao and Bo Yin},
keywords = {Meningioma, Image segmentation, Convolutional neural networks, Multi-scale receptive fields},
abstract = {Background and objective:
Treatment for meningiomas usually includes surgical removal, radiation therapy, and chemotherapy. Accurate segmentation of tumors significantly facilitates complete surgical resection and precise radiotherapy, thereby improving patient survival. In this paper, a deep learning model is constructed for magnetic resonance T1-weighted Contrast Enhancement (T1CE) images to develop an automatic processing scheme for accurate tumor segmentation.
Methods:
In this paper, a novel Convolutional Neural Network (CNN) model is proposed for the accurate meningioma segmentation in MR images. It can extract fused features in multi-scale receptive fields of the same feature map based on MR image characteristics of meningiomas. The attention mechanism is added as a helpful addition to the model to optimize the feature information transmission.
Results and conclusions:
The results were evaluated on two internal testing sets and one external testing set. Mean Dice Similarity Coefficient (DSC) values of 0.886, 0.851, and 0.874 are demonstrated, respectively. In this paper, a deep learning approach is proposed to segment tumors in T1CE images. Multi-center testing sets validated the effectiveness and generalization of the method. The proposed model demonstrates state-of-the-art tumor segmentation performance.}
}
@article{POLAP2020397,
title = {Strengthening the perception of the virtual worlds in a virtual reality environment},
journal = {ISA Transactions},
volume = {102},
pages = {397-406},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2020.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0019057820300860},
author = {Dawid Połap and Karolina Kęsik and Alicja Winnicka and Marcin Woźniak},
keywords = {Virtual reality, Mobile games, Player behavior, Image processing, Convolutional neural network},
abstract = {Virtual reality is becoming more and more improved primarily due to numerous applications and the powers of mobile devices. Using various sensors, precise displays and high computing powers smartphone are becoming devices that make the boost in technology. Now it is necessary to efficiently use various sensors without affecting system operation and improve control abilities for various purposes. Especially in practical applications received by mass users such as games and any kind of experience. In this article, we propose a system that allows to extend the perception of the virtual world by conveying information about the user’s movements in reality into the supervised model. The system retrieves data from several sources, quickly analyzes them using artificial intelligence techniques, and returns information to the mobile phone about the activity that is being processed. The concept extends the understanding of today’s virtual reality by allowing the user to move and perform simple gestures in a specially designed room. Moreover, we propose multiplayer mode in virtual reality, where players are in different places. The proposed architecture of the system has been tested on simple applications, and the results show high potential for implementations in various apps by achieving almost 90% efficiency in changing player direction in real time and only 7.5% of collision cases.}
}
@article{XIAO2022100110,
title = {Design of a virtual reality rehabilitation system for upper limbs that inhibits compensatory movement},
journal = {Medicine in Novel Technology and Devices},
volume = {13},
pages = {100110},
year = {2022},
issn = {2590-0935},
doi = {https://doi.org/10.1016/j.medntd.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2590093521000540},
author = {Bowen Xiao and Lin Chen and Xin Zhang and Zengyong Li and Xiaoyu Liu and Xiaoying Wu and Wensheng Hou},
keywords = {Kinect, Virtual reality, Compensatory pattern, Assessment},
abstract = {Stroke is currently the main cause of death and disability among the elderly in our country. According to medical research, a considerable number of patients with physical disabilities can be recovered through rehabilitation training. However, traditional motor rehabilitation is a continuous, repeated and slow process, and patients tend to feel bored and lose motivation for training. In this paper, we developed a virtual reality rehabilitation system based on Kinect, which is a vision capture sensor, for patients with movement disorders who are at or above Brunnstrom Stage III and have certain motor ability. Through the management platform of the system, physician can obtain the patient's personal information, formulate and adjust the training plan. Patients can control the role in the virtual scene through Kinect sensor, and complete the training action according to the guidance. The system collects the user's motion data in real time and detect the compensation. The system will adaptively evolve to guide the patient to self-correct the compensatory patterns. After the training, the system will evaluate the patients based on the their training performance. Two experiments are also carried out to verify the accuracy of the range of motion and the effectiveness of virtual guidance. It is proved that the virtual reality upper limb rehabilitation training system studied in this paper is reliable, stable, and can guide users to complete the training action and improve the rehabilitation effect.}
}
@article{TSAI2022104400,
title = {AR-based automatic pipeline planning coordination for on-site mechanical, electrical and plumbing system conflict resolution},
journal = {Automation in Construction},
volume = {141},
pages = {104400},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104400},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002734},
author = {Liang-Ting Tsai and Hung-Lin Chi and Tzong-Hann Wu and Shih-Chung Kang},
keywords = {MEP system, Pipeline planning, AR, Conflict resolution, Path coordination},
abstract = {Mechanical, Electrical and Plumbing (MEP) system conflicts are complicated problems given the involved multidisciplinary nature and constraint-bearing coordination. Although Building Information Modeling (BIM) shows benefit to resolving the conflicts before construction, the on-site pipeline inspection and further adjustment are inevitably necessary due to practical issues caused by deviations with plans. Given this, the research aims at developing an on-site pipeline inspection and automatic coordination approach by using Augmented Reality (AR) and a grid-based path-planning algorithm. The approach allows the site practitioners to compare the newly added pipe layout plan with the existing pipelines and re-plan the pipe layout where encountered conflicts, further obtaining a solution without clashes. The method considers MEP system constraints in practice and searches conflict-free pipe layout plans in near real-time. Furthermore, the path coordination process also considers MEP design criteria, including its function, construction, and maintenance. The planned pipe layout is displayed through AR to link the information from the virtual model to the real world. The performance tests are conducted in virtual scenarios with different grid cell sizes and priorities of pipes to evaluate the planning results. The test results show that under room and aisle space scenarios, the implemented approach can obtain path solutions with approximately 10 s, and most of them comply with the pre-defined MEP design requirements. Through this study, automatic pipe planning with constraints is proved to be feasible to provide an effective on-site conflict resolution approach to improve the MEP construction process and future maintenance.}
}