@article{LU2019111243,
title = {Analysis of nuclear response in CFETR toroidal field coils with density reduction VR technique},
journal = {Fusion Engineering and Design},
volume = {147},
pages = {111243},
year = {2019},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2019.111243},
url = {https://www.sciencedirect.com/science/article/pii/S0920379619307215},
author = {Peng Lu and Kun Xu and Yu Zheng and Xia Li and Songlin Liu and Jianjun Huang and Bin Yu},
keywords = {CFETR, TFC, McCad, Variance reduction, Density reduction},
abstract = {The China Fusion Engineering Test Reactor (CFETR) has been designed as steady-state operated tokamak device. The CFETR new engineering design has been initialized corresponding to the latest core parameters (major/minor radius is R = 7.2 m/a = 2.2 m). The nuclear analysis of CFETR was carried out to evaluate the neutron shielding capabilities and nuclear responses to toroidal field coils (TFC). The 3D neutronics model of CFETR was built with the support of McCad tool to convert the CAD model automatically. An extended variance reduction (VR) technique of combined MAGIC GVR with density reduction method has been involved to generate suitable weight windows to enhance the efficiency of particle transport. Highly detailed neutron and photon map has been obtained which meet the requirements for Monte Carlo nuclear analyses. The global neutron flux distribution of 3D CFETR with Pf = 200 MW to 1.5 GW was obtained to evaluate its shielding performance. The nuclear responses to TFC such as the nuclear heating and fast neutron fluence have been evaluated. Initial results show the shielding of current divertor design is not sufficient and the optimizing work has been done. The final results satisfy the referred ITER design limit and it can provide the support for the future more detailed design of divertor and other shielding systems.}
}
@article{SAKUMA2002243,
title = {Virtual reality experiments on a digital servosphere: guiding male silkworm moths to a virtual odour source},
journal = {Computers and Electronics in Agriculture},
volume = {35},
number = {2},
pages = {243-254},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(02)00021-2},
url = {https://www.sciencedirect.com/science/article/pii/S0168169902000212},
author = {Masayuki Sakuma},
keywords = {, Servosphere, Locomotion compensator, Orientation, Pheromone},
abstract = {A digital ‘servosphere’ locomotion-compensation apparatus has been developed for use in experiments on the mechanisms of spatial orientation in ambulatory animals. The sphere is driven by a pair of digital AC-servo motors, which provide a negative feedback response to any displacements of the test animal. A high-speed video tracker continually reports the position of the animal to a computer, which controls the servo-motors via a motor-control board and also logs the position of the motor axes. Movements of the servos define the path of the test animal on a virtual plane. The computer also generates odour cues in real time via a relay-control board and an airflow system. Odour presentation is programmed to occur in response to the animal's movements on the virtual plane, and can result in the animal being led to a virtual source. The servosphere was first used to investigate orientation of a walking male silkworm moth (Bombyx mori) towards a female in still air. When a calling female moth was placed beside the sphere, the male moth beat its wings and walked straight towards the female. Unilaterally dewinged males also walked in a straight line, though slightly to one side of the direction of the female, but bilaterally dewinged moths remained stationary. This indicates that in still air the male moths orientate to pheromone sources by fanning air from in front of them over their antennae with their wings. If a moth continually reverses its turning movements when it encounters a train of pheromone concentration peaks, it will maintain a course towards the source and eventually arrive there. This idea was demonstrated on the servosphere by incorporating a computer-controlled solenoid valve which releases pheromone when a moth points towards a virtual odour source. In this sensory field, both intact and bilaterally dewinged males reached the source irrespective of the wind direction. This result supports the proposed orientation mechanism and also demonstrates the applicability of the servosphere virtual-reality environment to the experimental investigation of insect orientation mechanisms.}
}
@article{AKDEMIRAKAR201687,
title = {Determination of optimal parameters for bilateral filter in brain MR image denoising},
journal = {Applied Soft Computing},
volume = {43},
pages = {87-96},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300953},
author = {Saime {Akdemir Akar}},
keywords = {Bilateral filter, Genetic algorithm, Rician noise, MR denoising, Optimal parameters},
abstract = {Noise elimination is an important pre-processing step in magnetic resonance (MR) images for clinical purposes. In the present study, as an edge-preserving method, bilateral filter (BF) was used for Rician noise removal in MR images. The choice of BF parameters affects the performance of denoising. Therefore, as a novel approach, the parameters of BF were optimized using genetic algorithm (GA). First, the Rician noise with different variances (σ=10, 20, 30) was added to simulated T1-weighted brain MR images. To find the optimum filter parameters, GA was applied to the noisy images in searching regions of window size [3×3, 5×5, 7×7, 11×11, and 21×21], spatial sigma [0.1–10] and intensity sigma [1–60]. The peak signal-to-noise ratio (PSNR) was adjusted as fitness value for optimization. After determination of optimal parameters, we investigated the results of proposed BF parameters with both the simulated and clinical MR images. In order to understand the importance of parameter selection in BF, we compared the results of denoising with proposed parameters and other previously used BFs using the quality metrics such as mean squared error (MSE), PSNR, signal-to-noise ratio (SNR) and structural similarity index metric (SSIM). The quality of the denoised images with the proposed parameters was validated using both visual inspection and quantitative metrics. The experimental results showed that the BF with parameters proposed by us showed a better performance than BF with other previously proposed parameters in both the preservation of edges and removal of different level of Rician noise from MR images. It can be concluded that the performance of BF for denoising is highly dependent on optimal parameter selection.}
}
@article{MICHAELKELM20131283,
title = {Spine detection in CT and MR using iterated marginal space learning},
journal = {Medical Image Analysis},
volume = {17},
number = {8},
pages = {1283-1292},
year = {2013},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2012.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1361841512001508},
author = {B. {Michael Kelm} and Michael Wels and S. {Kevin Zhou} and Sascha Seifert and Michael Suehling and Yefeng Zheng and Dorin Comaniciu},
keywords = {Marginal space learning, Spine detection, Spinal disk segmentation, Vertebra segmentation, Generative-discriminative detection},
abstract = {Examinations of the spinal column with both, Magnetic Resonance (MR) imaging and Computed Tomography (CT), often require a precise three-dimensional positioning, angulation and labeling of the spinal disks and the vertebrae. A fully automatic and robust approach is a prerequisite for an automated scan alignment as well as for the segmentation and analysis of spinal disks and vertebral bodies in Computer Aided Diagnosis (CAD) applications. In this article, we present a novel method that combines Marginal Space Learning (MSL), a recently introduced concept for efficient discriminative object detection, with a generative anatomical network that incorporates relative pose information for the detection of multiple objects. It is used to simultaneously detect and label the spinal disks. While a novel iterative version of MSL is used to quickly generate candidate detections comprising position, orientation, and scale of the disks with high sensitivity, the anatomical network selects the most likely candidates using a learned prior on the individual nine dimensional transformation spaces. Finally, we propose an optional case-adaptive segmentation approach that allows to segment the spinal disks and vertebrae in MR and CT respectively. Since the proposed approaches are learning-based, they can be trained for MR or CT alike. Experimental results based on 42 MR and 30 CT volumes show that our system not only achieves superior accuracy but also is among the fastest systems of its kind in the literature. On the MR data set the spinal disks of a whole spine are detected in 11.5s on average with 98.6% sensitivity and 0.073 false positive detections per volume. On the CT data a comparable sensitivity of 98.0% with 0.267 false positives is achieved. Detected disks are localized with an average position error of 2.4mm/3.2mm and angular error of 3.9°/4.5° in MR/CT, which is close to the employed hypothesis resolution of 2.1mm and 3.3°.}
}
@article{PONIS20201621,
title = {Augmented Reality and Gamification to Increase Productivity and Job Satisfaction in the Warehouse of the Future},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1621-1628},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.226},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320977},
author = {S.T. Ponis and G. Plakas and K. Agalianos and E. Aretoulaki and S.P. Gayialis and A. Andrianopoulos},
keywords = {B2C e-Commerce, Warehouse Logistics, Order Picking, Augmented Reality, Gamification, Job Satisfaction},
abstract = {The emergence of B2C e-commerce in the last decade is currently transforming the way warehouses fulfill online orders with just a few line items, broad and variable product assortments, varying workloads and extremely short delivery schedules. In this new business reality, order picking stands out as the most labor intensive, strenuous, time constrained, repetitive, prone to error and expensive process in the contemporary warehouse. These characteristics undoubtedly put significant strain to the human worker (picker) creating both body fatigue and feelings of monotony and dissatisfaction usually leading to employee underperformance and demotivation. In this paper, we conducted a questionnaire-based survey to supervisors and pickers from different large distribution centers to evaluate the individuals' acceptance of introducing Augmented Reality (AR) in the picking process to support pickers in their everyday tasks, gamification elements as a tool for increasing their motivation and last but not least, a combination of the two approaches.}
}
@article{HU2014207,
title = {Land deformation monitoring in mining area with PPP-AR},
journal = {International Journal of Mining Science and Technology},
volume = {24},
number = {2},
pages = {207-212},
year = {2014},
issn = {2095-2686},
doi = {https://doi.org/10.1016/j.ijmst.2014.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095268614000196},
author = {Hong Hu and Jingxiang Gao and Yifei Yao},
keywords = {Deformation monitoring precise point positioning, Ambiguity, Combination phase delay (CPD), Ionosphere-free combination},
abstract = {The mining area deformation monitoring theory and method using precise point positioning (PPP) ambiguity resolution (AR) were studied, and an ambiguity fixing model with satellite and receiver combination phase delay (CPD) was proposed for zero-differenced PPP ambiguity fixing and its corresponding formula derivation was given. The data processing results for 1h at six IGS stations in China show that 93% of ambiguities can be fixed within 10min and all ambiguities can be fixed within 15min. After ambiguity fixing, the positioning accuracy is improved by more than 85% in the E and N directions, with absolute positioning accuracy reaching millimeter level, and it was improved by 70% in the U direction, reaching centimeter level; the proposed zero-differenced ambiguity fixing model can effectively improve the convergence rate and positioning accuracy in PPP. Data monitoring continuously conducted for half a year at four CORS stations of Shanxi China Coal Pingshuo Group validated the feasibility of using PPP in mining area deformation monitoring.}
}
@article{ONOFREY201729,
title = {Learning Non-rigid Deformations for Robust, Constrained Point-based Registration in Image-Guided MR-TRUS Prostate Intervention},
journal = {Medical Image Analysis},
volume = {39},
pages = {29-43},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300452},
author = {John A. Onofrey and Lawrence H. Staib and Saradwata Sarkar and Rajesh Venkataraman and Cayce B. Nawaf and Preston C. Sprenkle and Xenophon Papademetris},
keywords = {Non-rigid registration, Statistical deformation model, Dimensionality reduction, Prostate, Magnetic resonance imaging, Trans-rectal ultrasound, Image-guided biopsy},
abstract = {Accurate and robust non-rigid registration of pre-procedure magnetic resonance (MR) imaging to intra-procedure trans-rectal ultrasound (TRUS) is critical for image-guided biopsies of prostate cancer. Prostate cancer is one of the most prevalent forms of cancer and the second leading cause of cancer-related death in men in the United States. TRUS-guided biopsy is the current clinical standard for prostate cancer diagnosis and assessment. State-of-the-art, clinical MR-TRUS image fusion relies upon semi-automated segmentations of the prostate in both the MR and the TRUS images to perform non-rigid surface-based registration of the gland. Segmentation of the prostate in TRUS imaging is itself a challenging task and prone to high variability. These segmentation errors can lead to poor registration and subsequently poor localization of biopsy targets, which may result in false-negative cancer detection. In this paper, we present a non-rigid surface registration approach to MR-TRUS fusion based on a statistical deformation model (SDM) of intra-procedural deformations derived from clinical training data. Synthetic validation experiments quantifying registration volume of interest overlaps of the PI-RADS parcellation standard and tests using clinical landmark data demonstrate that our use of an SDM for registration, with median target registration error of 2.98 mm, is significantly more accurate than the current clinical method. Furthermore, we show that the low-dimensional SDM registration results are robust to segmentation errors that are not uncommon in clinical TRUS data.}
}
@article{KIM2017370,
title = {Automatic localization of anatomical landmarks in cardiac MR perfusion using random forests},
journal = {Biomedical Signal Processing and Control},
volume = {38},
pages = {370-378},
year = {2017},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1746809417301349},
author = {Yoon-Chul Kim and Younjoon Chung and Yeon Hyeon Choe},
keywords = {MRI, Dynamic contrast enhanced imaging, Myocardial perfusion, Random forests, Machine learning, Landmark localization},
abstract = {Automatic localization of anatomical landmarks in myocardial perfusion magnetic resonance (MR) data is considered to be a preliminary step toward fully automatic quantification of regional upslope or blood flow in the myocardium. The goal of this work is to develop an automatic method based on supervised learning and detect anatomical landmarks with high accuracy from myocardial perfusion MR data. Dynamic contrast enhanced myocardial MR perfusion data were acquired using a standard perfusion sequence. To effectively extract characteristic features for left ventricle (LV) center point and anterior right ventricle (RV) insertion point, we performed feature extraction of the two landmarks independently from an LV enhancement frame and an RV enhancement frame. Feature extraction of pixel intensity, Sobel gradient, and Haar-like features was performed. Cross-validation from training data was used to build a random forests classifier. We used 38 subjects’ data as training datasets and 21 subjects’ data as test datasets. The proposed method provided high accuracy in localization of the LV center point and anterior RV insertion point. The mean (±SD) localization errors of the proposed method were 3.38 (±2.36) mm for the LV center point and 4.23 (±1.97) mm for the anterior RV insertion point. The proposed method shows the potential to automatically localize anatomical landmarks for the segmental analysis of myocardial perfusion in MRI.}
}
@article{KHANAL201449,
title = {Collaborative virtual reality based advanced cardiac life support training simulator using virtual reality principles},
journal = {Journal of Biomedical Informatics},
volume = {51},
pages = {49-59},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000902},
author = {Prabal Khanal and Akshay Vankipuram and Aaron Ashby and Mithra Vankipuram and Ashish Gupta and Denise Drumm-Gurnee and Karen Josey and Linda Tinker and Marshall Smith},
keywords = {Computer uses in education – , Multimedia information systems – , Serious games, Computer applications in medicine, Advanced cardiac life support, Medical team training},
abstract = {Background
Advanced Cardiac Life Support (ACLS) is a series of team-based, sequential and time constrained interventions, requiring effective communication and coordination of activities that are performed by the care provider team on a patient undergoing cardiac arrest or respiratory failure. The state-of-the-art ACLS training is conducted in a face-to-face environment under expert supervision and suffers from several drawbacks including conflicting care provider schedules and high cost of training equipment.
Objective
The major objective of the study is to describe, including the design, implementation, and evaluation of a novel approach of delivering ACLS training to care providers using the proposed virtual reality simulator that can overcome the challenges and drawbacks imposed by the traditional face-to-face training method.
Methods
We compare the efficacy and performance outcomes associated with traditional ACLS training with the proposed novel approach of using a virtual reality (VR) based ACLS training simulator. One hundred and forty-eight (148) ACLS certified clinicians, translating into 26 care provider teams, were enrolled for this study. Each team was randomly assigned to one of the three treatment groups: control (traditional ACLS training), persuasive (VR ACLS training with comprehensive feedback components), or minimally persuasive (VR ACLS training with limited feedback components). The teams were tested across two different ACLS procedures that vary in the degree of task complexity: ventricular fibrillation or tachycardia (VFib/VTach) and pulseless electric activity (PEA).
Results
The difference in performance between control and persuasive groups was not statistically significant (P=.37 for PEA and P=.1 for VFib/VTach). However, the difference in performance between control and minimally persuasive groups was significant (P=.05 for PEA and P=.02 for VFib/VTach). The pre-post comparison of performances of the groups showed that control (P=.017 for PEA, P=.01 for VFib/VTach) and persuasive (P=.02 for PEA, P=.048 for VFib/VTach) groups improved their performances significantly, whereas minimally persuasive group did not (P=.45 for PEA, P=.46 for VFib/VTach). Results also suggest that the benefit of persuasiveness is constrained by the potentially interruptive nature of these features.
Conclusions
Our results indicate that the VR-based ACLS training with proper feedback components can provide a learning experience similar to face-to-face training, and therefore could serve as a more easily accessed supplementary training tool to the traditional ACLS training. Our findings also suggest that the degree of persuasive features in VR environments have to be designed considering the interruptive nature of the feedback elements.}
}
@article{CHI2022104183,
title = {Rebar inspection integrating augmented reality and laser scanning},
journal = {Automation in Construction},
volume = {136},
pages = {104183},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104183},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000565},
author = {H.-L. Chi and M.-K. Kim and K.-Z. Liu and J.P.P. Thedja and J. Seo and D.-E. Lee},
keywords = {Rebar inspection, Laser scanning, Augmented reality (AR)},
abstract = {Assuring conformity regarding the number, spacing, and location of rebars is an important quality control task in a rebar operation. Nonconformity attributed to missing or mislocated rebars adversely affects structural performance and construct-ability. Rebar craftsmen normally read two-dimensional (2D) shop drawings and install rebars after converting 2D objects to three-dimensional objects in a cognitive image. To ensure conformity, the installation is inspected manually, hence, being tedious and costly. Existing sensing technologies do not lend themselves to effective visualization of the data obtained by the sensing process either in practice or academia. This paper presents an end-to-end method that addresses the existing limitation by hybridizing augmented reality (AR) and laser scanning technologies to provide intuitive and accurate rebar inspection. The AR prototype visualizes rebar inspection outputs and provides rework instructions in an effective manner. An experiment validating the method was performed using a laboratory-scale rebar layout. The results confirmed that the method successfully highlights detailed dimensional information of mislocated rebars and provides inspectors with intuitive rework instructions. Indeed, the method provides a way to detect and repair the nonconformity involved in rebar positions regardless of type, shape, and/or complexity. The method encourages accurate rebar dimensional inspection and intuitive visualization, hence, contributing to effective rebar quality control.}
}
@article{RIVESTHENAULT201556,
title = {Robust inverse-consistent affine CT–MR registration in MRI-assisted and MRI-alone prostate radiation therapy},
journal = {Medical Image Analysis},
volume = {23},
number = {1},
pages = {56-69},
year = {2015},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2015.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S136184151500064X},
author = {David Rivest-Hénault and Nicholas Dowson and Peter B. Greer and Jurgen Fripp and Jason A. Dowling},
keywords = {Prostate cancer, Radiation therapy, Robust, Multimodal registration, Inverse-consistent registration},
abstract = {Background: CT–MR registration is a critical component of many radiation oncology protocols. In prostate external beam radiation therapy, it allows the propagation of MR-derived contours to reference CT images at the planning stage, and it enables dose mapping during dosimetry studies. The use of carefully registered CT–MR atlases allows the estimation of patient specific electron density maps from MRI scans, enabling MRI-alone radiation therapy planning and treatment adaptation. In all cases, the precision and accuracy achieved by registration influences the quality of the entire process. Problem: Most current registration algorithms do not robustly generalize and lack inverse-consistency, increasing the risk of human error and acting as a source of bias in studies where information is propagated in a particular direction, e.g. CT to MR or vice versa. In MRI-based treatment planning where both CT and MR scans serve as spatial references, inverse-consistency is critical, if under-acknowledged. Purpose: A robust, inverse-consistent, rigid/affine registration algorithm that is well suited to CT–MR alignment in prostate radiation therapy is presented. Method: The presented method is based on a robust block-matching optimization process that utilises a half-way space definition to maintain inverse-consistency. Inverse-consistency substantially reduces the influence of the order of input images, simplifying analysis, and increasing robustness. An open source implementation is available online at http://aehrc.github.io/Mirorr/. Results: Experimental results on a challenging 35 CT–MR pelvis dataset demonstrate that the proposed method is more accurate than other popular registration packages and is at least as accurate as the state of the art, while being more robust and having an order of magnitude higher inverse-consistency than competing approaches. Conclusion: The presented results demonstrate that the proposed registration algorithm is readily applicable to prostate radiation therapy planning.}
}
@article{AZPIROZLEEHAN2000335,
title = {Selection of biorthogonal filters for image compression of MR images using wavelet packets},
journal = {Medical Engineering & Physics},
volume = {22},
number = {5},
pages = {335-343},
year = {2000},
issn = {1350-4533},
doi = {https://doi.org/10.1016/S1350-4533(00)00042-4},
url = {https://www.sciencedirect.com/science/article/pii/S1350453300000424},
author = {J Azpiroz-Leehan and J.-F Lerallut},
keywords = {Image compression, Wavelet packets, Magnetic resonance imaging},
abstract = {We present an analysis of different filter banks for the compression of magnetic resonance (MR) images of the human brain using wavelet packets based on biorthogonal filters. Initially, peak signal to noise ratio (PSNR) and normalized root mean square (RMS) error criteria are calculated for a series of images compressed with a 33:1 ratio, using filter banks based on biorthogonal wavelet packets. The results lead us to choose a few of these filter banks as optimal for image compression. One of these filters is employed to compress several images at four different compression ratios: 12.5:1, 25:1, 37.5:1 and 50:1. The quality of these images was evaluated by visual analysis by a group of seven experts who graded image quality on a 0–7 scale. Results show that using these filters, we can compress images to a rate of around 30:1 without introducing noticeable differences. Other applications for these filters are currently under study and include the compression/fusion of MR image stacks in order to obtain even better reductions in the amount of data needed to reconstruct complete MRI studies.}
}
@article{KHORASANI2023100037,
title = {Hands-on or hands-off: Deciphering the impact of interactivity on embodied learning in VR},
journal = {Computers & Education: X Reality},
volume = {3},
pages = {100037},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000314},
author = {Sara Khorasani and Brandon {Victor Syiem} and Sadia Nawaz and Jarrod Knibbe and Eduardo Velloso},
keywords = {Virtual reality, Embodied learning, VR learning, VR education, VR training, Interaction techniques, Interaction fidelity, Movement in VR, Learning analytics},
abstract = {Studies suggest that Sense of Embodiment (SoE) enabled by VR promotes embodied and active learning. However, it is unclear what features of VR learning environments tap into the concept of embodied learning. For example, interaction techniques, movement and purely observational scenarios in VR can all play a role in facilitating embodied learning. To understand how these mechanisms impact learning, we conducted 2 studies with a total of 64 participants who had no prior experience in the training task. Participants were taught how to use a table saw in 4 conditions and were tested on their task performance in a fully interactive VR assessment. The conditions were analyzed in pairs; 2 conditions with different interaction techniques, 2 conditions with differing ability to move and a cross-study analysis comparing conditions with purely observational learning to interactive learning. We used a mixed methods approach; Analysis of Variance (ANOVA), pairwise comparison of the learning outcomes in each condition as well as thematic analysis of the interview results. We found that some types of “hands-on” interactions can have a detrimental impact on learning and that observational learning can be as impactful as a fully interactive experience. Based on participant interviews, we explored how these mechanisms of the learning environment can impact participants’ learning ability.}
}
@article{AOKI2008841,
title = {Desktop-VR system for preflight 3D navigation training},
journal = {Acta Astronautica},
volume = {63},
number = {7},
pages = {841-847},
year = {2008},
note = {From Dream to Reality: Living, Working and Creating for Humans in Space - A selection of papers presented at the 16th IAA Humans in Space Symposium, Beijing, China, 2007},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2007.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0094576507002883},
author = {Hirofumi Aoki and Charles M. Oman and Daniel A. Buckland and Alan Natapoff},
keywords = {Spatial orientation, Navigation, Training, Virtual reality, Emergency egress},
abstract = {Crews who inhabit spacecraft with complex 3D architecture frequently report inflight disorientation and navigation problems. Preflight virtual reality (VR) training may reduce those risks. Although immersive VR techniques may better support spatial orientation training in a local environment, a non-immersive desktop (DT) system may be more convenient for navigation training in “building scale” spaces, especially if the two methods achieve comparable results. In this study trainees’ orientation and navigation performance during simulated space station emergency egress tasks was compared while using immersive head-mounted display (HMD) and DT-VR systems. Analyses showed no differences in pointing angular-error or egress time among the groups. The HMD group was significantly faster than DT group when pointing from destination to start location and from start toward different destination. However, this may be attributed to differences in the input device used (a head-tracker for HMD group vs. a keyboard touchpad or a gamepad in the DT group). All other 3D navigation performance measures were similar using the immersive and non-immersive VR systems, suggesting that the simpler desktop VR system may be useful for astronaut 3D navigation training.}
}
@article{VERHULST2021106951,
title = {Do VR and AR versions of an immersive cultural experience engender different user experiences?},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106951},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106951},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002740},
author = {Isabelle Verhulst and Andy Woods and Laryssa Whittaker and James Bennett and Polly Dalton},
keywords = {Virtual reality, Augmented reality, User experience, Presence, Enjoyment, Engagement},
abstract = {Although Virtual Reality (VR) and Augmented Reality (AR) user experiences have received large amounts of recent research interest, a direct comparison of different immersive technologies' user experiences has not often been conducted. This study compared user experiences of one VR and two AR versions of an immersive gallery experience ‘Virtual Veronese’, measuring multiple aspects of user experience, including enjoyment, presence, cognitive, emotional and behavioural engagement, using a between-subjects design, at the National Gallery in London, UK. Analysis of the self-reported survey data (N = 368) showed that enjoyment was high on all devices, with the Oculus Quest (VR) receiving higher mean scores than both AR devices, Magic Leap and Mira Prism. In relation to presence, the elements ‘spatial presence’, ‘involvement’, and ‘sense of being there’ received a higher mean score on the Oculus Quest than on both AR devices, and on ‘realism’ the Oculus Quest scored significantly higher than the Magic Leap. Cognitive engagement was similar between the three devices, with only ‘I knew what to do’ being rated higher for Quest than Mira Prism. Emotional engagement was similar between the devices. Behavioural engagement was high on all devices, with only ‘I would like to see more experiences like this’ being higher for Oculus Quest than Mira Prism. Negative effects including nausea were rarely reported. Differences in user experiences were likely partly driven by differences in immersion levels between the devices.}
}
@article{BAEK2024106085,
title = {Validation of tremor measurements using quantified drawing analysis in patients with essential tremor or Parkinson’s disease treated with MR-guided focused ultrasound thalamotomy},
journal = {Biomedical Signal Processing and Control},
volume = {92},
pages = {106085},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106085},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424001435},
author = {Hongchae Baek and Daniel Lockwood and Emmanuel Obusez and Matthew Poturalski and Jacqueline Chen and Sean J. Nagel and Stephen E. Jones},
keywords = {MR-guided focused ultrasound, Thalamotomy, Essential tremor, Parkinson’s disease, Kinetic tremor, Drawing analysis, Long-term follow up},
abstract = {Background
During MR–guided focused ultrasound (MRgFUS) thalamotomy, patients’ tremors are qualitatively assessed with drawing tests to provide near-real-time feedback on the treatment effect. The objective of this study was to validate an automated measure of kinetic tremor in patients with essential tremor (ET) or Parkinson’s disease (PD) by computerized analysis of standardized drawings.
Methods
Drawings were performed in three settings: (1) patients supine in the MR scanner during the procedure, (2) patients seated immediately before and after treatment, and (3) months after discharge. Drawings comprised of a signature, the Archimedes spiral, and a straight line. Once the drawings were digitized, the automated analysis was used to calculate quantitative metrics. For validation, the composite scores were compared with wrist angle simultaneously measured using accelerometers during the patients’ drawings.
Results
Quantitative drawing scores for the spiral and line showed significant improvement immediately after thalamotomy in patients with ET (spiral: p = 0.0002; line: p = 0.017) and PD (spiral: p = 0.12; line: p = 0.016); no significant change occurred in the months after treatment. In addition, quantitative drawing scores for spirals and lines were correlated with accelerometer-measured wrist angle in ET (spiral: R2 = 0.51, p = 0.029; line: R2 = 0.63, p = 0.004) and PD (spiral: R2 = 0.85, p = 0.0008; line: R2 = 0.69, p = 0.017) groups.
Conclusions
Quantitative drawing scores for patients with ET or PD accurately reflect the degree of tremor and can be used for objective assessment of tremor improvement during and after treatment.}
}
@article{WOO2024103089,
title = {Automated anomaly-aware 3D segmentation of bones and cartilages in knee MR images from the Osteoarthritis Initiative},
journal = {Medical Image Analysis},
volume = {93},
pages = {103089},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103089},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524000148},
author = {Boyeong Woo and Craig Engstrom and William Baresic and Jurgen Fripp and Stuart Crozier and Shekhar S. Chandra},
keywords = {Anomaly detection, Segmentation, U-Net, Knee osteoarthritis, MRI},
abstract = {In medical image analysis, automated segmentation of multi-component anatomical entities, with the possible presence of variable anomalies or pathologies, is a challenging task. In this work, we develop a multi-step approach using U-Net-based models to initially detect anomalies (bone marrow lesions, bone cysts) in the distal femur, proximal tibia and patella from 3D magnetic resonance (MR) images in individuals with varying grades of knee osteoarthritis. Subsequently, the extracted data are used for downstream tasks involving semantic segmentation of individual bone and cartilage volumes as well as bone anomalies. For anomaly detection, U-Net-based models were developed to reconstruct bone volume profiles of the femur and tibia in images via inpainting so anomalous bone regions could be replaced with close to normal appearances. The reconstruction error was used to detect bone anomalies. An anomaly-aware segmentation network, which was compared to anomaly-naïve segmentation networks, was used to provide a final automated segmentation of the individual femoral, tibial and patellar bone and cartilage volumes from the knee MR images which contain a spectrum of bone anomalies. The anomaly-aware segmentation approach provided up to 58% reduction in Hausdorff distances for bone segmentations compared to the results from anomaly-naïve segmentation networks. In addition, the anomaly-aware networks were able to detect bone anomalies in the MR images with greater sensitivity and specificity (area under the receiver operating characteristic curve [AUC] up to 0.896) compared to anomaly-naïve segmentation networks (AUC up to 0.874).}
}
@article{PAIGE2023152,
title = {MIT Zero-G outreach initiative: Using experiment design and virtual reality to inspire the next generation of space scientists and engineers},
journal = {Acta Astronautica},
volume = {212},
pages = {152-159},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2023.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0094576523003788},
author = {C. Paige and F. Ward and D.D. Haddad and J. MacNeil and P. McGaffigan and A. Ekblaw and D. Newman},
keywords = {Outreach, Microgravity, Virtual reality, Parabolic flight, Experiment design},
abstract = {MIT's Space Exploration Initiative offers a course on project development, prototyping, and deployment readiness for parabolic flights, culminating in an annually chartered research flight with Zero-G. MIT's Resource Exploration and Science of our Cosmic Environment (RESOURCE) team participated in the course in the fall of 2021 with a Zero-G flight in May of 2022 testing technology for a virtual reality platform to enable science on Lunar rover exploration missions. In parallel with the scientific effort, the team developed the MIT Zero-G Outreach Initiative (0G-OI), an outreach program to engage with the Cambridge Public School's (CPS) grade 7 classes to teach them about research in microgravity, parabolic flight, and experiment design. The goal of the outreach program is to inspire the next generation of space scientists and engineers using virtual reality. The CPS grade 7 curriculum covers ‘Mysteries of the Universe’ where students consider the role of gravity in the solar system. They also complete a unit on roller coasters in which they learn about key ideas of force, motion, and energy through the context of roller coasters. The 0G-OI Initiative ties these two units together through five videos covering: 1) an overview of the initiative, 2) the basics of parabolic flight and how we perceive gravity, 3) an overview of gravity and its function in the universe, 4) why and what we can study in microgravity and 5) how to design an experiment for microgravity. The students then design their own experiments for microgravity. These are down-selected to the top 5 experiments that are reviewed by an astronaut to select one experiment to fly on the MIT chartered Zero-G parabolic flight. The flight and the experiment are filmed in VR video by the MIT RESOURCE team. Each participating school is provided with an Oculus Quest 2 VR headset and a flight day is held at each school where the students experience the Zero-G flight and see the results of their experiment in an immersive VR environment. Using this initial setup, the 0G-OI can be run annually in parallel with MIT's SEI parabolic flight course. Some key development parameters included keeping the educational videos under 10 min, that having an astronaut select the winning experiment provided incentive and finally, using VR, as this technology is commonly associated with gaming and entertainment. Lessons learned focused on improvements to scheduling and coordination with Zero-G, in-person programming and classroom experience optimization. The 0G-OI aims to develop a lasting relationship with the CPSs and make the exciting experience of parabolic flight and space exploration an accessible experience.}
}
@article{LANIER201970,
title = {Virtual reality check: Statistical power, reported results, and the validity of research on the psychology of virtual reality and immersive environments},
journal = {Computers in Human Behavior},
volume = {100},
pages = {70-78},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302419},
author = {Madison Lanier and T. Franklin Waddell and Malte Elson and Daniel J. Tamul and James D. Ivory and Andrew Przybylski},
keywords = {Virtual reality, meta-Science, Statistical power, Questionable research practices, Quantitative methodology},
abstract = {Virtual reality (VR) is a popular subject of scientific study across a variety of academic fields. In the present study we evaluate methodological trends in behavioral research on VR with respect to data collection practices, statistical reporting, and data availability. In line with this goal, we conducted a meta-scientific analysis of 61 articles encompassing a total of 1122 statistical tests and highlight three emergent trends that inform our understanding of past and future studies focused on VR. Conclusions from analysis of the data include a high incidence of errors in statistical reporting, and a general lack of transparency with respect to the availability of study data. Transparency in data analysis, increased statistical power, and more careful reporting of statistical outcomes are suggested to heighten methodological rigor and improve reproducibility in the field of VR research.}
}
@article{LEE20131256,
title = {Breast lesion co-localisation between X-ray and MR images using finite element modelling},
journal = {Medical Image Analysis},
volume = {17},
number = {8},
pages = {1256-1264},
year = {2013},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2013.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1361841513000856},
author = {Angela W.C. Lee and Vijayaraghavan Rajagopal and Thiranja P. {Babarenda Gamage} and Anthony J. Doyle and Poul M.F. Nielsen and Martyn P. Nash},
keywords = {Breast biomechanical models, Breast image registration, Finite element modelling, Multimodal breast image registration},
abstract = {This paper presents a novel X-ray and MR image registration technique based on individual-specific biomechanical finite element (FE) models of the breasts. Information from 3D magnetic resonance (MR) images was registered to X-ray mammographic images using non-linear FE models subject to contact mechanics constraints to simulate the large compressive deformations between the two imaging modalities. A physics-based perspective ray-casting algorithm was used to generate 2D pseudo-X-ray projections of the FE-warped 3D MR images. Unknown input parameters to the FE models, such as the location and orientation of the compression plates, were optimised to provide the best match between the pseudo and clinical X-ray images. The methods were validated using images taken before and during compression of a breast-shaped phantom, for which 12 inclusions were tracked between imaging modalities. These methods were then applied to X-ray and MR images from six breast cancer patients. Error measures (such as centroid and surface distances) of segmented tumours in simulated and actual X-ray mammograms were used to assess the accuracy of the methods. Sensitivity analysis of the lesion co-localisation accuracy to rotation about the anterior–posterior axis was then performed. For 10 of the 12 X-ray mammograms, lesion localisation accuracies of 14mm and less were achieved. This analysis on the rotation about the anterior–posterior axis indicated that, in cases where the lesion lies in the plane parallel to the mammographic compression plates, that cuts through the nipple, such rotations have relatively minor effects.This has important implications for clinical applicability of this multi-modality lesion registration technique, which will aid in the diagnosis and treatment of breast cancer.}
}
@article{COUPE2010483,
title = {Robust Rician noise estimation for MR images},
journal = {Medical Image Analysis},
volume = {14},
number = {4},
pages = {483-493},
year = {2010},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2010.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841510000241},
author = {Pierrick Coupé and José V. Manjón and Elias Gedamu and Douglas Arnold and Montserrat Robles and D. Louis Collins},
keywords = {Rician noise, MR imaging, Wavelet, Noise estimation, MRI},
abstract = {In this paper, a new object-based method to estimate noise in magnitude MR images is proposed. The main advantage of this object-based method is its robustness to background artefacts such as ghosting. The proposed method is based on the adaptation of the Median Absolute Deviation (MAD) estimator in the wavelet domain for Rician noise. The MAD is a robust and efficient estimator initially proposed to estimate Gaussian noise. In this work, the adaptation of MAD operator for Rician noise is performed by using only the wavelet coefficients corresponding to the object and by correcting the estimation with an iterative scheme based on the SNR of the image. During the evaluation, a comparison of the proposed method with several state-of-the-art methods is performed. A quantitative validation on synthetic phantom with and without artefacts is presented. A new validation framework is proposed to perform quantitative validation on real data. The impact of the accuracy of noise estimation on the performance of a denoising filter is also studied. The results obtained on synthetic images show the accuracy and the robustness of the proposed method. Within the validation on real data, the proposed method obtained very competitive results compared to the methods under study.}
}
@article{CHEN2024102353,
title = {SC-GAN: Structure-completion generative adversarial network for synthetic CT generation from MR images with truncated anatomy},
journal = {Computerized Medical Imaging and Graphics},
volume = {113},
pages = {102353},
year = {2024},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102353},
url = {https://www.sciencedirect.com/science/article/pii/S0895611124000302},
author = {Xinru Chen and Yao Zhao and Laurence E. Court and He Wang and Tinsu Pan and Jack Phan and Xin Wang and Yao Ding and Jinzhong Yang},
keywords = {MRI, Synthetic CT, MR-guided adaptive radiotherapy, Cycle consistent generative adversarial networks, Deep learning},
abstract = {Creating synthetic CT (sCT) from magnetic resonance (MR) images enables MR-based treatment planning in radiation therapy. However, the MR images used for MR-guided adaptive planning are often truncated in the boundary regions due to the limited field of view and the need for sequence optimization. Consequently, the sCT generated from these truncated MR images lacks complete anatomic information, leading to dose calculation error for MR-based adaptive planning. We propose a novel structure-completion generative adversarial network (SC-GAN) to generate sCT with full anatomic details from the truncated MR images. To enable anatomy compensation, we expand input channels of the CT generator by including a body mask and introduce a truncation loss between sCT and real CT. The body mask for each patient was automatically created from the simulation CT scans and transformed to daily MR images by rigid registration as another input for our SC-GAN in addition to the MR images. The truncation loss was constructed by implementing either an auto-segmentor or an edge detector to penalize the difference in body outlines between sCT and real CT. The experimental results show that our SC-GAN achieved much improved accuracy of sCT generation in both truncated and untruncated regions compared to the original cycleGAN and conditional GAN methods.}
}
@article{SHAHZAD201744,
title = {Fully-automatic left ventricular segmentation from long-axis cardiac cine MR scans},
journal = {Medical Image Analysis},
volume = {39},
pages = {44-55},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300567},
author = {Rahil Shahzad and Qian Tao and Oleh Dzyubachyk and Marius Staring and Boudewijn P.F. Lelieveldt and Rob J. {van der Geest}},
keywords = {Atlas-based segmentation, Registration, Cardiac MRI, Left ventricular segmentation, Long-axis cine MRI},
abstract = {With an increasing number of large-scale population-based cardiac magnetic resonance (CMR) imaging studies being conducted nowadays, there comes the mammoth task of image annotation and image analysis. Such population-based studies would greatly benefit from automated pipelines, with an efficient CMR image analysis workflow. The purpose of this work is to investigate the feasibility of using a fully-automatic pipeline to segment the left ventricular endocardium and epicardium simultaneously on two orthogonal (vertical and horizontal) long-axis cardiac cine MRI scans. The pipeline is based on a multi-atlas-based segmentation approach and a spatio-temporal registration approach. The performance of the method was assessed by: (i) comparing the automatic segmentations to those obtained manually at both the end-diastolic and end-systolic phase, (ii) comparing the automatically obtained clinical parameters, including end-diastolic volume, end-systolic volume, stroke volume and ejection fraction, with those defined manually and (iii) by the accuracy of classifying subjects to the appropriate risk category based on the estimated ejection fraction. Automatic segmentation of the left ventricular endocardium was achieved with a Dice similarity coefficient (DSC) of 0.93 on the end-diastolic phase for both the vertical and horizontal long-axis scan; on the end-systolic phase the DSC was 0.88 and 0.85, respectively. For the epicardium, a DSC of 0.94 and 0.95 was obtained on the end-diastolic vertical and horizontal long-axis scans; on the end-systolic phase the DSC was 0.90 and 0.88, respectively. With respect to the clinical volumetric parameters, Pearson correlation coefficient (R) of 0.97 was obtained for the end-diastolic volume, 0.95 for end-systolic volume, 0.87 for stroke volume and 0.84 for ejection fraction. Risk category classification based on ejection fraction showed that 80% of the subjects were assigned to the correct risk category and only one subject (< 1%) was more than one risk category off. We conclude that the proposed automatic pipeline presents a viable and cost-effective alternative for manual annotation.}
}
@article{WANG2023104112,
title = {Usability evaluation of augmented reality visualizations on an optical see-through head-mounted display for assisting machine operations},
journal = {Applied Ergonomics},
volume = {113},
pages = {104112},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104112},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023001503},
author = {Chao-Hung Wang and Chih-Yu Hsiao and An-Ting Tai and Mao-Jiun J. Wang},
keywords = {Augmented reality, Optical see-through head-mounted display, Visual information},
abstract = {This study explores the effect of using different visual information overlays and guiding arrows on a machine operation task with an optical see-through head-mounted display (OST-HMD). Thirty-four participants were recruited in the experiment. The independent variables included visual information mode (text, animation, and mixed text and animation) and the use of guiding arrows (with and without arrows). In addition, gender difference was also an objective of this study. The task performance indicators were determined based on task completion time and error counts as well as subjective measures (system usability scale, NASA task load index, and immersion scale). This study used the mixed analysis of variance design to evaluate the main and interaction effects. The results showed that males performed better when using the mixed text and animation mode. Females performed better when using the text mode. In addition, using the mixed text and animation mode demonstrated the best outcome in system usability scale and NASA task load index. For the use of guiding arrows, the task completion time was reduced and the system usability scale, NASA task load index, and immersion scale showed positive effects.}
}
@article{JUAN2010756,
title = {Using augmented and virtual reality for the development of acrophobic scenarios. Comparison of the levels of presence and anxiety},
journal = {Computers & Graphics},
volume = {34},
number = {6},
pages = {756-766},
year = {2010},
note = {Graphics for Serious Games Computer Graphics in Spain: a Selection of Papers from CEIG 2009 Selected Papers from the SIGGRAPH Asia Education Program},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2010.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849310001263},
author = {M. Carmen Juan and David Pérez},
keywords = {Augmented reality, Acrophobia},
abstract = {Acrophobia has been treated using exposure in imagination, exposure “in vivo” and Virtual Reality (VR). This paper presents the development of an Augmented Reality (AR) system and a VR system that includes acrophobic scenarios. A study involving both these systems and non-phobic users has been carried out in order to compare the levels of presence and anxiety. In the acrophobic scenario, the floor fell away and the walls rose up. 20 participants took part in this study. After using each system (AR or VR), the participants were asked to fill out an adapted SUS questionnaire [26], and paired t-tests and ANOVA analyses were applied to the data obtained. For the sense of presence and anxiety levels, we did not find differences between the systems using an experiment with enough sensitivity to detect differences as large as d=0.66. For the anxiety level, the results show that there is a significant difference between the level of anxiety felt at the moment before starting the experiment and the level felt during the different stages of the experiment. For the correlation between anxiety and presence, the results show very low correlation between anxiety and presence. These results suggest that AR is likely to be as effective as VR in treating acrophobia.}
}
@article{NILKAR2019107482,
title = {Effects of annealing atmospheres (Ar, N2 and air) on structural, morphological, and surface corrosion properties of a-C:H thin films},
journal = {Diamond and Related Materials},
volume = {98},
pages = {107482},
year = {2019},
issn = {0925-9635},
doi = {https://doi.org/10.1016/j.diamond.2019.107482},
url = {https://www.sciencedirect.com/science/article/pii/S0925963519303164},
author = {M. Nilkar and F.E. Ghodsi},
keywords = {Amorphous hydrogenated carbon (a-C:H) films, Spin coating, Corrosion resistance, Potentiodynamic test, PVP (Polyvinylpyrrolidone)},
abstract = {In the present study, amorphous hydrogenated carbon (a-C:H) thin films were successfully prepared on Corning glass and stainless steel (SS) substrates via spin coating technique from a polymeric solution through thermal decomposition. For the first time, the effects of annealing atmospheres consisting of argon, nitrogen, and air on the structural, vibrational, and morphological properties of the films deposited on Corning glass substrates were investigated by Raman spectroscopy, FTIR spectroscopy, AFM, and FE-SEM. The surface corrosion tests of uncoated and a-C:H coated stainless steel (SS) substrates were evaluated by potentiodynamic polarization analysis. The results indicated that diamond like carbon (DLC) character, surface roughness as well as surface corrosion resistance behavior of the specimens depend on the type of annealing atmosphere. The films annealed under N2 exhibited smoother surface with lower sp3 fraction, while annealing in Ar caused to increase roughness with the higher sp3 fraction. It was also found that in all the samples, a-C:H coating acted as a proper barrier between the substrate and the corrosive medium. Especially, Ar-annealed a-C:H coating could be a candidate as a surface corrosion-resistance coating due to low surface corrosion current density and broad passivity region of anodic polarization branch. Furthermore, the fabricated a-C:H thin films involved with high film thickness controllability and good adhesion without needing the interlayer materials.}
}
@article{SEKARAN2009242,
title = {The effect of volume sub-sampling on motion estimation of joints via MR imaging},
journal = {Computerized Medical Imaging and Graphics},
volume = {33},
number = {3},
pages = {242-246},
year = {2009},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2009.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0895611109000032},
author = {D. Sekaran},
keywords = {, Joint imaging, Joint kinematics, Image sub-sampling, Registration},
abstract = {The technique of three-dimensional imaging is being explored as a means of better understanding the morphology and kinematics of the foot and ankle. To capture information about the dynamic nature of the joint, MRI pulse sequences are used to rapidly acquire single-slice kinematic images, which are then used to track the motion of the bone or other tissues. This approach cannot capture true 3D motion information. On the other hand, full 3D acquisitions are time consuming. A more time-efficient alternative to this method that may give accurate 3D motion information may be to use select MRI slices, instead of full resolution 3D models, that may be just enough to capture the vital information needed to track motion. This was tested by removing slices from already acquired full kinematic MRI datasets and progressively removing slices to determine up to what level data can be eliminated and still achieve accurate motion tracking. We evaluated the ability of the reduced data set in tracking motion in terms of both volume overlap and actual motion estimated, and compared these with the results from the full resolution data. We based our analysis of accuracy on the ability to transform the reduced images from one position of the foot to another. In tracking the motion of the bones of the tarsal joints, we were able to reduce the number of slices to about 25% of the full data set while maintaining an accurate representation of motion within about 0.5mm of translation and 0.5 degrees of rotation of the motion estimated from full data.}
}
@article{SUINESIAPUTRA201450,
title = {A collaborative resource to build consensus for automated left ventricular segmentation of cardiac MR images},
journal = {Medical Image Analysis},
volume = {18},
number = {1},
pages = {50-62},
year = {2014},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2013.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1361841513001217},
author = {Avan Suinesiaputra and Brett R. Cowan and Ahmed O. Al-Agamy and Mustafa A. Elattar and Nicholas Ayache and Ahmed S. Fahmy and Ayman M. Khalifa and Pau Medrano-Gracia and Marie-Pierre Jolly and Alan H. Kadish and Daniel C. Lee and Ján Margeta and Simon K. Warfield and Alistair A. Young},
keywords = {Segmentation challenge, Consensus images, LV myocardium},
abstract = {A collaborative framework was initiated to establish a community resource of ground truth segmentations from cardiac MRI. Multi-site, multi-vendor cardiac MRI datasets comprising 95 patients (73 men, 22 women; mean age 62.73±11.24years) with coronary artery disease and prior myocardial infarction, were randomly selected from data made available by the Cardiac Atlas Project (Fonseca et al., 2011). Three semi- and two fully-automated raters segmented the left ventricular myocardium from short-axis cardiac MR images as part of a challenge introduced at the STACOM 2011 MICCAI workshop (Suinesiaputra et al., 2012). Consensus myocardium images were generated based on the Expectation–Maximization principle implemented by the STAPLE algorithm (Warfield et al., 2004). The mean sensitivity, specificity, positive predictive and negative predictive values ranged between 0.63 and 0.85, 0.60 and 0.98, 0.56 and 0.94, and 0.83 and 0.92, respectively, against the STAPLE consensus. Spatial and temporal agreement varied in different amounts for each rater. STAPLE produced high quality consensus images if the region of interest was limited to the area of discrepancy between raters. To maintain the quality of the consensus, an objective measure based on the candidate automated rater performance distribution is proposed. The consensus segmentation based on a combination of manual and automated raters were more consistent than any particular rater, even those with manual input. The consensus is expected to improve with the addition of new automated contributions. This resource is open for future contributions, and is available as a test bed for the evaluation of new segmentation algorithms, through the Cardiac Atlas Project (www.cardiacatlas.org).}
}
@article{ZHANG20173,
title = {Concatenated spatially-localized random forests for hippocampus labeling in adult and infant MR brain images},
journal = {Neurocomputing},
volume = {229},
pages = {3-12},
year = {2017},
note = {Advances in computing techniques for big medical image data},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.05.082},
url = {https://www.sciencedirect.com/science/article/pii/S092523121630546X},
author = {Lichi Zhang and Qian Wang and Yaozong Gao and Hongxin Li and Guorong Wu and Dinggang Shen},
keywords = {Image segmentation, Random forest, Brain MR images, Atlas selection, Clustering},
abstract = {Automatic labeling of the hippocampus in brain MR images is highly demanded, as it has played an important role in imaging-based brain studies. However, accurate labeling of the hippocampus is still challenging, partially due to the ambiguous intensity boundary between the hippocampus and surrounding anatomies. In this paper, we propose a concatenated set of spatially-localized random forests for multi-atlas-based hippocampus labeling of adult/infant brain MR images. The contribution in our work is two-fold. First, each forest classifier is trained to label just a specific sub-region of the hippocampus, thus enhancing the labeling accuracy. Second, a novel forest selection strategy is proposed, such that each voxel in the test image can automatically select a set of optimal forests, and then dynamically fuses their respective outputs for determining the final label. Furthermore, we enhance the spatially-localized random forests with the aid of the auto-context strategy. In this way, our proposed learning framework can gradually refine the tentative labeling result for better performance. Experiments show that, regarding the large datasets of both adult and infant brain MR images, our method owns satisfactory scalability by segmenting the hippocampus accurately and efficiently.}
}
@article{SHU2022107568,
title = {A harmonic impedance estimation method based on AR model and Burg algorithm},
journal = {Electric Power Systems Research},
volume = {202},
pages = {107568},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2021.107568},
url = {https://www.sciencedirect.com/science/article/pii/S0378779621005496},
author = {Qin Shu and Yu Fan and Fangwei Xu and Chang Wang and Jinyan He},
keywords = {Smart grid, Utility impedance estimation, AR model, Burg algorithm, Apparent power},
abstract = {With the development of smart grid, a lot of nonlinear loads and new energy power stations are connected into the power network. There are a large number of harmonic sources in the power grid, harmonic distortion has become one of the main power quality concerns in recent years. Utility side harmonic impedance estimation is important for evaluating harmonic emission levels and determining the harmonic responsibility. In the traditional methods, the harmonic voltage and current at the PCC are considered to satisfy the normal distribution and the timing correlation of them is ignored which contains important information that can be used to estimate the harmonic impedance. This paper proposes a new method for harmonic impedance estimation based on the Burg algorithm and AR model. In this paper, harmonic voltage and current are considered to satisfy AR model including timing correlation. The reflection coefficient is calculated by minimizing the average power of the forward and backward errors in the AR model. Then, the utility side harmonic impedance is estimated by minimizing the apparent power. The proposed method is robust, and the error of the estimation result is small by the proposed method. Whats more, the proposed method is not limited by some traditional assumptions, such as the background harmonics are stable and small, the customer harmonic impedance is much larger than the utility, and the harmonic sources of both sides are independent, and so on.}
}
@article{LIN20111036,
title = {Adaptive pixon represented segmentation (APRS) for 3D MR brain images based on mean shift and Markov random fields},
journal = {Pattern Recognition Letters},
volume = {32},
number = {7},
pages = {1036-1043},
year = {2011},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2011.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167865511000596},
author = {Lei Lin and Daniel Garcia-Lorenzo and Chong Li and Tianzi Jiang and Christian Barillot},
keywords = {MRI segmentation, Markov random field, Adaptive mean shift, Pixon-representation, EM algorithm},
abstract = {In this paper, we proposed an adaptive pixon represented segmentation (APRS) algorithm for 3D magnetic resonance (MR) brain images. Different from traditional method, an adaptive mean shift algorithm was adopted to adaptively smooth the query image and create a pixon-based image representation. Then K-means algorithm was employed to provide an initial segmentation by classifying the pixons in image into a predefined number of tissue classes. By using this segmentation as initialization, expectation-maximization (EM) iterations composed of bias correction, a priori digital brain atlas information, and Markov random field (MRF) segmentation were processed. Pixons were assigned with final labels when the algorithm converges. The adoption of bias correction and brain atlas made the current method more suitable for brain image segmentation than the previous pixon based segmentation algorithm. The proposed method was validated on both simulated normal brain images from BrainWeb and real brain images from the IBSR public dataset. Compared with some other popular MRI segmentation methods, the proposed method exhibited a higher degree of accuracy in segmenting both simulated and real 3D MRI brain data. The experimental results were numerically assessed using Dice and Tanimoto coefficients.}
}
@article{FERREIRA2019107521,
title = {Diamond-like carbon coatings deposited by deep oscillation magnetron sputtering in Ar-Ne discharges},
journal = {Diamond and Related Materials},
volume = {98},
pages = {107521},
year = {2019},
issn = {0925-9635},
doi = {https://doi.org/10.1016/j.diamond.2019.107521},
url = {https://www.sciencedirect.com/science/article/pii/S0925963519303954},
author = {Fábio Ferreira and Ricardo Serra and Albano Cavaleiro and João Oliveira},
keywords = {DLC, DOMS, Ne, Hardness, Wear resistance},
abstract = {In a previous work, the authors have shown that a recently developed variant of high power impulse magnetron sputtering (HiPIMS), called deep oscillation magnetron sputtering (DOMS), was suitable for the deposition of hard hydrogen-free DLC films with tribological properties comparable to DLC films deposited by other state of the art deposition processes. Adding Ne to the discharge gas has been shown to be an effective method to increase the ionization fraction of the sputtered carbon species in an HiPIMS discharge. In this work, the effect of adding Ne to the discharge gas on the film growth processes and on the tribological performance of the films. On the overall, the substitution of Ar by Ne in the discharge gas up to 50% results in a progressive transformation of the dense columnar microstructure obtained in pure Ar into a featureless one. On the other hand, the hardness of the films increases from 14 to 22 GPa while a progressive increase of the sp3 content of the films is detected by Raman spectroscopy. The addition of Ne to the discharge gas results in much lower specific wear rates, reaching a minimum value of 4 × 10−17 m3/Nm, without any significant increase of the films residual stresses. Moreover, the higher wear resistance of the Ne films is achieved with only a small increase of the coefficient of friction, which remains close to 0.15, i.e., within the range of typical values for DLC films tested in relatively humid conditions. Thus the tribological properties of the DLC films deposited in this work are already interesting for application in the automotive industry, such as for the replacement of the CrN coatings nowadays standardly deposited onto the piston rings of internal combustion engines.}
}
@article{AGHABIGLOU2021106151,
title = {Projection-Based cascaded U-Net model for MR image reconstruction},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106151},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106151},
url = {https://www.sciencedirect.com/science/article/pii/S016926072100225X},
author = {Amir Aghabiglou and Ender M. Eksioglu},
keywords = {Magnetic resonance imaging, Image reconstruction, Deep learning, Cascaded networks, U-Net, Updated data consistency},
abstract = {Background and Objective
Background and Objective: Recent studies in deep learning reveal that the U-Net stands out among the diverse set of deep models as an effective network structure, especially for imaging inverse problems. Initially, the U-Net model was developed to solve segmentation problems for biomedical images while using an annotated dataset. In this paper, we will study a novel application of the U-Net structure for the important inverse problem of MRI reconstruction. Deep networks are particularly efficient for the speed-up of the MR image reconstruction process by decreasing the data acquisition time, and they can significantly reduce the aliasing artifacts caused by the undersampling in the k-space. Our aim is to develop a novel and efficient cascaded U-Net framework for reconstructing MR images from undersampled k-space data. The new framework should have improved reconstruction performance when compared to competing methodologies.
Methods
In this paper, a novel cascaded framework utilizing the U-Net as a sub-block is being proposed. The introduced U-Net cascade structure is applied to the magnetic resonance image reconstruction problem. The connection between the cascaded U-Nets is realized in the form of a recently developed projection-based updated data consistency layer. The novel structure is implemented in the PyTorch environment, which is one of the standards for deep learning implementations. The recently created fastMRI dataset which forms an important benchmark for MRI reconstruction is used for training and testing purposes.
Results
We present simulation results comparing the novel method with a variety of competitive deep networks. The new cascaded U-Net structures PSNR performance stands on average 1.28 dB higher than the baseline U-Net. The improvement, when compared to the standard CNN, is on average 3.32 dB.
Conclusions
The proposed cascaded U-Net configuration results in an improved reconstruction performance when compared to the CNN, the cascaded CNN, and also the singular U-Net structures, where the singular U-Net forms the baseline reconstruction method from the fastMRI package. The use of the projection-based updated data consistency layer also leads to improved quantitative (including SSIM, PSNR, and NMSE results) and qualitative results when compared to the use of the conventional data consistency layer.}
}
@article{UCHIDATE20041288,
title = {Generation of reference data of 3D surface texture using the non-causal 2D AR model},
journal = {Wear},
volume = {257},
number = {12},
pages = {1288-1295},
year = {2004},
note = {9th International Conference on Metrology and Properties of Engoneering Surfaces},
issn = {0043-1648},
doi = {https://doi.org/10.1016/j.wear.2004.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0043164804002054},
author = {M. Uchidate and T. Shimizu and A. Iwabuchi and K. Yanagi},
keywords = {Surface texture, Roughness, Reference data, AR model, Simulation},
abstract = {In this paper, the non-causal 2D AR model was proposed as a new method for generation of reference data of the 3D surface texture. Specification parameters to engender various topographical properties were the correlation distances in the x and y directions, the power index which determined the decaying pattern of the auto-correlation function, the root mean deviation, the skewness and the kurtosis. Isotropic surfaces were generated and compared with results by the causal 2D AR model which has been proposed in the past researches. It was found that the error of generation by the non-causal 2D AR model was smaller than that by the causal 2D AR model. Anisotropic surfaces and non-Gaussian distributed surfaces were also generated successfully.}
}
@article{CHEN2021102810,
title = {Harmonized neonatal brain MR image segmentation model for cross-site datasets},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102810},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102810},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421004079},
author = {Jian Chen and Yue Sun and Zhenghan Fang and Weili Lin and Gang Li and Li Wang},
keywords = {Neonatal brain, Artifacts, Cross-site datasets, Segmentation, CycleGAN, Cross-time},
abstract = {Accurate segmentation of white matter, gray matter and cerebrospinal fluid from neonatal brain MR images is of great importance in characterizing early brain development. Deep-learning-based methods have been successfully applied to neonatal brain MRIs with superior performance if testing subjects were acquired with the same imaging protocols/scanners as training subjects. However, for the testing subjects acquired with different imaging protocols/scanners, they cannot achieve accurate segmentation results due to large appearance/pattern differences between the testing and training subjects. Besides, imaging artifacts, like head motion, which are inevitable during the imaging acquisition process, also pose a challenge for the segmentation methods. To address these issues, in this paper, we propose a harmonized neonatal brain MR image segmentation model that harmonizes testing images acquired by different protocols/scanners into the domain of training images through a cycle-consistent generative adversarial network (CycleGAN). Meanwhile, the artifacts can be largely alleviated during the harmonization. Then, a densely-connected U-Net based segmentation model trained in the domain of training images can be applied robustly for segmenting the harmonized testing images. Comparisons with existing methods illustrate the better performance of the proposed method on neonatal brain MR images from cross-sites, a grand segmentation challenge, as well as images with artifacts.}
}
@article{SARICA2023104965,
title = {A dense residual U-net for multiple sclerosis lesions segmentation from multi-sequence 3D MR images},
journal = {International Journal of Medical Informatics},
volume = {170},
pages = {104965},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104965},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622002799},
author = {Beytullah Sarica and Dursun Zafer Seker and Bulent Bayram},
keywords = {Multiple sclerosis (MS), MS lesion segmentation, MRI, U-net, Convolutional neural networks, Deep learning, Residual blocks},
abstract = {Multiple Sclerosis (MS) is an autoimmune disease that causes brain and spinal cord lesions, which magnetic resonance imaging (MRI) can detect and characterize. Recently, deep learning methods have achieved remarkable results in the automated segmentation of MS lesions from MRI data. Hence, this study proposes a novel dense residual U-Net model that combines attention gate (AG), efficient channel attention (ECA), and Atrous Spatial Pyramid Pooling (ASPP) to enhance the performance of the automatic MS lesion segmentation using 3D MRI sequences. First, convolution layers in each block of the U-Net architecture are replaced by residual blocks and connected densely. Then, AGs are exploited to capture salient features passed through the skip connections. The ECA module is appended at the end of each residual block and each downsampling block of U-Net. Later, the bottleneck of U-Net is replaced with the ASSP module to extract multi-scale contextual information. Furthermore, 3D MR images of Fluid Attenuated Inversion Recovery (FLAIR), T1-weighted (T1-w), and T2-weighted (T2-w) are exploited jointly to perform better MS lesion segmentation. The proposed model is validated on the publicly available ISBI2015 and MSSEG2016 challenge datasets. This model produced an ISBI score of 92.75, a mean Dice score of 66.88%, a mean positive predictive value (PPV) of 86.50%, and a mean lesion-wise true positive rate (LTPR) of 60.64% on the ISBI2015 testing set. Also, it achieved a mean Dice score of 67.27%, a mean PPV of 65.19%, and a mean sensitivity of 74.40% on the MSSEG2016 testing set. The results show that the proposed model performs better than the results of some experts and some of the other state-of-the-art methods realized related to this particular subject. Specifically, the best Dice score and the best LTPR are obtained on the ISBI2015 testing set by using the proposed model to segment MS lesions.}
}
@article{LI201841,
title = {3D multi-scale FCN with random modality voxel dropout learning for Intervertebral Disc Localization and Segmentation from Multi-modality MR Images},
journal = {Medical Image Analysis},
volume = {45},
pages = {41-54},
year = {2018},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1361841518300136},
author = {Xiaomeng Li and Qi Dou and Hao Chen and Chi-Wing Fu and Xiaojuan Qi and Daniel L. Belavý and Gabriele Armbrecht and Dieter Felsenberg and Guoyan Zheng and Pheng-Ann Heng},
keywords = {Multi-modality, Magnetic resonance imaging, Intervertebral discs, Localization, Segmentation, Deep learning, Dropout},
abstract = {Intervertebral discs (IVDs) are small joints that lie between adjacent vertebrae. The localization and segmentation of IVDs are important for spine disease diagnosis and measurement quantification. However, manual annotation is time-consuming and error-prone with limited reproducibility, particularly for volumetric data. In this work, our goal is to develop an automatic and accurate method based on fully convolutional networks (FCN) for the localization and segmentation of IVDs from multi-modality 3D MR data. Compared with single modality data, multi-modality MR images provide complementary contextual information, which contributes to better recognition performance. However, how to effectively integrate such multi-modality information to generate accurate segmentation results remains to be further explored. In this paper, we present a novel multi-scale and modality dropout learning framework to locate and segment IVDs from four-modality MR images. First, we design a 3D multi-scale context fully convolutional network, which processes the input data in multiple scales of context and then merges the high-level features to enhance the representation capability of the network for handling the scale variation of anatomical structures. Second, to harness the complementary information from different modalities, we present a random modality voxel dropout strategy which alleviates the co-adaption issue and increases the discriminative capability of the network. Our method achieved the 1st place in the MICCAI challenge on automatic localization and segmentation of IVDs from multi-modality MR images, with a mean segmentation Dice coefficient of 91.2% and a mean localization error of 0.62 mm. We further conduct extensive experiments on the extended dataset to validate our method. We demonstrate that the proposed modality dropout strategy with multi-modality images as contextual information improved the segmentation accuracy significantly. Furthermore, experiments conducted on extended data collected from two different time points demonstrate the efficacy of our method on tracking the morphological changes in a longitudinal study.}
}
@article{GAMBERINI2015104,
title = {Psychological response to an emergency in virtual reality: Effects of victim ethnicity and emergency type on helping behavior and navigation},
journal = {Computers in Human Behavior},
volume = {48},
pages = {104-113},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215000540},
author = {Luciano Gamberini and Luca Chittaro and Anna Spagnolli and Claudio Carlesso},
keywords = {Emergency, Virtual reality, Helping behavior, Navigation behavior, Racial discrimination, Validation},
abstract = {Virtual environments are increasingly used for emergency training, but tend to focus mainly on teaching prescribed emergency procedures. However, social psychology literature highlights several factors that can bias individual response to an emergency in the real world, and would be worth considering in virtual training systems. In this paper, we focus on withdrawal of help due to racial discrimination and explore the potential of virtual environments to trigger this bias in emergency situations. We also test if a virtual emergency is actually reacted to as an emergency. We use an immersive virtual environment (IVE) where a victim issues help requests during two different emergency situations (time pressure or fire). While experiencing the emergency, white participants (N=96) receive a request for help from a black or white virtual human. The results show a psychological response to the virtual experience consistent with an emergency situation (increased state anxiety and increased frequency of collisions with objects in the environment) and biased by racial discrimination in help provision. In addition, racial discrimination increases under time pressure, but not in a fire. The implications for virtual training are discussed.}
}
@article{MOURTZIS2019574,
title = {Warehouse Design and Operation using Augmented Reality technology: A Papermaking Industry Case Study},
journal = {Procedia CIRP},
volume = {79},
pages = {574-579},
year = {2019},
note = {12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 18-20 July 2018, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.097},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302148},
author = {Dimitris Mourtzis and Vasilios Samothrakis and Vasilios Zogopoulos and Ekaterini Vlachou},
keywords = {Warehouse design, Augmented reality, Warehouse simulation, Manufacturing},
abstract = {In modern, high competitive markets, efficient warehousing is critical as it accounts for a great part of logistics costs. Companies try to adopt highly adaptive and flexible warehouse design that may support the integration of novel technologies such as Augmented Reality (AR). This paper proposes a framework for warehouse design which minimizes inventory cost while keeping a high degree of service by supporting the integration of an AR warehousing system. The AR system will support the effective management of operations, by providing meaningful information. The proposed methodology is tested and validated in a real-life case study of a papermaking industry.}
}
@article{ARCELOPERA2021397,
title = {Training birdsong recognition using virtual reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {5},
pages = {397-406},
year = {2021},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S209657962100067X},
author = {Carlos Arce-Lopera and María José Arias and Gustavo Corrales},
keywords = {Human computer interaction, Virtual environment, Birdsong, Audio training, User engagement},
abstract = {Background
In mega-biodiverse environments, where different species are more likely to be heard than seen, species monitoring is generally performed using bioacoustics methodologies. Furthermore, since bird vocalizations are reasonable estimators of biodiversity, their monitoring is of great importance in the formulation of conservation policies. However, birdsong recognition is an arduous task that requires dedicated training in order to achieve mastery, which is costly in terms of time and money due to the lack of accessibility of relevant information in field trips or even specialized databases. Immersive technology based on virtual reality (VR) and spatial audio may improve species monitoring by enhancing information accessibility, interaction, and user engagement.
Methods
This study used spatial audio, a Bluetooth controller, and a head-mounted display (HMD) to conduct an immersive training experience in VR. Participants moved inside a virtual world using a Bluetooth controller, while their task was to recognize targeted birdsongs. We measured the accuracy of recognition and user engagement according to the User Engagement Scale.
Results
The experimental results revealed significantly higher engagement and accuracy for participants in the VR-based training system than in a traditional computer-based training system. All four dimensions of the user engagement scale received high ratings from the participants, suggesting that VR-based training provides a motivating and attractive environment for learning demanding tasks through appropriate design, exploiting the sensory system, and virtual reality interactivity.
Conclusions
The accuracy and engagement of the VR-based training system were significantly high when tested against traditional training. Future research will focus on developing a variety of realistic ecosystems and their associated birds to increase the information on newer bird species within the training system. Finally, the proposed VR-based training system must be tested with additional participants and for a longer duration to measure information recall and recognition mastery among users.}
}
@article{CHEN2021464,
title = {Implications of Virtual Reality on Environmental Sustainability in Manufacturing Industry: A Case Study},
journal = {Procedia CIRP},
volume = {104},
pages = {464-469},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009768},
author = {Xiaoxia Chen and Liang Gong and Anton Berce and Björn Johansson and Mélanie Despeisse},
keywords = {environmental sustainability, Virtual Reality, manufacturing, VR, sustainability, case study},
abstract = {Research on Virtual Reality (VR) in manufacturing is rarely driven by environmental sustainability. This paper explores the feasibility of developing VR technologies to reduce environmental impact, drawing from a case study in an automotive company. We developed a VR demo to support the communication for design review between the technology centre in Sweden and the manufacturing site in China, thus reducing travel frequencies. This reduction was verified through user experience and feedback from focus group discussion and questionnaire. The results show a reduction of travel frequencies for design review, therefore contributing to environmental sustainability.}
}
@article{TAN201778,
title = {Convolutional neural network regression for short-axis left ventricle segmentation in cardiac cine MR sequences},
journal = {Medical Image Analysis},
volume = {39},
pages = {78-86},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300543},
author = {Li Kuo Tan and Yih Miin Liew and Einly Lim and Robert A. McLaughlin},
keywords = {Cardiac MRI, LV segmentation, Deep learning, Convolutional neural networks},
abstract = {Automated left ventricular (LV) segmentation is crucial for efficient quantification of cardiac function and morphology to aid subsequent management of cardiac pathologies. In this paper, we parameterize the complete (all short axis slices and phases) LV segmentation task in terms of the radial distances between the LV centerpoint and the endo- and epicardial contours in polar space. We then utilize convolutional neural network regression to infer these parameters. Utilizing parameter regression, as opposed to conventional pixel classification, allows the network to inherently reflect domain-specific physical constraints. We have benchmarked our approach primarily against the publicly-available left ventricle segmentation challenge (LVSC) dataset, which consists of 100 training and 100 validation cardiac MRI cases representing a heterogeneous mix of cardiac pathologies and imaging parameters across multiple centers. Our approach attained a .77 Jaccard index, which is the highest published overall result in comparison to other automated algorithms. To test general applicability, we also evaluated against the Kaggle Second Annual Data Science Bowl, where the evaluation metric was the indirect clinical measures of LV volume rather than direct myocardial contours. Our approach attained a Continuous Ranked Probability Score (CRPS) of .0124, which would have ranked tenth in the original challenge. With this we demonstrate the effectiveness of convolutional neural network regression paired with domain-specific features in clinical segmentation.}
}
@article{SUHIR20142594,
title = {Three-step concept (TSC) in modeling microelectronics reliability (MR): Boltzmann–Arrhenius–Zhurkov (BAZ) probabilistic physics-of-failure equation sandwiched between two statistical models},
journal = {Microelectronics Reliability},
volume = {54},
number = {11},
pages = {2594-2603},
year = {2014},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2014.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0026271414001978},
author = {E. Suhir},
keywords = {Microelectronics reliability, Reliability prediction, Physics of failure, Predictive modeling, Probabilistic design for reliability},
abstract = {When encountering a particular reliability problem at the design, fabrication, testing, or an operation stage of a product’s life, and considering the use of predictive modeling to assess the seriousness and the likely consequences of the a detected failure, one has to choose whether a statistical, or a physics-of-failure-based, or a suitable combination of these two major modeling tools should be employed to address the problem of interest and to decide on how to proceed. A three-step concept (TSC) is suggested as a possible way to go in such a situation. The classical statistical Bayes’ formula can be used at the first step in this concept as a technical diagnostics tool. Its objective is to identify, on the probabilistic basis, the faulty (malfunctioning) device(s) from the obtained signals (“symptoms of faults”). The recently suggested physics-of-failure-based Boltzmann–Arrhenius–Zhurkov’s (BAZ) model and particularly the multi-parametric BAZ model can be employed at the second step to assess the remaining useful life (RUL) of the faulty device(s). If the RUL is still long enough, no action might be needed; if it is not, corrective restoration action becomes necessary. In any event, after the first two steps are carried out, the device is put back into operation (testing), provided that the assessed probability of its continuing failure-free operation is found to be satisfactory. If the operational failure nonetheless occurs, the third, technical diagnostics step should be undertaken to update reliability. Statistical beta-distribution, in which the probability of failure is treated as a random variable, is suggested to be used at this step. While various statistical methods and approaches, including Bayes’ formula and beta-distribution, are well known and widely used in numerous applications for many decades, the BAZ model was introduced in the microelectronics reliability (MR) area only several years ago. Its attributes are addressed and discussed therefore in some detail. The suggested concept is illustrated by a numerical example geared to the use of the prognostics-and-health-monitoring (PHM) effort in actual operation, such as, e.g., en-route flight mission.}
}
@article{WANG2020103355,
title = {Adopting lean thinking in virtual reality-based personalized operation training using value stream mapping},
journal = {Automation in Construction},
volume = {119},
pages = {103355},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103355},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309353},
author = {Peng Wang and Peng Wu and Hung-Lin Chi and Xiao Li},
keywords = {Lean, Value stream mapping, Virtual reality, Personalized training, Productivity},
abstract = {Lean thinking has been proven effective in helping practitioners identify and eliminate wastes during engineering operations. However, systematic instructional mechanisms and training protocols based on individual trainee's performance are insufficient in existing training to define value-added activities for further productivity improvement in a training environment. This study aims to investigate how value stream mapping (VSM), as a lean tool, can be applied to help improve operation training performances through an immersive virtual reality (VR)-based personalized training program. A before–after experiment based on a virtual scaffolding erection scenario is established to simulate the training process. The training performance resulting from the VSM-based VR approach is compared with conventional VR training. Comparative results indicate that the waste time and errors reduce significantly. Compared with the conventional method, the overall productivity improvement of the erection process using VSM-based VR training is 12%. This demonstrates that integrating lean thinking into the operation training process can be a more effective approach for VR-based personalized operation training, provided that appropriate instructions are implemented.}
}
@article{TRUDEAU2023100009,
title = {“Breaking the fourth wall”: The effects of cinematic virtual reality film-viewing on adolescent students’ empathic responses},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100009},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100009},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300003X},
author = {Andrea Trudeau and Ying Xie and Olha Ketsman and Fatih Demir},
keywords = {Virtual reality, Film-viewing, Experiential learning, Empathy, Adolescence},
abstract = {This research study investigated the use of cinematic virtual reality (CVR) in a seventh-grade social studies classroom and its effects on adolescents' empathic responses. In this quantitative research study, participants (n ​= ​60) completed the Adolescent Measure of Empathy and Sympathy (AMES, Vossen et al., 2015) as a pretest a week before viewing The Displaced, a film about the lives of three refugee children, in either CVR or two-dimensional (2D), 360-degree format. Promptly after viewing the film, participants repeated the AMES as a posttest. Paired t-tests were conducted to explore the changes in mean scores for the AMES subscale scores between participants viewing the film in CVR and 2D formats as well as the changes in mean subscales between male and female participants viewing the film in CVR. Gain scores were also calculated and analyzed through a two-way MANOVA to examine the possible interaction effect between film format and gender on AMES subscale scores. The results of this study indicated that while the 2D, 360-degree film format affected adolescent students' affective empathy, there was a greater increase in both cognitive and affective empathy scores for those viewing the film in CVR with male adolescent students’ scores demonstrating the most remarkable increase.}
}
@article{CHANG20111388,
title = {A hybrid ANFIS model based on AR and volatility for TAIEX forecasting},
journal = {Applied Soft Computing},
volume = {11},
number = {1},
pages = {1388-1395},
year = {2011},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1568494610000864},
author = {Jing-Rong Chang and Liang-Ying Wei and Ching-Hsue Cheng},
keywords = {Adaptive network-based fuzzy inference system (ANFIS), Fuzzy time series, TAIEX forecasting, Autoregressive},
abstract = {Time series models have been applied to forecast stock index movements and make reasonably accurate predictions. There are, however, two major drawbacks of conventional time series models: (1) most conventional time series models use only one variable to forecast; and (2) the rules that are mined from artificial neural networks (ANNs) are not easily understandable. To solve these problems and enhance the forecasting performance of fuzzy time series models, this paper proposes a hybrid adaptive network-based fuzzy inference system (ANFIS) model that is based on AR and volatility to forecast stock price problems of the Taiwan stock exchange capitalization weighted stock index (TAIEX). To evaluate forecasting performance, the proposed model is compared with Chen's model and Yu's model. Our results indicate that the proposed model is superior to other methods with regard to root mean squared error (RMSE).}
}
@article{LI2021106776,
title = {MDFA-Net: Multiscale dual-path feature aggregation network for cardiac segmentation on multi-sequence cardiac MR},
journal = {Knowledge-Based Systems},
volume = {215},
pages = {106776},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106776},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000393},
author = {Feiyan Li and Weisheng Li and Sheng Qin and Linhong Wang},
keywords = {Multiscale dual-path feature aggregation network, Multi-sequence CMR, Cardiac segmentation},
abstract = {Deep convolutional neural networks have shown great potential in medical image segmentation. However, automatic cardiac segmentation is still challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this paper, we propose a multiscale dual-path feature aggregation network (MDFA-Net) to solve misclassification and shape discontinuity problems. The proposed network is aimed to maintain a realistic shape of the segmentation results and divided into two parts: the first part is a non-downsampling multiscale nested network (MN-Net) which restrains the cardiac continuous shape and maintains the shallow information, and the second part is a non-symmetric encoding and decoding network (nSED-Net) that can retain deep details and overcome misclassification. We conducted four-fold cross-validation experiments on balanced steady-state, free precession cine cardiac magnetic resonance (bSSFP cine CMR) sequence, edema-sensitive T2-weighted, black blood spectral presaturation attenuated inversion-recovery (T2-SPAIR) CMR sequence and late gadolinium enhancement (LGE) CMR sequence which include 45 cases in each sequence. The data are provided by the organizer of the Multi-sequence Cardiac MR Segmentation Challenge (MS-CMRSeg 2019) in conjunction with 2019 Medical Image Computing and Computer Assisted Interventions (MICCAI). We also conducted external validation experiments on the data of 2020 MICCAI myocardial pathology segmentation challenge (MyoPS 2020). Whether it is a four-fold cross-validation experiment or an external validation experiment, the proposed method ranks first or second in the segmentation tasks of multi-sequence CMR images. The subjective evaluation also shows the same results as the objective evaluation metrics. The code will be posted at https://github.com/fly1995/MDFA-Net/.}
}
@article{MILENKOVIC201555,
title = {Automated breast-region segmentation in the axial breast MR images},
journal = {Computers in Biology and Medicine},
volume = {62},
pages = {55-64},
year = {2015},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2015.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482515001195},
author = {Jana Milenković and Olga Chambers and Maja {Marolt Mušič} and Jurij Franc Tasič},
keywords = {Breast MRI, Breast-region segmentation, Tunable Gabor filter, Shortest-path search, Cost function},
abstract = {Purpose
The purpose of this study was to develop a robust breast-region segmentation method independent from the visible contrast between the breast region and surrounding chest wall and skin.
Materials and methods
A fully-automated method for segmentation of the breast region in the axial MR images is presented relying on the edge map (EM) obtained by applying a tunable Gabor filter which sets its parameters according to the local MR image characteristics to detect non-visible transitions between different tissues having a similar MRI signal intensity. The method applies the shortest-path search technique by incorporating a novel cost function using the EM information within the border-search area obtained based on the border information from the adjacent slice. It is validated on 52 MRI scans covering the full American College of Radiology Breast Imaging-Reporting and Data System (BI-RADS) breast-density range.
Results
The obtained results indicate that the method is robust and applicable for the challenging cases where a part of the fibroglandular tissue is connected to the chest wall and/or skin with no visible contrast, i.e. no fat presence, between them compared to the literature methods proposed for the axial MR images. The overall agreement between automatically- and manually-obtained breast-region segmentations is 96.1% in terms of the Dice Similarity Coefficient, and for the breast-chest wall and breast-skin border delineations it is 1.9mm and 1.2mm, respectively, in terms of the Mean-Deviation Distance.
Conclusion
The accuracy, robustness and applicability for the challenging cases of the proposed method show its potential to be incorporated into computer-aided analysis systems to support physicians in their decision making.}
}
@article{OLEKSY20173,
title = {Catch them all and increase your place attachment! The role of location-based augmented reality games in changing people - place relations},
journal = {Computers in Human Behavior},
volume = {76},
pages = {3-8},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303771},
author = {Tomasz Oleksy and Anna Wnuk},
keywords = {Place attachment, Gamification, Location-based AR games, Augmented reality, Pokémon Go},
abstract = {We examined how playing a game employing augmented reality (AR) technology increases attachment to the place of playing. Place attachment refers to the relationship between people and places, which has numerous benefits for individual well-being. Popular location-based AR games often include elements that are known to predict place attachment: exploration, social relations or the experience of enjoyment in a place. We argue that positive emotions triggered by playing can influence players’ place attachment via the process of gamification. We tested this hypothesis in a correlational study conducted among Pokémon Go players. Our analyses showed that satisfaction from playing and the social relations made during play positively predict place attachment, but the amount of time spent on playing does not. A series of mediation analyses showed that relations among game satisfaction, social relations, and place attachment were mediated by the appraisal of the place as exciting. This study demonstrated a mechanism of emotional transfer between positive experiences from playing and place attachment, which may prove useful in other domains, such as education, land conservation, or marketing.}
}
@article{LORENZ2016358,
title = {CAD to VR – A Methodology for the Automated Conversion of Kinematic CAD Models to Virtual Reality},
journal = {Procedia CIRP},
volume = {41},
pages = {358-363},
year = {2016},
note = {Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.12.115},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115011944},
author = {Mario Lorenz and Michael Spranger and Tino Riedel and Franziska Pürzel and Volker Wittstock and Philipp Klimant},
keywords = {Virtual reality, Engineering, Computer aided design (CAD), Kinematic model},
abstract = {Driven by the challenges of Industry 4.0 seamless data integration and conversion becomes an even more pressing topic than it already has been in the past. In this context virtual technologies such as Virtual Reality (VR) and Augmented Reality (AR) will play a defining role when it comes down to speed up the development process of new products, factory planning and supporting the workforce with novel intelligent assistance systems to keep up with the faster growing product diversity, especially in the face of the demographic challenge. The key enabling technology to foster this development is the seamless and automated integration of all product data defined in the CAD systems into VR/AR systems. Currently, there is still a major gap in this data integration when more than just plain geometry should be transferred, especially outside of the software ecosphere of a CAD system provider. Considerable effort by VR experts has to be put in to reduce the use case specific complexity of models and to rebuild the already in the CAD system defined kinematic mechanisms and animation. Whilst these obstacles were hindering SMEs in the past to participate of the benefits that virtual technologies provide, this cannot be afforded any longer. In this paper we will outline a CAD and VR system independent workflow for an automated model complexity reduction, animation and kinematic mechanism adoption. We do this by defining light weighted system interfaces and showing solution concepts for each area of the conversion problem. Finally, we verify the presented method on a proof of concept implementation.}
}
@article{TAKEDA202232,
title = {Light Guidance Control of Human Drivers: Driver Modeling, Control System Design, and VR Experiment},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {41},
pages = {32-37},
year = {2022},
note = {4th IFAC Workshop on Cyber-Physical and Human Systems CPHS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.099},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323001064},
author = {M. Takeda and M. Inoue and X. Fang and Y. Minami and J.M. Maestre},
keywords = {Light guidance control, Human-in-the-loop, Driver modeling, Virtual reality experiment},
abstract = {This paper addresses light guidance control for human-driven vehicles. We manipulate pace-making-light (PML) installed in the road environment to make drivers accelerate their vehicles unconsciously. Also, since the performance of the control system relies on the human reaction, we address the modeling of the driver's behavior guided by PML. To this end, we tested human subjects by using a driving simulator developed in a virtual reality environment. The collected data were used to model the driver's behavior and to perform a PML control simulation. In particular, the driver model is utilized for the design of a model predictive controller that is implemented as the PML logic.}
}
@article{PANG2018130,
title = {Variable universe fuzzy control for vehicle semi-active suspension system with MR damper combining fuzzy neural network and particle swarm optimization},
journal = {Neurocomputing},
volume = {306},
pages = {130-140},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218304958},
author = {Hui Pang and Fan Liu and Zeren Xu},
keywords = {Vehicle semi-active suspension, Variable universe, T–S fuzzy control, FNN, PSO},
abstract = {This study proposes a novel variable universe fuzzy control design for vehicle semi-active suspension system with magnetorheological (MR) damper through the combination of fuzzy neural network (FNN) and particle swarm optimization (PSO). By constructing a quarter-vehicle test rig equipped with MR damper and then collecting the measured data, a non-parametric model of MR damper based on adaptive neuro-fuzzy inference system is first presented. And then a Takagi–Sugeno (T–S) fuzzy controller is designed to achieve the effective control of the input current in MR damper by using the contraction-expansion factors. Furthermore, an appropriate FNN controller is proposed to obtain the contraction-expansion factors, in which particle swarm optimization and back propagation are introduced as the learning and training algorithm for the FNN controller. Lastly, a simulation investigation is provided to validate the proposed control scheme. The results of this study can provide the technical foundation for the development of vehicle semi-active suspension system.}
}
@article{CHEN2020106418,
title = {Using augmented reality to experiment with elements in a chemistry course},
journal = {Computers in Human Behavior},
volume = {111},
pages = {106418},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106418},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220301710},
author = {Shih-Yeh Chen and Shiang-Yao Liu},
keywords = {Augmented reality, Hands-on, Chemistry, Interest, Mobile, Collaborative},
abstract = {The use of augmented reality (AR) in education is an emerging trend aimed at improving students' learning outcomes and affective factors. This study further investigated the effects of AR learning activities combined with different approaches, including teacher-centered demonstration and student-centered hands-on, on the conceptual understanding of chemistry and interest in science. Participants of this study included 104 ninth graders from four classes at a junior high school, who were taught by the same chemistry teacher. A quasi-experimental research design was conducted to compare students' pretest, immediate posttest, and delayed posttest scores. Statistical findings were further supplemented by student interviews toward the activities. Results showed that the hands-on learning group performed significantly better on the chemical reactions concept test and interest questionnaire of the immediate posttest than the demonstration learning group. Retention effects also revealed that the students’ conceptual understanding of chemical elements remained effective four months after completion of the learning activities. No significant decline of situational interest and a slightly upward trend of individual interest were found in both groups. According to the interview responses, students were satisfied with the hands-on activities and appreciated the opportunity to be active learners. From this study, hands-on AR could serve as a promising strategy to motivate students in learning chemistry not only for immediate effectiveness but longstanding influence.}
}
@article{OPREA201977,
title = {A visually realistic grasping system for object manipulation and interaction in virtual reality environments},
journal = {Computers & Graphics},
volume = {83},
pages = {77-86},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849319301098},
author = {Sergiu Oprea and Pablo Martinez-Gonzalez and Alberto Garcia-Garcia and John A. Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez},
keywords = {Human-computer interaction, Virtual reality, User studies},
abstract = {Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.}
}
@article{TAKETOMI201411,
title = {Camera pose estimation under dynamic intrinsic parameter change for augmented reality},
journal = {Computers & Graphics},
volume = {44},
pages = {11-19},
year = {2014},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2014.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849314000648},
author = {Takafumi Taketomi and Kazuya Okada and Goshiro Yamamoto and Jun Miyazaki and Hirokazu Kato},
keywords = {Camera pose estimation, Augmented reality, Zoomable camera, Epipolar constraint},
abstract = {In this paper, we propose a method for estimating the camera pose for an environment in which the intrinsic camera parameters change dynamically. In video see-through augmented reality (AR) technology, image-based methods for estimating the camera pose are used to superimpose virtual objects onto the real environment. In general, video see-through-based AR cannot change the image magnification that results from a change in the camera׳s field-of-view because of the difficulty of dealing with changes in the intrinsic camera parameters. To remove this limitation, we propose a novel method for simultaneously estimating the intrinsic and extrinsic camera parameters based on an energy minimization framework. Our method is composed of both online and offline stages. An intrinsic camera parameter change depending on the zoom values is calibrated in the offline stage. Intrinsic and extrinsic camera parameters are then estimated based on the energy minimization framework in the online stage. In our method, two energy terms are added to the conventional marker-based method to estimate the camera parameters: reprojection errors based on the epipolar constraint and the constraint of the continuity of zoom values. By using a novel energy function, our method can accurately estimate intrinsic and extrinsic camera parameters. We confirmed experimentally that the proposed method can achieve accurate camera parameter estimation during camera zooming.}
}
@article{ZHAO2022108669,
title = {Single MR image super-resolution via channel splitting and serial fusion network},
journal = {Knowledge-Based Systems},
volume = {246},
pages = {108669},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108669},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003070},
author = {Xiaole Zhao and Yulun Zhang and Yun Qin and Qian Wang and Tao Zhang and Tianrui Li},
keywords = {Convolutional neural network, Magnetic resonance imaging, Channel splitting, Super-resolution, Serial fusion},
abstract = {In magnetic resonance imaging (MRI), spatial resolution is an important and critical imaging parameter that represents how much information is contained in a unit space. Acquiring high-resolution MRI data usually takes a long scanning time and is subject to motion artifacts due to hardware, physical, and physiological limitations. Single image super-resolution (SISR) based on deep learning is an effective and promising alternative technique to improve the native spatial resolution of magnetic resonance (MR) images. However, because of the simple diversity and single distribution of training samples, the effective training of deep models with medical training samples and improvement of the tradeoff between model performance and computing overhead are major challenges. In addition, deeper networks are more difficult to effectively train since the information is gradually weakened as the network deepens. In this paper, a novel channel splitting and serial fusion network (CSSFN) is presented for single MR image super-resolution. The proposed CSSFN splits hierarchical features into a series of subfeatures, which are then integrated together in a serial manner. Hence, the network becomes deeper and can discriminatively and reasonably deal with the subfeatures. Moreover, a dense global feature fusion (DGFF) is adopted to integrate the intermediate features, which further promotes the information flow in the network and helps to stabilize model training. Extensive experiments on several typical MR images show the superiority of our CSSFN models to other advanced SISR methods.}
}
@article{LOU2020106172,
title = {Augmented reality display device with continuous depth rendering capabilities},
journal = {Optics and Lasers in Engineering},
volume = {134},
pages = {106172},
year = {2020},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2020.106172},
url = {https://www.sciencedirect.com/science/article/pii/S0143816620301846},
author = {Yimin Lou and Jumanmei Hu and Aixi Chen and Fengmin Wu},
abstract = {True 3D display with continuous depth rendering is a key issue for augmented reality display system. However, the discrete sampling characteristics of current display devices makes the 3D images discontinuous in both lateral and depth directions. For depth information, very limited depth planes can be rendered, especially when the resolution of the display device is low. To address the limitation, a kind of augmented reality display device that can output continuous depth information using a modulated illumination integral imaging technique is demonstrated. Light field with a continuous directional component is achieved in a digitized integral imaging system by using a modulated convergent backlight. A custom-designed transparent off-axis spherical reflective lens is used as an optical combiner to project the light field images into the real world. Two prototype augmented reality display devices are fabricated that provided continuous depth information ranging from 0.5 m to 3 m for 3D images in front of the optical combiner. 3D images with continuous focus cues and double depth of field are realized using a simple optical structure. The imaging mechanisms of the augmented reality display devices are deduced analytically. The imaging characters of the augmented reality display devices are tested and verified.}
}
@article{HUANG2021101817,
title = {Difficulty-aware hierarchical convolutional neural networks for deformable registration of brain MR images},
journal = {Medical Image Analysis},
volume = {67},
pages = {101817},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101817},
url = {https://www.sciencedirect.com/science/article/pii/S136184152030181X},
author = {Yunzhi Huang and Sahar Ahmad and Jingfan Fan and Dinggang Shen and Pew-Thian Yap},
keywords = {Deformable registration, Brain MRI, Cascaded neural network, Difficulty-aware sampling},
abstract = {The aim of deformable brain image registration is to align anatomical structures, which can potentially vary with large and complex deformations. Anatomical structures vary in size and shape, requiring the registration algorithm to estimate deformation fields at various degrees of complexity. Here, we present a difficulty-aware model based on an attention mechanism to automatically identify hard-to-register regions, allowing better estimation of large complex deformations. The difficulty-aware model is incorporated into a cascaded neural network consisting of three sub-networks to fully leverage both global and local contextual information for effective registration. The first sub-network is trained at the image level to predict a coarse-scale deformation field, which is then used for initializing the subsequent sub-network. The next two sub-networks progressively optimize at the patch level with different resolutions to predict a fine-scale deformation field. Embedding difficulty-aware learning into the hierarchical neural network allows harder patches to be identified in the deeper sub-networks at higher resolutions for refining the deformation field. Experiments conducted on four public datasets validate that our method achieves promising registration accuracy with better preservation of topology, compared with state-of-the-art registration methods.}
}
@article{SINGH2020105,
title = {A novel enhanced hybrid recursive algorithm: Image processing based augmented reality for gallbladder and uterus visualisation},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {2},
pages = {105-118},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301744},
author = {T. Singh and Abeer Alsadoon and P.W.C. Prasad and Omar Hisham Alsadoon and Haritha Sallepalli Venkata and Ahmad Alrubaie},
keywords = {Augmented reality, 3D-2D image registration, Gallbladder surgery, Bowel surgery, Surgical and invasive medical procedures},
abstract = {Background: Current Augmented Reality systems in liver and bowel surgeries, are not accurate enough to classify the hidden parts such as gallbladder and uterus which are behind the liver and bowel. Therefore, we aimed to improve the visualization accuracy of bowel and liver augmented videos to avoid the unexpected cuttings on the hidden parts. Methodology: The proposed system consists of an Enhanced hybrid recursive matching and λ-parameterization techniques to improve the visualization. In addition, Mean Shift Filter is also added to improve the matching process while image registration. Results: Results proved that, the accuracy is improved in terms of liver and bowel surgeries Visualization errors about 0.53 mm and 0.22 mm respectively. Similarly, it can produce 2 more frames/sec compared to the current system. Conclusion: The proposed system worked towards the visualization of gallbladder and uterus while liver and bowel surgeries. So, this study solved the visualization issues, which are caused by neighbouring and hidden parts.}
}
@article{SUAREZWARDEN201517,
title = {Small Sample Size for Test of Training Time by Augmented Reality: An Aeronautical Case},
journal = {Procedia Computer Science},
volume = {75},
pages = {17-27},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036510},
author = {Fernando Suárez-Warden and Myrta Rodriguez and Nicolás Hendrichs and Salvador García-Lumbreras and Eduardo González Mendívil},
keywords = {small sample, estimation, training time, assembly time, Augmented Reality (AR), Operating Characteristic Curves (OCC)},
abstract = {Augmented Reality works for learning (growing tendency) in aeronautical sector demand an evaluation that requires experimental sampling, and this is often an expensive process. Usually, sample size is determined by resources availability constraint, which does not guarantee the level of significance (α) and the margin of error (E) required. This paper introduces a congruent sequence of statistical procedures to determine the estimated work sample size for traineeship in assembly operations. Sample size estimations for an aeronautical case are presented considering various scenarios. A minimum necessary sample size is calculated with statistical rigor via formulation and Operating Characteristic Curves.}
}
@article{DELIMA2022100515,
title = {Adaptive virtual reality horror games based on Machine learning and player modeling},
journal = {Entertainment Computing},
volume = {43},
pages = {100515},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100515},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000398},
author = {Edirlei Soares {de Lima} and Bruno M.C. Silva and Gabriel Teixeira Galam},
keywords = {Horror Games, Virtual Reality, Player Modeling, Adaptive Games},
abstract = {Fear is a basic human emotion that can be triggered by different situations, which vary from person to person. However, game developers usually design horror games based on a general knowledge about what most players fear, which does not guarantee a satisfying horror experience for every-one. When a horror game aims at intensifying the fear evoked in individual players, having useful information about the fears of the current player is vital to promote more frightening experiences. This work presents a new method to create adaptive virtual reality horror games, which combines player modeling techniques and an adaptive agent-based system that can identify what individual players fear and adapt the content of the game to intensify the fear evoked in players. The main contributions of this work are: (1) a new method to identify individual player’s fears using only gameplay data and machine learning techniques; and (2) a new agent-based adaptive game system that can track the horror intensity experienced by players and moderate the use of the horror elements feared by individual players in the game. The results show that the proposed method is capable of correctly identifying players’ fears (average accuracy of 79.4% for new players). In addition, results of a user study and statistical significance tests (ANOVA and post-hoc analyses) suggest that our method can intensify the fear evoked in players and positively improve immersion and flow.}
}
@article{CHENG20095254,
title = {Experimentation on MR fluid using a 2-axis wheel tool},
journal = {Journal of Materials Processing Technology},
volume = {209},
number = {12},
pages = {5254-5261},
year = {2009},
issn = {0924-0136},
doi = {https://doi.org/10.1016/j.jmatprotec.2009.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0924013609001010},
author = {H.B. Cheng and Yeung Yam and Y.T. Wang},
keywords = {Magnetorheological fluid, Fluid-assisted polishing, Material removal, Roughness},
abstract = {This paper is focused on magnetorheological (MR) fluid assistive polishing of optical aspheric components. MR fluid is a functional mixture of non-colloidal magnetic particle of micrometer size suspended in a host fluid, with the special property that its viscosity can be varied by the application of a magnetic field. This paper introduces the basic principles of the methodology and presents experiment results on MR fluids using a 2-axis wheel-shaped tool supporting dual magnetic fields. Mathematical models taking into account the pressure and the tool velocity are derived. The experiments serve to evaluate the effects of process parameters on material removal and performance using a K9 glass parabolic lens of 60mm diameter as work-piece. It is shown that surface roughness can be reduced from an initial value of 3.8–1.2nm after 10min of polishing. The form errors can also be improved from an initial 2.27μm rms and 7.89μm peak-to-valley to become 0.36μm rms and 2.01μm peak-to-valley after 60min of polishing.}
}
@article{HEMMATIAN201743,
title = {Sound transmission analysis of MR fluid based-circular sandwich panels: Experimental and finite element analysis},
journal = {Journal of Sound and Vibration},
volume = {408},
pages = {43-59},
year = {2017},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2017.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X17305345},
author = {Masoud Hemmatian and Ramin Sedaghati},
keywords = {Magnetorheological (MR) fluids, Sound transmission loss (STL), Sandwich panel, Finite element method},
abstract = {Magnetorheological Fluids (MR) have been recently utilized in sandwich panels to provide variable stiffness and damping to effectively control vibrations. In this study, the sound transmission behavior of MR based-sandwich panels is investigated through development of an efficient finite element model. A clamped circular sandwich panel with elastic face sheets and MR Fluid as the core layer has been considered. A finite element model utilizing circular and annular elements has been developed to derive the governing equations of motion in the finite element form. The transverse velocity is then calculated and utilized to obtain the sound radiated from the panel and subsequently the sound transmission loss. In order to validate the simulated results, a test setup including two anechoic spaces and an electro-magnet has been designed and fabricated. The magnetic flux density generated inside the electromagnet is simulated using magneto-static finite element analysis and validated with the measured magnetic flux density using Gaussmeter. The results from magneto-static analysis is used to derive an approximate polynomial function to evaluate the magnetic flux density as a function of the plate’s radius and applied current. The STL and first axisymmetric natural frequency of the MR sandwich panels with aluminum face sheets are simulated and compared with those obtained experimentally. Finally, a parametric study on the effect of applied magnetic field, the thickness of the core layer and the thickness of face sheets on the STL and natural frequency of the adaptive sandwich panel are presented.}
}
@article{FU201847,
title = {Sparse deformation prediction using Markove Decision Processes (MDP) for Non-rigid registration of MR image},
journal = {Computer Methods and Programs in Biomedicine},
volume = {162},
pages = {47-59},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718300506},
author = {Tianyu Fu and Qin Li and Jianjun Zhu and Danni Ai and Yong Huang and Hong Song and Yurong Jiang and Yongtian Wang and Jian Yang},
keywords = {Deformation prediction, Markove decision processes, Patch-wise registration, MR image},
abstract = {Background and Objective
A framework of sparse deformation prediction using Markove Decision Processes is proposed for achieving a rapid and accurate registration by providing a suitable initial deformation.
Methods
In the proposed framework, the tree is built based on the training set for each patch from the template image. The template patch is considered as the root. The node is the patch group in which multiple similar patches are extracted around a key point on the training image. Given the linkages between patch groups in the tree, MDP is introduced to select the optimal path with highest registration accuracy from each training patch to the template patch. The deformation between them is estimated along the selected path by patch-wise registration which can be realized by a non-learning-based method. Given the patches on a testing image, their best matching patches are fast chosen from the training patches and the corresponding deformations constitute a sparse deformation. A dense deformation for the entire test image is subsequently interpolated and used as an initial deformation for further registration.
Results
With the non-learning-based registration as the baseline method, the proposed framework is evaluated using three datasets of inter-subject brain MR images with three learning-based methods. Experimental results of the non-learning-based method using the proposed framework reveal that the computation time is reduced by fivefold after using the proposed framework. And, with the same baseline method, the proposed framework demonstrates the higher accuracy than three learning-based methods which predicts the initial deformation at image scale. The mean Dice of three datasets for the tissues of the brain are 73.52%, 70.73% and 64.82%, respectively.
Conclusions
The proposed framework rapidly registers the inter-subject brains and achieves the high mean Dice for the tissues of the brain.}
}
@article{EDGCUMBE201595,
title = {Pico Lantern: Surface reconstruction and augmented reality in laparoscopic surgery using a pick-up laser projector},
journal = {Medical Image Analysis},
volume = {25},
number = {1},
pages = {95-102},
year = {2015},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2015.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1361841515000584},
author = {Philip Edgcumbe and Philip Pratt and Guang-Zhong Yang and Christopher Nguan and Robert Rohling},
keywords = {Pico projector, Laparoscopic surgery, Augmented reality, Pico Lantern},
abstract = {The Pico Lantern is a miniature projector developed for structured light surface reconstruction, augmented reality and guidance in laparoscopic surgery. During surgery it will be dropped into the patient and picked up by a laparoscopic tool. While inside the patient it projects a known coded pattern and images onto the surface of the tissue. The Pico Lantern is visually tracked in the laparoscope’s field of view for the purpose of stereo triangulation between it and the laparoscope. In this paper, the first application is surface reconstruction. Using a stereo laparoscope and an untracked Pico Lantern, the absolute error for surface reconstruction for a plane, cylinder and ex vivo kidney, is 2.0 mm, 3.0 mm and 5.6 mm, respectively. Using a mono laparoscope and a tracked Pico Lantern for the same plane, cylinder and kidney the absolute error is 1.4 mm, 1.5 mm and 1.5 mm, respectively. These results confirm the benefit of the wider baseline produced by tracking the Pico Lantern. Virtual viewpoint images are generated from the kidney surface data and an in vivo proof-of-concept porcine trial is reported. Surface reconstruction of the neck of a volunteer shows that the pulsatile motion of the tissue overlying a major blood vessel can be detected and displayed in vivo. Future work will integrate the Pico Lantern into standard and robot-assisted laparoscopic surgery.}
}
@article{SUN2016206,
title = {Surface modification and etch process optimization of fused silica during reaction CHF3–Ar plasma etching},
journal = {Optik},
volume = {127},
number = {1},
pages = {206-211},
year = {2016},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2015.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0030402615014011},
author = {Laixi Sun and Huang Jin and Xin Ye and Hongjie Liu and Fengrui Wang and Xiaodong Jiang and Weidong Wu and Wanguo Zheng},
keywords = {Fused silica, Damage, Etching, Defects},
abstract = {The ability to predict and control the influence of reaction ion etching process parameters on laser damage resistance of fused silica optical components is vital for their development in specialized applications such as inertial confinement fusion. In this study, a set of experiments was designed and performed to determine an optimized process condition by characterizing the evolution of the surface roughness and absorption of the fused silica samples treated at various gas flow rates. The macroscopic morphology, carbon concentration and laser damage resistance of the etched surface was measured. The optimized surface was obtained when the gas flow rate R (CHF3/(CHF3+Ar)) decreased down to 38.4%. With the optimized etch process, the presence of subsurface defects known to lead to laser damage initiation was removed and prevented. The C atomic concentration of the sample surface etched was decreased by 64.7%. The laser damage resistance increased dramatically; the average threshold fluence for damage initiation increased from 6.79J/cm2 to 8.99J/cm2.}
}
@article{SKULMOWSKI2023100023,
title = {Ethical issues of educational virtual reality},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100023},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100023},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300017X},
author = {Alexander Skulmowski},
keywords = {Virtual reality, Education, Ethics, Realism, Autonomy, Privacy},
abstract = {In response to the high demand for digital learning as a surrogate for physical experiences, virtual reality (VR) is positioning itself as a tool for creating educational virtual experiences. VR technology faces a number of ethical issues, including a reduction of users’ autonomy, health problems, and privacy concerns. The use of VR and realism in education can turn out to be a double-edged sword. While realistic visualizations can promote learning for some content domains, they can hinder comprehension in others. Furthermore, the effects of realism on learning also depend on learners’ spatial abilities. Letting young children and teenagers engage in virtual educational experiences can expose them to manipulation, could lead to health issues, and may infringe on their privacy. In short, realism and virtual experiences may severely limit learners’ autonomy in a number of ways. Based on a review of the literature and considerations of emerging technologies such as generative artificial intelligence, this paper presents guidelines for the ethically sound utilization of VR and realism. By applying findings and conclusions established in the context of research on the ethics of VR to the educational utilization of this technology, I develop several suggestions that may help to avoid negative consequences of educational VR. These suggestions include the utilization of spatial ability testing, requiring virtual experiences to offer alternative paths to prevent manipulation, as well as using algorithms that deidentify the highly detailed developmental profiles that can be generated through educational VR use.}
}
@article{DONOVAN20022635,
title = {Demonstration of temperature and OH mole fraction diagnostic in SiH4/H2/O2/Ar flames using narrow-line UV OH absorption spectroscopy},
journal = {Proceedings of the Combustion Institute},
volume = {29},
number = {2},
pages = {2635-2643},
year = {2002},
issn = {1540-7489},
doi = {https://doi.org/10.1016/S1540-7489(02)80321-2},
url = {https://www.sciencedirect.com/science/article/pii/S1540748902803212},
author = {M.T. Donovan and D.L. Hall and P.V. Torek and C.R. Schrock and M.S. Wooldridge},
abstract = {Results of an experimental study of the application of frequency-modulated UV laser absorption spectroscopyto silica (SiO2) particle-forming flames (SiH4/H2/O2/Ar) are presented. An argon-ion pumped ring-dye laser system in the rapid wavelength scanning configuration was used to obtain multiple line shape prifoles of the R1(7) and R1(11) transitions in the A2Σ+«X2IIi (0,0) band of the OH spectrum (vo=32,625,547 and 32,625.072 cm−1, respectively). Temperature and OH mole fraction were determined by a best fit a convolved Voigt absorption profile to the data. Measurements were made in the multiphase regions of silane/hydrogen/oxygen/argon flames, verifying the applicability of the diagnostic approach to combustion synthesis systems. Absorption measurements were taken over a range of particle environments found at increasing heights above the burner surface (5–20 mm) and equivalence ratios (φ=1.0 and 1.2). The experimental data were compared with thermocouple measurements, equilibrium, and one-dimensional modeling simulations. The results of the study successfully demonstrate OH UV absorption spectroscopy as a highly sensitive and accurate (uncertainties less than±10% in the current work) diagnostic approach for in situ measurements of temperature and Oh mole fractions in combustion synthesis flames.}
}
@article{MAKRANSKY2017276,
title = {Development and validation of the Multimodal Presence Scale for virtual reality environments: A confirmatory factor analysis and item response theory approach},
journal = {Computers in Human Behavior},
volume = {72},
pages = {276-285},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.02.066},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217301474},
author = {Guido Makransky and Lau Lilleholt and Anders Aaby},
keywords = {Presence, Virtual reality, Confirmatory factor analysis, Item response theory, Virtual simulations},
abstract = {Presence is one of the most important psychological constructs for understanding human-computer interaction. However, different terminology and operationalizations of presence across fields have plagued the comparability and generalizability of results across studies. Lee's (2004) unified understanding of presence as a multidimensional construct made up of physical, social, and self-presence, has created a unified theory of presence; nevertheless, there are still no psychometrically valid measurement instruments based on the theory. Two studies were conducted that describe the development of a standardized multidimensional measure of presence (the MPS) for a VR learning context based on this theory, and its validation using confirmatory factor analysis and item response theory. The results from Study 1 which included 161 medical students from Denmark indicated that the items used in the MPS measure a three dimensional theoretical model of presence: physical, social, and self-presence. Furthermore, IRT analyses indicated that it was possible to limit the number of items in the MPS to 15 (five items per sub-dimension) while maintaining the construct validity and reliability of the measure. The results of Study 2, which included 118 biology students from Scotland, supported the validity and generalizability of the MPS in a new context.}
}
@article{OH2024108098,
title = {Domain transformation learning for MR image reconstruction from dual domain input},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {108098},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108098},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524001823},
author = {Changheun Oh and Jun-Young Chung and Yeji Han},
keywords = {Deep learning, End to end, Magnetic resonance, Image reconstruction, Parallel imaging, Recurrent neural network},
abstract = {Medical images are acquired through diverse imaging systems, with each system employing specific image reconstruction techniques to transform sensor data into images. In MRI, sensor data (i.e., k-space data) is encoded in the frequency domain, and fully sampled k-space data is transformed into an image using the inverse Fourier Transform. However, in efforts to reduce acquisition time, k-space is often subsampled, necessitating a sophisticated image reconstruction method beyond a simple transform. The proposed approach addresses this challenge by training a model to learn domain transform, generating the final image directly from undersampled k-space input. Significantly, to improve the stability of reconstruction from randomly subsampled k-space data, folded images are incorporated as supplementary inputs in the dual-input ETER-net. Moreover, modifications are made to the formation of inputs for the bi-RNN stages to accommodate non-fixed k-space trajectories. Experimental validation, encompassing both regular and irregular sampling trajectories, validates the method's effectiveness. The results demonstrated superior performance, measured by PSNR, SSIM, and VIF, across acceleration factors of 4 and 8. In summary, the dual-input ETER-net emerges as an effective both regular and irregular sampling trajectories, and accommodating diverse acceleration factors.}
}
@article{PARLAK2012890,
title = {Optimal design of MR damper via finite element analyses of fluid dynamic and magnetic field},
journal = {Mechatronics},
volume = {22},
number = {6},
pages = {890-903},
year = {2012},
note = {Special Issue on Intelligent Mechatronics (LSMS2010 & ICSEE2010)},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2012.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957415812000815},
author = {Zekeriya Parlak and Tahsin Engin and İsmail Çallı},
keywords = {Magnetorheological fluid, MR damper, MR fluid, Finite element, Magnetic field, Computational fluid dynamics, CFD},
abstract = {In the last decade many researchers have been carried out on semi-active control systems, a large number of academic publications have been presented. Semi-active control systems which are used the magnetic field controlled fluid have been shown significant improvements by the researchers. In the study, a design optimization method that has been carried out for the objectives of target damper force and maximum magnetic flux density of an MR damper has been presented. Finite element methods, electromagnetic analysis of magnetic field and CFD analysis of MR flow, have been used to obtain optimal value of design parameters. The new approach that is use of magnetic field and MR flow together and simultaneously has specified optimal design values. Two optimal design of MR damper obtained have been verified with experimental study by manufacturing and testing of the dampers.}
}
@article{LAMB2022100003,
title = {The moderating role of creativity and the effect of virtual reality on stress and cognitive demand during preservice teacher learning},
journal = {Computers & Education: X Reality},
volume = {1},
pages = {100003},
year = {2022},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2022.100003},
url = {https://www.sciencedirect.com/science/article/pii/S2949678022000034},
author = {Richard Lamb and Jonah Firestone},
keywords = {Preservice teacher preparation, Teacher education, Virtual reality, Stress, Protective factors, Sensors, fNIRS},
abstract = {Virtual reality (VR) has received considerable attention related to its use in teacher development over the last decade. Despite this attention, there is insufficient understanding of how specific underlying cognitive system responses and autonomic nervous system responses moderate the use of VR and the associated learning outcomes in the development of preservice teachers. This work intends to explore and evaluate preservice service teachers’ (PSTs) experiences using VR and the effects of creativity, mental flexibility (MF), acute stress, and cognitive demand (CD). Forty-eight undergraduate college students in year two of their teacher preparation program were recruited for the study. Each of the preservice teachers was assigned randomly to one of two conditions, microteaching (n ​= ​24) or VR (n ​= ​24). The use of these two conditions allowed the researchers to compare the effects of creativity and MF [measured using the Torrance Test of Creative Thinking] on cognitive demand and stress responses [measured using heart rate variability and electrodermal activity]. Results from analysis of the hemodynamic data and stress response data illustrate that the protective factors of creativity and MF may moderate success in VR and the reduction of cognitive demand and stress when VR is used to develop skills related to teaching.}
}
@article{SANTANAMANCILLA2012721,
title = {Service Oriented Architecture to Support Mexican Secondary Education through Mobile Augmented Reality},
journal = {Procedia Computer Science},
volume = {10},
pages = {721-727},
year = {2012},
note = {ANT 2012 and MobiWIS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.06.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912004498},
author = {Pedro C. Santana-Mancilla and Miguel A. Garc’a-Ruiz and Ricardo Acosta-Diaz and Celso U. Juárez},
keywords = {Augmented reality, Service-Oriented Architecture, Mexican Basic Education, Mobile computing},
abstract = {This paper proposes a mobile augmented reality system that allows Mexican secondary education students to access additional educational contents related to their textbooks. Our system recognizes the images printed in the book as part of regular taught topics and shows multimedia contents that complement the topics covered in the book. These contents are generated through a service-oriented architecture. Initial usability testing of our augmented reality system showed a user satisfaction of 97%.}
}
@article{FU2021101845,
title = {Biomechanically constrained non-rigid MR-TRUS prostate registration using deep learning based 3D point cloud matching},
journal = {Medical Image Analysis},
volume = {67},
pages = {101845},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101845},
url = {https://www.sciencedirect.com/science/article/pii/S1361841520302097},
author = {Yabo Fu and Yang Lei and Tonghe Wang and Pretesh Patel and Ashesh B. Jani and Hui Mao and Walter J. Curran and Tian Liu and Xiaofeng Yang},
keywords = {MR-TRUS, Image registration, Point cloud matching, Finite element, Deep learning},
abstract = {A non-rigid MR-TRUS image registration framework is proposed for prostate interventions. The registration framework consists of a convolutional neural networks (CNN) for MR prostate segmentation, a CNN for TRUS prostate segmentation and a point-cloud based network for rapid 3D point cloud matching. Volumetric prostate point clouds were generated from the segmented prostate masks using tetrahedron meshing. The point cloud matching network was trained using deformation field that was generated by finite element analysis. Therefore, the network implicitly models the underlying biomechanical constraint when performing point cloud matching. A total of 50 patients’ datasets were used for the network training and testing. Alignment of prostate shapes after registration was evaluated using three metrics including Dice similarity coefficient (DSC), mean surface distance (MSD) and Hausdorff distance (HD). Internal point-to-point registration accuracy was assessed using target registration error (TRE). Jacobian determinant and strain tensors of the predicted deformation field were calculated to analyze the physical fidelity of the deformation field. On average, the mean and standard deviation were 0.94±0.02, 0.90±0.23 mm, 2.96±1.00 mm and 1.57±0.77 mm for DSC, MSD, HD and TRE, respectively. Robustness of our method to point cloud noise was evaluated by adding different levels of noise to the query point clouds. Our results demonstrated that the proposed method could rapidly perform MR-TRUS image registration with good registration accuracy and robustness.}
}
@article{XIANG201831,
title = {Deep embedding convolutional neural network for synthesizing CT image from T1-Weighted MR image},
journal = {Medical Image Analysis},
volume = {47},
pages = {31-44},
year = {2018},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1361841518301257},
author = {Lei Xiang and Qian Wang and Dong Nie and Lichi Zhang and Xiyao Jin and Yu Qiao and Dinggang Shen},
keywords = {Image synthesis, Deep convolutional neural network, Embedding block},
abstract = {Recently, more and more attention is drawn to the field of medical image synthesis across modalities. Among them, the synthesis of computed tomography (CT) image from T1-weighted magnetic resonance (MR) image is of great importance, although the mapping between them is highly complex due to large gaps of appearances of the two modalities. In this work, we aim to tackle this MR-to-CT synthesis task by a novel deep embedding convolutional neural network (DECNN). Specifically, we generate the feature maps from MR images, and then transform these feature maps forward through convolutional layers in the network. We can further compute a tentative CT synthesis from the midway of the flow of feature maps, and then embed this tentative CT synthesis result back to the feature maps. This embedding operation results in better feature maps, which are further transformed forward in DECNN. After repeating this embedding procedure for several times in the network, we can eventually synthesize a final CT image in the end of the DECNN. We have validated our proposed method on both brain and prostate imaging datasets, by also comparing with the state-of-the-art methods. Experimental results suggest that our DECNN (with repeated embedding operations) demonstrates its superior performances, in terms of both the perceptive quality of the synthesized CT image and the run-time cost for synthesizing a CT image.}
}
@article{HUA2024108218,
title = {EEG classification model for virtual reality motion sickness based on multi-scale CNN feature correlation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {251},
pages = {108218},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108218},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400213X},
author = {Chengcheng Hua and Jianlong Tao and Zhanfeng Zhou and Lining Chai and Ying Yan and Jia Liu and Rongrong Fu},
keywords = {Channel attention, Feature correlation matrix, Multi-scale feature fusion, Resting state EEG, Virtual reality motion sickness},
abstract = {Background
Virtual reality motion sickness (VRMS) is a key issue hindering the development of virtual reality technology, and accurate detection of its occurrence is the first prerequisite for solving the issue.
Objective
In this paper, a convolutional neural network (CNN) EEG detection model based on multi-scale feature correlation is proposed for detecting VRMS.
Methods
The model uses multi-scale 1D convolutional layers to extract multi-scale temporal features from the multi-lead EEG data, and then calculates the feature correlations of the extracted multi-scale features among all the leads to form the feature adjacent matrixes, which converts the time-domain features to correlation-based brain network features, thus strengthen the feature representation. Finally, the correlation features of each layer are fused. The fused features are then fed into the channel attention module to filter the channels and classify them using a fully connected network. Finally, we recruit subjects to experience 6 different modes of virtual roller coaster scenes, and collect resting EEG data before and after the task to verify the model. Results: The results show that the accuracy, precision, recall and F1-score of this model for the recognition of VRMS are 98.66 %, 98.65 %, 98.68 %, and 98.66 %, respectively. The proposed model outperforms the current classic and advanced EEG recognition models.
Significance
It shows that this model can be used for the recognition of VRMS based on the resting state EEG.}
}
@article{PRASTAWA2009297,
title = {Simulation of brain tumors in MR images for evaluation of segmentation efficacy},
journal = {Medical Image Analysis},
volume = {13},
number = {2},
pages = {297-311},
year = {2009},
note = {Includes Special Section on Functional Imaging and Modelling of the Heart},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841508001357},
author = {Marcel Prastawa and Elizabeth Bullitt and Guido Gerig},
keywords = {Brain MRI, Segmentation validation, Tumor simulation, Simulation of tumor infiltration, Diffusion tensor imaging, Ground truth, Gold standard},
abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors).}
}
@article{BASNET2021103063,
title = {A deep dense residual network with reduced parameters for volumetric brain tissue segmentation from MR images},
journal = {Biomedical Signal Processing and Control},
volume = {70},
pages = {103063},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.103063},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421006601},
author = {Ramesh Basnet and M. Omair Ahmad and M.N.S. Swamy},
keywords = {Brain tissue, Convolutional neural network, Deep learning, Magnetic resonance imaging, Segmentation},
abstract = {Deep convolutional neural networks (DCNN) have proven to be the state-of-the-art methods for brain tissue segmentation; however, their complex architectures, and the large number of parameters make them computationally expensive and difficult to optimize. In this paper, a novel 3D DCNN architecture, which is built upon the U-Net structure, is presented for compact feature representation and efficient parameter reduction in order to segment the brain tissues into white matter, gray matter, and cerebrospinal fluid (Code is available at: https://github.com/basnetr/U-DenseResNet). The basic idea in the proposed method is to use densely connected convolutional layers and residual skip-connections in order to increase the representation capacity, improve the gradient flow, facilitate easier and better learning, and reduce the number of parameters of the network. The loss functions, cross-entropy, dice similarity, and a combination of the two are used for the training of the proposed network. Experimental results show that the proposed approach provides the best performance on the test dataset of the single-modality IBSR18 dataset containing MR scans of diverse age groups and competitive performance on the multi-modality brain tissue segmentation challenge, iSeg-2017, containing MR scans of infants while reducing, for both the datasets, the parameters ranging from 40% to 98% compared to that of the other deep-learning based architectures. The proposed method significantly reduces the number of parameters of DCNNs while still providing high degree of accuracy. The proposed method can be used for the study of brain structure and development, in detecting a wide range of abnormal tissues, to aid diagnosis, and for guiding surgical procedures.}
}
@article{RUCKERT2018164,
title = {Implementation of virtual reality systems for simulation of human-robot collaboration},
journal = {Procedia Manufacturing},
volume = {19},
pages = {164-170},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300234},
author = {Patrick Rückert and Laura Wohlfromm and Kirsten Tracht},
keywords = {Virtual Reality, Simulation, Human-Robot Collaboration},
abstract = {A collaboration between human and robot can implicate many advantages for complex industrial assembly processes, especially as increased flexibility and adaptability become a key feature of production systems. The use of virtual reality (VR) systems has the potential to simulate cooperative processes in advance and to include workers and their individual behavior into the simulation. The use of VR simulations makes it possible to secure processes and reduce physical and mental barriers between human and robot. This paper presents a methodical approach for the implementation of systems for the virtual testing of collaborative assembly processes. Following the aim of replicating the assembly process with the highest possible immersion, a specific VR system is derived from an analysis of the assembly process. Core features of the system are the physical simulation of the assembly process, the integration of the robot control and a haptic feedback for the operator.}
}
@article{SWATI201934,
title = {Brain tumor classification for MR images using transfer learning and fine-tuning},
journal = {Computerized Medical Imaging and Graphics},
volume = {75},
pages = {34-46},
year = {2019},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0895611118305937},
author = {Zar Nawab Khan Swati and Qinghua Zhao and Muhammad Kabir and Farman Ali and Zakir Ali and Saeed Ahmed and Jianfeng Lu},
keywords = {Brain tumor classification, Block-wise fine-tuning, Convolutional neural networks, Deep learning, Magnetic resonance images, Transfer learning},
abstract = {Accurate and precise brain tumor MR images classification plays important role in clinical diagnosis and decision making for patient treatment. The key challenge in MR images classification is the semantic gap between the low-level visual information captured by the MRI machine and the high-level information perceived by the human evaluator. The traditional machine learning techniques for classification focus only on low-level or high-level features, use some handcrafted features to reduce this gap and require good feature extraction and classification methods. Recent development on deep learning has shown great progress and deep convolution neural networks (CNNs) have succeeded in the images classification task. Deep learning is very powerful for feature representation that can depict low-level and high-level information completely and embed the phase of feature extraction and classification into self-learning but require large training dataset in general. For most of the medical imaging scenario, the training datasets are small, therefore, it is a challenging task to apply the deep learning and train CNN from scratch on the small dataset. Aiming this problem, we use pre-trained deep CNN model and propose a block-wise fine-tuning strategy based on transfer learning. The proposed method is evaluated on T1-weighted contrast-enhanced magnetic resonance images (CE-MRI) benchmark dataset. Our method is more generic as it does not use any handcrafted features, requires minimal preprocessing and can achieve average accuracy of 94.82% under five-fold cross-validation. We compare our results not only with the traditional machine learning but also with deep learning methods using CNNs. Experimental results show that our proposed method outperforms state-of-the-art classification on the CE-MRI dataset.}
}
@article{KHATIB2021102030,
title = {Human-robot contactless collaboration with mixed reality interface},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102030},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102030},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302416},
author = {Maram Khatib and Khaled {Al Khudir} and Alessandro {De Luca}},
keywords = {Control system, Robotics, Human-robot collaboration, Redundancy control, Collision avoidance, Mixed reality},
abstract = {A control system based on multiple sensors is proposed for the safe collaboration of a robot with a human. New constrained and contactless human-robot coordinated motion tasks are defined to control the robot end-effector so as to maintain a desired relative position to the human head while pointing at it. Simultaneously, the robot avoids any collision with the operator and with nearby static or dynamic obstacles, based on distance computations performed in the depth space of a RGB-D sensor. The various tasks are organized with priorities and executed under hard joint bounds using the Saturation in the Null Space (SNS) algorithm. A direct human-robot communication is integrated within a mixed reality interface using a stereo camera and an augmented reality system. The proposed system is significant for on-line, collaborative quality assessment phases in a manufacturing process. Various experimental validation scenarios using a 7-dof KUKA LWR4 robot are presented.}
}
@article{ZHANG2022564,
title = {Augmented reality material management system based on post-processing of aero-engine blade code recognition},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {564-578},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001777},
author = {Jie Zhang and Shuxia Wang and Weiping He and Jianghong Li and Shixin Wu and Jiaxu Huang and Qiang Zhang and Manxian Wang},
keywords = {Aero-engine blades, Post-processing, Line classification, Bayes, Augmented reality, Material management},
abstract = {The efficient management of aero-engine blades plays a vital role in improving material turnover and productivity, while the automatic recognition of blade code provides support for digital production. However, there is a lack of discussion about intelligent recognition, result correction, and effective solutions to material management methods for aero-engine blades. There is still a lot of manual work. Therefore, we developed an augmented reality (AR) material management system based on the post-processing of aero-engine blade code recognition, which not only supports the semi-automatic recognition of blade codes but also provides users with guidance on efficient storage and selection. Firstly, we proposed a line classification method of code Optical Character Recognition (OCR) recognition results to determine the structural relationship of discrete characters. Secondly, we constructed a Bayes error correction model which fuses the noise channel model to achieve the post-processing correction of wrong codes. Lastly, we developed the AR auxiliary guidance module to guide users on warehousing. The results show that the proposed line classification algorithm outperforms the baseline method, achieving a high success rate in terms of line classification. Post-processing error correction is effective in improving the accuracy of OCR results. In addition, AR visualization guidance can lead to a significant improvement in efficiency in material entry and out for aero-engine blades. In conclusion, the system proposed in this paper provides an effective solution to material management for the aero-engine blade.}
}
@article{FITZGERALD2012492,
title = {Aggrandizing power output from Shewanella oneidensis MR-1 microbial fuel cells using calcium chloride},
journal = {Biosensors and Bioelectronics},
volume = {31},
number = {1},
pages = {492-498},
year = {2012},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2011.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0956566311007688},
author = {Lisa A. Fitzgerald and Emily R. Petersen and Benjamin J. Gross and Carissa M. Soto and Bradley R. Ringeisen and Mohamed Y. El-Naggar and Justin C. Biffinger},
keywords = { MR-1, Microbial fuel cell, Calcium chloride, EIS},
abstract = {There are several interconnected metabolic pathways in bacteria essential for the conversion of carbon electron sources directly into electrical currents using microbial fuel cells (MFCs). This study establishes a direct exogenous method to increase power output from a Shewanella oneidensis MR-1 containing MFC by adding calcium chloride to the culture medium. The current output from each CaCl2 concentration tested revealed that the addition of CaCl2 to 1400μM increased the current density by >80% (0.95–1.76μA/cm2) using sodium lactate as the sole carbon source. Furthermore, polarization curves showed that the maximum power output could be increased from 157 to 330μW with the addition of 2080μM CaCl2. Since the conductivity of the culture medium did not change after the addition of CaCl2 (confirmed by EIS and bulk conductivity measurements), this increase in power was primarily biological and not based on ionic effects. Thus, controlling the concentration of CaCl2 is a pathway to increase the efficiency and performance of S. oneidensis MR-1 MFCs.}
}
@article{FURUKI20061312,
title = {PHYSICAL PARAMETER IDENTIFICATION OF STRUCTURES INSTALLED WITH MR DAMPER},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {1},
pages = {1312-1317},
year = {2006},
note = {14th IFAC Symposium on Identification and System Parameter Estimation},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060329-3-AU-2901.00212},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015354483},
author = {Satoshi Furuki and Itthisek Nilkhamhang and Akira Sano},
keywords = {System identification, health monitoring, MR damper, subspace method, vibration isolation},
abstract = {This paper is concerned with a new practical identification algorithm for all physical model parameters of a multi-story structure installed with an MR damper between a ground level and first floor. By using not only acceleration measurements of ground and all stories but also the MR damper force, the proposed method can directly give damping and stiffness parameters as well as mass parameters of all floors. Optimal weighting on each mode information is also proposed to improve the physical parameter estimation errors. Validity of the proposed algorithm is investigated in numerical simulation with application to structure health monitoring, and experimental study dealing with vibration isolation of a simple structure with an MR damper installed.}
}
@article{LIM2022102621,
title = {Generalized self-calibrating simultaneous multi-slice MR image reconstruction from 3D Fourier encoding perspective},
journal = {Medical Image Analysis},
volume = {82},
pages = {102621},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522002493},
author = {Eun Ji Lim and Taehoon Shin and Joonyeol Lee and Jaeseok Park},
keywords = {Magnetic resonance imaging, Simultaneous multi-slice, Parallel imaging, Image reconstruction},
abstract = {This work introduces a novel, k-space based one-step solution for simultaneous multi-slice MR image reconstruction from 3D Fourier encoding perspective. With undersampled SMS imaging, image reconstruction suffers from both inter-slice leakages and in-plane aliasing artifacts. Aliasing separation becomes further challenging in the presence of discrepancies between calibration and imaging. To address them, in this work a measured SMS 3D k-space with additional calibrating signals is decomposed into SMS imaging and self-calibrating data sets. Extended controlled aliasing is performed by upsampling the measured data in the kz-direction. A slice-specific null space operator is then learned using extended self-calibration exploiting target slices and additional in-plane-shifted images. Inter-slice leakages and in-plane aliasing artifacts are jointly resolved in a single step by solving a constrained optimization problem in which null space reconstruction consistency is balanced with a Hankel-structured low rank prior while data fidelity in 3D Fourier space is enforced. Retrospective and prospective studies are performed to validate the effectiveness of the proposed method in various regions including knee and L-spine.}
}
@article{ESHAGHI2020103997,
title = {The effect of magnetorheological fluid and aerodynamic damping on the flutter boundaries of MR fluid sandwich plates in supersonic airflow},
journal = {European Journal of Mechanics - A/Solids},
volume = {82},
pages = {103997},
year = {2020},
issn = {0997-7538},
doi = {https://doi.org/10.1016/j.euromechsol.2020.103997},
url = {https://www.sciencedirect.com/science/article/pii/S0997753819309404},
author = {Mehdi Eshaghi},
keywords = {Flutter suppression, Aeroelastic stability, MR Fluid, Supersonic airflow},
abstract = {The present study deals with aeroelastic stability analysis of sandwich plates, containing magnetorheological fluid as the core layer, subjected to the supersonic airflow. The classical plate theory, Hamilton's principle and linear first-order piston theory have been employed to extract the governing equations of motion of the structure. The assumed mode method is used to solve the derived equations and identify the instability region of the sandwich plate. In order to demonstrate validity of the proposed solution, an experiment on a cantilever sandwich plate consisting of polyethylene terephthalate (PET) as the face layers and MR fluid (MRF 132DG) as the core layer is conducted. In this study, the primary attention is focused on the effect of MR fluid and aerodynamic damping on the stability of the MR sandwich plates. Furthermore, the effect of magnetic flux, boundary conditions and plate parameters on the flutter boundaries of the sandwich MR plates are investigated. The results highlight the significance of MR fluid and aerodynamic damping in the flutter suppression of the sandwich plate structures, in a wide range of aerodynamic pressure.}
}
@article{RUNDO201777,
title = {A fully automatic approach for multimodal PET and MR image segmentation in gamma knife treatment planning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {144},
pages = {77-96},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716300311},
author = {Leonardo Rundo and Alessandro Stefano and Carmelo Militello and Giorgio Russo and Maria Gabriella Sabini and Corrado D'Arrigo and Francesco Marletta and Massimo Ippolito and Giancarlo Mauri and Salvatore Vitabile and Maria Carla Gilardi},
keywords = {Multimodal image segmentation, PET/MR imaging, Fuzzy C-means clustering, Random Walker algorithm, Brain tumors, Gamma knife treatments},
abstract = {Background and objectives: Nowadays, clinical practice in Gamma Knife treatments is generally based on MRI anatomical information alone. However, the joint use of MRI and PET images can be useful for considering both anatomical and metabolic information about the lesion to be treated. In this paper we present a co-segmentation method to integrate the segmented Biological Target Volume (BTV), using [11C]-Methionine-PET (MET-PET) images, and the segmented Gross Target Volume (GTV), on the respective co-registered MR images. The resulting volume gives enhanced brain tumor information to be used in stereotactic neuro-radiosurgery treatment planning. GTV often does not match entirely with BTV, which provides metabolic information about brain lesions. For this reason, PET imaging is valuable and it could be used to provide complementary information useful for treatment planning. In this way, BTV can be used to modify GTV, enhancing Clinical Target Volume (CTV) delineation. Methods: A novel fully automatic multimodal PET/MRI segmentation method for Leksell Gamma Knife® treatments is proposed. This approach improves and combines two computer-assisted and operator-independent single modality methods, previously developed and validated, to segment BTV and GTV from PET and MR images, respectively. In addition, the GTV is utilized to combine the superior contrast of PET images with the higher spatial resolution of MRI, obtaining a new BTV, called BTVMRI. A total of 19 brain metastatic tumors, undergone stereotactic neuro-radiosurgery, were retrospectively analyzed. A framework for the evaluation of multimodal PET/MRI segmentation is also presented. Overlap-based and spatial distance-based metrics were considered to quantify similarity concerning PET and MRI segmentation approaches. Statistics was also included to measure correlation among the different segmentation processes. Since it is not possible to define a gold-standard CTV according to both MRI and PET images without treatment response assessment, the feasibility and the clinical value of BTV integration in Gamma Knife treatment planning were considered. Therefore, a qualitative evaluation was carried out by three experienced clinicians. Results: The achieved experimental results showed that GTV and BTV segmentations are statistically correlated (Spearman's rank correlation coefficient: 0.898) but they have low similarity degree (average Dice Similarity Coefficient: 61.87 ± 14.64). Therefore, volume measurements as well as evaluation metrics values demonstrated that MRI and PET convey different but complementary imaging information. GTV and BTV could be combined to enhance treatment planning. In more than 50% of cases the CTV was strongly or moderately conditioned by metabolic imaging. Especially, BTVMRI enhanced the CTV more accurately than BTV in 25% of cases. Conclusions: The proposed fully automatic multimodal PET/MRI segmentation method is a valid operator-independent methodology helping the clinicians to define a CTV that includes both metabolic and morphologic information. BTVMRI and GTV should be considered for a comprehensive treatment planning.}
}
@article{SONG20101,
title = {An automated three-dimensional plus time registration framework for dynamic MR renography},
journal = {Journal of Visual Communication and Image Representation},
volume = {21},
number = {1},
pages = {1-8},
year = {2010},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2009.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1047320309001291},
author = {Ting Song and Vivian S. Lee and Qun Chen and Henry Rusinek and Andrew F. Laine},
keywords = {MR renography, Dynamic MR, 3D plus time registration, Dynamic contrast-enhanced imaging, Wavelet representation, Anisotropic diffusion, Fourier-based registration, Automated respiratory motion correction, WRFT},
abstract = {Dynamic contrast-enhanced 3D images of the kidneys, or 3D MR renography, has the potential for broad clinical applications, but suffers from respiratory motion that limits analysis and interpretation. Manual registration is prohibitively labor-intensive. In this paper, a fully automated technique, Wavelet Representation and the Fourier Transform (WRFT) method, that corrects for translation and rotation motion in 3D MR renography is presented. The method was composed by anisotropic denoising, wavelet-based feature extraction, and Fourier-based registration. This was first evaluated on a set of simulated MR renography images with defined degrees of kidney motion. The method was then tested on 24 clinical patient MR renography data sets. Results of clinical testing were compared with the results obtained using a mutual information registration method. Based on intrarenal time-intensity curves, our method showed robust and consistent agreement with the results of manually coregistered data sets.}
}
@article{BASILI20133113,
title = {Shaking table experimentation on adjacent structures controlled by passive and semi-active MR dampers},
journal = {Journal of Sound and Vibration},
volume = {332},
number = {13},
pages = {3113-3133},
year = {2013},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2012.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X13000199},
author = {M. Basili and M. {De Angelis} and G. Fraraccio},
abstract = {This paper presents the results of shaking table tests on adjacent structures controlled by passive and semi-active MR dampers. The aim was to demonstrate experimentally the effectiveness of passive and semi-active strategies in reducing structural vibrations due to seismic excitation. The physical model at issue was represented by two adjacent steel structures, respectively of 4 and 2 levels, connected at the second level by a MR damper. When the device operated in semi-active mode, an ON–OFF control algorithm, derived by the Lyapunov stability theory, was implemented and experimentally validated. Since the experimentation concerned adjacent structures, two control objectives have been reached: global and selective protection. In case of global protection, the attention was focused on protecting both structures, whereas, in case of selective protection, the attention was focused on protecting only one structure. For each objective the effectiveness of passive control has been compared with the situation of no control and then the effectiveness of semi-active control has been compared with the passive one. The quantities directly compared have been: measured displacements, accelerations and force–displacement of the MR damper, moreover some global response quantities have been estimated from experimental measures, which are the base share force and the base bending moment, the input energy and the energy dissipated by the device. In order to evaluate the effectiveness of the control action in both passive and semi-active case, an energy index EDI, previously defined and already often applied numerically, has been utilized. The aspects investigated in the experimentation have been: the implementation and validation of the control algorithm for selective and global protection, the MR damper input voltage influence, the kind of seismic input and its intensity.}
}
@article{ABRAMOVICI201718,
title = {Context-aware Maintenance Support for Augmented Reality Assistance and Synchronous Multi-user Collaboration},
journal = {Procedia CIRP},
volume = {59},
pages = {18-22},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.09.042},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116311520},
author = {Michael Abramovici and Mario Wolf and Stefan Adwernat and Matthias Neges},
keywords = {Collaboration, Augmented Reality, mobile devices, maintenance},
abstract = {Maintenance processes are generally subdivided into tasks with specified goals for the concerned practitioners. Collaboration is achieved through coordination, cooperation and communication. Smart devices and the Internet of Things (IoT) improve communication between men and machine alike, so the potential gain on cooperation and coordination in an IoT-enabled environment is examined. In this approach a concept for a framework is proposed to create Augmented Reality based collaboration assistant systems. AR is not only a tool for the visualization of maintenance data, but also for team-wide communication and the display of warnings or other coordination-related indications. The framework is validated with the presented use case in a laboratory environment.}
}
@article{CHA2016192,
title = {Seismic fragility estimates of a moment-resisting frame building controlled by MR dampers using performance-based design},
journal = {Engineering Structures},
volume = {116},
pages = {192-202},
year = {2016},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2016.02.055},
url = {https://www.sciencedirect.com/science/article/pii/S0141029616300190},
author = {Young-Jin Cha and Jong-Wha Bai},
keywords = {Fragility analysis, Performance-based design, Magnetorheological damper, Multi-objective, Genetic algorithm, High-rise building},
abstract = {Seismic fragility was estimated for a controlled high-rise building using 200kN magnetorheological (MR) dampers with direct performance-based design (DPBD) to assess seismic vulnerability and to validate the performance of the DPBD which was previously developed. The DPBD offers multiple control design layouts for various performance levels subjected to different hazard levels using multi-objective optimization approaches. These multiple control design layouts for the given performance levels need to be validated using random seismic excitations because those performance-based designs (PBD) had been devolved based on the specific strength of design objective earthquakes (i.e., hazard levels) from the DPBD. In order to evaluate those PBD cases using MR dampers, two different approaches for fragility estimation of the four PBD cases under two hazard levels are conducted: traditional approach using the overall maximum interstory drift and system reliability approach which considers multiple limit states associated with the maximum interstory drift for stories within the entire system. The results are compared using 41 earthquake ground motions. From this study, overall seismic fragility relations have been derived from extensive fragility analyses in terms of broad range of hazard levels for multiple performance levels which were achieved by new direct performance-based design using MR dampers. Moreover, it is observed that the multiple performance-based control design cases obtained from DPBD clearly show significant reduction in seismic vulnerability compared to the uncontrolled case. It also shows different seismic fragility estimates against seismic hazards reflecting the performance enhancement based on the initial objective of the DPBD. Based on the results, the system reliability approach can identify the stories that have close interstory drifts to the overall maximum value allowing for more accurate estimates of the seismic fragility of multi-story buildings.}
}
@article{REYBECERRA2023104077,
title = {Improvement of short-term outcomes with VR-based safety training for work at heights},
journal = {Applied Ergonomics},
volume = {112},
pages = {104077},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104077},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023001151},
author = {Estefany Rey-Becerra and Lope H. Barrero and Rolf Ellegast and Annette Kluge},
keywords = {Serious games, Virtual environment, Construction industry},
abstract = {Serious games and virtual reality offer engaging learning opportunities and a cost-effective solution within an immersive and safe environment for safety training in construction. However, there have been few examples of safety training for work at heights developed using these technologies, especially commercial training. To fill this literature gap, a new VR-based safety training was developed and compared with lecture-based training across time. We conducted a quasi-experiment with a non-equivalent group design with 102 workers from six construction sites in Colombia. Learning objectives, observations from training centers, and national regulations were considered during the design of the training methods. Training outcomes were assessed using Kirkpatrick's model. We found that both training approaches were effective in improving knowledge test results and self-reported attitudes in the short-term; and risk-perception, self-reported behavior and safety climate in the long-term. In particular, participants of the VR-based training got significantly higher results in knowledge and reported higher attitudes (commitment and motivation) than participants of the lecture-based training. We suggest that safety managers and practitioners should invest in VR using serious games as an alternative to training programs based on short-term outcomes. Future work is needed to test VR for long-term outcomes.}
}
@article{SABIDUSSI2021102220,
title = {Recurrent inference machines as inverse problem solvers for MR relaxometry},
journal = {Medical Image Analysis},
volume = {74},
pages = {102220},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102220},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521002656},
author = {E.R. Sabidussi and S. Klein and M.W.A. Caan and S. Bazrafkan and A.J. {den Dekker} and J. Sijbers and W.J. Niessen and D.H.J. Poot},
keywords = {Quantitative MRI, Relaxometry, Deep learning, Mapping, Recurrent inference machines},
abstract = {In this paper, we propose the use of Recurrent Inference Machines (RIMs) to perform T1 and T2 mapping. The RIM is a neural network framework that learns an iterative inference process based on the signal model, similar to conventional statistical methods for quantitative MRI (QMRI), such as the Maximum Likelihood Estimator (MLE). This framework combines the advantages of both data-driven and model-based methods, and, we hypothesize, is a promising tool for QMRI. Previously, RIMs were used to solve linear inverse reconstruction problems. Here, we show that they can also be used to optimize non-linear problems and estimate relaxometry maps with high precision and accuracy. The developed RIM framework is evaluated in terms of accuracy and precision and compared to an MLE method and an implementation of the Residual Neural Network (ResNet). The results show that the RIM improves the quality of estimates compared to the other techniques in Monte Carlo experiments with simulated data, test-retest analysis of a system phantom, and in-vivo scans. Additionally, inference with the RIM is 150 times faster than the MLE, and robustness to (slight) variations of scanning parameters is demonstrated. Hence, the RIM is a promising and flexible method for QMRI. Coupled with an open-source training data generation tool, it presents a compelling alternative to previous methods.}
}
@article{LI2023102471,
title = {An AR-assisted Deep Reinforcement Learning-based approach towards mutual-cognitive safe human-robot interaction},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102471},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102471},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001533},
author = {Chengxi Li and Pai Zheng and Yue Yin and Yat Ming Pang and Shengzeng Huo},
keywords = {Smart manufacturing, Human robot interaction, Augmented reality, Deep reinforcement learning, Manufacturing safety},
abstract = {With the emergence of Industry 5.0, the human-centric manufacturing paradigm requires manufacturing equipment (robots, etc.) interactively assist human workers to deal with dynamic and complex production tasks. To achieve symbiotic human–robot interaction (HRI), the safety issue serves as a prerequisite foundation. Regarding the growing individualized demand of manufacturing tasks, the conventional rule-based safe HRI measures could not well address the safety requirements due to inflexibility and lacking synergy. To fill the gap, this work proposes a mutual-cognitive safe HRI approach including worker visual augmentation, robot velocity control, Digital Twin-enabled motion preview and collision detection, and Deep Reinforcement Learning-based robot collision avoidance motion planning in the Augmented Reality-assisted manner. Finally, the feasibility of the system design and the performance of the proposed approach are validated by establishing and executing the prototype HRI system in a practical scene.}
}
@incollection{KARAASLAN2022261,
title = {Chapter 10 - Mixed reality-assisted smart bridge inspection for future smart cities},
editor = {Amir H. Alavi and Maria Q. Feng and Pengcheng Jiao and Zahra Sharif-Khodaei},
booktitle = {The Rise of Smart Cities},
publisher = {Butterworth-Heinemann},
pages = {261-280},
year = {2022},
isbn = {978-0-12-817784-6},
doi = {https://doi.org/10.1016/B978-0-12-817784-6.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128177846000023},
author = {Enes Karaaslan and Mahta Zakaria and F. Necati Catbas},
keywords = {Real-time machine learning, Mixed reality, Human-computer interaction, Structural health monitoring, Smart cities, Smart infrastructure inspection},
abstract = {Smart infrastructures aim more efficient and accurate methods of routine inspection for long-term monitoring of the infrastructure to make smarter decision on maintenance and rehabilitation. Although some recent technologies (i.e., robotic techniques) that are currently in practice can collect objective, quantified data, the inspector’s own expertise is still critical in many instances. Yet, these technologies are designed to replace human expertise, or are ineffective in terms of saving time and labor. This chapter investigates a new methodology for structural inspections with the help of mixed reality technology and real-time machine learning to accelerate certain tasks of the inspector such as detection, measurement, and assessment of defects, and easy accessibility to defect locations. A functional, real-time machine learning system that can be ideally deployed in mixed reality devices and headsets which can be used by inspectors during their routine concrete infrastructure inspection is introduced. The deep learning models to be employed in the AI system can localize a concrete defect in real time and further analyze it by performing pixel wise segmentation while running on a mobile device architecture. First, a sufficiently large database of concrete defect images is gathered from various sources including publicly available crack and spalling datasets, real-world images taken during bridge inspections, and the public images from the internet search results. For defect localization, various state-of-the-art deep learning model architectures are investigated based on their memory allocation, inference speed, and flexibility to deploy different deep learning platforms. YoloV5s model was found to be the optimal model architecture for concrete defect localization to be deployed in the mixed reality system. For defect quantification, several segmentation architectures with three different classification backbones are trained on the collected image dataset with segmentation labels. Based on the model evaluation results, the PSPNet with EfficientNet-b0 backbone is found to be the best performing model in terms of inference speed and accuracy. The selected models for defect localization and quantification are deployed to the mixed reality platform and image tracking libraries are configured in the platform environment, and accurate distance estimation is accomplished using a calibration process. Lastly, a methodology for condition assessment of concrete defects using the mixed reality system is discussed. The proposed methodology can locate and track the defects using the mixed reality platform, which can eventually be transferred to cloud data and potentially used for remote assessments or updating a digital twins or BIMs.}
}
@article{KOLEY2016453,
title = {Delineation and diagnosis of brain tumors from post contrast T1-weighted MR images using rough granular computing and random forest},
journal = {Applied Soft Computing},
volume = {41},
pages = {453-465},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300096},
author = {Subhranil Koley and Anup K. Sadhu and Pabitra Mitra and Basabi Chakraborty and Chandan Chakraborty},
keywords = {MRI, Brain tumor, Rough entropy, Feature extraction, Tumor characterization},
abstract = {This paper presents a new approach of delineation and characterization of four different types of brain tumors viz. Glioblastoma multiforme (GBM), metastasis (MET), meningioma (MG) and granuloma (GN) from magnetic resonance imaging (MRI) slices of post contrast T1-weighted (T1C) sequence to improve the computer assistive diagnostic accuracy. An integrated framework of identification and extraction of tumor region, quantification of histogram, shape and textural features followed through pattern classification by machine learning algorithm has been proposed. Rough entropy based thresholding in granular computing paradigm has been adopted for delineation of tumor area. After accomplishing quantitative validation and comparison with existing methods, experimental results prove the efficiency and applicability of proposed segmentation approach. In the next stage, the extracted lesions have been quantified with 86 features to develop the training dataset. Random forest (RF), an ensemble learning scheme has been implemented, which learns the training data for accurate prediction of the class label of a given input. The performance of RF has been evaluated by statistical measures from 3 fold cross-validation and compared with five different classifiers. The same experiment has been repeated over the reduced set of features generated by correlation based feature selection strategy. Experimental results show the superiority of RF (Sensitivity achieved in %: GBM-96.7, MET-96.2, MG-98.1 and GN-97.7) with the complete set of features. The comparison of proposed methodology with the existing works signifies its applicability and effectiveness. Additionally a 10 fold cross-validation has been accomplished to justify the statistical significance of the classification accuracy achieved from proposed methodology.}
}
@article{KANNAN20081599,
title = {A new segmentation system for brain MR images based on fuzzy techniques},
journal = {Applied Soft Computing},
volume = {8},
number = {4},
pages = {1599-1606},
year = {2008},
note = {Soft Computing for Dynamic Data Mining},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2007.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S1568494608000045},
author = {S.R. Kannan},
keywords = {Fuzzy membership C-mean (FMCM), Fuzzy C-mean, Unsupervised clustering algorithm, MRI},
abstract = {This work concerns a new method called fuzzy membership C-means (FMCMs) for segmentation of magnetic resonance images (MRI), and an efficient program implementation of it to the segmentation of MRI. Classical unsupervised clustering methods including the FCM by Bezdek, suffer many problems that can be partially treated with a proper rule to construct the initial membership matrix to clusters. This work develops a specific method to construct the initial membership matrix to clusters in order to improve the strength of the clusters. The new FMCM is tested on a set of benchmarks and then the application to the segmentation of MR images is presented and compared with the results obtained using FCM.}
}
@article{SUK2020110737,
title = {Radial reflector discontinuity factors iteration scheme at VR-1},
journal = {Nuclear Engineering and Design},
volume = {366},
pages = {110737},
year = {2020},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2020.110737},
url = {https://www.sciencedirect.com/science/article/pii/S0029549320302314},
author = {Pavel Suk},
keywords = {Data preparation process, PARCS, SCALE Newt, Research reactors, VR-1, Neutron flux distribution},
abstract = {The macroscopic data preparation process is the essential part of the safe operation nuclear power station and despite the increasing calculation power this method stays essential in the near future. The data preparation process and mainly the assembly discontinuity factors (ADF) calculation are analysed in the paper. Developed models are connected with the C7 core of the VR-1 research reactor, which is operated by the Czech Technical University in Prague. The fuel models as well as the connection of the fuel and the reflector models are analysed. The multiplication factor and the neutron flux distribution are compared. The fuel ADF usage showed better prediction of the multiplication factor and also the neutron flux distribution. The new external iteration reflector ADF calculation method is developed, implemented and tested in the paper. The most realistic model of C7 core was used as a test case. The best results of the multiplication factor as well as the neutron flux distribution were obtained with the external iteration process ADF calculation. The multiplication factor was calculated with only 452 pcm discrepancy, more than 6600 pcm better result than without ADF and more than 3400 pcm better result than with reflector ADF calculated via SCALE Newt.}
}
@article{ZHAN201691,
title = {MR image bias field harmonic approximation with histogram statistical analysis},
journal = {Pattern Recognition Letters},
volume = {83},
pages = {91-98},
year = {2016},
note = {Geometric, topological and harmonic trends to image processing},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2016.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167865516000568},
author = {Shu Zhan and Xiong Yang},
keywords = {MRI, Energy function, Bias field correction, Harmonic approximation, Statistical analysis},
abstract = {This study investigates a method to correct the intensity inhomogeneity field of magnetic resonance image. The algorithm takes full advantage of the properties of the magnetic resonance image, namely, the piecewise character (piecewise constant and piecewise smooth) of true image which characterizes a physical property of the tissue anatomical structure and the smoothly varying property of bias field which accounts for the intensity inhomogeneity. An energy function was constructed by embedding this characters into the image model. We can get the estimation of bias field and the segmentation of tissue by minimizing the energy function. The initial parameter of energy function is calculated automatically by statistical analysis. By mixing the fitting basis function with cosine function and polynomial function, we can obtain an accurate approximation of the bias field. A comparative performance evaluation is carried out over a large set of experiments using synthetic magnetic resonance data. Besides, a set of tests on real prostate magnetic resonance image with severe intensity inhomogeneity field is shown to demonstrate the validity of our method.}
}
@article{CHA201212,
title = {A virtual reality based fire training simulator integrated with fire dynamics data},
journal = {Fire Safety Journal},
volume = {50},
pages = {12-24},
year = {2012},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2012.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0379711212000136},
author = {Moohyun Cha and Soonhung Han and Jaikyung Lee and Byungil Choi},
keywords = {Virtual reality, Training simulator, CFD (Computational Fluid Dynamics), Fire simulation},
abstract = {VR (virtual reality)-based fire training simulators provide the general public or inexperienced firefighters or commanders with wide-ranging second-hand experience so that they can make prompt decisions and safe and organized responses in actual fire situations. In order to effectively achieve this training goal, it is crucial to reliably express fire dynamics as realistic graphics. In the field of engineering, computational fluid dynamics (CFD) is widely used to precisely predict the behaviors of fluid phenomena. The resultant data, however, have structures and capacities that are not readily applied to real-time virtual reality systems. This study proposes a series of data conversion techniques and a real-time processing framework to develop a fire training simulator on the basis of a precise CFD simulation that is capable of calculating various invisible physical quantities such as toxic gases and heat as well as visible factors such as smoke and flame. By exploiting safety level-based visualization mapping, this study also proposes a new method to intuitively experience dangerous fire environments and perform training and evaluation. Lastly, this study implements a simulator that can undertake simple firefighting activities such as evacuation and rescue in fire situations at road tunnels; the functions and real-time performance of the simulator have been experimentally measured to verify the applicability of the proposed framework.}
}