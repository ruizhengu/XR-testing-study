@inproceedings{10.1145/2669592.2669651,
author = {Reisman, Ronald J. and Feiner, Steven K. and Brown, David M.},
title = {Augmented reality tower technology flight test},
year = {2014},
isbn = {9781450325608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669592.2669651},
doi = {10.1145/2669592.2669651},
abstract = {Augmented reality technology adapted for air traffic control tower applications was used to track an OH-58C helicopter in proximity to an airport. A camera and 'see-through' display system was used to measure the registration error of static airport features and dynamic test aircraft. The observed registration errors of the test aircraft were largely attributable to two terms of error: 1) aircraft surveillance transport latency, and 2) registration error (from all sources) of the static environment. Compensating for registration errors of static objects and modeling aircraft movement reduces registration errors for dynamic (aircraft) objects to ≤2° of error for aircraft-surveillance transport latency ≤ 5 seconds, and to ≤1° of error for transport latency ≤ 2 seconds.},
booktitle = {Proceedings of the International Conference on Human-Computer Interaction in Aerospace},
articleno = {8},
numpages = {8},
keywords = {static registration, head mounted display, flight test, dynamic registration, augmented reality, air traffic control towers, ADS-B},
location = {Santa Clara, California},
series = {HCI-Aero '14}
}

@article{10.1145/3626238,
author = {Bartlett, Kristin A. and Palacios-Ib\'{a}\~{n}ez, Almudena and Camba, Jorge Dorribo},
title = {Design and Validation of a Virtual Reality Mental Rotation Test},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1544-3558},
url = {https://doi.org/10.1145/3626238},
doi = {10.1145/3626238},
abstract = {Mental rotation, a common measure of spatial ability, has traditionally been assessed through paper-based instruments like the Mental Rotation Test (MRT) or the Purdue Spatial Visualization Test: Rotations (PSVT:R). The fact that these instruments present 3D shapes in a 2D format devoid of natural cues like shading and perspective likely limits their ability to accurately assess the fundamental skill of mentally rotating 3D shapes. In this paper, we describe the Virtual Reality Mental Rotation Assessment (VRMRA), a virtual reality-based mental rotation assessment derived from the Revised PSVT:R and MRT. The VRMRA reimagines traditional mental rotation assessments in a room-scale virtual environment and uses hand-tracking and elements of gamification in attempts to create an intuitive, engaging experience for test-takers. To validate the instrument, we compared response patterns in the VRMRA with patterns observed on the MRT and Revised PSVT:R. For the PSVT:R-type questions, items requiring a rotation around two axes were significantly harder than items requiring rotations around a single axis in the VRMRA, which is not the case in the Revised PSVT:R. For the MRT-type questions in the VRMRA, a moderate negative correlation was found between the degree of rotation in the X direction and item difficulty. While the problem of occlusion was reduced, features of the shapes and distractors accounted for 50.6\% of the variance in item difficulty. Results suggest that the VRMRA is likely a more accurate tool to assess mental rotation ability in comparison to traditional instruments which present the stimuli through 2D media. Our findings also point to potential problems with the fundamental designs of the Revised PSVT:R and MRT question formats.},
journal = {ACM Trans. Appl. Percept.},
month = {jan},
articleno = {5},
numpages = {22},
keywords = {PSVT:R, MRT, visual perception, virtual reality, mental rotation, Spatial ability}
}

@inproceedings{10.1145/3552327.3552337,
author = {Liinasuo, Marja and Kuula, Timo and Goriachev, Vladimir and Helin, Kaj},
title = {Remote Testing of an Augmented Reality System},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552327.3552337},
doi = {10.1145/3552327.3552337},
abstract = {Remote test settings have become more common due to COVID-19. Our paper presents two user tests focusing on the usability and user experience of an augmented reality-based solution, i.e., augmented reality system. We describe the proceeding of the tests from the perspective of what party has participated in the test in the same location as the test participant, i.e., locally, and what party remotely. The importance - or unimportance - of physical presence is contemplated from the perspective of the successfulness of the test. The physical presence of a person providing technical support to the test participant during the testing proved vital for the augmented reality related testing; the location of other test organisers appears more indifferent in this context.},
booktitle = {Proceedings of the 33rd European Conference on Cognitive Ergonomics},
articleno = {3},
numpages = {4},
keywords = {User experience, Usability, Testing methodology, Augmented reality},
location = {<conf-loc>, <city>Kaiserslautern</city>, <country>Germany</country>, </conf-loc>},
series = {ECCE '22}
}

@inproceedings{10.1145/3473856.3474034,
author = {Peintner, Jakob and Funk Drechsler, Maikol and Reway, Fabio and Seifert, Georg and Huber, Werner and Riener, Andreas},
title = {Mixed Reality Environment for Complex Scenario Testing},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3474034},
doi = {10.1145/3473856.3474034},
abstract = {Driver assistance systems are currently evaluated using standardized test procedures as defined, for example, by Euro NCAP, but which only represent a narrow, idealized spectrum of real-life situations. Test conditions hardly take into account the variability that occur in everyday traffic – adverse weather conditions such as fog, rain, snow or darkness are typically not considered, nor are the resulting changes in, e.g., the behavior of pedestrians. In an emergency braking situation, the behavior of the VRU can also be directly affected by the interaction with the vehicle. For example, a pedestrian might be alarmed by an approaching car. This could elicit a very different response in the human that cannot be simulated in the dummy motion as defined in the standard test procedure. In order to create a more versatile and interactive simulation and test environment, we have developed the Mixed Reality Test Environment MiRE. The goal of Mixed Reality Test Environment MiRE is to enable testing of a broader range of scenarios, including interaction between VRUs and vehicles. During the demo, we will demonstrate MiRE live to interested visitors and show its full potential for future driving safety tests.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {605–608},
numpages = {4},
keywords = {Vehicle in the Loop, Test Procedures, Mixed Reality, Automated Driving Systems},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.5555/2525493.2525500,
author = {Irlitti, Andrew and Von Itzstein, Stewart},
title = {Validating constraint driven design techniques in spatial augmented reality},
year = {2013},
isbn = {9781921770241},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {We describe new techniques to allow constraint driven design using spatial augmented reality (SAR), using projectors to animate a physical prop. The goal is to bring the designer into the visual working space, interacting directly with a dynamic design, allowing for intuitive interactions, while gaining access to affordance through the use of physical objects. We address the current industrial design process, expressing our intended area of improvement with the use of SAR. To corroborate our hypothesis, we have created a prototype system, which we have called SARventor. Within this paper, we describe the constraint theory we have applied, the interaction techniques devised to help illustrate our ideas and goals, and finally the combination of all input and output tasks provided by SARventor.To validate the new techniques, an evaluation of the prototype system was conducted. The results of this evaluation indicated promises for a system allowing a dynamic design solution within SAR. Design experts see potential in leveraging SAR to assist in the collaborative process during industrial design sessions, offering a high fidelity, transparent application, presenting an enhanced insight into critical design decisions to the projects stakeholders. Through the rich availability of affordance in SAR, designers and stakeholders have the opportunity to see first-hand the effects of the proposed design while considering both the ergonomic and safety requirements.},
booktitle = {Proceedings of the Fourteenth Australasian User Interface Conference - Volume 139},
pages = {63–72},
numpages = {10},
keywords = {tangible user interface, spatial augmented reality, industrial design process},
location = {Melbourne, Australia},
series = {AUIC '13}
}

@inproceedings{10.1109/ISMAR.2005.23,
author = {Gomez, Javier-Flavio Vigueras and Simon, Gilles and Berger, Marie-Odile},
title = {Calibration Errors in Augmented Reality: A Practical Study},
year = {2005},
isbn = {0769524591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2005.23},
doi = {10.1109/ISMAR.2005.23},
abstract = {This paper confronts some theoretical camera models to reality and evaluates the suitability of these models for effective augmented reality (AR). It analyses what level of accuracy can be expected in real situations using a particular camera model and how robust the results are against realistic calibration errors. An experimental protocol is used that consists of taking images of a particular scene from different quality cameras mounted on a 4DOF micro-controlled device. The scene is made of a calibration target and three markers placed at different distances of the target. This protocol enables us to consider assessment criteria specific to AR as alignment error and visual impression, in addition to the classical camera positioning error.},
booktitle = {Proceedings of the 4th IEEE/ACM International Symposium on Mixed and Augmented Reality},
pages = {154–163},
numpages = {10},
series = {ISMAR '05}
}

@inproceedings{10.5555/850976.854950,
author = {Geiger, C. and Paelke, V. and Reimann, C. and Rosenbach, W. and Stoecklein, J.},
title = {Testable Design Representations for Mobile Augmented Reality Authoring},
year = {2002},
isbn = {0769517811},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper applies the idea of a continuously testable design representation to authoring of augmented realities for mobile devices.},
booktitle = {Proceedings of the 1st International Symposium on Mixed and Augmented Reality},
pages = {281},
keywords = {Prototyping, Authoring Augmented Realities},
series = {ISMAR '02}
}

@inproceedings{10.1145/769953.769966,
author = {Bierbaum, Allen and Hartling, Patrick and Cruz-Neira, Carolina},
title = {Automated testing of virtual reality application interfaces},
year = {2003},
isbn = {1581136862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/769953.769966},
doi = {10.1145/769953.769966},
abstract = {We describe a technique for supporting testing of the interaction aspect of virtual reality (VR) applications. Testing is a fundamental development practice that forms the basis of many software engineering methodologies. It is used to ensure the correct behavior of applications. Currently, there is no common pattern for automated testing of VR application interaction. We review current software engineering practices used in testing and explore how they may be applied to the specific realm of VR applications. We then discuss the ways in which current practices are insuficient to test VR application interaction and propose a testing architecture for addressing the problems. We present an implementation of the design written on top of the VR Juggler platform. This system allows VR developers to employ standard software engineering techniques that require automated testing methods.},
booktitle = {Proceedings of the Workshop on Virtual Environments 2003},
pages = {107–114},
numpages = {8},
keywords = {unit testing, extreme programming, VR Juggler},
location = {Zurich, Switzerland},
series = {EGVE '03}
}

@inproceedings{10.1145/3573381.3597215,
author = {Bakk, \'{A}gnes Karolina and T\"{o}lgyesi, Borb\'{a}la and Bark\'{o}czi, M\'{a}t\'{e} and Buri, Bal\'{a}zs and Szab\'{o}, Andr\'{a}s and Tobai, Botond and Georgieva, Iva and Roth, Christian},
title = {Zenctuary VR: Simulating Nature in an Interactive Virtual Reality Application: Description of the design process of creating a garden in Virtual Reality with the aim of testing its restorative effects.},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597215},
doi = {10.1145/3573381.3597215},
abstract = {In this paper we present the design process of a virtual reality experience the aim of which is to have a restorative effect on users. In the simulated natural site, the user can interact with some elements of the environment and can also explore the view. We describe how we tried to create a more realistic sense of nature by relying on high quality graphics, the use of free-roaming space, and naturalistic interactions. During the design process we avoided gameful interactions and instead created playful interactions, while also relying on the multimodal aspect of the virtual reality technology.&nbsp;},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {165–169},
numpages = {5},
keywords = {virtual restorative environments, restorative effect of nature, nature simulation&nbsp;, immersive virtual reality},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3473682.3481878,
author = {Funk Drechsler, Maikol and Peintner, Jakob Benedikt and Seifert, Georg and Huber, Werner and Riener, Andreas},
title = {Mixed Reality Environment for Testing Automated Vehicle and Pedestrian Interaction},
year = {2021},
isbn = {9781450386418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473682.3481878},
doi = {10.1145/3473682.3481878},
abstract = {The test and development of Automated Driving Systems is usually realized by scenario based testing or virtual testing environments. These methods apply artificial targets to trigger the safety critical functions under specific predefined scenarios, as the NCAP or IIHS test catalogues. Despite having a good reproducibility, these approaches hardly permit the evaluation of new interaction concepts like external Human-Machine Interfaces (eHMIs), since the interaction between real users and the vehicle cannot be realistic reproduced without risks to the participants. The novel Mixed Reality Test Environment (MiRE) overcomes this limitation by the integration of Virtual Reality (VR) technologies, Dynamic Vehicle-in-the-Loop (DynViL) and the Virtual Environment. In MiRE the movement and positioning of the vehicles and the VRUs are tracked in real time and reproduced in the virtual environment. Synthetic data from the virtual environment is generated to stimulate the vehicle and the human participant, enabling a safe interaction between both entities.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {229–232},
numpages = {4},
keywords = {Vehicle-in-the-Loop, Test Procedures, Sensor stimulation, External Human-Machine Interfaces, Automated Driving Systems},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21 Adjunct}
}

@inproceedings{10.1145/3565387.3565448,
author = {Wang, Peng and Hu, Yaoguang and Yang, Xiaonan and Wang, Jingfei},
title = {Augmented Reality-Based Rapid Digital Verification of the Body-in-white for Intelligent Manufacturing},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565448},
doi = {10.1145/3565387.3565448},
abstract = {The body-in-white (BIW) is the structural assembly of the automobile after it is welded, which is the skeleton of the automobile. BIW design verification is an important part of the automobile development, not only to test the structural performance, but also to verify its manufacturability in subsequent production lines. Augmented reality, as an important tool for rapid digital verification, has been widely used in intelligent manufacturing systems. This paper presents an augmented reality-based method and associated system for rapid digital verification of BIW manufacturability during the BIW design phase. The system approach is described: 1) AR based auto BIW verification environment construction, 2) Bare hand interaction assisted AR verification, 3) Distance measurement and collision detection. Finally, an experimental scenario was built to illustrate how this AR system assist in verifying BIW structures on a production line and reveal the huge potential of AR in product design verification.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {61},
numpages = {5},
keywords = {Digital verification, Body-in-white, Augmented reality},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3359996.3364247,
author = {Gradl, Stefan and Wirth, Markus and M\"{a}chtlinger, Nico and Poguntke, Romina and Wonner, Andrea and Rohleder, Nicolas and Eskofier, Bjoern M.},
title = {The Stroop Room: A Virtual Reality-Enhanced Stroop Test},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359996.3364247},
doi = {10.1145/3359996.3364247},
abstract = {The Stroop Test is a well known and regularly employed stressor in laboratory research. In contrast to other methods, it is not based on fear of physical harm or social shame. Consequently, it is more likely accepted by a wide population. In our always-on, technology-driven, social-media centered world, large-scale in-field stress research will need adequate experimental tools to explore the increasing prevalence of stress-related diseases without bringing subjects into laboratories. This is why we designed the Stroop Room: A virtual reality-based adaptation of the Stroop Test using elements of the virtual world to extend the demands of the original test and at the same time make it easily accessible. It is open source and can be used and improved by anyone as an in-the-wild, repeatable, laboratory-quality stressor. In this work, the method is presented and an evaluation study described, to demonstrate its effectiveness in provoking cognitive stress. 16 male and 16 female subjects were tested in the Stroop Room while recording the electrocardiogram, electrodermal activity, saliva based cortisol and alpha-amylase, performance metrics and an array of questionnaire-based assessments regarding psychological confounders, stress state and likability of the simulation. Our results show that the Stroop Room increases heart rate on average by 19\%, other heart rate variability time-domain parameters (RMSSD, pNN50) decrease by 24\%-47\%, and its most stress-correlated frequency-parameter (LF/HF) increases by 107\%. Skin conductance (SC) level increases by 63\% and non-specific SC responses by 135\% on average. Salivary cortisol and alpha-amylase concentrations increase significantly in some specific conditions. Compared to related work using the Stroop Test, this is an improvement for some metrics by around 30\%-40\%. Questionnaire evaluation show a strong engagement of users with the simulation and some aspects of a flow-induction. These findings support the effectiveness of a Stroop Test involving 3-dimensional interactivity and thus the Stroop Room demonstrates how this can be applied in a playful interaction that could be used pervasively.},
booktitle = {Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {12},
keywords = {virtual reality, stroop test, psychological stress, heart rate, cortisol, amylase, HRV, EDA},
location = {Parramatta, NSW, Australia},
series = {VRST '19}
}

@inproceedings{10.1145/3505284.3532988,
author = {Mevlevio\u{g}lu, Deniz and Tabirca, Sabin and Murphy, David},
title = {Emotional Virtual Reality Stroop Task: an Immersive Cognitive Test},
year = {2022},
isbn = {9781450392129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505284.3532988},
doi = {10.1145/3505284.3532988},
abstract = {Stroop Colour-Word Task has been widely used as a cognitive task. There are computerised and Virtual Reality versions of this task that are commonly used. The emotional version of the task, called the Emotional Stroop Colour-Word task is commonly used to induce certain emotions in a person. We are developing an application that brings the Emotional Stroop Colour-Word task into Virtual Reality. The aim of this application is to elicit different stress levels on the user and to record associated brain, heart and skin activity using wearable sensors. It is an immersive application that includes a tutorial, artificial intelligence generated audio instructions and a logging system for the user activity.},
booktitle = {Proceedings of the 2022 ACM International Conference on Interactive Media Experiences},
pages = {387–390},
numpages = {4},
keywords = {emotional Stroop, biosignals, biosensors, anxiety, VR, Stroop color-word test},
location = {Aveiro, JB, Portugal},
series = {IMX '22}
}

@inproceedings{10.1145/3603421.3603432,
author = {Gulrez, Tauseef and Kekoc, Vlado and Gaurvit, Emmanuel and Schuhmacher, Mark and Mills, Tony},
title = {Machine Learning Enabled Mixed Reality Systems - For Evaluation and Validation of Augmented Experience in Aircraft Maintenance},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603432},
doi = {10.1145/3603421.3603432},
abstract = {In this paper, a framework to assess the usability of mixed reality (MR) systems in terms of physical load metric is presented. The participants used a MR system for an aircraft maintenance task, while a computer vision (CV) based machine learning (ML) method extracted the skeletal data of the participants. A bi-manual musculoskeletal computational model of human upper-limb was overlayed onto the skeletal data to derive upper-limb muscle forces/energy spent during the aircraft maintenance tasks. The experiments were conducted with and without the use of MR devices, while CV based ML was applied to detect the energy used of aircraft maintainers in real-time while they performed tasks on an MRH90 helicopter. An analysis of augmented experience has been presented, which will act as a driver for the use of MR based applications in aircraft maintenance as well as battle-ready platforms. Preliminary results are encouraging and we envisage to use the proposed methodology for an objective assessment and detection of physical workload.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {77–83},
numpages = {7},
keywords = {Physical load, Mixed reality, Machine learning, Aircraft maintenance},
location = {<conf-loc>, <city>Sydney</city>, <country>Australia</country>, </conf-loc>},
series = {ICVARS '23}
}

@inproceedings{10.1145/3551349.3561160,
author = {Rafi, Tahmid and Zhang, Xueling and Wang, Xiaoyin},
title = {PredART: Towards Automatic Oracle Prediction of Object Placements in Augmented Reality Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561160},
doi = {10.1145/3551349.3561160},
abstract = {While the emerging Augmented Reality (AR) technique allows a lot of new application opportunities, from education and communication to gaming, current augmented apps often have complaints about their usability and/or user experience due to placement errors of virtual objects. Therefore, identifying noticeable placement errors is an important goal in the testing of AR apps. However, placement errors can only be perceived by human beings and may need to be confirmed by multiple users, making automatic testing very challenging. In this paper, we propose PredART, a novel approach to predict human ratings of virtual object placements that can be used as test oracles in automated AR testing. PredART is based on automatic screenshot sampling, crowd sourcing, and a hybrid neural network for image regression. The evaluation on a test set of 480 screenshots shows that our approach can achieve an accuracy of 85.0\% and a mean absolute error, mean squared error, and root mean squared error of 0.047, 0.008, and 0.091, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {77},
numpages = {13},
keywords = {Virtual Objects, Placement Error, Augmented Reality},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3411764.3445216,
author = {Uriu, Daisuke and Obushi, Noriyasu and Kashino, Zendai and Hiyama, Atsushi and Inami, Masahiko},
title = {Floral Tribute Ritual in Virtual Reality: Design and Validation of SenseVase with Virtual Memorial},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445216},
doi = {10.1145/3411764.3445216},
abstract = {While floral tributes are commonly used for the public commemoration of victims of disasters, war, and other accidents, flowers in vases color everyday life. In this research, these features of flowers are intertwined with the recent phenomenon of online memorials to develop a virtual floral tribute concept that includes physical rituals. We designed SenseVase, a smart vase to detect flowers placed in it, and a 3DCG Virtual Memorial that illustrates floral tributes given by people using SenseVases at home. This paper describes how we developed our design concept by reviewing previous literature and social aspects, and presents a video illustrating the concept. To validate the current concept, we interviewed several experts knowledgeable in public commemorations, virtual and online communities, and the floral business. Through a discussion of our findings from the design process and interviews, we propose a new direction for how HCI technology can contribute to public commemoration in addition to personal memorialization.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {628},
numpages = {15},
keywords = {Thanatosensitive Design, Techno-spiritual Practices, Research through Design, Online Memorial, Mourning, Memorialization, Death Rituals, Commemoration},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI '21}
}

@inproceedings{10.1145/1851600.1851655,
author = {Schinke, Torben and Henze, Niels and Boll, Susanne},
title = {Visualization of off-screen objects in mobile augmented reality},
year = {2010},
isbn = {9781605588353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851600.1851655},
doi = {10.1145/1851600.1851655},
abstract = {An emerging technology for tourism information systems is mobile Augmented Reality using the position and orientation sensors of recent smartphones. State-of-the-art mobile Augmented Reality application accompanies the Augmented Reality visualization with a small mini-map to provide an overview of nearby points of interest (POIs). In this paper we develop an alternative visualization for nearby POIs based on off-screen visualization techniques for digital maps. The off-screen visualization uses arrows directly embedded into the Augmented Reality scene which point at the POIs. In the conducted study 26 participants explored nearby POIs and had to interpret their position. We show that participants are faster and can interpret the position of POIs more precisely with the developed visualization technique.},
booktitle = {Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {313–316},
numpages = {4},
keywords = {orientation, mobile phone, augmented reality},
location = {Lisbon, Portugal},
series = {MobileHCI '10}
}

@inproceedings{10.1145/3597926.3598134,
author = {Rzig, Dhia Elhaq and Iqbal, Nafees and Attisano, Isabella and Qin, Xue and Hassan, Foyzul},
title = {Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598134},
doi = {10.1145/3597926.3598134},
abstract = {Virtual Reality (VR) is an emerging technique that provides a unique real-time experience for users. VR technologies have provided revolutionary user experiences in various scenarios (e.g., training, education, gaming, etc.). However, testing VR applications is challenging due to their nature which necessitates physical interactivity, and their reliance on specific hardware systems. Despite the recent advancements in VR technology and its usage scenarios, we still know little about VR application testing. To fill up this knowledge gap, we performed an empirical study on 314 open-source VR applications. Our analysis identified that 79\% of the VR projects evaluated did not have any automatic tests, and for the VR projects that did, the median functional-method to test-method ratios were lower than those of other project types. Moreover, we uncovered tool support issues concerning the measurement of VR code coverage, and the assertion density results we were able to generate were relatively low, with an average of 17.63\%. Finally, through a manual analysis of 370 test cases, we identified the different categories of test cases being used to validate VR application quality attributes. Furthermore, we extracted which of these categories are VR-attention, meaning that test writers need to pay special attention to VR characteristics when writing tests of these categories. We believe that our findings constitute a call to action for the VR development community to improve their automatic testing practices and provide directions for software engineering researchers to develop advanced techniques for automatic test case generation and test quality analysis for VR applications. Our replication package containing the dataset we used, software tools we developed, and the results we found, is accessible at ‍https://doi.org/10.6084/m9.figshare.19678938.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1269–1281},
numpages = {13},
keywords = {Virtual Reality, Software Testing, Empirical Study},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611659.3617210,
author = {Graf, Max and Barthet, Mathieu},
title = {Reducing Sensing Errors in a Mixed Reality Musical Instrument},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3617210},
doi = {10.1145/3611659.3617210},
abstract = {This paper describes the design and evaluation of Netz, a novel mixed reality musical instrument that leverages artificial intelligence for reducing errors in gesture interpretation by the system. We followed a participatory design approach over three months through regular sessions with a professional musician. We explain our design process and discuss technological sensing errors in mixed reality devices, which emerged during the design sessions. We investigate the use of interactive machine learning techniques to mitigate such errors. Results from statistical analyses indicate that a deep learning model based on interactive machine learning can significantly reduce the number of technological errors in a set of musical performance tasks with the mixed reality musical instrument. Based on our findings, we argue that the application of interactive machine learning techniques can be beneficial for embodied, hand-controlled musical instruments in the mixed reality domain.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {72},
numpages = {2},
keywords = {hand-pose estimation, mixed reality musical instruments, participatory design},
location = {<conf-loc>, <city>Christchurch</city>, <country>New Zealand</country>, </conf-loc>},
series = {VRST '23}
}

@inproceedings{10.1145/3154943.3154947,
author = {Jun, Seung-Hwa and Kim, Jung-Ho},
title = {5G will popularize virtual and augmented reality: KT's trials for world's first 5G olympics in Pyeongchang},
year = {2017},
isbn = {9781450353120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3154943.3154947},
doi = {10.1145/3154943.3154947},
abstract = {As the telegraph and the Internet served in the previous industrial revolutions, the fifth generation (5G) network works as a general purpose technology (GPT) for the Fourth Industrial Revolution. Virtual and augmented reality, one of the most attractive technologies leading the Fourth is ideal for 5G, benefitting from enhanced bandwidth and latency. KT's attempt to showcase various virtual and augmented reality services for the world's first 5G Olympics in 2018 will be historic.1 The world will take the first step into the 5G era and get a glimpse of the infinite possibilities of VR/AR.},
booktitle = {Proceedings of the International Conference on Electronic Commerce},
articleno = {4},
numpages = {8},
keywords = {virtual reality, the fourth industrial revolution, augmented reality, KT, GPT (General Purpose Technology), ACM proceedings, 5G, 2018 winter olympics},
location = {Pangyo, Seongnam, Republic of Korea},
series = {ICEC '17}
}

@inproceedings{10.1145/2087756.2087816,
author = {Yuan, Miaolong and Khan, Ishtiaq Rasool and Farbiz, Farzam and Niswar, Arthur and Huang, Zhiyong},
title = {A mixed reality system for virtual glasses try-on},
year = {2011},
isbn = {9781450310604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2087756.2087816},
doi = {10.1145/2087756.2087816},
abstract = {In this paper we present an augmented reality system for automatic try-on of 3D virtual eyeglasses. The user can select from various virtual models of eyeglasses for trying-on and the system will automatically fit the selected virtual glasses on the user's face. The user can see his/her face as in a mirror with the 3D virtual glasses fitted on it. We also propose a method for handling the occlusion problem, to display only those parts of the glasses that are not occluded by the face. This system can be used for online shopping, or short listing a large set of available models to a few before physical try-on at a retailer's site.},
booktitle = {Proceedings of the 10th International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {363–366},
numpages = {4},
keywords = {mixed reality, computer vision and virtual try-on system},
location = {Hong Kong, China},
series = {VRCAI '11}
}

@inproceedings{10.1145/3543407.3543412,
author = {Liu, Qinghe and Kong, Lingming and Zhang, Guanzhe and Zhao, Lijun},
title = {Application of Mixed Reality in Driverless Vehicles Technology Courses Testing and Teaching Activities},
year = {2022},
isbn = {9781450396790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543407.3543412},
doi = {10.1145/3543407.3543412},
abstract = {At present, driverless vehicles technology has become the most concerned hot spot in the automotive field, and college students' learning enthusiasm in this field is unprecedented. However, the existing teaching and testing methods and resources are far from meeting the requirements. The main difficulties are (1) it is difficult to realize the traffic scene of driverless vehicles technology; (2) Using real vehicle experiment is prone to collision, resulting in loss of personnel and property. In this paper, a technical scheme of constructing driverless vehicles technology testing and teaching environment in the laboratory by using mixed reality technology is proposed. The complex traffic scene is realized in the simulation software by using virtual reality technology, and the driverless vehicles adopts the real scaling-down automatic chassis model. Through hardware in the loop simulation, the mixture of virtual traffic environment and the actual reality of driverless vehicle model is realized. This method has the advantages of rich traffic scenes and flexible adjustment, and the collision between real vehicles and virtual vehicles will not cause actual property damage.},
booktitle = {Proceedings of the 4th International Conference on Modern Educational Technology},
pages = {28–32},
numpages = {5},
keywords = {Teaching and Testing, Mixture of Virtual and Reality, Mixed Reality, Driverless vehicles Technologies},
location = {Macau, China},
series = {ICMET '22}
}

@inproceedings{10.1145/3272973.3274048,
author = {Siri Jr., John Scott and Khalid, Hamna and Nguyen, Luong and Wohn, Donghee Yvette},
title = {Screen-viewing Practices in Social Virtual Reality},
year = {2018},
isbn = {9781450360180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3272973.3274048},
doi = {10.1145/3272973.3274048},
abstract = {This study reports screen-viewing practices in the social virtual reality platform Altspace, based on immersive observations. We identified what type of content is being watched and how the screen content facilitates, complements, or is the focus of social interactions.},
booktitle = {Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {173–176},
numpages = {4},
keywords = {watching together, video, social vr, social virtual reality, social television, online community, altspace},
location = {<conf-loc>, <city>Jersey City</city>, <state>NJ</state>, <country>USA</country>, </conf-loc>},
series = {CSCW '18 Companion}
}

@inproceedings{10.1145/3349801.3349812,
author = {Aksu, Ridvan and Chakareski, Jacob and Velisavljevic, Vladan},
title = {Displacement Error Analysis of 6-DoF Virtual Reality},
year = {2019},
isbn = {9781450371896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349801.3349812},
doi = {10.1145/3349801.3349812},
abstract = {Virtual view synthesis is a critical step in enabling Six-Degrees of Freedom (DoF) immersion experiences in Virtual Reality (VR). It comprises synthesis of virtual viewpoints for a user navigating the immersion environment, based on a small subset of captured viewpoints featuring texture and depth maps. We investigate the extreme values of the displacement error in view synthesis caused by depth map quantization, for a given 6DoF VR video dataset, particularly based on the camera settings, scene properties, and the depth map quantization error. We establish a linear relationship between the displacement error and the quantization error, scaled by the sine of the angle between the location of the object and the virtual view in the 3D scene, formed at the reference camera location. In the majority of cases the horizontal and vertical displacement errors induced at a pixel location of a reconstructed 360° viewpoint comprising the immersion environment are respectively proportional to 3/5 and 1/5 of the respective quantization error. Also, the distance between the reference view and the synthesized view severely increases the displacement error. Following these observations: displacement error values can be predicted for given pixel coordinates and quantization error, and this can serve as a first step towards modeling the relationship between the encoding rate of reference views and the quality of synthesized views.},
booktitle = {Proceedings of the 13th International Conference on Distributed Smart Cameras},
articleno = {11},
numpages = {6},
keywords = {Virtual Reality, View synthesis, Omnidirectional video, Depth-image-based rendering, 6DoF},
location = {Trento, Italy},
series = {ICDSC 2019}
}

@inproceedings{10.1145/3551349.3560510,
author = {Qin, Xue and Hassan, Foyzul},
title = {DyTRec: A Dynamic Testing Recommendation tool for Unity-based Virtual Reality Software},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560510},
doi = {10.1145/3551349.3560510},
abstract = {Virtual Reality (VR) technology has been utilized in other fields besides gaming, such as education, training, arts, shopping, and e-commerce. However, the technical support of VR software is not growing as fast as its market size, especially for testing. Because of the immersive feature that requires VR apps to act and react to all the interactions dynamically, the traditional static testing techniques such as unit test generation cannot fully guarantee the correctness of the tested functions. In this paper, we proposed a Dynamic Testing Recommendation tool (DyTRec) to suggest the potential types of dynamic testing for the target VR projects. Specifically, we categorize the dynamic testing types by analyzing the official APIs from the VR engine documentation and then apply the extracting and searching on all VR script files. We evaluated DyTRec on 20 VR projects and successfully reported 39 suggested results.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {227},
numpages = {5},
keywords = {Virtual Reality, Unity, Software Testing},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/166117.166134,
author = {Cruz-Neira, Carolina and Sandin, Daniel J. and DeFanti, Thomas A.},
title = {Surround-screen projection-based virtual reality: the design and implementation of the CAVE},
year = {1993},
isbn = {0897916018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/166117.166134},
doi = {10.1145/166117.166134},
abstract = {This paper describes the CAVE (CAVE Automatic Virtual Environment) virtual reality/scientific visualization system in detail and demonstrates that projection technology applied to virtual-reality goals achieves a system that matches the quality of workstation screens in terms of resolution, color, and flicker-free stereo. In addition, this format helps reduce the effect of common tracking and system latency errors. The off-axis perspective projection techniques we use are shown to be simple and straightforward. Our techniques for doing multi-screen stereo vision are enumerated, and design barriers, past and current, are described. Advantages and disadvantages of the projection paradigm are discussed, with an analysis of the effect of tracking noise and delay on the user. Successive refinement, a necessary tool for scientific visualization, is developed in the virtual reality context. The use of the CAVE as a one-to-many presentation device at SIGGRAPH '92 and Supercomputing '92 for computational science data is also mentioned.},
booktitle = {Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {135–142},
numpages = {8},
keywords = {virtual reality, stereoscopic display, real-time manipulation, projection paradigms, head-tracking},
location = {Anaheim, CA},
series = {SIGGRAPH '93}
}

@inbook{10.1145/3596711.3596718,
author = {Cruz-Neira, Carolina and Sandin, Daniel J. and DeFanti, Thomas A.},
title = {Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3596711.3596718},
abstract = {This paper describes the CAVE (CAVE Automatic Virtual Environment) virtual reality/scientific visualization system in detail and demonstrates that projection technology applied to virtual-reality goals achieves a system that matches the quality of workstation screens in terms of resolution, color, and flicker-free stereo. In addition, this format helps reduce the effect of common tracking and system latency errors. The off-axis perspective projection techniques we use are shown to be simple and straightforward. Our techniques for doing multi-screen stereo vision are enumerated, and design barriers, past and current, are described. Advantages and disadvantages of the projection paradigm are discussed, with an analysis of the effect of tracking noise and delay on the user. Successive refinement, a necessary tool for scientific visualization, is developed in the virtual reality context. The use of the CAVE as a one-to-many presentation device at SIGGRAPH '92 and Supercomputing '92 for computational science data is also mentioned.},
booktitle = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
articleno = {6},
numpages = {8}
}

@inproceedings{10.1145/3580585.3606282,
author = {von Sawitzky, Tamara and Himmels, Chantal and L\"{o}cken, Andreas and Grauschopf, Thomas and Riener, Andreas},
title = {Investigating Hazard Notifications for Cyclists in Mixed Reality: A Comparative Analysis with a Test Track Study},
year = {2023},
isbn = {9798400701054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580585.3606282},
doi = {10.1145/3580585.3606282},
abstract = {One way to improve road safety for cyclists is the development of hazard notification systems. Instead of in field experiments, such systems could be tested in safe and more controlled simulated environments; however, their validity needs verification. We evaluated the validity of mixed reality (MR) simulation for bicycle support systems notifying of dooring hazards. In a mixed-design study (N=43) with environment type(MR/test track) as within and hazard notifications (with/without) as between factor, comparing subjective and objective measures across environments. In conclusion, MR simulation is absolutely valid for user experience and perceived safety and relatively valid for workload, standard deviation of lateral position, and speed. However, MR simulation was not valid for lateral distance, as participants cycled more in the center of the street than on the test track, perhaps to avoid simulator sickness. Thus, we conclude that MR simulation is valuable for studying bicycle safety.},
booktitle = {Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {202–212},
numpages = {11},
keywords = {Validation, Test Track Study, Mixed Reality Study, Hazard Notifications, Dooring, Cyclist Safety},
location = {Ingolstadt, Germany},
series = {AutomotiveUI '23}
}

@inproceedings{10.1145/3343055.3359719,
author = {Perea, Patrick and Morand, Denis and Nigay, Laurence},
title = {Spotlight on Off-Screen Points of Interest in Handheld Augmented Reality: Halo-based techniques},
year = {2019},
isbn = {9781450368919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343055.3359719},
doi = {10.1145/3343055.3359719},
abstract = {Navigating Augmented Reality (AR) environments with a handheld device often requires users to access digital contents (i.e. Points of Interests - POIs) associated with physical objects outside the field of view of the device's camera. Halo3D is a technique that displays the location of off-screen POIs as halos (arcs) along the edges of the screen. Halo3D reduces clutter by aggregating POIs but has not been evaluated. The results of a first experiment show that an enhanced version of Halo3D was 18\% faster than the focus+context technique AroundPlot* for pointing at a POI, and perceived as 34\% less intrusive than the arrow-based technique Arrow2D. The results of a second experiment in more realistic settings reveal that two variants of Halo3D that show the spatial distribution of POIs in clusters (1) enable an effective understanding of the off-screen environment and (2) require less effort than AroundPlot* to find POIs in the environment.},
booktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
pages = {43–54},
numpages = {12},
keywords = {visualization, off-screen point of interest, halo., augmented reality},
location = {<conf-loc>, <city>Daejeon</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {ISS '19}
}

@inproceedings{10.1145/2379636.2379644,
author = {Marzo, Asier and Bossavit, Beno\^{\i}t and Ardaiz, Oscar},
title = {Interacting with multi-touch handheld devices in augmented reality spaces: effects of marker and screen size},
year = {2012},
isbn = {9781450313148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379636.2379644},
doi = {10.1145/2379636.2379644},
abstract = {During the last years popularity of Augmented Reality (AR) has grown considerably. Nowadays most people have handheld devices equipped with camera, a variety of sensors and high computational power capable of rendering graphics previously only available on desktop computers. Such devices have replaced buttons with multi-touch screens as their main input system. In this paper we design and implement a technique for multi-touch handheld devices to manipulate objects in three-dimensional spaces. By mean of three experiments we studied the effect of workspace and device size on the performance.},
booktitle = {Proceedings of the 13th International Conference on Interacci\'{o}n Persona-Ordenador},
articleno = {8},
numpages = {2},
keywords = {multi-touch, handheld, augmented reality},
location = {Elche, Spain},
series = {INTERACCION '12}
}

@inproceedings{10.1145/3613904.3642158,
author = {Qian, Xun and Wang, Tianyi and Xu, Xuhai and Jonker, Tanya R. and Todi, Kashyap},
title = {Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642158},
doi = {10.1145/3613904.3642158},
abstract = {Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user’s context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {951},
numpages = {17},
keywords = {Context-Aware Policy, Extended Reality, Unit Test, Validation},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{10.1145/3544549.3585728,
author = {Matulic, Fabrice and Kashima, Taiga and Beker, Deniz and Suzuo, Daichi and Fujiwara, Hiroshi and Vogel, Daniel},
title = {Above-Screen Fingertip Tracking with a Phone in Virtual Reality},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585728},
doi = {10.1145/3544549.3585728},
abstract = {Using a phone as a VR controller is challenging because users cannot see their fingers when aiming for targets on the touchscreen. We propose using two mirrors mounted above the screen that reflect the front camera and a purpose-built deep neural network to robustly infer the 3D position of fingertips manipulating the phone. Network training is self-supervised after only a few initial labelled images and does not require any external sensor. We present a few example scenarios showing potential applications that use our phone-based fingertip tracker for precise touch input and above-screen interaction.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {18},
numpages = {7},
keywords = {hand pose estimation, virtual reality},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI EA '23}
}

@inproceedings{10.1145/3340764.3344435,
author = {Volkmann, Torben and Wessel, Daniel and Franke, Thomas and Jochems, Nicole},
title = {Testing the Social Presence Aspect of the Multimodal Presence Scale in a Virtual Reality Game},
year = {2019},
isbn = {9781450371988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340764.3344435},
doi = {10.1145/3340764.3344435},
abstract = {Presence is a key variable in virtual reality (VR), however, it is a complex variable consisting of different aspects. The Multimodal Presence Scale (MPS) by Makransky, Lilleholt, and Aaby was developed to measure physical, social and self-presence. But how well can it actually detect changes in one aspect of presence, and differentiate between different aspects of presence? To answer these questions, we use a German translation of the MPS in an experiment with a VR game with 45 participants. We examine social presence -the sense of being with another - specifically, and compare three conditions (abstraction levels) that should differ in the degree of social presence, but not in other aspects of presence. Results indicate that the MPS is reliable and useful for measuring social presence, while still correlating strongly with presence overall.},
booktitle = {Proceedings of Mensch Und Computer 2019},
pages = {433–437},
numpages = {5},
keywords = {virtual reality, virtual characters, scale, presence, measurement, HMD},
location = {Hamburg, Germany},
series = {MuC '19}
}

@inproceedings{10.1145/3644116.3644208,
author = {Chen, Yali and Zhu, Ruiyan and Xiong, Wei},
title = {Investigating the Efficacy of Virtual Reality Forest Meditation as an Intervention for Test Anxiety among College Students},
year = {2024},
isbn = {9798400708138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644116.3644208},
doi = {10.1145/3644116.3644208},
abstract = {The primary objective of this study is to verify the effectiveness of virtual reality forest meditation as an intervention for test anxiety among college students and to investigate the impact of three factors: namely forest style, meditation state, and pranayama guidance, on the intervention effect. A total of thirty participants exhibiting moderate or higher levels of test anxiety were subjected to an induction of test anxiety followed by a virtual reality forest meditation intervention comprising various combinations of the aforementioned three factors. Participants' levels of test anxiety were measured by the TAS scores and the quantification of brainwave energy. The results of the paired sample T-test demonstrate that virtual reality forest meditation constitutes an efficacious intervention for test anxiety among college students. The results of a three-factor analysis of variance revealed that both meditation state and pranayama guidance significantly influenced the effectiveness of virtual reality forest meditation intervention, while forest style did not demonstrate a significant impact. Specifically, wandering meditation was found to be more effective than sitting meditation, and interventions with pranayama guidance were more successful than those without pranayama guidance. Therefore, it can be concluded that guided wandering meditation combined with pranayama in a virtual reality forest environment offers superior alleviation of test anxiety among college students.},
booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science},
pages = {558–562},
numpages = {5},
location = {<conf-loc>, <city>Chengdu</city>, <country>China</country>, </conf-loc>},
series = {ISAIMS '23}
}

@article{10.1145/3604279,
author = {Qiu, Huajian and Streli, Paul and Luong, Tiffany and Gebhardt, Christoph and Holz, Christian},
title = {ViGather: Inclusive Virtual Conferencing with a Joint Experience Across Traditional Screen Devices and Mixed Reality Headsets},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604279},
doi = {10.1145/3604279},
abstract = {Teleconferencing is poised to become one of the most frequent use cases of immersive platforms, since it supports high levels of presence and embodiment in collaborative settings. On desktop and mobile platforms, teleconferencing solutions are already among the most popular apps and accumulate significant usage time---not least due to the pandemic or as a desirable substitute for air travel or commuting.In this paper, we present ViGather, an immersive teleconferencing system that integrates users of all platform types into a joint experience via equal representation and a first-person experience. ViGather renders all participants as embodied avatars in one shared scene to establish co-presence and elicit natural behavior during collocated conversations, including nonverbal communication cues such as eye contact between participants as well as body language such as turning one's body to another person or using hand gestures to emphasize parts of a conversation during the virtual hangout. Since each user embodies an avatar and experiences situated meetings from an egocentric perspective no matter the device they join from, ViGather alleviates potential concerns about self-perception and appearance while mitigating potential 'Zoom fatigue', as users' self-views are not shown. For participants in Mixed Reality, our system leverages the rich sensing and reconstruction capabilities of today's headsets. For users of tablets, laptops, or PCs, ViGather reconstructs the user's pose from the device's front-facing camera, estimates eye contact with other participants, and relates these non-verbal cues to immediate avatar animations in the shared scene.Our evaluation compared participants' behavior and impressions while videoconferencing in groups of four inside ViGather with those in Meta Horizon as a baseline for a social VR setting. Participants who participated on traditional screen devices (e.g., laptops and desktops) using ViGather reported a significantly higher sense of physical, spatial, and self-presence than when using Horizon, while all perceived similar levels of active social presence when using Virtual Reality headsets. Our follow-up study confirmed the importance of representing users on traditional screen devices as reconstructed avatars for perceiving self-presence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {232},
numpages = {27},
keywords = {virtual reality, video conferencing, teleconferencing, social VR, mixed reality, immersive social interaction, embodied presence, cross-platform, collaboration, co-presence, avatars}
}

@inproceedings{10.1145/3613904.3641992,
author = {Bonnail, Elise and Frommel, Julian and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},
title = {Was it Real or Virtual? Confirming the Occurrence and Explaining Causes of Memory Source Confusion between Reality and Virtual Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641992},
doi = {10.1145/3613904.3641992},
abstract = {Source confusion occurs when individuals attribute a memory to the wrong source (e.g., confusing a picture with an experienced event). Virtual Reality (VR) represents a new source of memories particularly prone to being confused with reality. While previous research identified causes of source confusion between reality and other sources (e.g., imagination, pictures), there is currently no understanding of what characteristics specific to VR (e.g., immersion, presence) could influence source confusion. Through a laboratory study (n=29), we 1) confirm the existence of VR source confusion with current technology, and 2) present a quantitative and qualitative exploration of factors influencing VR source confusion. Building on the Source Monitoring Framework, we identify VR characteristics and assumptions about VR capabilities (e.g., poor rendering) that are used to distinguish virtual from real memories. From these insights, we reflect on how the increasing realism of VR could leave users vulnerable to memory errors and perceptual manipulations.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {796},
numpages = {17},
keywords = {Memory, Source Confusion, Source Misattribution, Virtual Reality},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{10.1145/3510454.3516870,
author = {Wang, Xiaoyin},
title = {VRTest: an extensible framework for automatic testing of virtual reality scenes},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516870},
doi = {10.1145/3510454.3516870},
abstract = {Virtual Reality (VR) is an emerging technique that attracts interest from various application domains such as training, education, remote communication, gaming, and navigation. Despite the ever growing number of VR software projects, the quality assurance techniques for VR software has not been well studied. Therefore, the validation of VR software largely rely on pure manual testing. In this paper, we present a novel testing framework called VRTest to automate the testing of scenes in VR software. In particular, VRTest extracts information from a VR scene and controls the user camera to explore the scene and interact with the virtual objects with certain testing strategies. VRTest currently supports two built-in testing strategies: VRMonkey and VRGreed, which use pure random exploration and greedy algorithm to explore interact-able objects in VR scenes. The video of our tool is available on Youtube at https://www.youtube.com/watch?v=TARqTEaa7_Q},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {232–236},
numpages = {5},
keywords = {virtual reality, software testing, scene exploration},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3385956.3418955,
author = {Rodil, Kasper and Maasz, Donovan and Winschiers-Theophilus, Heike},
title = {Moving Virtual Reality out of its Comfort Zone and Into the African Kalahari Desert Field: Experiences From Technological Co-Exploration With an Indigenous San Community in Namibia},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418955},
doi = {10.1145/3385956.3418955},
abstract = {Indigenous people (IP) living in remote areas, at the margins of mainstream society, are often the last ones to experience emerging technologies and even less to shape those experiences. It could be argued technology exposure and experience is necessary for IP to gain agency in making informed decisions on the rejection or appropriation of novel technologies. In this paper, VR is introduced to a remote San community within a broader community-based research collaboration considering political and ethical perspectives of technology inclusion. The intent was to familiarise the community with the technology through the development and playthrough of a game, to explore future opportunities for joint co-designs of VR applications, meanwhile gauging the barriers for how VR operates outside of its intended setting. The community members expressed their excitement about the experience and the desire to re-create traditional San games in VR. The paper reflects on the community experiences, the setup and use of VR in remote settings, and the choices made to facilitate the familiarization of emerging technology.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {5},
numpages = {10},
keywords = {Virtual Reality, User Experiences, San People, Namibia, Indigenous People, Indigenous Knowledge, Cultural Heritage},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3474085.3481537,
author = {An, Shan and Che, Guangfu and Guo, Jinghao and Zhu, Haogang and Ye, Junjie and Zhou, Fangru and Zhu, Zhaoqi and Wei, Dong and Liu, Aishan and Zhang, Wei},
title = {ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481537},
doi = {10.1145/3474085.3481537},
abstract = {Virtual try-on technology enables users to try various fashion items using augmented reality and provides a convenient online shopping experience. However, most previous works focus on the virtual try-on for clothes while neglecting that for shoes, which is also a promising task. To this concern, this work proposes a real-time augmented reality virtual shoe try-on system for smartphones, namely ARShoe. Specifically, ARShoe adopts a novel multi-branch network to realize pose estimation and segmentation simultaneously. A solution to generate realistic 3D shoe model occlusion during the try-on process is presented. To achieve a smooth and stable try-on effect, this work further develop a novel stabilization method. Moreover, for training and evaluation, we construct the very first large-scale foot benchmark with multiple virtual shoe try-on task-related labels annotated. Exhaustive experiments on our newly constructed benchmark demonstrate the satisfying performance of ARShoe. Practical tests on common smartphones validate the real-time performance and stabilization of the proposed approach.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1111–1119},
numpages = {9},
keywords = {virtual try-on, stabilization, segmentation, pose estimation, benchmark, augmented reality},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/2535597.2535608,
author = {Siu, Teresa and Herskovic, Valeria},
title = {SidebARs: improving awareness of off-screen elements in mobile augmented reality},
year = {2013},
isbn = {9781450322003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535597.2535608},
doi = {10.1145/2535597.2535608},
abstract = {In a high-stress situation, such as an emergency, first responders (e.g. police, firefighters) require relevant information to be delivered in a timely, efficient way. Augmented reality seems like a natural way for emergency responders to find relevant information that is close to them. However, due to the limited angle and distance seen through the camera, many relevant points will be off-screen, making it difficult to quickly find the needed information. Several approaches for this problem have been proposed in previous works, however, most are designed for 2D maps, and those proposed for augmented reality do not allow users to quickly find a certain type of point of interest. We studied the emergency response scenario through a development project and several focus groups. Then, we implemented SidebARs: a prototype that implements two sidebars that allow users to quickly find the relevant information they are interested in, combined with layer filters and a slide bar to set a radius of interest. This visualization technique not only gives users awareness about the distance and direction of relevant points of interest, but also about their type. This paper presents the design and implementation of this prototype. A preliminary evaluation with firefighters found it to be a promising mechanism to find information during an emergency.},
booktitle = {Proceedings of the 2013 Chilean Conference on Human - Computer Interaction},
pages = {36–41},
numpages = {6},
keywords = {off-screen POIs, emergency response, awareness, augmented reality},
location = {Temuco, Chile},
series = {ChileCHI '13}
}

@inproceedings{10.1145/3613904.3642408,
author = {Ring, Patrizia and Tietenberg, Julius and Emmerich, Katharina and Masuch, Maic},
title = {Development and Validation of the Collision Anxiety Questionnaire for VR Applications},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642408},
doi = {10.1145/3613904.3642408},
abstract = {The high degree of sensory immersion is a distinctive feature of head-mounted virtual reality (VR) systems. While the visual detachment from the real world enables unique immersive experiences, users risk collisions due to their inability to perceive physical obstacles in their environment. Even the mere anticipation of a collision can adversely affect the overall experience and erode user confidence in the VR system. However, there are currently no valid tools for assessing collision anxiety. We present the iterative development and validation of the Collision Anxiety Questionnaire (CAQ), involving an exploratory and a confirmatory factor analysis with a total of 159 participants. The results provide evidence for both discriminant and convergent validity and a good model fit for the final CAQ with three subscales: general collision anxiety, orientation, and interpersonal collision anxiety. By utilizing the CAQ, researchers can examine potential confounding effects of collision anxiety and evaluate methods for its mitigation.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {605},
numpages = {13},
keywords = {assessment, collision anxiety, discomfort, fear, user experience, virtual reality},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{10.1145/2984751.2984760,
author = {Yamashita, Shogo and Zhang, Xinlei and Rekimoto, Jun},
title = {AquaCAVE: Augmented Swimming Environment with Immersive Surround-Screen Virtual Reality},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984760},
doi = {10.1145/2984751.2984760},
abstract = {AquaCAVE is a system for enhancing the swimming experience. Although swimming is considered to be one of the best exercises to maintain our health, swimming in a pool is normally monotonous; thus, maintaining its motivation is sometimes difficult. AquaCAVE is a computer-augmented swimming pool with rear-projection acrylic walls that surround a swimmer, providing a CAVE-like immersive stereoscopic projection environment. The swimmer wears goggles with liquid-crystal display (LCD) shutter glasses, and cameras installed in the pool tracks swimmer's head position. Swimmers can be immersed into synthetic scenes such as coral reefs, outer space, or any other computer generated environments. The system can also provide swimming training with projections such as record lines and swimming forms as 3D virtual characters in the 3D space.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {183–184},
numpages = {2},
keywords = {virtual reality, swimming, projection-based systems, immersive environment, cave, augmented sports},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/3565474.3569072,
author = {Bedin, Andrea and Marin\v{s}ek, Alexander and Shahcheraghi, Shaghayegh and Gholian, Nairy Moghadas and Van der Perre, Liesbet},
title = {DOPAMINE: Doppler frequency and angle of arrival minimization of tracking error for extended reality},
year = {2022},
isbn = {9781450399340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565474.3569072},
doi = {10.1145/3565474.3569072},
abstract = {In this paper, we investigate how Joint Communication And Sensing (JCAS) can be used to improve the Inertial Measurement Unit (IMU)-based tracking accuracy of eXtended Reality (XR) Head-Mounted Displays (HMDs). Such tracking is used when optical and InfraRed (IR) tracking is lost, and its lack of accuracy can lead to disruption of the user experience. In particular, we analyze the impact of using doppler-based speed estimation to aid the accelerometer-based position estimation, and Angle of Arrival (AoA) estimation to aid the gyroscope-based orientation estimation. Although less accurate than IMUs for short times in fact, the JCAS based methods require one fewer integration step, making the tracking more sustainable over time. Based on the proposed model, we conclude that at least in the case of the position estimate, introducing JCAS can make long lasting optical/IR tracking losses more sustainable.},
booktitle = {Proceedings of the 1st International Workshop on Emerging Topics in Wireless},
pages = {24–29},
numpages = {6},
keywords = {mmWave, joint communication and sensing, extended reality},
location = {<conf-loc>, <city>Rome</city>, <country>Italy</country>, </conf-loc>},
series = {EmergingWireless '22}
}

@inproceedings{10.1145/3132129.3132144,
author = {Perea, Patrick and Morand, Denis and Nigay, Laurence},
title = {Halo3D: a technique for visualizing off-screen points of interest in mobile augmented reality},
year = {2017},
isbn = {9781450351096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132129.3132144},
doi = {10.1145/3132129.3132144},
abstract = {When working with mobile Augmented Reality (AR), users often need to visualize off-screen points of interest (POIs). These POIs belong to the context since they are not directly observable in the 3D first-person view on screen. The aim is to present the 3D direction and distance of each POI in a 3D first-person view. The context in mobile AR can include a large number of POIs including locally dense clusters as in mobile AR systems for production plant machine maintenance.Existing solutions display 3D arrows or an area on the edges of the screen to represent the POIs of the context. These techniques display the direction but not the distance of each POI. We present Halo3D, a mobile AR adaptation of a 2D visualization technique. Halo3D displays the 3D direction and distance of off-screen POIs in a high POI-density environment. The paper describes the design elements of Halo3D and outlines the experimental study to be conducted. The first experimental results indicate that users prefer visual attributes that minimize the visual intrusion on screen.},
booktitle = {Proceedings of the 29th Conference on l'Interaction Homme-Machine},
pages = {43–51},
numpages = {9},
keywords = {visualization, off-screen points of interest, mobility, halo, augmented reality},
location = {Poitiers, France},
series = {IHM '17}
}

@inproceedings{10.1145/3599640.3599668,
author = {Wang, Juan and He, Chunlin and Bai, Xinye and Li, Qiuhong and Li, Yunhao and Cai, Minghan},
title = {Construction and validation of a junior high school biology laboratory class teaching model based on desktop VR},
year = {2023},
isbn = {9781450399593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599640.3599668},
doi = {10.1145/3599640.3599668},
abstract = {In the context of continuous integration of information technology and subject education, more and more educators consider combining VR technology with actual education teaching, and desktop VR technology provides fresh vitality for education teaching as an emerging technology. In junior high school biology experimental teaching, there are problems such as some experimental materials are not easy to obtain, resulting in less-than-ideal experimental effects. Therefore, how to apply desktop VR in junior high school biology laboratory teaching and whether integrating desktop VR into junior high school biology laboratory teaching can help students better master biology knowledge? In this study, the tower of Dell's experience theory is used as the theoretical basis for model construction. Based on the ARCS learning motivation model and the features of desktop VR, a model of junior high school biology laboratory class teaching based on desktop VR is explored. In this study, taking a middle school in China as an example, classes with similar grades were selected as the experimental class and the control class, each with 50 students, and the lesson of middle school biology "Observe the results of flowers" was used as an example for teaching practice. The control class was taught using traditional teaching methods, and the experimental class was taught using the teaching model of middle school biology laboratory class based on desktop VR, and a paper-and-pencil quiz, the Amabile et al. The experimental data were collected in the form of paper-and-pencil tests, learning motivation scales, etc. The statistical analysis of the data led to the conclusion that the model can promote students' learning effectiveness and have different degrees of improvement on students' learning motivation, thus verifying the effectiveness of the desktop VR-based junior high school biology laboratory class teaching model.},
booktitle = {Proceedings of the 9th International Conference on Education and Training Technologies},
articleno = {26},
numpages = {8},
location = {<conf-loc>, <city>Macau</city>, <country>China</country>, </conf-loc>},
series = {ICETT '23}
}

@inproceedings{10.1145/3565387.3565450,
author = {Li, Chengshun and Yang, Xiaonan and Hu, Yaoguang and Mao, Wanting and Fang, Haonan},
title = {AR-based Accessibility Verification Method for Smart Manufacturing System with Human Motion Capture},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565450},
doi = {10.1145/3565387.3565450},
abstract = {With the development of industrial production towards personalization and intelligence, process validation of manufacturability has become a complex and time-consuming problem in manufacturing system. Smart manufacturing requires manufacturing system to have the ability of rapid response to various product designs. However, there is an obvious time span between product design and accessibility verification process. Augmented reality (AR) constructs an environment of virtual reality integration, which brings people intuitive visual feeling and natural interaction feedback. It can be easily applied to the early stage of product accessibility verification. This paper presents a digital human assisted accessibility verification system based on AR, which can perform accessibility verification in the product design stage. In this system, augmented reality glasses are used to realize the natural interaction between human and product 3D model, and a motion capture device is used to realize the superposition of digital human and operators. This paper expounds the above methods and related devices in detail, verifies the feasibility of this system through an industrial example, and shows its application potential in smart manufacturing system.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {63},
numpages = {6},
keywords = {Smart Manufacturing System, Motion capture, Augmented Reality, Accessibility verification},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3340764.3344885,
author = {Jelonek, Markus and Herrmann, Thomas},
title = {Atentiveness for Potential Accidents at the Construction Site: Virtual Reality Test Environment with Tactile Warnings for Behavior Tests in Hazardous Situations},
year = {2019},
isbn = {9781450371988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340764.3344885},
doi = {10.1145/3340764.3344885},
abstract = {Construction sites are still considered to be one of the most dangerous working environments for human beings. In 2017, approximately one third of all fatal accidents at a workplace in Germany occurred in the construction industry. To a certain extent, this is because double tasks are performed continuously in this rather dynamic working environment: On the one hand, construction workers have to carry out their work tasks and on the other hand, they have to take into account their own safety. This paper presents a concept for a test environment based on virtual reality, which should make it possible to realistically design several hazard scenarios and examine to what extent the chosen type of vibration-based warnings helps to avoid danger in hazardous situations. Finally, different challenges for such a test environment will be discussed.},
booktitle = {Proceedings of Mensch Und Computer 2019},
pages = {649–653},
numpages = {5},
keywords = {Warnungen, Virtual Reality, Vibration, Aufmerksamkeit},
location = {Hamburg, Germany},
series = {MuC '19}
}

@inproceedings{10.1145/3491101.3519698,
author = {Mitchell, Daxton and Choi, HeeSun},
title = {Assessing the Spatial Distribution of Visual Attention in a Virtual Environment: Development and Validation of a Novel VR-based Attentional Visual Field (AVF) Task},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519698},
doi = {10.1145/3491101.3519698},
abstract = {Visual attention is critical for everyday task performance and safety. The Attentional Visual Field Task (AVF) is an established, computerized method for assessing the distribution of visual attention across a wide visual field. High-fidelity virtual reality (VR) presents an opportunity for more ecological methods for assessing and training visual attention; however, this novel approach has not been examined. We developed a new VR-based AVF task, AVF-VE, using a Head-Mounted Display (HMD) VR device with an integrated eye-tracker, and conducted a study to validate this newly developed visual attention task. We further examined how visual attention is distributed in a virtual visual field. The findings suggest that the VR-based visual attention task is a valid and useful tool that can be used for future attention research and training. Unique characteristics of the spatial distribution of visual attention in the virtual environment observed in the current evaluation study are discussed.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {379},
numpages = {7},
keywords = {Visual attention, Virtual reality, Head-mounted display, Attentional visual field},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3489849.3489959,
author = {Mukhopadhyay, Abhishek and Reddy, G S Rajshekar and Ghosh, Subhankar and L R D, Murthy and Biswas, Pradipta},
title = {Validating Social Distancing through Deep Learning and VR-Based Digital Twins},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489959},
doi = {10.1145/3489849.3489959},
abstract = {The Covid-19 pandemic resulted in a catastrophic loss to global economies, and social distancing was consistently found to be an effective means to curb the virus's spread. However, it is only as effective when every individual partakes in it with equal alacrity. Past literature outlined scenarios where computer vision was used to detect people and to enforce social distancing automatically. We have created a Digital Twin (DT) of an existing laboratory space for remote monitoring of room occupancy and automatically detecting violation of social distancing. To evaluate the proposed solution, we have implemented a Convolutional Neural Network (CNN) model for detecting people, both in a limited-sized dataset of real humans, and a synthetic dataset of humanoid figures. Our proposed computer vision models are validated for both real and synthetic data in terms of accurately detecting persons, posture, and intermediate distances among people.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {104},
numpages = {2},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.5555/1108368.1108425,
author = {Hutchins, Matthew and Stevenson, Duncan and Gunn, Chris and Krumpholz, Alexander and Pyman, Brian and O'Leary, Stephen},
title = {"I think i can see it now!": evidence of learning in video transcripts of a collaborative virtual reality surgical training trial},
year = {2005},
isbn = {1595932224},
publisher = {Computer-Human Interaction Special Interest Group (CHISIG) of Australia},
address = {Narrabundah, AUS},
abstract = {Networked collaborative virtual reality systems have been proposed for surgical education. They allow an instructor to teach a student using a shared virtual model, even if separated by distance. For these systems to be accepted within the surgical community there must be a compelling body of evidence that demonstrates that learning occurs in the training environment, and is transferable to the operating theatre. We have developed a networked multisensory virtual reality system for teaching surgery of the temporal bone and conducted a training transfer trial. To augment the quantitative analysis of the results, we have performed a qualitative analysis of the transcripts of videotapes of the learning phase of the trial, using techniques from Conversation Analysis. In this short paper we present a single case study that convincingly demonstrates that learning occurred within the instruction phase of the trial.},
booktitle = {Proceedings of the 17th Australia Conference on Computer-Human Interaction: Citizens Online: Considerations for Today and the Future},
pages = {1–4},
numpages = {4},
keywords = {collaborative virtual environment, conversation analysis, evaluation, surgical simulation},
location = {<conf-loc>, <city>Canberra</city>, <country>Australia</country>, </conf-loc>},
series = {OZCHI '05}
}

@article{10.1145/3656340,
author = {Xu, Congying and Terragni, Valerio and Zhu, Hengcheng and Wu, Jiarong and Cheung, Shing-Chi},
title = {MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3656340},
doi = {10.1145/3656340},
abstract = {Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs), that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities. In this paper, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97\% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52\% and 9.42\% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76\% to 76.92\% of codified MRs are easily comprehensible for developers.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
keywords = {Software Testing, Metamorphic Testing, Metamorphic Relation, Automated Test Case Generation}
}

