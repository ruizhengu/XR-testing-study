@article{DONG201345,
title = {Collaborative visualization of engineering processes using tabletop augmented reality},
journal = {Advances in Engineering Software},
volume = {55},
pages = {45-55},
year = {2013},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2012.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997812001287},
author = {Suyang Dong and Amir H. Behzadan and Feng Chen and Vineet R. Kamat},
keywords = {Tabletop augmented reality, Visualization, Simulation, Planar tracking library, Validation, Collaboration},
abstract = {3D computer visualization has emerged as an advanced problem-solving tool for engineering education and practice. For example in civil engineering, the integration of 3D/4D CAD models in the construction process helps to minimize the misinterpretation of the spatial, temporal, and logical aspects of construction planning information. Yet despite the advances made in visualization, the lack of collaborative problem-solving abilities leaves outstanding challenges that need to be addressed before 3D visualization can become widely accepted in the classroom and in professional practice. The ability to smoothly and naturally interact in a shared workspace characterizes a collaborative learning process. This paper introduces tabletop Augmented Reality to accommodate the need to collaboratively visualize computer-generated models. A new software program named ARVita is developed to validate this idea, where multiple users wearing Head-Mounted Displays and sitting around a table can all observe and interact with dynamic visual simulations of engineering processes. The applications of collaborative visualization using Augmented Reality are reviewed, the technical implementation is covered, and the program’s underlying tracking libraries are presented.}
}
@article{FOKIDES2024100048,
title = {Development and testing of a model for explaining learning and learning-related factors in immersive virtual reality},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100048},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000429},
author = {Emmanuel Fokides and Panagiotis Antonopoulos},
keywords = {Immersive virtual reality, Learning, Model},
abstract = {Many believe that immersive virtual reality (IVR) possesses transformative potential for a plethora of human activities that are mediated via technology, including education. In light of this, it is critically important to understand the determinants that influence learning in IVR environments and the interrelations among these determinants. For that matter, a model was developed that encapsulated ten potential factors influencing learning outcomes. Three hundred and thirty-four university students interacted with a purpose-built application that presented ancient Greek inventions through the use of head-mounted displays. Data analysis, adhering to a structural equation modeling approach, indicated that a multitude of factors exerted a positive influence on learning outcomes. These included the perceived quality of graphics, the perceived quality of feedback and content, and the perceived degree of interaction. Moreover, intrinsic motivation and the immersive experience that IVR provides also demonstrated a positive impact. Conversely, the perceived cognitive load and symptoms of simulator sickness manifested a negative impact. Interestingly, these factors did not appear to inhibit learners' motivation or their positive feelings. Age did not have any effect, while gender seemed to have an impact only on immersion. The implications of the results are also discussed.}
}
@article{PENG2000153,
title = {Application and evaluation of VR-based CAPP system},
journal = {Journal of Materials Processing Technology},
volume = {107},
number = {1},
pages = {153-159},
year = {2000},
issn = {0924-0136},
doi = {https://doi.org/10.1016/S0924-0136(00)00677-4},
url = {https://www.sciencedirect.com/science/article/pii/S0924013600006774},
author = {Q Peng and F.R Hall and P.M Lister},
keywords = {CAPP, Virtual reality, CAD/CAM, Engineering simulation},
abstract = {This paper proposes an approach to an integrated CAPP/VR system. A desktop VR environment is used to explore the machining process element of CAPP. An architecture of a virtual CAPP system is proposed based on an incorporated VR design interface, which can respond to the dynamic manufacturing process. The key problems under discussion are to find an effective method to quickly capture the 3D information from the manufacturing environment, and to define a uniform and well-organised structure of the program layout for achieving improved sharing of VR resources. Examples of the simulation for cutting process are supplied to demonstrate and verify the proposed CAPP/VR integration system.}
}
@article{MOTAMEDI2017248,
title = {Signage visibility analysis and optimization system using BIM-enabled virtual reality (VR) environments},
journal = {Advanced Engineering Informatics},
volume = {32},
pages = {248-262},
year = {2017},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301203},
author = {Ali Motamedi and Zhe Wang and Nobuyoshi Yabuki and Tomohiro Fukuda and Takashi Michikawa},
keywords = {Signage visibility, Signage system optimization, Virtual reality, Building Information Modeling (BIM)},
abstract = {The proper placement of signage greatly influences pathfinding and information provision in public spaces. Clear visibility, easy comprehension, and efficient placement are all important for successful signage. We propose a signage visibility analysis and optimization system, utilizing an updated Building Information Model (BIM) and a game engine software application to simulate the movement of pedestrians. BIM can provide an up-to-date digital representation of a building and its assets, while computer simulation environments have the potential to simulate the movement of pedestrians. Combining these two technologies provides an opportunity to create a tool that analyzes the efficiency of installed signage and visualizes them in VR environments. The proposed tool contains algorithms, functions and predefined scenarios to calculate the coverage and the visibility of a building’s signage system. This system assists building managers to analyze (visually or by using statistics) the visibility of signboards, to assess their proper placement, and to optimize their placement. The applicability of the method has been validated in case studies performed in subway stations in Japan.}
}
@article{FORTUNA2024104057,
title = {A comparative study of Augmented Reality rendering techniques for industrial assembly inspection},
journal = {Computers in Industry},
volume = {155},
pages = {104057},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523002075},
author = {Santina Fortuna and Loris Barbieri and Emanuele Marino and Fabio Bruno},
keywords = {Augmented Reality, Industrial assembly inspection, Quality inspection, AR rendering techniques, Industry 4.0},
abstract = {In the manufacturing industry, Augmented Reality (AR) has shown significant potential in enhancing operators’ capabilities while performing inspection and assembly activities. However, the augmented visualization of virtual models on physical components can present challenges and potential misunderstandings, as the visualization mode greatly influences the perception of components and the amount of information received. This study investigates the impact of rendering techniques on user performance during industrial assembly inspection tasks. In particular, a set of rendering techniques, suitable for industrial AR applications, have been selected and implemented through a dedicated AR tool. The selected AR rendering techniques have been compared, in terms of qualitative and quantitative metrics, in order to test their efficiency and effectiveness for industrial assembly inspection activities. 17 domain experts and 33 representative users have been involved in the experimental study which outcomes reveal that the rendering techniques play a significant role in assisting operators for identifying design discrepancies in industrial products. Furthermore, a correlation has been observed between the two AR rendering techniques best suited to support industrial inspection activities and the types of assembly errors detected.}
}
@article{ZHANG2022104113,
title = {Virtual reality technology in construction safety training: Extended technology acceptance model},
journal = {Automation in Construction},
volume = {135},
pages = {104113},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104113},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005641},
author = {Ming Zhang and Lei Shu and Xiaowei Luo and Mengqi Yuan and Xiazhong Zheng},
keywords = {Virtual reality, Training and education, Construction, TAM, Safety training},
abstract = {VR technology has been demonstrated great potential as a training platform for construction training & education (T&E). Although the effectiveness of VR technology has been widely validated, their low acceptance and usage are still observed, lacking the understanding of users' attitudes towards using VR technology. Therefore, this research aims to investigate the factors behind low acceptance of VR technology through the extended TAM. Perceived price value, self-efficacy, and perceived playfulness were adopted as the external motivators integrated with the TAM framework. Results revealed that perceived usefulness (PU) and perceived ease of use (PEU) are significant direct predictors of attitude. Moreover, PU does not directly affect intention to use (IU), but a significant influence of perceived playfulness on IU was observed. These findings provide theoretical support for predicting users' acceptance of VR technology in construction T&E and empirical implications for guiding decisions in the design and development of VR systems.}
}
@article{PAES2023103953,
title = {Investigating the relationship between three-dimensional perception and presence in virtual reality-reconstructed architecture},
journal = {Applied Ergonomics},
volume = {109},
pages = {103953},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103953},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022002769},
author = {Daniel Paes and Javier Irizarry and Mark Billinghurst and Diego Pujoni},
keywords = {Virtual reality, Virtual environments, Human factors, Three-dimensional perception, Presence, Design technology},
abstract = {Identifying and characterizing the factors that affect presence in virtual environments has been acknowledged as a critical step to improving Virtual Reality (VR) applications in the built environment domain. In the search to identify those factors, the research objective was to test whether three-dimensional perception affects presence in virtual environments. A controlled within-group experiment utilizing perception and presence questionnaires was conducted, followed by data analysis, to test the hypothesized unidirectional association between three-dimensional perception and presence in two different virtual environments (non-immersive and immersive). Results indicate no association in either of the systems studied, contrary to the assumption of many scholars in the field but in line with recent studies on the topic. Consequently, VR applications in architectural design may not necessarily need to incorporate advanced stereoscopic visualization techniques to deliver highly immersive experiences, which may be achieved by addressing factors other than depth realism. As findings suggest that the levels of presence experienced by users are not subject to the display mode of a 3D model (whether immersive or non-immersive display), it may still be possible for professionals involved in the review of 3D models (e.g., designers, contractors, clients) to experience high levels of presence through non-stereoscopic VR systems provided that other presence-promoting factors are included.}
}
@article{KARAGIANNIS2021290,
title = {Operators Training Using Simulation And VR Technology},
journal = {Procedia CIRP},
volume = {96},
pages = {290-294},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121001177},
author = {Panagiotis Karagiannis and Theodoros Togias and George Michalos and Sotiris Makris},
keywords = {Type your keywords here, separated by semicolons},
abstract = {This paper introduces a method for training operators using digital technologies. More specifically the training is focused on the teleoperation and remote control of resources and sensors using a simulated environment with virtual elements, before entering the real one. The objective is to improve the training of the operators and bring them closer to Industry 4.0 technologies where robotic and other automation solutions are tackled to support production activities. In this training activity the benefits of using a simulated environment as well as VR technology are presented. More specifically, a simulation application has been created and can be executed on a PC showing a virtual environment where a mobile platform can navigate freely around, calculating its path, and use its camera for obstacle or part detection purposes. On the other side, the VR application can be executed on a smartphone device with Android OS and can be used to see the field of view of the camera attached on the robot. The aforementioned applications can be connected through TCP/IP connection and enable the operator in real time to see and navigate in the virtual world using either the VR or simulation application. Furthermore, a specific scenario has been created and elaborated in the paper, where the robot needs to navigate inside a virtual infrastructure and perform inspection activities using its camera. Lastly, the benefits of each application are further detailed in the paper, focusing mainly on the validation of the inspection automated solution and its remote control in an intuitive and immersive way.}
}
@article{WASSERMANN2018161,
title = {Intuitive robot programming through environment perception, augmented reality simulation and automated program verification},
journal = {Procedia CIRP},
volume = {76},
pages = {161-166},
year = {2018},
note = {7th CIRP Conference on Assembly Technologies and Systems (CATS 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.01.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300507},
author = {Jonas Wassermann and Axel Vick and Jörg Krüger},
keywords = {Industrial Robotics, Robot Programming, Augmented Reality},
abstract = {The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.}
}
@article{KHORCHI2024108647,
title = {An OpenBIM-based 4D approach to support coordination meetings in virtual reality environments},
journal = {Journal of Building Engineering},
volume = {85},
pages = {108647},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.108647},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224002158},
author = {Aissa Khorchi and Conrad Boton},
keywords = {Construction virtual reality, 4D simulation, BIM collaboration format (BCF), OpenBIM, Collaboration},
abstract = {Notwithstanding the emergence of Virtual Reality technologies, the solutions available on the market for integrating 4D and Virtual Reality are not well adapted to the principles of BIM and only propose certain visualization functionalities. Indeed, generally, such solutions are mainly simple visualization tools which do not offer BIM-based collaborative tools such as BIM Collaboration Format (BCF)-based exchanges. Our research aims to develop a method and subsequent prototype to integrate an IFC-compliant 3D model and a planning schedule in order to perform a 4D simulation in a Virtual Reality environment having BCF-based information exchange capabilities. The prototype developed achieved its theoretical and technical objectives, namely, to validate our hypothesis that the BCF can be implemented in a VR environment and support 4D simulation-based collaboration. The proposed method allows to identify some lessons learned, in terms of success factors as well as areas for improvement.}
}
@article{DEMIRTAS2022102764,
title = {Development and Implementation of a Collaborative Workspace for Industrial Robots Utilizing a Practical Path Adaptation Algorithm and Augmented Reality},
journal = {Mechatronics},
volume = {84},
pages = {102764},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102764},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822000204},
author = {Serhat Demirtas and Tolga Cankurt and Evren Samur},
keywords = {Human-robot collaboration, Augmented reality, Industrial robots},
abstract = {The idea of humans and robots coexisting in manufacturing environments has increased the importance of personnel safety. Current practice to provide human safety in a conventional industrial robotic application is to put the robot in a cell enclosed by fences that occupies a large area in the environment. If someone enters this robotic cell, the robot ceases its movement immediately that leads to significant delays in the production cycle. In this paper, we present a new collaborative workspace concept for conventional industrial robots in order to eliminate fences and to avoid delays as much as possible. The proposed concept relies on a novel path adaptation algorithm and an augmented reality-based warning system. The algorithm, which makes the robot adaptable to human interventions, is unique in a sense that it can be applied to variety of industrial robots without requiring changes in the robot controller. Besides, the warning system provides additional safety features to the collaborative workspace through a projector and a head-mounted display. We demonstrate the feasibility of the proposed concept with an industrial robot in a realistic palletizing application. Several experiments were carried out to test the performance along with a user study. Performance test results indicate that the robot is able to perform three actions according to the minimum distance between the human and the robot in case of a human violation: slowdown, avoidance, and full stop. Qualitative performance tests results show that augmented reality improves the perception of safety and collaboration but negatively affects physical comfort. Overall, the proposed concept provides a practical and implementable solution for conventional industrial robots missing collaborative features.}
}
@article{CAO2015115,
title = {Feature selection for low bit rate mobile augmented reality applications},
journal = {Signal Processing: Image Communication},
volume = {36},
pages = {115-126},
year = {2015},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2015.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0923596515001010},
author = {Yi Cao and Christian Ritz and Raad Raad},
keywords = {Feature selection, Mobile augmented reality, Retrieval accuracy, Low bit-rate transmission},
abstract = {Mobile augmented reality applications rely on automatically matching a captured visual scene to an image in a database. This is typically achieved by deriving a set of features from the captured image, transmitting them through a network and then matching with features derived from a database of reference images. A fundamental problem is to select as few and robust features as possible such that the matching accuracy is invariant to distortions caused by camera capture whilst minimizing the bit rate required for their transmission. In this paper, novel feature selection methods are proposed, based on the entropy of the image content, entropy of extracted features and the Discrete Cosine Transformation (DCT) coefficients. The methods proposed in the descriptor domain and DCT domain achieve better retrieval accuracy under low bit rate transmission than state-of-the-art peak based feature selection used within the MPEG-7 Compact Descriptor for Visual Search (CDVS). The robustness of the proposed methods is evaluated under controlled single distortion and the retrieval performance is verified from image retrieval experiments and results for a realistic dataset with complex real world capturing distortion. Results show that the proposed method can improve the matching accuracy for various detectors. On the one side, feature selection can achieve low bit rate transmission, on the other side, when coping with distorted images, it can result in a higher matching accuracy than using all features. Hence, even if all the features can be transmitted in high transmission bandwidth scenarios, feature selection should still be applied to the distorted query image to ensure high matching accuracy.}
}
@article{BAROROH2022146,
title = {Human-centric production system simulation in mixed reality: An exemplary case of logistic facility design},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {146-157},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001479},
author = {Dawi Karomati Baroroh and Chih-Hsing Chu},
keywords = {Mixed reality, Human-centric, Production planning, Facility layout, Simulation},
abstract = {A production system with manual operations tends to suffer from variations in system performance induced by individual differences of human actions. To re-design the system under such uncertainty can be time-consuming without the support of advanced simulation tools. Hence, this research proposes a novel idea of production system simulation in mixed reality (MR) that facilitates production planning involving human operators. An exemplary case of logistic facility design demonstrates a MR-based planning tool implementing this idea. In this case, a manager and an operator, both wearing a HoloLens, simultaneously collaborate in a facility constructed by overlaying virtual equipment with existing one in real environment. The manager attempts to adjust the production parameters in the facility to reach a new target created by mixed-model production, while the operator accomplishes manual tasks related to material transportation and storage. The planning results show that the MR tool is advantageous over traditional simulation software in terms of planning quality and flexibility. This work validates the feasibility of MR as a new approach to realizing human-centric production system design.}
}
@article{DASDEMIR2022103942,
title = {Cognitive investigation on the effect of augmented reality-based reading on emotion classification performance: A new dataset},
journal = {Biomedical Signal Processing and Control},
volume = {78},
pages = {103942},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103942},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422004414},
author = {Yaşar Daşdemir},
keywords = {Augmented reality, Real reading, AR reading, EEG, Emotion, Metaverse},
abstract = {As a result of stimulating the basic human senses, emotional states occur in humans. Of these senses, the visual sense is the most basic human sense. This sense perceives visual stimuli and elicits emotional states. Augmented Reality (AR) applications also work with these visual stimuli. This study investigates how AR systems, which are among immersive environments, are effective in distinguishing the emotional states of students in book reading activities, with the support of Electroencephalography (EEG). The BOOKAR dataset obtained within the scope of this study is among the first AR-supported datasets in emotion recognition using physiological signals with immersive methods. To reveal the emotional states of the readers, texts that stimulate emotional states such as disgusting, happy, neutral, and 2-dimensional pictures are presented within these texts. In the AR-based reading section, the 3-dimensional models of these 2-dimensional pictures and the rig-processed conditions of these models are presented as a stimulus to the reader. The results show that AR-based reading has a significant discriminatory effect, especially on the valence-arousal emotional states of readers, and achieves higher classification performance than real reading. The results also show that the proposed method is good at classifying emotional states from EEG signals with accuracy scores close to 100%. It has been observed that the designed AR application also meets the usability characteristics. The proposed emotion recognition method in AR applications has significant potential for integration into various Metaverse-based applications.}
}
@article{TUMA20141015,
title = {The Process Simulation Using by Virtual Reality},
journal = {Procedia Engineering},
volume = {69},
pages = {1015-1020},
year = {2014},
note = {24th DAAAM International Symposium on Intelligent Manufacturing and Automation, 2013},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2014.03.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877705814003300},
author = {Zdenek Tůma and Jiri Tůma and Radek Knoflíček and Petr Blecha and Frantisek Bradáč},
keywords = {Virtual reality, simulation, process, augmented reality},
abstract = {Nowadays companies can solve production systems problems by variety of software solutions, but in practice it is known that it is often difficult to interpret these outputs, illustrate and to further integrate. One possibility is the use of augmented virtual reality for the planning of these systems. This article deals with the construction of the experimental sorting workplace as a model case of the production system. Weak points of solution are solved using augmented virtual reality, which helps to simulate the process in real time, in this case the virtual displayed product moving on belt conveyor before testing the real piece.}
}
@article{SUKIRMAN2019247,
title = {Self-Evacuation Drills by Mobile Virtual Reality Application to Enhance Earthquake Preparedness},
journal = {Procedia Computer Science},
volume = {157},
pages = {247-254},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.164},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919310828},
author = { Sukirman and Reza Arif Wibisono and  Sujalwo},
keywords = {virtual reality, VR application, mobile VR, earthquake preparedness, self-evacuation drills},
abstract = {Traditional evacuation drills by self-rescue practice are a common method used to educate society in communities, companies or schools in Indonesia which involving participants in large numbers. However, the participants in this approach are not always committed and doing it seriously in some cases. Besides, the participants were also unable to feel the earthquake sensation because no shaking at all when practice evacuation drills. For this reason, it is necessary to design a tool that can be used to practice by themselves with no involving many people, but the sensation of shaking can be felt by the users, one way is using Virtual Reality (VR) technology. This study aims to develop VR-based applications that can create a sensation of shaking such as an earthquake and can be used individually. The method used in this study is initial research, development, testing and analyzing, and recommendations. The development carried out by Unity 3D software which is equipped with GoogleVR plugin. The testing was conducted on 14 students of SMK Batik Batik 1 Surakarta, Indonesia, whose average age of 16 years old and the habit of playing games is 2-3 hours per day on average. From the experiments conducted, 71% stated that earthquake simulations using VR applications are more interesting than animated video, because they can interact with virtual objects directly and the simulations are more realistic. Thus, it can be concluded that the VR application can be used for earthquake evacuation exercises independently, further hope that user preparedness will be better in dealing with natural disasters, especially earthquakes}
}
@article{BROSCH2021100078,
title = {Model-based segmentation using neural network-based boundary detectors: Application to prostate and heart segmentation in MR images},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100078},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000396},
author = {Tom Brosch and Jochen Peters and Alexandra Groth and Frank Michael Weber and Jürgen Weese},
keywords = {Model-based segmentation, Boundary detection, Neural networks, Prostate, Heart, MRI},
abstract = {Model-based segmentation (MBS) is a variant of active surfaces and active shape models that has successfully been used to segment anatomical structures such as the heart or the brain. We propose to integrate neural networks (NNs) into MBS for boundary detection. We formulate boundary detection as a regression task and use a NN to predict the distances between a surface mesh and the corresponding boundary points. The proposed approach has been applied to two tasks — prostate segmentation in MR images and the segmentation of the left and right ventricle in MR images. For the first task, data from the Prostate MR Image Segmentation 2012 (PROMISE12) challenge has been used. For the second task, a diverse database with cardiac MR images from six clinical sites has been used. We compare the results to the popular U-net approaches using the nnU-net implementation that is among the top performing segmentation algorithms in various challenges. In cross-validation experiments, the mean Dice scores are very similar and no statistically significant difference is observed. On the PROMISE12 test set, nnU-net Dice scores are significantly better. This is achieved by using an ensemble of 2D and 3D U-nets to generate the final segmentation, a concept that may also be adapted to NN-based boundary detection in the future. While the U-net provides a voxel labeling, our approach provides a 3D surface mesh with pre-defined mesh topology, establishes correspondences with respect to the reference mesh, avoids isolated falsely segmented regions and ensures proper connectivity of different regions.}
}
@article{KANADE2024103548,
title = {Exploring the effectiveness of virtual reality as a learning tool in the context of task interruption: A systematic review},
journal = {International Journal of Industrial Ergonomics},
volume = {99},
pages = {103548},
year = {2024},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2024.103548},
url = {https://www.sciencedirect.com/science/article/pii/S0169814124000040},
author = {Sameeran G. Kanade and Vincent G. Duffy},
keywords = {Virtual reality, Task interruption, Skill training, Learning, VOSviewer},
abstract = {As the cost of rendering immersive virtual reality goes down, VR technology has the potential to improve the learning outcomes of educational and training programs. In this context, it is important to identify the advantages and pitfalls of using VR as a training tool. This review is an attempt to evaluate VR technology as a training tool and various factors that impact its effectiveness. It was found that virtual reality has been used and tested extensively in surgical training with mixed results. Apart from the effectiveness and the cost of VR as a training tool, the other factors that will influence the widespread adoption of this technology for training are skill retention and task interruption. If VR is to be used for longer training sessions, interruptions are inevitable in the form of restroom breaks, texting etc., and hence it is important to study their impact on learning outcomes. Hence, a brief review of these topics in the context of VR and learning has been included in this study. In conclusion, it was found that although the use of VR as a training tool holds a lot of promise, its widespread adoption will depend on its adaptability to different training settings and its impact on learning outcomes, including factors like skill retention and task interruption.}
}
@article{LARAPRIETO201559,
title = {An Innovative Self-learning Approach to 3D Printing Using Multimedia and Augmented Reality on Mobile Devices},
journal = {Procedia Computer Science},
volume = {75},
pages = {59-65},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036674},
author = {Vianney Lara-Prieto and Efraín Bravo-Quirino and Miguel Ángel Rivera-Campa and José Enrique Gutiérrez-Arredondo},
keywords = {3D Printing, Augmented Reality, Self-learning},
abstract = {Technology is evolving rapidly and it is becoming harder to keep up its pace. At a university environment, important investments are required so that students and professors can have access to novel technology. However, it is also fundamental to know how to use this novel technology to exploit its benefits. Tecnológico de Monterrey, Campus Monterrey, recently acquired several 3D printers so that the students could get familiar with this rapid prototyping technology and use them to deliver better quality projects. Nevertheless, the new challenge was to administrate the 3D printing process including 3D model verification, STL file generation, printing time and raw material. Since students were not familiar with 3D printing, the expert had to spend a lot of time with them explaining the whole process from the modelling to the printing stage, and verifying their work to make an efficient use of 3D printing time and materials. To overcome this situation, it was decided to make use of augmented reality and multimedia applications to generate tutorials for self-learning the whole process of 3D printing. Nowadays, the wide spread of mobile devices and wireless technologies brings a huge potential to e-learning changing dramatically the traditional instructor-oriented scheme. The learning process can take place in an informal setting having the tutorials available on mobile devices by just scanning a quick response code, usually known as QR code. The first QR code enables a link to download the free augmented reality Layar app. Then, several images can be scanned using Layar to open each of the video tutorials. These videos explain graphically step by step of all the processes involved and give different options of software to be used. The tutorials are easy to follow so that any engineering or design student can learn from them. This case-study offers an innovative learning approach that fosters self-learning and a more efficient use of technology resources.}
}
@article{TZONGMING2009520,
title = {A fast parametric deformation mechanism for virtual reality applications},
journal = {Computers & Industrial Engineering},
volume = {57},
number = {2},
pages = {520-538},
year = {2009},
note = {Challenges for Advanced Technology},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2008.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0360835208001721},
author = {Cheng Tzong-Ming and Tsung-Han Tu},
keywords = {Emulated deformation, Virtual reality, Collaborative design, Parametric design, Neural networks},
abstract = {Virtual reality technologies have been adopted in a wide variety of applications for its interactive ability and realistic senses. Despite early implementations regard VR only as a medium for lively animation; a practical VR work must deliver precise deformation on virtual objects based on real-time interactions. The exact ability is especially important for users who utilize VR to do collaborative design, for it will greatly reduce the amount of on-line computations on operating substance-based interactions, and consequently facilitates the collaboration. Therefore, this research will employ neural networks to memorize the deformation behavior of solid objects, and then perform instant and accurate deformations in the virtual environment. The proposed method also allows design variations for parametric features, and uses feature parameters as variable switches to adjust the deformation mechanism. There are three steps in the method: (1) For a sample object, generate force-induced deformations using the finite-element method; (2) memorize the surface displacements with artificial neural networks; and (3) convert the parametric deformation matrices into Behavioral Modules for the virtual reality engine. In the implementations, ANSYS is used to generate model deformations, and MATLAB is used to perform neural training. Finally, a virtual environment is built using Virtools where customized Building Blocks are created to present interactive deformation behavior. Experiments were carried out on an Intel XEON workstation with nVIDIA Quadro4 750GL display device. Sample workparts are tested to examine the ability of the method. The results show that both training accuracy and real-time capability are more than satisfactory.}
}
@article{LIU20151065,
title = {A bio-system inspired nonline ar intelligent controller with application to bio-reactor system},
journal = {Neurocomputing},
volume = {168},
pages = {1065-1075},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215006372},
author = {Bao Liu and Yongsheng Ding and Na Gao and Xin Zhang},
keywords = {Nonlinear intelligent control, Bi-cooperative regulation, Bio-reactor process, Bio-system, Nonlinear process},
abstract = {In order to improve the control effectiveness of some nonlinear processes, we propose a novel nonlinear guided intelligent controller (NGIC) inspired by the bi-cooperative regulation mechanism and the regulation characteristics of glucose in human body. The NGIC comprises a cooperative control unit (CCU), an enhanced control unit (ECU), an inhibited control unit (ICU) and an auxiliary control unit (ACU). To shorten the rise time of the process variable, the ECU first takes effect with a rapid step action when the set-point changes or a large error appears. After a prescribed time, the ICU takes effect with an opposite step action to reduce the system overshoot or to avoid oscillation. When the control system achieves a prescribed steady state or the control time exceeds a pre-defined limitation, the ACU becomes effective to improve the control precision of the NGIC by applying a central proportional law with a guided final control output. As a super managing unit, the CCU coordinates all the work of the ECU, the ICU and the ACU to make the NGIC eliminate the control error with short rise time, small overshoot and short settling time. To testify the control performance of the NGIC, we apply the NGIC to a bio-reactor process with large time variation. The simulation results indicate that the NGIC has better control performance than the conventional PID, internal model controller (IMC) and artificial neural network (ANN) controller. Hence, the NGIC is significant to improve the control effect for some complex practical processes.}
}
@article{MALIK2019346,
title = {An Application of 3D Model Reconstruction and Augmented Reality for Real-Time Monitoring of Additive Manufacturing},
journal = {Procedia CIRP},
volume = {81},
pages = {346-351},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119303658},
author = {Ammar Malik and Hugo Lhachemi and Joern Ploennigs and Amadou Ba and Robert Shorten},
keywords = {Additive Manufacturing, Augmented Reality, Real-Time 3D Model Reconstruction, Real-Time Monitoring},
abstract = {This paper presents a novel scan-based method for the real-time monitoring of additive manufacturing processes. Most traditional scanning techniques used for generating 3D models capture the only outer shape of the object after completion of the printing process. The method proposed in this paper differs as it relies on a layer-by-layer scanning of the 3D object directly during the printing process. This strategy has been successfully implemented with a fused filament 3D printer (PRUSA i3 MK3). Furthermore, in order to offer an increased interaction between the obtained 3D model and the user, a virtual environment has been developed for the augmented reality glasses HoloLens. The novelty of this method lies in the layer-by-layer 3D model reconstruction of both the outer shape and the inner layers of the printed part. It enables the user, directly during the printing process, to view and detect potential defects, not only at the surface but also in the inner layers of the printed object. Therefore, it can provide detailed information about the build quality and can be used as the basis of a decision-making tool.}
}
@article{PLAKAS20201629,
title = {Augmented Reality in Manufacturing and Logistics: Lessons Learnt from a Real-Life Industrial Application},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1629-1635},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.227},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320989},
author = {G. Plakas and S.T. Ponis and K. Agalianos and E. Aretoulaki and S.P. Gayialis},
keywords = {Industry 4.0, Augmented Reality, Head-Worn Displays, Smart Glasses, Smart Factories, Logistics 4.0},
abstract = {One of the most prominent technologies in the Industry 4.0’s arsenal, when it comes to manufacturing and logistics operations, is Augmented Reality (AR). AR takes the capabilities of computer-generated display, sound, text and effects to enhance the user’s real-world experience and support workers in their every-day tasks such as assembly, order picking and maintenance. This paper, after a short review of AR in manufacturing and logistics, showcases the introduction of a real-life AR-Smart Glasses application in support of one of the most repetitive, prone to error, and expensive warehouse logistics processes, i.e. order picking. The implementation of the AR system takes place in actual production settings and is integrated in the picking process of a distribution centre operated by a large Telecommunications Service Provider. This paper describes the functionalities of the developed AR system and highlights functional details and lessons learnt from its pilot implementation.}
}
@article{NETLAND20236,
title = {Interactive mixed reality live streaming technology in manufacturing},
journal = {Manufacturing Letters},
volume = {38},
pages = {6-10},
year = {2023},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.141},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323002018},
author = {Torbjørn Netland and Michael Stegmaier and Cesare Primultini and Omid Maghazei},
keywords = {Extended reality, Mixed reality, Virtual reality, Live-streaming, Industry 4.0},
abstract = {This paper presents the first field-tested application of interactive mixed reality live streaming technology in a manufacturing setting and elaborates on its implications for practitioners and scholars. A proof-of-concept of a mixed reality live streaming device that uses 360° video calls for remote presence is developed and applied in a field test during a tool changeover at a Stanley Black & Decker factory. This study documents that interactive mixed reality live streaming for shop-floor environments has passed Technology Readiness Level 6, representing a radical new way of organizing remote shop-floor tours, inspections, audits, and training.}
}
@article{KIND2020802,
title = {Haptic Interaction in Virtual Reality Environments for Manual Assembly Validation},
journal = {Procedia CIRP},
volume = {91},
pages = {802-807},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.238},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120309045},
author = {Simon Kind and Andreas Geiger and Nora Kießling and Michael Schmitz and Rainer Stark},
keywords = {Virtual assembly, Virtual reality, Haptic interaction, Prototype},
abstract = {Based on an increased level of product complexity in product development processes new technologies and validation approaches are researched and applied in industry to validate assembly processes. Especially technologies that are focusing on Mixed or Virtual Reality environments are getting very popular in the field of assembly validation, because of a huge potential to reduce the necessary number of physical prototypes. Therefore costs and time within product integration phase can be saved. These technologies can provide a high level of immersion for pick-and place or assembly tasks and allow a scale 1:1 interaction with the future product or production system. Still plenty of limitations for a complete substitution of physical prototypes exist due to the lack of physical and haptic behavior of these virtual prototypes. Within this paper, a development method and applications for haptic interaction with force-feedback devices for assembly validation is presented. Focusing especially on the development process of a mixed prototype with virtual and physical elements, so called hybrid prototypes, a hard- and software framework is presented. Nowadays virtual assembly methods for Virtual Reality and computer haptics are still based on a high level of expertise in providing technology and testbeds. To keep a high level of flexibility regarding virtual validation models, easily applicable procedures for modelling, im-/exporting and configuration of the prototype are needed for a virtual assembly use case. In this paper, a new and connected processing pipeline is presented that describes and guides a non-expert user from CAD environment to the implementation and configuration of prototypes for haptic interaction.}
}
@article{OSTANIN20222791,
title = {Interactive Industrial Robot Programming based on Mixed Reality and Full Hand Tracking},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2791-2796},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.153},
url = {https://www.sciencedirect.com/science/article/pii/S240589632202167X},
author = {Mikhail Ostanin and Stanislav Zaitsev and Adelia Sabirova and Alexandr Klimchik},
keywords = {human-robot interaction, mixed reality, industrial robots, hand tracking},
abstract = {The paper show a state-of-the-art approach for programming industrial robotics manipulator via mixed reality holographic interface. In mixed reality, we implement basic functionality for robot programming: setting up the base and tool frames, setting the sequence of Cartesian poses and Joint positions for robot motion, robot movements simulation. The software developed is based on Unity, ROS, and MoveIt frameworks. We compared cursor-based interface (Microsoft HoloLens 1) and full hand tracking (Microsoft HoloLens 2 and Oculus Quest 2) functionalities for interactive robot programming. We tested our approach on the UR10e collaborative robot in the virtual scene and real environment. The full hand tracking human-robot interaction approach improves the setup of Cartesian and Joints robot goals in respect to cursor based interaction.}
}
@article{LOTSARIS2021301,
title = {Augmented Reality (AR) based framework for supporting human workers in flexible manufacturing},
journal = {Procedia CIRP},
volume = {96},
pages = {301-306},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.091},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121001190},
author = {Konstantinos Lotsaris and Nikos Fousekis and Spyridon Koukas and Sotiris Aivaliotis and Niki Kousi and George Michalos and Sotiris Makris},
keywords = {human robot interaction, augmented reality, mobile robots, remote guidance},
abstract = {This paper presents an Augmented Reality (AR) application that aims to facilitate the operator’s work in an industrial, human-robot collaboration environment with mobile robots. In such a flexible environment, with robots and humans working and moving in the same area, the ease of communication between the two sides is critical and prerequisite. The developed application provides the user with handy tools to interact with the mobile platform, give direct instructions to it and receive information about the robot’s and the broader system’s state, through an AR headset. The communication between the headset and the robot is achieved through a ROS based system, that interconnects the resources. The discussed tool has been deployed and tested in a use case inspired from the automotive industry, assisting the operators during the collaborative assembly tasks.}
}
@article{REN2024113,
title = {Exploring the effect of fingertip aero-haptic feedforward cues in directing eyes-free target acquisition in VR},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {113-131},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000839},
author = {Xiaofei Ren and Jian He and Teng Han and Songxian Liu and Mengfei Lv and Rui Zhou},
keywords = {Haptic, Feedforward, Virtual reality, Aero-haptic},
abstract = {Background
The sense of touch plays a crucial role in interactive behavior within virtual spaces, particularly when visual attention is absent. Although haptic feedback has been widely used to compensate for the lack of visual cues, the use of tactile information as a predictive feedforward cue to guide hand movements remains unexplored and lacks theoretical understanding.
Methods
This study introduces a fingertip aero-haptic rendering method to investigate its effectiveness in directing hand movements during eyes-free spatial interactions. The wearable device incorporates a multichannel micro-airflow chamber to deliver adjustable tactile effects on the fingertips.
Results
The first study verified that tactile directional feedforward cues significantly improve user capabilities in eyes-free target acquisition and that users rely heavily on haptic indications rather than spatial memory to control their hands. A subsequent study examined the impact of enriched tactile feedforward cues on assisting users in determining precise target positions during eyes-free interactions, and assessed the required learning efforts.
Conclusions
The haptic feedforward effect holds great practical promise in eyeless design for virtual reality. We aim to integrate cognitive models and tactile feedforward cues in the future, and apply richer tactile feedforward information to alleviate users' perceptual deficiencies.}
}
@article{GALAMBOS201568,
title = {Design, programming and orchestration of heterogeneous manufacturing systems through VR-powered remote collaboration},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {33},
pages = {68-77},
year = {2015},
note = {Special Issue on Knowledge Driven Robotics and Manufacturing},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2014.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0736584514000738},
author = {Péter Galambos and Ádám Csapó and Péter Zentay and István Marcell Fülöp and Tamás Haidegger and Péter Baranyi and Imre J. Rudas},
keywords = {Virtual reality/augmented reality, Mixed virtual and physical reality, Remote collaboration, Virtual commissioning, Future internet, Cognitive infocommunications},
abstract = {Modern manufacturing systems are often composed of a variety of highly customized units and specifically designed manufacturing cells. Optimization of assembly and training of staff requires a series of demo installations and excessive use of costly operational resources. In some cases, components are located at different sites, making the orchestration of the whole system even more difficult. Virtual Reality (VR) collaboration environments offer a solution by enabling high fidelity testing and training of complex manufacturing systems. On the other hand, such platforms are difficult to implement in an engineering perspective, as they are required to provide reliable, standard interfaces towards both robotic components and human operators. The VirCA (Virtual Collaboration Arena) platform is a software framework that supports various means of collaboration through the use of 3D augmented/virtual reality as a communication medium. VirCA offers functions for the high-level interoperability of heterogeneous components in a wide range of domains, spanning from research & development, through remote education to orchestration and management of industrial processes in manufacturing applications. This paper provides an overview of the industrial requirements behind high-fidelity virtual collaboration and demonstrates how the VirCA platform meets these requirements. Use cases are provided to illustrate the usability of the platform.}
}
@article{BEJCZY2020620,
title = {Mixed Reality Interface for Improving Mobile Manipulator Teleoperation in Contamination Critical Applications},
journal = {Procedia Manufacturing},
volume = {51},
pages = {620-626},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.087},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920319442},
author = {Bence Bejczy and Rohat Bozyil and Evaldas Vaičekauskas and Sune Baagø {Krogh Petersen} and Simon Bøgh and Sebastian Schleisner Hjorth and Emil Blixt Hansen},
keywords = {Human-Robot Interaction, Teleoperation, Cleanroom Environments, Robotics, Mixed Reality, Mobile Manipulator},
abstract = {This paper presents a mixed reality teleoperation interface for mobile manipulation tasks in contamination critical production environments, where human presence is undesirable. This is achieved by using an intuitive control approach and providing the operator with a sense of depth through various visual feedback modalities. The different visual feeds from a mono-and stereoscopic multi-camera setup are displayed for the operator, in a mixed reality control room developed in Unity. The control interface employs the differentiation of the VR controller’s pose, interpolated into a trajectory for the end-effector. The communication between the operator and the robot is facilitated through ROS for control commands and visual feedback. Speed of operation is typically not crucial in current use cases, while task safety, accuracy, and perception are paramount. The paper presents the latest research developments of a mixed reality interface designed and tested for a mobile manipulator.}
}
@article{ZUK2022110627,
title = {Three-dimensional gait analysis using a virtual reality tracking system},
journal = {Measurement},
volume = {188},
pages = {110627},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110627},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121014974},
author = {Magdalena Żuk and Magdalena Wojtków and Michał Popek and Jakub Mazur and Katarzyna Bulińska},
keywords = {Motion capture, Kinematics, Virtual reality, Gait analysis, HTC Vive Trackers},
abstract = {The development of virtual reality (VR) technology has prompted low-budget hardware accessories for VR headsets that support six degrees of freedom (6DOF) tracking. In this study, two methods for describing lower-limb joint kinematics were proposed, implemented in custom software, and validated: Simplified 6DOF protocol and a previously proposed ISB 6DOF based on reporting standard for joint kinematics introduced by International Society of Biomechanics (ISB) adjusted to the HTC Vive Trackers. The low-cost system allowed for the collection of complete kinematic data of the lower limb, obtaining continuous tracker trajectories without time-consuming system calibration. The obtained data can serve as reference data for normal gait. The performed analysis confirmed the high repeatability, comparable with advanced optoelectronic motion capture systems. The proposed tools enable low-cost analysis of the kinematics without the use of additional equipment but in accordance with the protocols used in biomechanical research and consistent with clinical practice.}
}
@article{LOTSARIS2021459,
title = {AR based robot programming using teaching by demonstration techniques},
journal = {Procedia CIRP},
volume = {97},
pages = {459-463},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.09.186},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314906},
author = {Konstantinos Lotsaris and Christos Gkournelos and Nikos Fousekis and Niki Kousi and Sotiris Makris},
keywords = {Augmented reality, Human robot interaction, HoloLens, Remote guidance},
abstract = {This paper presents an Augmented Reality tool for supporting the operator's interaction with the robot hardware in production systems. We are focusing on the development of an AR application, which allows the user to interact with a robotic arm and move it by demonstration. The application's purpose is to simplify and accelerate the industrial manufacturing process by introducing an easy and intuitive way of interaction with the hardware, without requiring special programming skills or long training time from the worker. The proposed software is developed for the Microsoft's HoloLens Mixed Reality Headset, integrated with ROS and it has been tested in a case study inspired from the automotive industry.}
}
@article{MARTINSEN2023120988,
title = {Positive climate effects when AR customer support simultaneous trains AI experts for the smart industries of the future},
journal = {Applied Energy},
volume = {339},
pages = {120988},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120988},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923003525},
author = {Madeleine Martinsen and Yuanye Zhou and Erik Dahlquist and Jinyue Yan and Konstantinos Kyprianidis},
keywords = {Intelligent systems, Augmented reality (AR), CO₂ emissions, Digitalization, Internet of things (IoT), Artificial Intelligence},
abstract = {Initially, Artificial Intelligence (AI) focused on diagnostics during the 70s and 80s. Unfortunately, it did not gain trust and few industries embraced it, mostly due to the extensive manual programming effort that AI required for interpreting data and act. In addition, the computer capacity, for handling the amounts of data necessary to train AI, was lacking the disc dimensions we are used to today, which made it go slowly. Not until the 2000 s confidence in AI was established in parallel with the introduction of new tools that was paving the way for PLS, PCA, ANN and soft sensors. Year 2011, IBM Watson (an AI application) was developed and won over the jeopardy champion. Today's machine learning (ML) such as “deep learning” and artificial neural networks (ANN) have created interesting use cases. AI has therefore regained confidence and industries are beginning to embrace where they see appropriate uses. Simultaneously, Internet of Things (IoT) tools have been introduced and made it possible to develop new capabilities such as virtual reality (VR), augmented reality (AR), mixed reality (MR) and extended reality (XR). These technologies are maturing and could be used in several application areas for the industries and form part of their digitalization journey. Furthermore, it is not only the industries that could benefit from introducing these technologies. Studies also show several areas and use cases where augmented reality has a positive impact, such as on students' learning ability. Yet few teachers know or use this technology. This paper evaluates and analyze AR, remote assistance tool for industrial purposes. The potential of the tool is discussed for frequent maintenance cases in the mining industry. Further on, if we look into the future, it is not surprising if we will be able to see that today's concepts of reality tools have evolved to become smarter by being trained by multimedia recognition and from people who have thus created an AI expert. Where the AI expert will support customers and be able to solve simple errors but also those that occur rarely and thus be a natural part of the solution for future completely autonomous processes for the industry. The article demonstrates a framework for creating smarter tools by combining AR, ML and AI and forms part of the basis creating the smarter industry of the future. Natural Language Processing (NLP) toolbox has been utilized to train and test an AI expert to give suitable resolutions to a specific maintenance request. The motivation for AR is the possible energy savings and reduction of CO₂ emissions in the maintenance field for all business trips that can be avoided. At the same time saving money for the industries and expert manhours that are spent on traveling and finally enhancing the productivity for the industries. Tests cases have verified that with AR, the resolution time could be significantly reduced, minimizing production stoppages by more than 50% of the time, which ultimately has a positive effect on a country's GDP. How much energy can be saved is predicted by the fact that 50% of all the world's business flights are replaced by one of the reality concepts and are estimated to amount to at least 50 Mton CO₂ per year. This figure is probably slightly higher as business trips also take place by other means of transport such as trains, buses, and cars. With today's volatile employees changing jobs more frequently, industry experts are becoming fewer and fewer. Since new employee stays for a maximum of 3–5 years per workplace, they will not stay long enough to become experts. Introducing an AI expert trained by today's experts, there is a chance that this knowledge can be maintained.}
}
@article{MALEK2022104542,
title = {Realtime conversion of cracks from pixel to engineering scale using Augmented Reality},
journal = {Automation in Construction},
volume = {143},
pages = {104542},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104542},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004137},
author = {Kaveh Malek and Fernando Moreu},
keywords = {Augmented reality, Crack characterization, Pixel unit conversion, Euler angles, Edge extraction},
abstract = {A key step in image-based crack characterization techniques is pixel to engineering scale conversion. The conversion factor corresponds to Camera-to-Crack-Distance (CCD) and camera obliquity angle. To date, nonstationary image-based crack characterization methods are not designed for real-time unit conversion. This study develops a new crack characterization algorithm for Augmented Reality (AR) headsets; then implements it in Unity-game-software on a computer; and finally deploys and validates it in AR headsets. The proposed algorithm includes: (1) the transformation of pixel information to AR platform; (2) edge extraction utilizing Canny algorithm; (3) crack measurement at the pixel level adopting the horizontal approach; (4) CCD and camera angle measurement using AR orientation capabilities and applying this information for perspective correction; (5) Engineering scale dimensions computation using the results of (3)–(4). A scaler equation for perspective correction streamlines the algorithm toward real-time implementation. The results show maximum errors of 8.45% and 12.05% for laboratory and field experiments, respectively.}
}
@article{ZHOU2018239,
title = {Promoting Knowledge Construction: A Model for Using Virtual Reality Interaction to Enhance Learning},
journal = {Procedia Computer Science},
volume = {130},
pages = {239-246},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918303867},
author = {Yun Zhou and Shangpeng Ji and Tao Xu and Zi Wang},
keywords = {Virtual Reality, Knowledge Construction, Evaluation, Learning Experience, Learning Model},
abstract = {VR technologies, offering powerful immersion and rich interaction, have gained great interest from researchers and practitioners in the field of education. However, current learning theories and models either mainly take into account the technology perspectives, or focus more on the pedagogy. In this paper, we propose a learning model benefiting from both the Human-Computer Interaction aspects and pedagogical aspects. This model takes full account of the impact of different factors including pedagogical contexts, VR roles and scenarios, and output specifications, which would be combined to inform the design and realize VR education applications. Based on this model, we design and implement an educational application of computer assembly under virtual reality using HTC Vive, which is a headset providing immersion experience. To analyze users’ learning behaviors and evaluate their performance and experience, we conduct an evaluation with 32 college students as participants. We design a questionnaire including usability tests and emotion state measures. Results showed that our proposed learning model gave a good guidance for informing the design and use of VR-supported learning application. The use of the natural interaction not only makes the learning interesting and fosters the engagement, but also improves the construction of knowledge in practices.}
}
@article{HAN2021101398,
title = {A framework for semi-automatically identifying fully occluded objects in 3D models: Towards comprehensive construction design review in virtual reality},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101398},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101398},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001506},
author = {Bing Han and Jong Won Ma and Fernanda Leite},
keywords = {Virtual Reality, Occluded Objects, Construction Design Review, Automation, Visualization},
abstract = {Virtual Reality (VR)-based construction design review applications have shown potential to enhance user performance in many research projects and experiments. Currently, visualizing occluded objects in VR is a challenge, and this function is indispensable for construction design review and coordination. This paper proposes an occlusion detection framework that semi-automatically identifies occluded objects in 3D construction models. The framework determines the visibility status of an object by converting the object to a point cloud and comparing the point cloud to the virtual laser scanning result of the original model. It exports models that are interoperable with VR development software so that visualization effects can be easily employed to occluded objects. The authors validated the framework using two building information models. The algorithm achieved a recall rate of 90.30% and a precision rate of 75.05% in a gasoline refinery facility model. It reached a higher 98.06% recall rate and a 97.53% precision rate in an academic building model. This paper contributes to the body of knowledge by proposing a semi-automatic occlusion detection framework and validating that point cloud-based algorithms are appropriate for this classification task.}
}
@article{PUJIASTUTI20241738,
title = {The Effectiveness of Using Augmented Reality on the Geometry Thinking Ability of Junior High School Students},
journal = {Procedia Computer Science},
volume = {234},
pages = {1738-1745},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.180},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924005398},
author = {Heni Pujiastuti and Rudi Haryadi},
keywords = {augmented reality, thinking ability, geometry},
abstract = {This study aimed to test the effectiveness of using augmented reality applications in learning geometry in junior high school students. The method used in this study is the quasi-experiment method. In this study, researchers used learning with augmented reality to find its effect on student learning outcomes in grade 8 junior high school, which has an average age of 13 years. The research design used in this study was a quasi-experimental nonequivalent control group design. This study, 2 sample classes were taken: an experimental class (learning with augmented reality) and a control class (learning without augmented reality). The experimental and control classes have the same characteristics; each has 20 male and 20 female students. The instrument used in this research is a geometric thinking ability test. This study uses a test instrument in the form of essay questions. The instrument was given twice, namely during the pretest and post-test. There are ten essay questions made. After obtaining the data from the pretest and post-test results, the normalized gain score formula (N-Gain) was used to know the increase in geometric thinking skills. The N-Gain result of the experimental class is 78% or 0.78, which is in the high category. Whereas in the control class, it was 50% or 0.58, this result is in the moderate category. The N-gain results show that there is a difference between the experimental class and the control class. In this case, the effectiveness of using augmented reality for learning geometry in junior high school students is higher than those who did not receive augmented reality treatment. From the N-gain value, it proves that learning using augmented reality learning media is effective in increasing the ability to think Geometry. Then the results of observations made during learning activities using augmented reality show that students look active and make students more enthusiastic when learning Geometry. The results of the responses showed that 39 students had become more proactive in learning Geometry by using augmented reality applications.}
}
@article{TALO2019176,
title = {Application of deep transfer learning for automated brain abnormality classification using MR images},
journal = {Cognitive Systems Research},
volume = {54},
pages = {176-188},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718310933},
author = {Muhammed Talo and Ulas Baran Baloglu and Özal Yıldırım and U {Rajendra Acharya}},
keywords = {MRI classification, Abnormal brain images, Deep transfer learning, CNN},
abstract = {Magnetic resonance imaging (MRI) is the most common imaging technique used to detect abnormal brain tumors. Traditionally, MRI images are analyzed manually by radiologists to detect the abnormal conditions in the brain. Manual interpretation of huge volume of images is time consuming and difficult. Hence, computer-based detection helps in accurate and fast diagnosis. In this study, we proposed an approach that uses deep transfer learning to automatically classify normal and abnormal brain MR images. Convolutional neural network (CNN) based ResNet34 model is used as a deep learning model. We have used current deep learning techniques such as data augmentation, optimal learning rate finder and fine-tuning to train the model. The proposed model achieved 5-fold classification accuracy of 100% on 613 MR images. Our developed system is ready to test on huge database and can assist the radiologists in their daily screening of MR images.}
}
@article{VGRAVES2023102283,
title = {Siamese pyramidal deep learning network for strain estimation in 3D cardiac cine-MR},
journal = {Computerized Medical Imaging and Graphics},
volume = {108},
pages = {102283},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102283},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123001015},
author = {Catharine {V. Graves} and Marina F.S. Rebelo and Ramon A. Moreno and Roberto N. Dantas-Jr and Antonildes N. Assunção-Jr and Cesar H. Nomura and Marco A. Gutierrez},
keywords = {Myocardium strain, Cardiac magnetic resonance, Deep learning},
abstract = {Strain represents the quantification of regional tissue deformation within a given area. Myocardial strain has demonstrated considerable utility as an indicator for the assessment of cardiac function. Notably, it exhibits greater sensitivity in detecting subtle myocardial abnormalities compared to conventional cardiac function indices, like left ventricle ejection fraction (LVEF). Nonetheless, the estimation of strain poses considerable challenges due to the necessity for precise tracking of myocardial motion throughout the complete cardiac cycle. This study introduces a novel deep learning-based pipeline, designed to automatically and accurately estimate myocardial strain from three-dimensional (3D) cine-MR images. Consequently, our investigation presents a comprehensive pipeline for the precise quantification of local and global myocardial strain. This pipeline incorporates a supervised Convolutional Neural Network (CNN) for accurate segmentation of the cardiac muscle and an unsupervised CNN for robust left ventricle motion tracking, enabling the estimation of strain in both artificial phantoms and real cine-MR images. Our investigation involved a comprehensive comparison of our findings with those obtained from two commonly utilized commercial software in this field. This analysis encompassed the examination of both intra- and inter-user variability. The proposed pipeline exhibited demonstrable reliability and reduced divergence levels when compared to alternative systems. Additionally, our approach is entirely independent of previous user data, effectively eliminating any potential user bias that could influence the strain analyses.}
}
@article{BAVELOS2022276,
title = {Integrating AR based operator support features for reconfigurable production systems},
journal = {Procedia CIRP},
volume = {106},
pages = {276-281},
year = {2022},
note = {9th CIRP Conference on Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.191},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001925},
author = {Angelos Christos Bavelos and Dimosthenis Dimosthenopoulos and Thodoris Togias and Niki Kousi and Sotiris Makris},
keywords = {Augmented Reality, digital instructions, operator support, asset administration shell, alerts, reconfigurable production, components manufacturing},
abstract = {Augmented Reality (AR) applications have seen a rise in industry in the last few years, due to technological advancements in many of its fundamental areas, such as display and sensing technologies. AR has been used as a driver for supporting human operators in fully manual and semi-automated systems, proving its usefulness in areas such as operation and reconfiguration instructions, safety and resource status awareness. This paper presents an AR application that integrates a set of support features based on the latest advances in AR. The application covers all the aforementioned areas of interest, by including digital instructions, malfunction alerts and resource state visualization in one unified framework. An Asset Administration Shell (AAS) framework has been developed for integration and data exchange between the application and the overall industrial network. The developed application has been validated in an industrial-components-manufacturing case study. Nevertheless, its modularity allows for the adaption in other varying industrial sectors as well.}
}
@article{SANDOVALPEREZ2024335,
title = {Teaching three-phase half-wave power electronic rectifier with gamified augmented reality support},
journal = {Alexandria Engineering Journal},
volume = {99},
pages = {335-346},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.077},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824004599},
author = {Sergio {Sandoval Pérez} and Juan Miguel {González López} and Manuel {Brambila Pelayo} and Jesús Ezequiel {Molinar Solis}},
keywords = {Virtual Reality, Teaching power electronics, Augmented reality, Gamification},
abstract = {This paper presents a support tool designed for instructors in technical education within engineering programs, focusing specifically on power electronics and laboratory procedures related to the three-phase half-wave rectifier. The Augmented Reality (AR) application was developed for Android devices using Unity+Vuforia, employing a gamification-based approach. The application allows students to evaluate their knowledge and skills in real-time and offers an interactive learning environment. It provides learning diagnostics and sends each student's evaluation to a database stored on Google-Forms that is accessible by the instructor or assessor in question. It's important to note that this APP is freely accessible through the Android APP store. The learning strategy is around a game that tests players on their knowledge of measuring devices and theoretical concepts. Additionally, there is a puzzle task that encourages learners to correctly arrange the waveforms in a rectifier. Students can also connect the half-wave rectifier's essential connections and the transformer's delta-star connections through touch interaction. A comparison study between traditional teaching methods and the innovative AR-based approach has been conducted. This educational proposal itself is an innovative solution that will be especially helpful for laboratories lacking useful equipment in Mexican colleges. To test the effect of AR application on student success, an experimental study was conducted on two groups of students (a control group [non-using AR] and a test group [using AR]). To determine significant difference in acquired learning, an ANOVA statistical analysis was conducted using independent sample tests. According to tests result, there is a significant difference in the acquired knowledge for those who used the designed tool. The probability value (p = 1e-5 <0.05) for the use of AR is found. They increase their initial average grade from 27.08% to 72.3%, in contrast to the control group with no significant increasing learning, reporting an initial average grade of 28.75–43.33%. These results show that the use of AR in electronic practice makes a significant difference in student success.}
}
@article{KURNIAWAN201880,
title = {Human Anatomy Learning Systems Using Augmented Reality on Mobile Application},
journal = {Procedia Computer Science},
volume = {135},
pages = {80-88},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314388},
author = {Michael H Kurniawan and  Suharjito and  Diana and Gunawan Witjaksono},
keywords = {Augmented Reality, Human Anatomy Learning, Android Application},
abstract = {Students generally experience difficulties in learning human body anatomy due to constraints to visualize the body anatomy from 2D into 3D image. This research aims to develop a human anatomy learning system using augmented reality technology. By using this system, it is expected that students can easily understand the anatomy of the human body using a 3D image visualization. The method used in this system is augmented reality marker on mobile computing platform. The marker is captured by taking a picture. Then, the captured image is divided into pieces and the pattern is matched with images stored in the database. In this research, we use Floating Euphoria Framework and combine it with the SQLite database. Augmented reality anatomy system of the human body has features that can interactively display the whole body or parts of the human organs. To evaluate the usefulness of the application, we tested the augmented reality anatomy system with high school students and medical students for learning the anatomy of the human body. The results show that the human anatomy learning system with interactive augmented reality visualization helps students learn human anatomy more easily.}
}
@article{PILATI2020189,
title = {Learning manual assembly through real-time motion capture for operator training with augmented reality},
journal = {Procedia Manufacturing},
volume = {45},
pages = {189-195},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311355},
author = {Francesco Pilati and Maurizio Faccio and Mauro Gamberi and Alberto Regattieri},
keywords = {operator training, aided assembly, augmented reality, motion capture, learning factory, industry 4.0, digital manufacturing},
abstract = {The current fourth industrial revolution significantly impacts on production processes. The personalized production paradigm which distinguishes Industry 4.0 enables customers to order unique products, defined by the specific features selected. The operators involved in the manual assembly of such workpieces have to process an enormous component variety adapting their tasks from product to product with limited learning opportunities. On the other hand, digital technologies significantly evolved in the last decade and their adoption in industrial shop floors in increasingly wider. In particular, camera-based marker-less motion capture achieved a large popularity since it represents a cheap, reliable and non-invasive solution to track, trace and digitalize human movements in different environments. Considering the presented framework, this research proposes an original hardware/software architecture to assist in real-time operators involved in manual assembly processes during the training phase to support their learning process, both in terms of rate and quality. A marker-less depth camera captures human motions in relation with the workstation environment whereas an augmented reality application based on visual feedback guides the operator through consecutive assembly tasks during the training phase. An experimental campaign is performed at the Learning factory of the Digital production university laboratory to validate the proposed architecture compared to traditional paper-based instructions provided for trainings. A real industrial case study is adopted to test and quantitatively evaluate the benefits of the developed technology compared to traditional approach in terms learning rate, which increases by 22% with a reduction in manual process duration up to -51% during the first assembly cycles.}
}
@article{FERRARINI2009133,
title = {A Mixed-Reality approach to Test Automation Function For Manufacturing Systems},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {4},
pages = {133-138},
year = {2009},
note = {13th IFAC Symposium on Information Control Problems in Manufacturing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090603-3-RU-2001.0431},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016337806},
author = {Luca Ferrarini and Alessio Dedè},
keywords = {3D graphic simulation, discrete manufacturing systems, closed-loop testing},
abstract = {The paper deals with the problem of building a test environment for MES (Manufacturing Execution System) control of manufacturing system. In the paper, the MES control layer has been developed according to agent-based technology inside the Pabadis'Promise Project. The test system is built as a user-defined combination of the real automation components and a 3D graphical simulator. For that reason, it has been named as mixed-reality simulation. The advantage is to allow the system integrator to test some control SW (SoftWare) parts in closed-loop as soon as they become available, and in parallel to proceed with the plant construction. At the beginning the test will be made in simulation, and then some virtual components will be removed and substituted by the real components as soon as they become available. The paper investigates the possibility to automatize this process, and will discuss an application of a real cell producing carter parts for motorbikes.}
}
@article{POTSELUYKO2022104496,
title = {Game-like interactive environment using BIM-based virtual reality for the timber frame self-build housing sector},
journal = {Automation in Construction},
volume = {142},
pages = {104496},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104496},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003697},
author = {Lilia Potseluyko and Farzad {Pour Rahimian} and Nashwan Dawood and Faris Elghaish and Aso Hajirasouli},
keywords = {VR, BIM, Gamification, Integration, Design automation, Unreal engine, Parametric design, Collaboration},
abstract = {BIM, gamification, and Virtual Reality applications are more often used to serve the interests of Design for Manufacture and Assembly (DfMA). This paper presents a comprehensive study to exploit these technologies' innovative approaches and capabilities. The study is specifically adopted to implement small and medium-size architectural and construction practices with a limited budget and time dedicated to visualisation creation. The collected evidence proved that a game-like platform combined with BIM could provide simplified data delivery to a client, leading to customer satisfaction, confidence and increased sales. The designed workflow and templates were tested in the case study of a small self-build construction company. The staff was trained to provide BIM data correctly and use supplied game templates. The case study demonstrated that automation of the VR House Configurator creation is achievable. The study's outcome is an integrated solution to regenerate BIM models in the game environment and utilise the house configurator's organised furniture library and costing interface. Furthermore, the usability tests confirmed the applicability, practicability, and validity of the developed framework and tools to deal with the revealed challenges in the self-build sector. Finally, the research provided a fresh approach for the companies in the sector, a step-by-step guide for implementing the innovative changes, and detailed descriptions of the methodologies and workflows.}
}
@article{WENJUAN2016426,
title = {Analysis and Simulation of a 6R Robot in Virtual Reality},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {16},
pages = {426-430},
year = {2016},
note = {5th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.078},
url = {https://www.sciencedirect.com/science/article/pii/S240589631631641X},
author = {Li {Wen. Juan} and Song {Zheng. He} and Zhu {Zhong. Xiang} and Mao {En. Rong}},
keywords = {inverse kinematics, 6R robot, virtual reality, MATLAB, EON},
abstract = {Abstract:
To analyse the motion of a 6R robot, the inverse kinematics analysis was performed. Virtual reality makes it visible and verisimilar to simulate the robot motion as well as to verify the inverse kinematics analysis and the consistency of pose for the robot. In virtual reality, which is developed by EON Studio software, the robot motion is realized by joints control which requires the joint control functions instead of the use of teach pendant in practice. Addressing the particular difficulties, the MATLAB software was used for performing inverse kinematics analysis as well as to fit the control functions of the joints. The combination of MATLAB and EON can realize the control of a 6R robot to validate smooth motions in virtual reality.}
}
@article{SHIN2008882,
title = {Identification of application areas for Augmented Reality in industrial construction based on technology suitability},
journal = {Automation in Construction},
volume = {17},
number = {7},
pages = {882-894},
year = {2008},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2008.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580508000289},
author = {Do Hyoung Shin and Phillip S. Dunston},
keywords = {Augmented Reality, Human factors, Visual information, Industrial construction, 3D drawing, Suitability},
abstract = {Research studies in the application of Augmented Reality (AR) in the Architecture, Engineering, and Construction (AEC) industry have suggested its feasibility. However, realization of the use of AR in AEC requires not only demonstration of feasibility but also validation of its suitability. This paper comprehensively identifies AR application areas in industrial construction based on suitability of AR technologies. In order to successfully explore suitability of AR, this paper assesses work tasks from the viewpoint of human factors regarding visual information requirements to find rationale for the benefits of AR in work tasks. Based on the assessment of work tasks, this paper presents a comprehensive map that identifies AR application areas in industrial construction. The comprehensive map reveals that eight work tasks (layout, excavation, positioning, inspection, coordination, supervision, commenting, and strategizing) out of 17 classified work tasks may potentially benefit from AR support.}
}
@article{FARQUHAR2023109489,
title = {Marconi-Rosenblatt Framework for Intelligent Networks (MR-iNet Gym): For Rapid Design and Implementation of Distributed Multi-agent Reinforcement Learning Solutions for Wireless Networks},
journal = {Computer Networks},
volume = {222},
pages = {109489},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109489},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005230},
author = {Collin Farquhar and Swatantra Kafle and Kian Hamedani and Anu Jagannath and Jithin Jagannath},
keywords = {Machine learning, Reinforcement learning, Wireless network, CDMA, Ns-3, OpenAI gym, LPD/I networks, Power and frequency control},
abstract = {We present the Marconi-Rosenblatt Framework for Intelligent Networks (MR-iNet Gym) an open-source architecture designed for accelerating research and development of novel reinforcement learning applied to distributed wireless networks. To ensure an end-to-end architecture, we leverage the existing work of ns3-gym, a software package that allows for using ns-3, a wireless network simulator, as an environment within the OpenAI Gym framework for RL. In addition to this, we have implemented the first known custom CDMA module for ns-3 as well as a framework for RL models with a core suite of implemented algorithms. The software framework capturing the interaction between wireless transceiver (agent) and RL decision engine has been designed to maximize the ease-of-use when testing different RL algorithms and models. In the rest of the paper, we describe these new software components and demonstrate some of the results and capabilities that can be achieved when used in conjunction with the existing open-source ecosystem.}
}
@article{BLACK2024184,
title = {Mixed reality human teleoperation with device-agnostic remote ultrasound: Communication and user interaction},
journal = {Computers & Graphics},
volume = {118},
pages = {184-193},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000037},
author = {David Black and Mika Nogami and Septimiu Salcudean},
keywords = {Human computer interaction, Mixed reality, Augmented reality, Teleoperation, Tele-ultrasound},
abstract = {For many applications, remote guidance and telerobotics provide great advantages. For example, tele-ultrasound can bring much-needed expert healthcare to isolated communities. However, existing tele-guidance methods have serious limitations including either low precision for video conference-based systems, or high complexity and cost for telerobotics. A new concept called human teleoperation leverages mixed reality, haptics, and high-speed communication to provide tele-guidance that gives an expert nearly-direct remote control without requiring a robot. This paper provides an overview of the human teleoperation concept and its application to tele-ultrasound. The concept and its impact are discussed. A new approach to remote streaming and control of point-of-care ultrasound systems independent of their manufacturer is described, as is a high-speed communication system for the HoloLens 2 that is compatible with ResearchMode API sensor stream access. Details of these systems are shown in supplementary video demonstrations. Novel interaction methods enabled by HoloLens 2-based pose tracking are also introduced and tests of the communication and user interaction are presented. The results show continued improvement of the system compared to previous work in instrumentation, HCI, and communication. The system thus has good potential for tele-ultrasound, as well as possible other applications of human teleoperation including remote maintenance, inspection, and training. The remote ultrasound streaming and control application is made available open source.}
}
@article{LIU2023106568,
title = {A mixed reality-based navigation method for dental implant navigation method: A pilot study},
journal = {Computers in Biology and Medicine},
volume = {154},
pages = {106568},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106568},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523000331},
author = {Lin Liu and Xiaoyu Wang and Miaosheng Guan and Yiping Fan and Zhongliang Yang and Deyu Li and Yuming Bai and Hongbo Li},
keywords = {Three-dimensional imaging, Oral and maxillofacial surgeons augmented reality, Dentition, Surgical instruments},
abstract = {This in vitro study aimed to put forward the development and investigation of a novel Mixed Reality (MR)-based dental implant navigation method and evaluate implant accuracy. Data were collected using 3D-cone beam computed tomography. The MR-based navigation system included a Hololens headset, an NDI (Northern Digital Inc.) Polaris optical tracking system, and a computer. A software system was developed. Resin models of dentition defects were created for a randomized comparison study with the MR-based navigation implantation system (MR group, n = 25) and the conventional free-hand approach (FH group, n = 25). Implant surgery on the models was completed by an oral surgeon. The precision and feasibility of the MR-based navigation method in dental implant surgery were assessed and evaluated by calculating the entry deviation, middle deviation, apex deviation, and angular deviation values of the implant. The system, including both the hardware and software, for the MR-based dental implant navigation method were successfully developed and a workflow of the method was established. Three-Dimensional (3D) reconstruction and visualization of the surgical instruments, dentition, and jawbone were achieved. Real-time tracking of implant tools and jaw model, holographic display via the MR headset, surgical guidance, and visualization of the intraoperative implant trajectory deviation from the planned trajectory were captured by our system. The MR-based navigation system was with better precise than the free-hand approach for entry deviation (MR: 0.6914 ± 0.2507 mm, FH: 1.571 ± 0.5004 mm, P = 0.000), middle deviation (MR: 0.7156 ± 0.2127 mm, FH: 1.170 ± 0.3448 mm, P = 0.000), apex deviation (MR: 0.7869 ± 0.2298 mm, FH: 0.9190 ± 0.3319 mm, P = 0.1082), and angular deviation (MR: 1.849 ± 0.6120°, FH: 4.933 ± 1.650°, P = 0.000).}
}
@article{DEGIORGIO20171279,
title = {Human-machine Collaboration in Virtual Reality for Adaptive Production Engineering},
journal = {Procedia Manufacturing},
volume = {11},
pages = {1279-1287},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.255},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917304638},
author = {Andrea {de Giorgio} and Mario Romero and Mauro Onori and Lihui Wang},
keywords = {Virtual Reality, Augmented Reality, Unity Game Engine, Human-Robot Collaboration, Industry 4.0, Robotics, Adaptive Production},
abstract = {This paper outlines the main steps towards an open and adaptive simulation method for human-robot collaboration (HRC) in production engineering supported by virtual reality (VR). The work is based on the latest software developments in the gaming industry, in addition to the already commercially available hardware that is robust and reliable. This allows to overcome VR limitations of the industrial software provided by manufacturing machine producers and it is based on an open-source community programming approach and also leads to significant advantages such as interfacing with the latest developed hardware for realistic user experience in immersive VR, as well as the possibility to share adaptive algorithms. A practical implementation in Unity is provided as a functional prototype for feasibility tests. However, at the time of this paper, no controlled human-subject studies on the implementation have been noted, in fact, this is solely provided to show preliminary proof of concept. Future work will formally address the questions that are raised in this first run.}
}
@article{KIM2012184,
title = {Implementation of Martian virtual reality environment using very high-resolution stereo topographic data},
journal = {Computers & Geosciences},
volume = {44},
pages = {184-195},
year = {2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2011.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0098300411003475},
author = {Jung-Rack Kim and Shih-Yuan Lin and Jeong-Woo Hong and Young-Hwi Kim and Chin-Kang Park},
keywords = {Mars, High resolution DTM, Ortho-image, Virtual reality, Stereo analysis, Geomorphology},
abstract = {Topography over terrestrial or other planetary surfaces is an important base data for virtual reality construction. In particular, with inaccessible topography such as the Martian surface, virtual reality provides great value not only for public interaction but also for scientific research. For the latter application, since field surveys are essential for the geological and geomorphological researches, the virtual reality environment created based on verified topographic products provides an alternative solution for planetary research. The performance of virtual reality implementation over a planetary surface can be assessed by two major factors: (1) The geodetically controlled base topographic products, such as DTM and ortho-image, and (2) Technological integration of topographic products into virtual reality software and hardware. For the first aspect, the multi-resolution stereo analysis approach has already provided a solid basis so that specific topographic data sets over testing areas were generated by the hierarchical processor. To address the second problem, a parallel processor with multiple screen display combining 3D display software was employed in this research. As demonstrated in this paper, the constructed Martian virtual environment showed highly detailed features over the Athabasca Valles (one of former potential Mars Exploration Rover landing sites) and Eberswalde crater (one of the main original landing candidates for the NASA’s rover mission scheduled to launch in late 2011). The employment of such virtual reality environments is expected to be a powerful simulator after integrating a 3D Martian model, engineering and environment constraints for Martian geological and geomorphic researches including landing site selection and rover navigation.}
}
@article{WIEPKE2024100064,
title = {A systematic literature review on user factors to support the sense of presence in virtual reality learning environments},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100064},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100064},
url = {https://www.sciencedirect.com/science/article/pii/S294967802400014X},
author = {Axel Wiepke and Birte Heinemann},
keywords = {Virtual reality, Learning environments, Presence, User factor, Systematic review},
abstract = {Immersive VR media are increasingly being integrated into education and academic studies. Various forms of teaching benefit from the sense of presence, wherein users feel fully engaged in the virtual environment. Research indicates that the presence experienced in VR environments enhances user satisfaction, reduces errors during tasks, and promotes more enduring training effects. Since presence is a subjective phenomenon, it is expected to be influenced by user demographics, cognitive abilities, personality traits, interests, and emotions. In this systematic literature review, we examined scientific articles using Google Scholar to identify significant influences of these user factors on presence. Employing the PRISMA methodology, we analyzed a total of 33 articles that addressed our research question. The results indicate that only a subset of the anticipated factors significantly affect presence. These factors include the user's level of interest in the subject being experienced, and any mental disorders associated with it. Additionally, factors such as the user's ability to perceive the spatial qualities of the virtual environment, their disposition toward kindness and generosity, and their inclination to engage with objects—such as media products—affect presence.}
}
@article{BOTTANI2021103429,
title = {Wearable and interactive mixed reality solutions for fault diagnosis and assistance in manufacturing systems: Implementation and testing in an aseptic bottling line},
journal = {Computers in Industry},
volume = {128},
pages = {103429},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103429},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000361},
author = {Eleonora Bottani and Francesco Longo and Letizia Nicoletti and Antonio Padovano and Giovanni Paolo Carlo Tancredi and Letizia Tebaldi and Marco Vetrano and Giuseppe Vignali},
keywords = {Augmented reality, Smart technologies, System usability scale, Industry 4.0, Wearable technologies, Industrial safety},
abstract = {Thanks to the spread of technologies stemming from the fourth industrial revolution, also the topic of fault diagnosis and assistance in industrial contexts has benefited. Indeed, several smart tools were developed for assisting with maintenance and troubleshooting, without interfering with operations and facilitating tasks. In line with that, the present manuscript aims at presenting a web smart solution with two possible applications installed on an Android smartphone and Microsoft HoloLens. The solution aims at alerting the operators when an alarm occurs on a machine through notifications, and then at providing the instructions needed for solving the alarm detected. The two devices were tested by the operators of an industrial aseptic bottling line consisting of five machines in real working conditions. The usability of both devices was positively rated by these users based on the System Usability Scale (SUS) and additional appropriate statements. Moreover, the in situ application brought out the main difficulties and interesting issues for the practical implementation of the solutions tested.}
}
@article{MAIER2022244,
title = {Experiencing the structure and features of a machine tool with mixed reality},
journal = {Procedia CIRP},
volume = {106},
pages = {244-249},
year = {2022},
note = {9th CIRP Conference on Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.186},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001871},
author = {Walther Maier and Johannes Rothmund and Hans-Christian Möhring and Phuc-Dat Dang and Evelyn Hoffarth and Bernd Zinn and Matthias Wyrwal},
keywords = {Structure, augmented reality, learning},
abstract = {Augmented and mixed reality (AR/MR) enable a completely new way of visualizing the connections between components and assemblies of a complex machine tool. By means of AR/MR, users can immerse themselves in areas of interest of a machine or an assembly process and not only observe the connections from outside a real machine or a process. Experiencing virtual contents in the real surroundings is completely different from virtually seeing a design or an assembly process on a screen by means of a conventional CAD application or a simulation. Therefore, this paper presents a mixed reality app for the visualization of information about components and assemblies in the holograms of a complex machine tool. Within a research project on the improvement of teaching methods by augmented reality (AR), virtual reality (VR) and mixed reality (MR), it is planned to concept, develop and test teaching units in studies. At the Institute for Machine Tools (IfW) of the University of Stuttgart, the Index R200 high-end turning/milling centre has been chosen as demonstration model for an MR app. It is a 10-axis machining centre with several driven spindles and tool holders in the working space. It can be used both as real machine tool for complex experiments and as virtual machine for simulations or visualizations of CNC programs, among other things for developing new teaching methods and contents.}
}
@incollection{LANZONI2023829,
title = {Chapter 32 - A design procedure for the development of VR platforms for the rehabilitation of patients after stroke},
editor = {Gunther Paul and Mohamed {Hamdy Doweidar}},
booktitle = {Digital Human Modeling and Medicine},
publisher = {Academic Press},
pages = {829-848},
year = {2023},
isbn = {978-0-12-823913-1},
doi = {https://doi.org/10.1016/B978-0-12-823913-1.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239131000117},
author = {Daniel Lanzoni and Andrea Vitali and Daniele Regazzoni and Caterina Rizzi},
keywords = {Cognitive rehabilitation, Motor skills rehabilitation, Tele-rehabilitation, User-centered design, Virtual reality},
abstract = {Virtual Reality (VR) opens new frontiers in the rehabilitation field. Low-cost head mounted displays and motion capture systems are available and easily integrated in consumer technologies, such as laptops, smartphones, and tablets. The main limit of VR is related to its acceptance because the technologies are usually considered too complex for patients' daily rehabilitation. Furthermore, available VR hardware solutions are usually designed for generic categories of users and there is a considerable margin to specific pathologies or patients. This chapter presents the development of VR platforms for both cognitive and motor skills rehabilitation starting from physicians' requirements and usability issues. Three VR applications are described: a platform for cognitive rehabilitation of patients with severe memory loss, an application to test the level of extra-personal neglect, and a web platform for hand motor skill rehabilitation.}
}
@article{CHEN2024102330,
title = {One model, two brains: Automatic fetal brain extraction from MR images of twins},
journal = {Computerized Medical Imaging and Graphics},
volume = {112},
pages = {102330},
year = {2024},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102330},
url = {https://www.sciencedirect.com/science/article/pii/S0895611124000077},
author = {Jian Chen and Ranlin Lu and Bin Jing and He Zhang and Geng Chen and Dinggang Shen},
keywords = {Twin fetal brains, Brain extraction, MR images, Transfer learning},
abstract = {Fetal brain extraction from magnetic resonance (MR) images is of great importance for both clinical applications and neuroscience studies. However, it is a challenging task, especially when dealing with twins, which are commonly existing in pregnancy. Currently, there is no brain extraction method dedicated to twins, raising significant demand to develop an effective twin fetal brain extraction method. To this end, we propose the first twin fetal brain extraction framework, which possesses three novel features. First, to narrow down the region of interest and preserve structural information between the two brains in twin fetal MR images, we take advantage of an advanced object detector to locate all the brains in twin fetal MR images at once. Second, we propose a Twin Fetal Brain Extraction Network (TFBE-Net) to further suppress insignificant features for segmenting brain regions. Finally, we propose a Two-step Training Strategy (TTS) to learn correlation features of the single fetal brain for further improving the performance of TFBE-Net. We validate the proposed framework on a twin fetal brain dataset. The experiments show that our framework achieves promising performance on both quantitative and qualitative evaluations, and outperforms state-of-the-art methods for fetal brain extraction.}
}
@article{MARZANO2015242,
title = {Design of a Virtual Reality Framework for Maintainability and Assemblability Test of Complex Systems},
journal = {Procedia CIRP},
volume = {37},
pages = {242-247},
year = {2015},
note = {CIRPe 2015 - Understanding the life cycle implications of manufacturing},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.08.067},
url = {https://www.sciencedirect.com/science/article/pii/S221282711500921X},
author = {Adelaide Marzano and Imelda Friel and John Ahmet Erkoyuncu and Samuel Court},
keywords = {Virtual Reality, Assemblability, Manufacturing Systems},
abstract = {This paper presents a unique environment whose features are able to satisfy requirements for both virtual maintenance and virtual manufacturing through the conception of original virtual reality (VR) architecture. Virtual Reality for the Maintainability and Assemblability Tests (VR_MATE) encompasses VR hardware and software and a simulation manager which allows customisation of the architecture itself as well as interfacing with a wide range of devices employed in the simulations. Two case studies are presented to illustrate VR_MATE's unique ability to allow for both maintainability tests and assembly analysis of an aircraft carriage and a railway coach cooling system respectively. The key impact of this research is the demonstration of the potentialities of using VR techniques in industry and its multiple applications despite the subjective character within the simulation. VR_MATE has been presented as a framework to support the strategic and operative objectives of companies to reduce product development time and costs whilst maintaining product quality for applications which would be too expensive to simulate and evaluate in the real world.}
}
@article{LEUTERT2015153,
title = {Augmented Reality for Telemaintenance and -inspection in Force-Sensitive Industrial Robot Applications},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {10},
pages = {153-158},
year = {2015},
note = {2nd IFAC Conference on Embedded Systems, Computer Intelligence and Telematics CESCIT 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.08.124},
url = {https://www.sciencedirect.com/science/article/pii/S240589631500991X},
author = {Florian Leutert and Klaus Schilling},
keywords = {Industrial Robots, Augmented Reality, Telecontrol, Human-Robot-Interaction, Tele-robotics, Control Interfaces},
abstract = {Telematic solutions allow fast remote support in industrial robot work cells: Robotic experts can assist the local staff in work process modifications or in tracking down errors without having to actually travel to the remote site. In order to be able to perform such teleinspection and telemaintenance tasks the operator needs to establish a data connection to the robot and get an understanding of the remote work process, as well as of the transmitted data. This contribution introduces a telematic control system developed for remote monitoring and control of an industrial robot performing force-sensitive work applications. An Augmented Reality interface is proposed for intuitive representation of complex robotic data, allowing quick analysis of the current working cycle and process data and altering the robot's program remotely while still having a clear spatial reference to the work environment.}
}
@article{HU201885,
title = {A VR simulation framework integrated with multisource CAE analysis data for mechanical equipment working process},
journal = {Computers in Industry},
volume = {97},
pages = {85-96},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518300289},
author = {Liang Hu and Zhenyu Liu and Jianrong Tan},
keywords = {Modeling and simulation, DEVS, VR simulation framework, Working process modeling, CAE analysis data integration},
abstract = {Nowadays, the virtual reality (VR) technology has been widely used in machinery to verify the working process and performance of equipment prototype in early design stage. To gain insight into the coupling of process verifying the timing and rhythm of equipment working and performance indicating the quality of equipment, a VR framework is proposed in this paper to integrate the modeling and simulation of working process with the visualization of multisource analysis data from various computer-aided engineering (CAE) software. The framework is equipped with an extended discrete event system specification (DEVS) method, named scripted DEVS, which divides the process modeling into two stages, namely, behavior modeling and data customization, and frees the mechanical designer from programming. Based on the method, a uniform procedure of data integration is designed to hide the specific structure and visualization implementation of diverse data from the modeler by a script format called PDTree (Process&Data Tree) and a series of visualization interfaces in model-view-controller (MVC) pattern. Three applications of different working processes are also presented finally to verify the flexibility and compatibility of the framework.}
}
@article{POONJA2023104720,
title = {Engagement detection and enhancement for STEM education through computer vision, augmented reality, and haptics},
journal = {Image and Vision Computing},
volume = {136},
pages = {104720},
year = {2023},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2023.104720},
url = {https://www.sciencedirect.com/science/article/pii/S026288562300094X},
author = {Hasnain Ali Poonja and Muhammad Ayaz Shirazi and Muhammad Jawad Khan and Kashif Javed},
keywords = {Engagement detection, Engagement enhancement, Computer vision, Augmented reality, Haptics, Online learning},
abstract = {STEM (Science, technology, engineering, and mathematics) based education is being revolutionized with the rapid development of the applications of intelligent Haptics. Theoretical knowledge may now be put into practice, and complex ideas can now be presented using three-dimensional models that are enhanced in the actual environment via Augmented Reality (AR) and computer Haptics to enhance the student's interest in concepts and studies. This article presents a novel design and development of an AR and Haptics-based STEM product comprising of World Map with Haptic feedback using Vuforia, Unity 3D, and Open-Haptics to enhance student engagement. To detect students' engagement in online learning, a computer vision-based system is also designed and deployed in the real-time website that uses the features, such as facial emotions, pose estimation, and head rotation. Marker-based augmentation is employed, and a sense of touch is included for an immersive experience for the students with the use of the haptic Touch Omni device. Hence, the proposed approach can improve classroom learning activities by using Augmented reality and Haptics-based STEM product and by its evaluation using a computer vision system. To validate the improvement in engagement, a comprehensive user study is performed and analyzed on the proposed setup.}
}
@article{MOURTZIS20221144,
title = {Integration of Mixed Reality to CFD in Industry 4.0: A Manufacturing Design Paradigm},
journal = {Procedia CIRP},
volume = {107},
pages = {1144-1149},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.122},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004061},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Simulation, Computational Fluid Dynamics, Industry 4.0, Industrial Internet of Things},
abstract = {Simulation, as a pillar technology of Industry 4.0, is a key tool for improving manufacturing processes. Simulation is an indispensable set of technological tools and methods for the successful implementation of digital manufacturing, since it allows for the experimentation and validation of product, process, and system design and configuration. The value of simulation is evident, especially in today’s volatile manufacturing environment, which is influenced by globalization and ever-increasing demands for higher levels of product customization and personalization. Moreover, a significant manufacturing issue is the interoperability and communication among the several simulation tools. To that end, an Industrial Internet of Things (IIoT) framework for simulation and visualization of selected monitored values based on Mixed Reality (MR) and Computational Fluid Dynamics (CFD) in a cloud-computing environment for intuitive data visualization on smart devices is presented in this paper. As such, in light of the possibilities offered by digital technologies such as CFD simulation, smart factories can gain a competitive advantage because of the low cost, low risk, and the provided useful insights. The range of CFD applications encountered in process manufacturing and engineering varies greatly depending on the industry. Summarizing, this review paper identifies the gaps in current practices, and future trends and challenges to be met on the field are outlined. The contribution of the research work extends to the proposal of a workflow for the integration of Mixed Reality (MR) to the CFD simulation, in an attempt to create simulation holograms on the user’s field of view.}
}
@article{URIEL2020389,
title = {Improving the understanding of Basic Sciences concepts by using Virtual and Augmented Reality},
journal = {Procedia Computer Science},
volume = {172},
pages = {389-392},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.165},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314952},
author = {Cukierman Uriel and Silvestri Sergio and González Carolina and González Mariano and Dellepiane Paola and Agüero Martín},
keywords = {Basic Sciences, Virtual, Augmented Reality, Student-Centered Learning},
abstract = {One of the most important problems in the early years of engineering programs is the difficulty that students have to understand Basic Sciences concepts, their application in technology and, particularly, in engineering. One of the ways to solve this situation is by implementing methodologies that promote an "Active and Student-Centered Learning" (ASCL). There are many techniques to develop this kind of approach, some of them are facilitated and/or enhanced through the use of technology. One of the most developed technologies in recent times and with greater expectations for the near future is Virtual and Augmented Reality (VAR). Our objective with this project is to develop and test VAR applications for improving the understanding of Basic Sciences concepts in freshmen courses.}
}
@article{DEPAOLIS2022e00238,
title = {Virtual reality for the enhancement of cultural tangible and intangible heritage: The case study of the Castle of Corsano},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {27},
pages = {e00238},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00238},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000273},
author = {Lucio Tommaso {De Paolis} and Sofia Chiarello and Carola Gatto and Silvia Liaci and Valerio {De Luca}},
keywords = {Cultural heritage, Intangible heritage, 3D modelling, Virtual reality, User experience},
abstract = {This paper concerns the development of an immersive VR application for the enhancement of an inaccessible old Castle in Corsano, a small village in Salento - Italy. Starting from the 3D reconstruction of the building, the project allowed the development of an interaction system aimed at providing the user with the historical information about the Castle. These contents come from the analysis of the tangible cultural heritage (such as the architectural elements, the furniture, the decorative motifs of the castle, and their evolution through the centuries), but also from the collection of intangible heritage, such as reminiscences on folk customs and traditions related to the context of the castle. Moreover, in order to evaluate the user experience of the developed application, some tests were carried out on a heterogeneous sample of users, obtaining positive feedback on the degree of immersiveness and sense of presence of the application.}
}
@article{FAZEL2018135,
title = {An interactive augmented reality tool for constructing free-form modular surfaces},
journal = {Automation in Construction},
volume = {85},
pages = {135-145},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516302497},
author = {Alireza Fazel and Abbasali Izadi},
keywords = {Digital fabrication, Multi-marker augmented reality, Edge detection, Free-form construction, Complex brick wall},
abstract = {Although modern software has paved the way for architects to design complex forms, such as free-forms, construction remains challenging, costly, and time-consuming which requires skilled workers. Advanced digital fabrication technologies can offer new ways to fill the gap between design and construction. Augmented Reality (AR) technology is one such technology that has many potentials in various fields, however, its capabilities are not sufficiently explored yet, especially in the field of digital fabrication. This study presents a new affordable interactive multi-marker augmented reality tool for constructing free-form modular surfaces implemented by integrating common accessible devices. The proposed tool consists of two digital cameras, a head-mounted display, a processor, and two markers that enable the user to virtually see the accurate location of any proposed object in the real world. A controlling subsystem was also designed to enhance the accuracy of construction. Method efficiency was studied in five full-scale prototypes. The results showed that the majority of errors (91%) were less than 6mm, and 2° for lateral placements and orientation errors.}
}
@article{ZHOU2023188,
title = {Web-based mixed reality video fusion with remote rendering},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {2},
pages = {188-199},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000274},
author = {Qiang Zhou and Zhong Zhou},
keywords = {Mixed reality, Video fusion, WebRTC, Remote rendering},
abstract = {Background
Mixed reality (MR) video fusion systems merge video imagery with 3D scenes to make the scene more realistic and help users understand the video content and temporal–spatial correlation between them, reducing the user′s cognitive load. MR video fusion are used in various applications; however, video fusion systems require powerful client machines because video streaming delivery, stitching, and rendering are computationally intensive. Moreover, huge bandwidth usage is another critical factor that affects the scalability of video-fusion systems.
Methods
Our framework proposes a fusion method for dynamically projecting video images into 3D models as textures.
Results
Several experiments on different metrics demonstrate the effectiveness of the proposed framework.
Conclusions
The framework proposed in this study can overcome client limitations by utilizing remote rendering. Furthermore, the framework we built is based on browsers. Therefore, the user can test the MR video fusion system with a laptop or tablet without installing any additional plug-ins or application programs.}
}
@article{JIANG2022101001,
title = {Adjacent surface trajectory planning of robot-assisted tooth preparation based on augmented reality},
journal = {Engineering Science and Technology, an International Journal},
volume = {27},
pages = {101001},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001130},
author = {Jingang Jiang and Yafeng Guo and Zhiyuan Huang and Yongde Zhang and Dianhao Wu and Yi Liu},
keywords = {Dental assisted robot, Adjacent surface tooth preparation, Trajectory planning, Augmented reality, NURBS curve},
abstract = {Dental caries is a common disease affecting the health of humans. Tooth preparation is an essential method for the treatment of dental caries, and its effect determines the quality of dental caries restoration directly. At present, the doctor-patient ratio of dentistry is incredibly unbalanced, and it is challenging to meet the increasing demand for tooth preparation. Besides, it is prone to produce visual deviation and positioning error if the tooth preparation is operated manually by doctors. This paper focuses on the adjacent surface tooth preparation of the full crown. By analyzing the preparation process, the robot body as well as the end-effector are selected. Further, the pose and position between the robot and the tooth are established. Through the inverse calculation and interpolation of the NURBS curve, the robot-assisted trajectory planning of adjacent surface tooth preparation is realized. On the basis of the traditional offline programming method, mobile App “Dental Preparation Assisting Software” is developed based on augmented reality, which can interactively display the planned adjacent preparation curves and preparation information. And the robot can be controlled by the mobile App, which improves the visualization and efficiency of the preparation process. Finally, the experimental system based on the Dobot Magician robot is established, and the robot experiments of adjacent surface tooth preparation are carried out. The maximum relative fixed-point error of each feature point in the XYZ axis is 0.24 mm, 0.29 mm and 0.37 mm, respectively. The confidence interval width of each feature point in the XYZ direction is stable at about 0.31 mm on average, and the correlation between the feature points in the three directional variables is not strong. The proposed method of robot-assisted trajectory planning based on augmented reality for adjacent surface tooth preparation can improve the efficiency of tooth preparation and relieve the pressure of manual tooth preparation within the margin of error, and its feasibility and validity are verified.}
}
@article{NIKITIN2020622,
title = {VR Training for Railway Wagons Maintenance: architecture and implementation},
journal = {Procedia Computer Science},
volume = {176},
pages = {622-631},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318883},
author = {Alexandr Nikitin and Nina Reshetnikova and Ivan Sitnikov and Olesya Karelova},
keywords = {VR Training, railway maintenances, digital railway, virtual reality, software architecture, implementation, HTC Vive, Unity},
abstract = {The problems of supporting the required professional skills of railway wagons maintenance workers are regarded in this article. It is proposed to design a simulator based on virtual reality to test the skills of a car inspector in accordance with the instructions. The requirements for the simulator, its architecture and the architecture of the software simulator, platforms and implementation tools are considered. The paper provides features of the implemented prototype of the simulator based on HTC Vive and Unity platform for training skills of car inspection by the inspector and their further verification by the instructor in accordance with the regulations illustrated by a 3D model of the wagon with the fault and the tools of the inspector. Analysis of the simulator prototype’s feasibility showed its sufficient ergonomic adequacy to the actual process of wagons maintenance and identified a number of implementation problems that will be taken into account in further research and development.}
}
@article{KELLER20112105,
title = {ITER Equatorial Port plug engineering: Design and remote handling activities supported by Virtual Reality tools},
journal = {Fusion Engineering and Design},
volume = {86},
number = {9},
pages = {2105-2108},
year = {2011},
note = {Proceedings of the 26th Symposium of Fusion Technology (SOFT-26)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2011.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S0920379611004595},
author = {Delphine Keller and Christian Dechelle and Louis Doceul and Sylvain Madeleine and Jean Pierre Martins and Yvan Measson and Jean Claude Patterlini and Julien Wagrez},
keywords = {ITER, Equatorial Port plug, Maintenance, Test blanket module system, Remote handling, Virtual Reality},
abstract = {In the context of ITER, CEA/IRFM has participated to the design and integration of several components in the Equatorial Port plug region. Particularly, in the framework of the grant F4E-2008-GRT-09-PNS-TBM, CEA/IRFM has contributed to the test blanket module system (TBS) design and robot access feasibility study in the Port Cell. Simulations of the maintenance procedure were studied and fully integrated to the design process, enabling to provide space reservation for human and robotic access. For this mean, CEA/IRFM has used a CEA LIST Virtual Reality simulation software directly integrated to the Solidworks CAD software. The feasibility to connect/dis-connect the pipes in front of the Bioshield by a set of potential standard industrial arms was demonstrated. Aiming to give more realism to maintenance scenario and CAD models, CEA IRFM has decided to build a Virtual Reality platform in the institute, integrated to the design office. With the expertise of CEA LIST, this platform aims to provide the nearest possible links between design and remote handling needs. This paper presents the outcome of the robot access study and discusses about the Virtual Reality tools that are being developed for these applications.}
}
@article{ZHAO2024200089,
title = {VR interactive input system based on INS and binocular vision fusion},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200089},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200089},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000188},
author = {Hongxia Zhao and Bei Wang},
keywords = {Virtual reality, Space position measurement, Pose estimation, Binocular vision},
abstract = {In virtual reality interactive input systems, real-time and accurate spatial position and pose measurement is key to achieving a natural user interface. This study explores the application of inertial measurement units and binocular vision fusion technology in virtual reality interactive input systems, with the aim of improving the tracking accuracy of the system through optimized pose models and visual algorithms. A virtual reality measurement technology that integrates inertial measurement units and binocular vision is proposed by using an improved Kalman filtering algorithm to process inertial measurement units data, and combining it with the SURF algorithm optimized binocular vision system. Experimental results showed that fusion technology could reduce sensor noise and bias, improve the accuracy of pose estimation, and promote stable and robust motion tracking in virtual reality systems. In the angle range of -90 ° to 90 °, the average absolute error of the fusion system compared to the simple inertial measurement units pose calculation decreased from 3.371 ° to 1.369 ° In the experiment of measuring distance with a binocular vision system, the average absolute error value decreased to 1.532 mm, and the error range of the marker within the distance range of 500 mm to 650 mm was controlled within -1.3 mm to 1.2 mm. This study provides an effective solution for achieving high-precision virtual reality interactive input systems and is meaningful for the advancement of virtual reality technology.}
}
@article{MIHALYI20151,
title = {Robust 3D object modeling with a low-cost RGBD-sensor and AR-markers for applications with untrained end-users},
journal = {Robotics and Autonomous Systems},
volume = {66},
pages = {1-17},
year = {2015},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2015.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015000135},
author = {Rãzvan-George Mihalyi and Kaustubh Pathak and Narunas Vaskevicius and Tobias Fromm and Andreas Birk},
keywords = {Object modeling, RGBD sensor, Augmented Reality (AR) marker, Pose uncertainty, Graph-SLAM},
abstract = {An approach for generating textured 3D models of objects without the need for complex infrastructure such as turn-tables or high-end sensors on precisely controlled rails is presented. The method is inexpensive as it uses only a low-cost RGBD sensor, e.g., Microsoft Kinect or ASUS Xtion, and Augmented Reality (AR) markers printed on paper sheets. The sensor can be moved by hand by an untrained person and the AR-markers can be arbitrarily placed in the scene, thus allowing the modeling of objects of a large range of sizes. Due to the use of the simple AR markers, the method is significantly more robust than just using the RGBD sensor or a monocular camera alone and it hence avoids the typical need for manual post-processing of alternative approaches like Kinect-Fusion, 123D Catch, Photosynth, or similar. This article has two main contributions: First, the development of a simple, inexpensive method for the quick and easy digitization of physical objects is presented. Second, the development of an uncertainty model for AR-marker pose estimation is introduced. The latter is of interest beyond the object modeling application presented here. The uncertainty model is used in a graph-based relaxation method to improve model-consistency. Realistic modeling of various objects, such as parcels, sport balls, coffee sacks, human dolls, etc., is experimentally demonstrated. Good model-accuracy is shown for several ground-truth objects with simple geometries and known dimensions. Furthermore, it is shown that the models obtained using the uncertainty model have fewer errors than the ones obtained without it.}
}
@article{DESOUZACARDOSO2020106712,
title = {Mobile augmented reality to support fuselage assembly},
journal = {Computers & Industrial Engineering},
volume = {148},
pages = {106712},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106712},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220304381},
author = {Luís Fernando {de Souza Cardoso} and Flávia Cristina Martins Queiroz Mariano and Ezequiel Roberto Zorzal},
keywords = {Augmented reality, Mixed reality, Markerless tracking, Hand-held mobile device, Aeronautical structural assembly, Industry 4.0},
abstract = {One major manufacturing challenge of the fourth industrial revolution is the quality of product development information used in the assembly line. One of the most studied information visualization technologies is Augmented Reality (AR), whose deployment via mobile devices is currently being promoted. However, the industrial sector still has doubts regarding the technology’s maturity and the costs incurred by such production processes, restricting its deployment in production environments. Thus, in the present study, we statistically analyze a markerless AR application, using a hand-held mobile device, that addresses the requirements of the structural assemblies used in aeronautical industries comparing them to those of traditional assembly methods. The main technological benefits and limitations of such applications are identified. To perform this analysis, we conducted a field evaluation where qualified professionals marked the positions of small brackets in an aeronautical structure by using AR. After collecting the time and measurement data for each bracket’s position, a statistical analysis was performed to compare these positionings against those achieved using a traditional, metric scale-based method. The results indicate that AR offers a faster solution for highly complex assemblies, despite several limitations regarding the positioning tolerance when using the technology. As a future study, several application modifications are suggested to reduce markup errors and overall usage time.}
}
@article{DUAN20214299,
title = {Design of online volleyball remote teaching system based on AR technology},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {5},
pages = {4299-4306},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821001617},
author = {Chunxia Duan},
keywords = {AR technology, Online volleyball, Remote teaching system, Motion capture},
abstract = {To help individuals deal with volleyball learning and training, an online volleyball remote teaching system is designed based on Augmented Reality (AR) technology. Firstly, the action images of the volleyball coach were captured, followed by being input into the computer. The hardware of the system was mainly composed of motion capture equipment, data transmission equipment, central control processing equipment, and AR display equipment. The system’s software included four modules of motion acquisition and recognition. The testing results showed that the functional modules and performance of the system could meet the expected requirements, verifying the effectiveness and practicability.}
}
@article{WOLFARTSBERGER201927,
title = {Analyzing the potential of Virtual Reality for engineering design review},
journal = {Automation in Construction},
volume = {104},
pages = {27-37},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518312093},
author = {Josef Wolfartsberger},
keywords = {Design review, Virtual Reality, Engineering, Usability},
abstract = {Virtual Reality (VR) technology still needs to evolve, but as the pace of innovations accelerates, systems allow for more novel modes of visualization and interaction to support engineering design reviews. Currently, the classic design review process is often performed on a PC with the support of CAD software packages. However, CAD on a screen cannot always meet all the requirements in regard to the functional and ergonomic validations of complex 3D models. In this paper, the development and evaluation of a VR-based tool to support engineering design review is described. “VRSmart” visualizes CAD data and allows for an intuitive interaction. In a preliminary user study, the tool was checked for its usability and user experience. VRSmart was then evaluated in a real industrial environment and tested in an authentic design review. The results indicate that a VR-supported design review allows users to see slightly more faults in a 3D model than in a CAD software-based approach on a PC screen. Furthermore, VR reduces the risk of exclusion of certain professional groups from the design review process. In addition, the intuitive interaction with the VR system allowed for a much faster entry into the design review. In summary, VR will not replace the traditional design review process on screen, but it provides a useful addition to engineering companies.}
}
@article{TATIC20171,
title = {The application of augmented reality technologies for the improvement of occupational safety in an industrial environment},
journal = {Computers in Industry},
volume = {85},
pages = {1-10},
year = {2017},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516302718},
author = {Dušan Tatić and Bojan Tešić},
keywords = {Augmented reality, Occupational safety, Mobile devices, Virtual reality, Real world interaction},
abstract = {In many branches of industry, occupational safety experts identified two main causes of worker injuries related to the usage of modern electro-mechanical machines and systems: inadequate training and insufficient work experience, and monotonicity of the tasks often performed repeatedly. In this paper, we present a system based on augmented reality (AR) technologies that can be useful in reducing these factors of risk at work and decreasing the error rate and preventing injuries. The system that is implemented on mobile devices is intended to project augmented reality instructions directly at the work place. A worker is led by the AR-system step by step through various work and safety procedures that should be performed. Each procedure consists of steps specified by a series of instructions accessed through an interactive check list. To ensure the safeness, if a confirmation is missing because of a skipped, incompletely, or wrongly performed step of a procedure, the AR-system blocks further implementation of the procedure and returns the worker to the previous step until the correct actions are carried out. At the same time, interactive work with the checklist breaks the monotonicity of the job. The system is personalized according to skills of a worker by taking into account his professional training and work experience. Depending on that it is determined the amount of data to be displayed to a worker helping even less skilled workers to perform a task. As a case study, the proposed approach is implemented as an instructional and occupational safety system for work at a universal lathe, which is an element of many technological processes of Thermal Power Plant Ugljevik in the Republika Srpska, Bosnia and Herzegovina, where this AR-system was experimentally implemented and verified.}
}
@article{LI2023104935,
title = {Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network},
journal = {Tunnelling and Underground Space Technology},
volume = {133},
pages = {104935},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104935},
url = {https://www.sciencedirect.com/science/article/pii/S0886779822005764},
author = {Wei Li and Zhoujing Ye and Yajian Wang and Hailu Yang and Songli Yang and Zhenlong Gong and Linbing Wang},
keywords = {Underground Pipeline Network, Mixed Reality, IoT Cloud Platform, Data Communication, Operation and Maintenance},
abstract = {The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN.}
}
@article{GARBETT2021103487,
title = {A multi-user collaborative BIM-AR system to support design and construction},
journal = {Automation in Construction},
volume = {122},
pages = {103487},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103487},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520310670},
author = {James Garbett and Thomas Hartley and David Heesom},
keywords = {BIM, Augmented reality, Collaboration, Cloud, Database},
abstract = {Augmented Reality (AR) is fast becoming an established tool for the construction industry. Previous research reports on the conversion of BIM geometric models and the implementation of these with marker-based AR, or the use of more wide area AR taking positional input from GPS. Much of this focused on the use of AR in an individual context, so there is need to align AR with the more collaborative nature of BIM. By implementing marker-based AR, and connecting to a cloud-based database, the presented BIM-AR system provides the ability to view, interact and collaborate with 3D and 2D BIM data via AR with geographically dispersed teams. An Agile Scrum Method was used to develop the prototype system including a mobile AR application and a Large Touch Screen application based on and a Model, View, Controller (MVC) approach. Finally, the system was tested and verified using a focus group of construction practitioners.}
}
@article{CHEN2018135,
title = {SLAM-based dense surface reconstruction in monocular Minimally Invasive Surgery and its application to Augmented Reality},
journal = {Computer Methods and Programs in Biomedicine},
volume = {158},
pages = {135-146},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717301694},
author = {Long Chen and Wen Tang and Nigel W. John and Tao Ruan Wan and Jian Jun Zhang},
keywords = {SLAM, Surface reconstruction, Augmented Reality, Minimally Invasive Surgery},
abstract = {Background and objective: While Minimally Invasive Surgery (MIS) offers considerable benefits to patients, it also imposes big challenges on a surgeon’s performance due to well-known issues and restrictions associated with the field of view (FOV), hand-eye misalignment and disorientation, as well as the lack of stereoscopic depth perception in monocular endoscopy. Augmented Reality (AR) technology can help to overcome these limitations by augmenting the real scene with annotations, labels, tumour measurements or even a 3D reconstruction of anatomy structures at the target surgical locations. However, previous research attempts of using AR technology in monocular MIS surgical scenes have been mainly focused on the information overlay without addressing correct spatial calibrations, which could lead to incorrect localization of annotations and labels, and inaccurate depth cues and tumour measurements. In this paper, we present a novel intra-operative dense surface reconstruction framework that is capable of providing geometry information from only monocular MIS videos for geometry-aware AR applications such as site measurements and depth cues. We address a number of compelling issues in augmenting a scene for a monocular MIS environment, such as drifting and inaccurate planar mapping. Methods: A state-of-the-art Simultaneous Localization And Mapping (SLAM) algorithm used in robotics has been extended to deal with monocular MIS surgical scenes for reliable endoscopic camera tracking and salient point mapping. A robust global 3D surface reconstruction framework has been developed for building a dense surface using only unorganized sparse point clouds extracted from the SLAM. The 3D surface reconstruction framework employs the Moving Least Squares (MLS) smoothing algorithm and the Poisson surface reconstruction framework for real time processing of the point clouds data set. Finally, the 3D geometric information of the surgical scene allows better understanding and accurate placement AR augmentations based on a robust 3D calibration. Results: We demonstrate the clinical relevance of our proposed system through two examples: (a) measurement of the surface; (b) depth cues in monocular endoscopy. The performance and accuracy evaluations of the proposed framework consist of two steps. First, we have created a computer-generated endoscopy simulation video to quantify the accuracy of the camera tracking by comparing the results of the video camera tracking with the recorded ground-truth camera trajectories. The accuracy of the surface reconstruction is assessed by evaluating the Root Mean Square Distance (RMSD) of surface vertices of the reconstructed mesh with that of the ground truth 3D models. An error of 1.24 mm for the camera trajectories has been obtained and the RMSD for surface reconstruction is 2.54 mm, which compare favourably with previous approaches. Second, in vivo laparoscopic videos are used to examine the quality of accurate AR based annotation and measurement, and the creation of depth cues. These results show the potential promise of our geometry-aware AR technology to be used in MIS surgical scenes. Conclusions: The results show that the new framework is robust and accurate in dealing with challenging situations such as the rapid endoscopy camera movements in monocular MIS scenes. Both camera tracking and surface reconstruction based on a sparse point cloud are effective and operated in real-time. This demonstrates the potential of our algorithm for accurate AR localization and depth augmentation with geometric cues and correct surface measurements in MIS with monocular endoscopes.}
}
@article{SU2018190,
title = {A multi-plane optical see-through holographic three-dimensional display for augmented reality applications},
journal = {Optik},
volume = {157},
pages = {190-196},
year = {2018},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2017.11.081},
url = {https://www.sciencedirect.com/science/article/pii/S0030402617315000},
author = {Yanfeng Su and Zhijian Cai and Lingyan Shi and Feng Zhou and Peiliang Guo and Yifan Lu and Jianhong Wu},
keywords = {Three-dimensional display, Holographic display, Augmented reality, Spatial light modulators, Programmable zoom lenses},
abstract = {In this paper, a multi-plane optical see-through holographic three-dimensional (3D) display system for augmented reality (AR) applications is proposed and implemented. This system is composed of a holographic projection module and an optical see-through display module. The holographic projection module manages to reconstruct the multi-plane 3D scene by using a spatial light modulator (SLM), and the correct depth information can be expressed by programmable zoom lenses encoded on the SLM. The reconstructed 3D scene and the real physical world will fuse together through the optical see-through display module. An experimental verification system for the proposed multi-plane optical see-through holographic 3D display is demonstrated. The experimental results prove that the proposed system can achieve the multi-plane AR holographic 3D display effect without any image bearing structure, and can solve the accommodation-vergence conflict problem effectively. This system has the advantages of large depth range, continuous expression capacity of depth information and no visual fatigue.}
}
@article{BEZBORODOV2017169,
title = {Calculation of Vibration of Pipeline Bundle with Damping Support Made of MR Material},
journal = {Procedia Engineering},
volume = {176},
pages = {169-174},
year = {2017},
note = {Proceedings of the 3rd International Conference on Dynamics and Vibroacoustics of Machines (DVM2016) June 29–July 01, 2016 Samara, Russia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817307932},
author = {S.A. Bezborodov and A.M. Ulanov},
keywords = {Pipeline, bundle, vibration, damping, support, MR material, Finite Element Method},
abstract = {Wire damping materials (such as Metal Rubber, MR) are used widely for protection of pipelines against vibration. A calculation method for bundle of arbitrary shape pipelines with damping supports made of MR material by finite element method ANSYS software is developed. The method takes into account complexity of pipelines shape, interaction of pipelines vibration in the bundle, multi-axial linear and torsion stiffness and damping of nipple supports. A constant damping coefficient is obtained by calculation-experimental method from vibration of one pipeline. The method allows obtaining amplitude of pipelines vibration and stress in them. Calculation results are proved by experiment. Errors of natural frequencies are less than 7%, thus it is possible to use this method for practical application: placement of pipeline supports and design of these supports parameters. It allows reducing a large experimental work of measurement a stress in pipelines during pipeline systems design.}
}
@article{ZOGOPOULOS202284,
title = {Authoring Tool for Automatic Generation of Augmented Reality Instruction Sequence for Manual Operations},
journal = {Procedia CIRP},
volume = {106},
pages = {84-89},
year = {2022},
note = {9th CIRP Conference on Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.159},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001603},
author = {Vasilios Zogopoulos and Eva Geurts and Dorothy Gors and Steven Kauffmann},
keywords = {Assembly, Augmented reality, Computer aided design (CAD)},
abstract = {Despite the large-scale digitalization and automation of production lines, human operators still play a vital role in the shopfloor. Operators provide flexibility and agile responsiveness with their actions, as well as capability to react autonomously based on their technical knowledge and experience. Bridging the gap between modern digital systems of information creation and management and humans in the shopfloor is a current challenge for both the academia and the technology integrators. One emerging way for delivering instructions to the operators is Augmented Reality (AR). AR allows the visualization of the instructions in the operator’s field of view, using digital designs, animations and text instructions. As this technology gains increasingly more ground in the shopfloor, there is a need for agile generation of content, automatizing most of the instructions’ generation process. Towards that end, this paper presents an authoring tool for generating digital instructions for manual operations. The authoring tool allows the identification of the (dis-)assembly sequence of a product and also to include intermediate manual operations, such as surface treatment, and visualizations. In order to determine which assembly tasks and instructions are the most commonly needed, contextual inquiries were conducted in collaboration with the industry. These findings, together with the existing literature on manual processes on the shopfloor, served as a starting point for a taxonomy of assembly task types that are also presented in this paper. The taxonomy serves as a guide to define the processes on which digital instructions and visualizations can be provided. The result can be delivered as digital on-screen instructions or as an augmented reality application that may target mobile devices and headsets. The proposed approach is validated in an industrial use case of a compressor assembly.}
}
@article{WU2020102802,
title = {Hand pose estimation in object-interaction based on deep learning for virtual reality applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102802},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102802},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300523},
author = {Min-Yu Wu and Pai-Wen Ting and Ya-Hui Tang and En-Te Chou and Li-Chen Fu},
keywords = {Hand pose estimation, Deep learning, Convolutional neural network, Spherical part model},
abstract = {Hand Pose Estimation aims to predict the position of joints on a hand from an image, and it has become popular because of the emergence of VR/AR/MR technology. Nevertheless, an issue surfaces when trying to achieve this goal, since a hand tends to cause self-occlusion or external occlusion easily as it interacts with external objects. As a result, there have been many projects dedicated to this field for a better solution of this problem. This paper develops a system that accurately estimates a hand pose in 3D space using depth images for VR applications. We propose a data-driven approach of training a deep learning model for hand pose estimation with object interaction. In the convolutional neural network (CNN) training procedure, we design a skeleton-difference loss function, which effectively can learn the physical constraints of a hand. Also, we propose an object-manipulating loss function, which considers knowledge of the hand-object interaction, to enhance performance. In the experiments we have conducted for hand pose estimation under different conditions, the results validate the robustness and the performance of our system and show that our method is able to predict the joints more accurately in challenging environmental settings. Such appealing results may be attributed to the consideration of the physical joint relationship as well as object information, which in turn can be applied to future VR/AR/MR systems for more natural experience.}
}
@article{CHOKWITTHAYA2023101903,
title = {Ontology for experimentation of human-building interactions using virtual reality},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101903},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101903},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000319},
author = {Chanachok Chokwitthaya and Yimin Zhu and Weizhuo Lu},
keywords = {Ontology, Virtual human-building interaction, Competency question, EXPO, DOGMA},
abstract = {Scientific experiments significantly enhance the understanding of human-building interactions in building and engineering research. Recently, conducting virtual reality (VR) experiments has gained acceptance and popularity as an approach to studying human-building interactions. However, little attention has been given to the standardization of the experimentations. Proper standardization can promote the reusability, replicability, and repeatability of VR experiments and accelerate the maturity of this emerging experimentation method. Responding to such needs, the authors proposed a virtual human-building interaction experimentation ontology (VHBIEO). It is an ontology at the domain level, extending the ontology of scientific experiments (EXPO) to standardize virtual human-building interaction experimentation. It was developed based on state-of-the-art ontology development approaches. Competency questions (CQs) were used to derive requirements and regulate the development. Semantic Web technologies were applied to make VHBIEO machine-readable, accessible, and processable. VHBIEO incorporates an application view (APV) to support the inclusion of unique information for particular applications. The authors performed taxonomy evaluations to assess the consistency, completeness, and redundancy, affirming no occurrence of errors in its structure. Application evaluations were applied for investigating its ability to standardize and support generating of machine-readable, accessible, and processable information. Application evaluations also verified the capability of APV to support the inclusion of unique information.}
}
@article{PLEWAN2021103250,
title = {Exploring the benefits and limitations of augmented reality for palletization},
journal = {Applied Ergonomics},
volume = {90},
pages = {103250},
year = {2021},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2020.103250},
url = {https://www.sciencedirect.com/science/article/pii/S0003687018307646},
author = {Thorsten Plewan and Benedikt Mättig and Veronika Kretschmer and Gerhard Rinkenauer},
abstract = {Superimposing virtual visual information within the real world is referred to as augmented reality (AR). This technique becomes increasingly popular and due to technical advances can be applied to various purposes ranging from consumer entertainment to industrial use cases. Workplaces equipped with AR assistance-systems promise fast and accurate performance. Within the field of intralogistics, palletization constitutes a field of application which may benefit from guidance by AR. In the present investigation, participants performed a palletization task while they received instructions via three different devices (paper list, tablet, AR glasses). Performance data and subjective ratings were obtained. Participants subjectively preferred to work with the AR device. Number of errors was reduced while at the same time handling times were prolonged when the AR device was used. In all experimental conditions, performance improved in the course of the experiment. Least improvement was observed in the AR condition, which emphasizes that such applications allow decent performance without prior experience. Overall, the present results indicate that the utility of AR applications is limited when task demands are low. Such applications might be more useful in unclear and complex situations. Furthermore, AR possesses high hedonic qualities which may ease the integration of novel work processes.}
}
@article{MONTEIRO20231,
title = {Exploring the user experience of hands-free VR interaction methods during a Fitts’ task},
journal = {Computers & Graphics},
volume = {117},
pages = {1-12},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932300242X},
author = {Pedro Monteiro and Hugo Coelho and Guilherme Gonçalves and Miguel Melo and Maximino Bessa},
keywords = {Hands-free, HCI, Immersive Virtual Reality, Interaction, Usability},
abstract = {Despite advancements in interaction with immersive Virtual Reality (VR) systems, using hand gestures for all interactions still imposes some challenges, especially in interactions with graphical user interfaces that are usually performed with point-and-click interfaces. Therefore, exploring the use of alternative hands-free methods for selection is essential to overcome usability problems and provide natural interaction for users. The results and insights gained from this exploration can lead to enhanced user experiences in VR applications. This study aims to contribute to the literature with the evaluation of the usability of the most commonly used hands-free methods for selection and system control tasks in immersive VR and their impact on standard and validated experience and usability metrics, namely the sense of presence, cybersickness, system usability, workload, and user satisfaction. A Fitts’ selection task was performed using a within-subjects design by nine participants experienced in VR. The methods evaluated were the handheld controllers, the head gaze, eye gaze, and voice commands for pointing at the targets, and dwell time and voice commands to confirm the selections. Results show that the methods provide similar levels of sense of presence and low cybersickness while showing low workload values and high user satisfaction, matching the experience of traditional handheld controllers for non-multimodal approaches. The assisted eye gaze with dwell was the preferred hands-free method and the one with the highest values of usability. Still, developers should minimize the number of gaze movements to reduce fatigue. The evaluation also showed that using a multimodal approach for selections, especially using the voice, decreases user satisfaction and increases users’ frustration.}
}
@article{ESFAHLANI201942,
title = {Mixed reality and remote sensing application of unmanned aerial vehicle in fire and smoke detection},
journal = {Journal of Industrial Information Integration},
volume = {15},
pages = {42-49},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X18300773},
author = {Shabnam Sadeghi Esfahlani},
keywords = {Fire detection, Autonomous flight, Crazyflie 2.0, Monocular camera, Computer vision},
abstract = {This paper proposes the development of a system incorporating inertial measurement unit (IMU), a consumer-grade digital camera and a fire detection algorithm simultaneously with a nano Unmanned Aerial Vehicle (UAV) for inspection purposes. The video streams are collected through the monocular camera and navigation relied on the state-of-the-art indoor/outdoor Simultaneous Localisation and Mapping (SLAM) system. It implements the robotic operating system (ROS) and computer vision algorithm to provide a robust, accurate and unique inter-frame motion estimation. The collected onboard data are communicated to the ground station and used the SLAM system to generate a map of the environment. A robust and efficient re-localization was performed to recover from tracking failure, motion blur, and frame lost in the data received. The fire detection algorithm was deployed based on the color, movement attributes, temporal variation of fire intensity and its accumulation around a point. The cumulative time derivative matrix was utilized to analyze the frame-by-frame changes and to detect areas with high-frequency luminance flicker (random characteristic). Color, surface coarseness, boundary roughness, and skewness features were perceived as the quadrotor flew autonomously within the clutter and congested area. Mixed Reality system was adopted to visualize and test the proposed system in a physical environment, and the virtual simulation was conducted through the Unity game engine. The results showed that the UAV could successfully detect fire and flame, autonomously fly towards and hover around it, communicate with the ground station and simultaneously generate a map of the environment. There was a slight error between the real and virtual UAV calibration due to the ground truth data and the correlation complexity of tracking real and virtual camera coordinate frames.}
}
@article{BHATTACHARYA201961,
title = {Augmented reality via expert demonstration authoring (AREDA)},
journal = {Computers in Industry},
volume = {105},
pages = {61-79},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517307972},
author = {Bhaskar Bhattacharya and Eliot H. Winer},
keywords = {Digital signal processing, Authoring systems, Industry applications, Augmented reality, Expert demonstration},
abstract = {Augmented Reality (AR) has in the past decade shown great potential in manufacturing, specifically AR guided assembly. This is the use of AR to provide product assembly instructions to factory workers. AR guided assembly has shown to outperform conventional (paper based) and virtual based methods of instruction delivery in terms of accuracy and time. However, authoring the AR content displayed to the worker is difficult and time consuming. The main challenge for an AR authoring tool is to allow the author the ability to register the content intuitively without having to understand complicated AR concepts. Many authoring tools have tried to overcome this challenge but have been unable to sufficiently create a good working abstraction for the novice author. The work presented in this paper focused on creating AR work instructions for bench top assemblies using a novel form of authoring termed expert demonstration. The working concept is that an expert is recorded performing the assembly steps by a computer vision system. The recording is then automatically processed to generate AR work instructions. Augmented Reality via Expert Demonstration Authoring (AREDA) is the name of the representative software developed in this research. AREDA is divided into two phases: the demonstration phase and refinement phase. Using computer vision algorithms along with a novel skin detection algorithm known as Particle Swarm Optimized Gaussian Mixture Model, the demonstration phase was created. This phase takes 3D point clouds of assembly sequences and converts them into ordered 3D parts along with their transformations. The refinement phase then allows a user to modify the automatically generated AR work instructions to fix errors and add useful virtual content (textual instructions). Finally, three use cases are presented using AREDA to generate the work instructions. Through these use cases, this paper shows the significance of the expert demonstration authoring approach.}
}
@article{SVEINSSON2021105836,
title = {ARmedViewer, an augmented-reality-based fast 3D reslicer for medical image data on mobile devices: A feasibility study},
journal = {Computer Methods and Programs in Biomedicine},
volume = {200},
pages = {105836},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720316692},
author = {Bragi Sveinsson and Neha Koonjoo and Matthew S Rosen},
keywords = {Augmented reality, Medical imaging, Mobile device},
abstract = {Background and objective: Medical images obtained by methods such as magnetic resonance imaging (MRI) or computed tomography (CT) are typically displayed as a stack of 2D slices, comprising a 3D volume. Often, the anatomy of interest does not fall neatly into the slice plane but rather extends obliquely through several slices. Reformatting the data to show the anatomy in one slice in conventional medical imaging software can require expertise and time. In this work, we present ARmedViewer, a medical image viewing app designed for mobile devices that uses augmented reality technology to display medical image data. An arbitrary plane for displaying the data can be chosen quickly and intuitively by moving the mobile device. Methods: The app ARmedViewer, compiled for an iOS device, was designed to allow a user to easily select from a list of 3D image datasets consisting of header information and image data. The user decides where to place the data, which can be overlaid on actual human anatomy. After loading the dataset, the user can move and rotate the data as desired. 15 users compared the user experience of the app to a common image viewer by answering two user surveys each, one custom and one standardized. The utility of the app was also tested by having two users find a plane through a 3D dataset that displayed 3 randomly placed lesions. This operation was timed and compared between the app and a standard medical image viewer. Results: ARmedViewer was successfully developed and run on an iPhone XS. User interfaces for selecting, placing, moving, reslicing, and displaying the data were operated with ease, even by naïve users. The custom user survey indicated that freely selecting a slice through the data was significantly more intuitive and easier using the app than using a conventional image viewer on a computer workstation, and changing the viewing angle was also significantly more intuitive. The standardized survey indicated a significantly better user experience for the app in several categories, and never significantly worse. The timed reslicing experiments demonstrated the app being faster than the standard image viewer by an average factor of 9. Conclusions: The newly developed ARmedViewer is a portable software tool for easily displaying 3D medical image data overlaid on human anatomy, allowing for easy choice of the viewing plane by intuitively moving the mobile device.}
}
@article{ALHEJRI20221,
title = {Reconstructing real object appearance with virtual materials using mobile augmented reality},
journal = {Computers & Graphics},
volume = {108},
pages = {1-10},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322001480},
author = {Aisha Alhejri and Naizheng Bian and Entesar Alyafeai and Mousa Alsharabi},
keywords = {Augmented reality, Deep learning, Depth map, Light estimation, RGB-D images, Materials design},
abstract = {In augmented reality (AR) applications, detecting and tracking real-world objects remains a challenge. Another challenge is making the loaded virtual objects truly reflect the lighting and shadow conditions of the actual environment in which they are located. In this paper, we proposed an improved method for real-time light estimation of environmental lighting in AR applications, tracking the movement of physical objects, and making the corresponding virtual objects reflect the lighting and shadows of the actual environment. First, we proposed an effective method for estimating light in an AR scene by training a deep neural network once on different scenes from the input of RGB-D images to determine the direction of the brightest light. Second, we experimented with advanced methods of detecting and tracking different types of real objects (flat surfaces, simple geometries, and complex geometries) for visualizing virtual materials on the top of these surfaces, where virtual materials will acquire the conditions of the real place in terms of lighting, reflection, and shadows depending on our proposed method for light estimation. Our method of light estimation was tested via experiments on the three types of real objects mentioned above. Results indicate that our method achieves higher resolution and lower error rate compared with other work. Our methods have achieved very good results in terms of detection accuracy, tracking accuracy, realistic visualization of materials, matching real shade, and lighting conditions.}
}
@article{ULANOV2016101,
title = {Calculation Method of Pipeline Vibration with Damping Supports Made of the MR Material},
journal = {Procedia Engineering},
volume = {150},
pages = {101-106},
year = {2016},
note = {2nd International Conference on Industrial Engineering (ICIE-2016)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.06.725},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816312747},
author = {A.M. Ulanov and S.A. Bezborodov},
keywords = {Pipeline, vibration, damping, MR material, ANSYS.},
abstract = {Wire damping materials (such as Metal Rubber, MR, or “spring cushion”, etc.) are widely used for protection of a pipeline against vibration. A calculation method for an arbitrary shaped pipeline with damping supports made of the MR material was developed by means of the finite element ANSYS software package. Dependencies of MR support stiffness and energy dissipation coefficient on various parameters (material density, wire diameter, thickness, preliminary static deformation) are obtained as these two characteristics influence the quality of MP supports. An equivalence viscous damping coefficient was obtained on the basis of the equality of the hysteretic loop area for the viscous damping system and dry friction in the wire material. The method takes into account a non-linearity of the MR material by the iteration method. The method allows obtaining a pipeline vibration amplitude and stress in the pipeline for its 3D vibration. Calculation results are proved by experiment with the ARAMIS non-contact measurement system. The error is less than 9%, thus, it is possible to use this method for practical application: placement of pipeline supports and design of these supports parameters. It allows reducing a large experimental work of stress measurement in pipelines during pipeline system design. Any engineer who is able to work with the ANSYS software can use it. It is possible to develop this method for calculation of a pipeline system with many pipelines connected by damping supports.}
}
@article{CHU2024463,
title = {Experimental analysis of augmented reality interfaces for robot programming by demonstration in manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {463-476},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000657},
author = {Chih-Hsing Chu and Chen-Yu Weng},
keywords = {Augmented reality, Robot, Programming by demonstration, Eye gaze, Head gaze, Hand ray. user interface},
abstract = {Augmented Reality (AR) technology has been effectively utilized to support various manual operations in the manufacturing industry. An important application is serving as a user interface for human-robot collaboration. This paper presents an experimental study on the feasibility of robot programming by demonstration (PbD) through AR interfaces in the context of manufacturing. Our focus is on comparing the pointing and line tracing processes using three input methods provided by an AR headset: hand ray, head gaze, and eye gaze, based on both objective and subjective measures obtained from the experiment. The hand ray method performs the best in terms of accuracy, precision, and completion time in most experimental conditions. The SUS and NASA-TLX scores indicate acceptable usability for the hand ray method but low usability for the others. A prototyping AR tool using the hand ray as non-contact input in the real world is developed for motion planning of an industrial robotic arm. A test case of tire mold welding verifies the feasibility of the AR tool while also showing its limited capability in precision manufacturing. This work demonstrates a new approach for robot PbD on tangible objects enabled by AR.}
}
@article{JULIANTINO2023310,
title = {The development of virtual healing environment in VR platform},
journal = {Procedia Computer Science},
volume = {216},
pages = {310-318},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022190},
author = {Chris Juliantino and Mega Putri Nathania and Religiana Hendarti and Herru Darmadi and Bonny A Suryawinata},
keywords = {Android, Environment, Smartphone, Virtual Reality, Virtual space},
abstract = {The purpose of this research is to build a healing environment that can be experienced through virtual reality (VR) to reduce stress levels of the user because of the lengthy period of COVID-19 pandemic that forced people to stay at home. The healing environment is created based on several theories on the principles of Healing Environments related to Architectural design. And as for the development of the simulation software, it is using the method of Extreme Programming that is based on Agile principle that combined with Architectural design flow. The VR is targeted to run optimally in low-end devices with cardboard and common Bluetooth controller so that it can be accessed inclusively. The evaluation is conducted using a user acceptance test with expert judgment and survey to 32 respondents. The findings are that they enjoyed the simulation because of the guidance is clear and the virtual environment looks more real compared to others that looks cartoonish.}
}
@article{BAL2016267,
title = {Computer Hardware Course Application through Augmented Reality and QR Code Integration: Achievement Levels and Views of Students},
journal = {Procedia Computer Science},
volume = {102},
pages = {267-272},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.400},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325807},
author = {Erkan Bal and Hüseyin Bicen},
keywords = {Augmented Reality, QR Code, Computer Hardware, Achievement, View},
abstract = {This study examined the effect of Augmented Reality and QR Code Integration on achievements and views of undergraduate students taking computer course. Study group of the research included 50 voluntary students taking compulsory computer course studying in the department of Guidance and Psychological Counseling at Near East University. Experimental research design was used in the study. In the beginning of the term, students were seperated into two groups including experimental and control group consisting of 25 students in each group. Pre-test was administered to the two groups in the first lecture and lecture was given with traditional methods, computer features were defined and functions of computer features were explained to the 1st group. Three-dimensional images of hardware features in the hardware chapter of computer course were explained through augmented reality and QR technology in laboratory environment with computer, projection and voice systems to the 2nd group. Students examined computer hardware with QR code cards and augmented reality technology during the lecture from their own mobile devices and all lectures were given by this way until the end of the semester. At the end of the term, post-test was administered to the experimental and control group and a questionnaire related with augmented reality and computer hardware course application with QR code integration was also applied to the experimental group. Results obtained from the study revealed that achievement level of experimental group is higher. In this context, results showed that computer hardware course application with augmented reality and QR code integration has a positive effect on academic achievements of students and students have positive views towards this course. Based on the results, it is expected that method used in this study might contribute to the integration of technology into education and therefore technology and education could support each other.}
}
@article{XIN2024100440,
title = {Enhancing healthcare environment design evaluations through an interactive virtual reality-based approach: A design science research},
journal = {Developments in the Built Environment},
volume = {18},
pages = {100440},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924001212},
author = {Yangpeng Xin and Ying Zhou and Le Yang and Yuanyuan Liu and Tan Tan},
keywords = {Virtual reality, Healthcare environment, Design evaluation, Human-environment interaction, Dynamic virtual crowds, Design science research},
abstract = {Ignoring a large number of occupants and their behavior in the Healthcare Environment Design Evaluations (HEDE) may lead to biased evaluation results and inefficiencies in the collaborative design process. This study aims to propose a VR-based approach integrating dynamic virtual crowds and human-environment interactions to improve participants’ spatial perception thereby increasing the effectiveness and efficiency of HEDE. This study followed Design Science Research (DSR) to develop an interactive VR-based workflow which was tested in a controlled experiment on the renovation of a real trauma intensive care unit and evaluated by 14 stakeholders including staff and architects. Results of the evaluation showed that dynamic virtual crowds and human-environment interactions helped participants improve spatial perception and were considered crucial for HEDE. The interactive VR-based approach met the criteria of “satisfaction” in terms of accuracy and efficiency. This study underscores the synergy between advanced technology and human-centered design, contributing to social sustainability.}
}
@article{WANG2022100190,
title = {Augmented reality (AR) and fracture mapping model on middle-aged femoral neck fracture: A proof-of-concept towards interactive visualization},
journal = {Medicine in Novel Technology and Devices},
volume = {16},
pages = {100190},
year = {2022},
issn = {2590-0935},
doi = {https://doi.org/10.1016/j.medntd.2022.100190},
url = {https://www.sciencedirect.com/science/article/pii/S2590093522000777},
author = {Yong-Qin Wang and Peng-Fei Li and Zi-Huan Xu and Ying-Qi Zhang and Qua-Na Lee and James {Chung-Wai Cheung} and Ming Ni and Duo {Wai-Chi Wong}},
keywords = {Biomedical visualization, Fracture pattern, Fracture characteristics, Virtual reality, X-reality, Surgical navigation, Digital medicine},
abstract = {A thorough understanding of the fracture characteristics can assist the decision-making process for surgery. This study aimed to characterize the femoral neck fractures among middle-aged patients and illustrated a biomedical visualization method using a fracture mapping model and augmented reality. We collected plain radiography and computed tomography (CT) data from 156 adult patients with a femoral neck fracture. The descriptive study showed that Type I and Type II fractures accounted for 8 (5%) and 64 (41%) cases. In comparison, Type IV fractures accounted for 44 (28%) and 40 (25%) cases according to the Garden classification. Comminuted fractures and cortical defects were identified in 14.74% and 29.49% of the cases. A fracture mapping model was reconstructed based on the CT data and demonstrated the location and distribution of the major fracture lines surrounding the head-neck junction. We also illustrated the application of augmented reality technology to visualize and interact with the patient-specific fracture model and the fracture mapping model that facilitated education, training, and surgical planning. Future studies may consider mapping other biomechanical data, such as joint loading and stress distribution, and exploring artificial intelligence via deep reinforcement learning for computer-aided fracture reduction and procedure planning.}
}
@article{HAN20224027,
title = {Virtual reality assisted techniques in field tests and engineering application of the mechanical parameters of a horizontally layered rock mass},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {5},
pages = {4027-4039},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821006116},
author = {Xianmin Han and Wenjiang Li and Xinzhi Li and Zhengguo Zhu},
keywords = {Layered rock mass, Transverse isotropy, Mechanical parameters, Field test, Numerical simulation},
abstract = {The mechanical indices used to describe the anisotropy of layered rock mass are numerous and difficult to be measured. Take the medium - thick horizontal layered limestone rock mass as an example, rock mechanical tests, engineering geological survey, field tests, numerical back-analysis were done to obtain transverse isotropic deformation parameters and strength indices. Normal and transverse moduli of elasticity of rock mass could be preliminarily confirmed by rock uniaxial compression tests, RQD, and empirical formula. Then vertical compression testing of layered rock mass, vertical and horizontal plate bearing tests, and numerical back-analysis were conducted to determine the other deformation parameters. These parameters include normal and transverse moduli of elasticity, normal and transverse Poisson’s ratio, and shear modulus of the rock mass. The cohesion and internal friction angle of the rock mass were then confirmed by direct shear and shear testing. Test results of bidirectional elasticity modulus showed the obvious anisotropy of the layered rock mass. Engineering application indicated that vault settlement was the main deformation mode of the horizontally layered surrounding rock. This differed from the homogeneous rock. Rock falling and delamination readily occurred in the arch of the tunnel.}
}
@article{GAZCON201830,
title = {Fieldwork in Geosciences assisted by ARGeo: A mobile Augmented Reality system},
journal = {Computers & Geosciences},
volume = {121},
pages = {30-38},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417313237},
author = {Nicolás F. Gazcón and Juan M. {Trippel Nagel} and Ernesto A. Bjerg and Silvia M. Castro},
keywords = {Fieldwork, Augmented reality, GIS, Mobile visualization, Geo-data},
abstract = {Gathering data on the field is a crucial task in many disciplines. Moreover, in the Geosciences the GPS positioning is relevant to the data acquisition as well as to the planning of the fieldwork. Nowadays, mobile devices are widespread, constituting an accessible platform consisting of single devices coupled with different sensors for data acquisition, such as GPS, compass, magnetometer, etc. Emerging technologies such as Augmented Reality (AR) enable the use of these devices as a software platform to complement the perception of our surrounding environment. Despite the fact that there are several AR applications focused on different aspects of Geosciences, a comprehensive fieldwork oriented system, which can be easily adapted to the community needs, is still missing. Based on a multidisciplinary work between professionals from Geosciences and Computer Sciences, we designed and developed a framework based AR system named ARGeo. The proposed system constitutes a complementing tool for the geologist's fieldwork carefully designed to be used in remote sites, using only the mobile device hardware and without the need of an internet connection for data acquisition. ARGeo complements the fieldwork observation and the precise recording of geological data needed, e.g., for the interpretation of folded structures. It allows the user to register geo-tagged information such as points of interests (i.e. references on the field or the fold itself) using natural interactions. In addition, it provides two novel features: an interactive dipping plane measurement from distant points of view and a back-in-time visualization of previous recorded positions, in order to help understanding the spatial relations among the recorded samples. Pilot studies were conducted with domain experts of geology and they agree on the potential of the system. Results point out that ARGeo naturally integrates mobile devices and AR to the geologists' fieldwork. Its possible extension with new features to the Geosciences as well as to other application fields is detailed.}
}
@article{BELLAZZI2022108674,
title = {Virtual reality for assessing visual quality and lighting perception: A systematic review},
journal = {Building and Environment},
volume = {209},
pages = {108674},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108674},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321010647},
author = {Alice Bellazzi and Laura Bellia and Giorgia Chinazzo and Federica Corbisiero and Pierpaolo D'Agostino and Anna Devitofrancesco and Francesca Fragliasso and Matteo Ghellere and Valentino Megale and Francesco Salamone},
keywords = {Virtual reality, Light, Daylight, Perception, Preference, Human behavior},
abstract = {The achievement of a good visual environment is key to guaranteeing human satisfaction indoors. In this context, it is crucial to assess the visual environment through the measurement of human perception. However, the assessment of the visual environment through human perception is often complicated. Using real spaces or mock-ups is time consuming, costly, and does not allow the control of all possible variables (e.g., daylight). Photorealistic rendered images present several limitations, starting from the veracity of the visual stimulus presented to participants. Virtual Reality (VR) is emerging as a valid alternative for evaluating the perception of the indoor visual environment due to the ability to control selected variables, analyse cause-effect relationships, and save time and cost, especially for the evaluation of daylit spaces. The high level of immersion and the possibility of interaction provide an opportunity to study users' perceptions and behaviors. However, some aspects of light assessment in VR need further investigations, such as the comparability of the perception of light in real and virtual environments. This paper reviews the available literature on the topic, highlighting the advantages and disadvantages related to the use of VR for lighting research and design. Previous research is classified into 1) studies focused on the comparability between lighting conditions in VR and real environments; 2) studies about users’ perception and behavior with respect to lighting scenarios in VR; and 3) studies exploiting VR for lighting design. Hardware and software used in existing literature are further analyzed. This paper highlights that more studies are needed to define a common investigation protocol to make VR a valid investigation tool for lighting research studies aimed at evaluating visual quality and lighting perception.}
}
@article{BAO2022104565,
title = {Cross-platform virtual reality for real-time construction safety training using immersive web and industry foundation classes},
journal = {Automation in Construction},
volume = {143},
pages = {104565},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104565},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004356},
author = {Lan Bao and Si Van-Tien Tran and Truong Linh Nguyen and Hai Chien Pham and Dongmin Lee and Chansik Park},
keywords = {Safety training, Virtual reality, Cross-platform, WebXR, IFC, Real-time, User interaction and communication},
abstract = {Virtual reality (VR) has been studied extensively in the construction industry, particularly for safety training, as it is capable of simulating the hazardous areas of a construction site in a virtual environment that enables workers to visualize the real scenario before being introduced to job sites. However, at present, VR safety training requires specialized hardware and software, thus limiting worker access as only a few workers can participate in each training session. Therefore, this paper proposes a cross-platform framework based on Industry Foundation Classes (IFC) and WebXR (Immersive Web) for conveniently accessible VR safety training (CPVR) in the construction industry. Workers can access VR safety training sessions using their mobile device or desktop with the aid of the recommended framework. Furthermore, they can interact and communicate with other participants in real-time. A prototype based on the CPVR framework was developed, and its practicality and effectiveness for safety training were validated by senior students, managers, site engineers, and workers in the construction field. The proposed framework would help improve employees, particularly the worker, accessibility to VR safety training, which enables them to receive training at any time and from any location to enhance their awareness and knowledge. Furthermore, it could generate collaboration among employees and help assist managers in assessing employees' safety knowledge.}
}
@article{XU20201,
title = {Co-axial depth sensor with an extended depth range for AR/VR applications},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {1},
pages = {1-11},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300012},
author = {Mohan Xu and Hong Hua},
keywords = {Depth map sensor, 3D camera, Controlled aberration},
abstract = {Background
Depth sensor is an essential element in virtual and augmented reality devices to digitalize users' environment in real time. The current popular technologies include the stereo, structured light, and Time-of-Flight (ToF). The stereo and structured light method require a baseline separation between multiple sensors for depth sensing, and both suffer from a limited measurement range. The ToF depth sensors have the largest depth range but the lowest depth map resolution. To overcome these problems, we propose a co-axial depth map sensor which is potentially more compact and cost-effective than conventional structured light depth cameras. Meanwhile, it can extend the depth range while maintaining a high depth map resolution. Also, it provides a high-resolution 2D image along with the 3D depth map.
Methods
This depth sensor is constructed with a projection path and an imaging path. Those two paths are combined by a beamsplitter for a co-axial design. In the projection path, a cylindrical lens is inserted to add extra power in one direction which creates an astigmatic pattern. For depth measurement, the astigmatic pattern is projected onto the test scene, and then the depth information can be calculated from the contrast change of the reflected pattern image in two orthogonal directions. To extend the depth measurement range, we use an electronically focus tunable lens at the system stop and tune the power to implement an extended depth range without compromising depth resolution.
Results
In the depth measurement simulation, we project a resolution target onto a white screen which is moving along the optical axis and then tune the focus tunable lens power for three depth measurement subranges, namely, near, middle and far. In each sub-range, as the test screen moves away from the depth sensor, the horizontal contrast keeps increasing while the vertical contrast keeps decreasing in the reflected image. Therefore, the depth information can be obtained by computing the contrast ratio between features in orthogonal directions.
Conclusions
The proposed depth map sensor could implement depth measurement for an extended depth range with a co-axial design.}
}