@article{SHARMA2020723,
title = {Osmotic computing-based service migration and resource scheduling in Mobile Augmented Reality Networks (MARN)},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {723-737},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19310738},
author = {Vishal Sharma and Dushantha Nalin K. Jayakody and Marwa Qaraqe},
keywords = {Osmotic computing, Edge-computing, Augmented reality, Resource scheduling, Service migrations},
abstract = {Resources and services between the servers in Mobile Augmented Reality Networks (MARN) are tedious to manage. These networks comprise users possessing Augmented Reality (AR)-Virtual Reality (VR) applications. Low latency, robustness, and tolerance are the key requirements of these networks, which can be attained by using near-user solutions such as edge computing. However, management of services and scheduling them to near-user servers in an integrated environment of edge and public/private infrastructure are complex tasks. These require an optimal solution, which can be obtained by using “Osmotic Computing”, that has been recently proposed as a paradigm for the integration of edge and public/private cloud. This paper uses osmotic computing for effectively migrating and scheduling the services between the servers of the different layers. The paper also presents the details on various components that are used for applying osmotic computing to a network followed by core applications, types, service classification, migration, and scheduling through the rules of osmotic game formulated for its operations. The evaluations are conducted on 100,000 requests and the proposed approach shows significant performance with the probability of the error being 0.1 at 55.72% conservation of the energy and memory resources for the entire network despite the increasing number of users. The proposed approach also satisfies the conditions of the joint optimization functions presented in the system model and demonstrates that the system holds true even with varying users, thus, proving its robustness and tolerance against the number of users.}
}
@article{WINDHAUSEN2024108153,
title = {Exploring the impact of augmented reality smart glasses on worker well-being in warehouse order picking},
journal = {Computers in Human Behavior},
volume = {155},
pages = {108153},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108153},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224000207},
author = {Anne Windhausen and Jonas Heller and Tim Hilken and Dominik Mahr and Roberta {Di Palma} and Lieven Quintens},
keywords = {Augmented reality, Augmented reality smart glasses, Logistics, Warehousing, Order-picking, Well-being},
abstract = {This paper examines the use of Augmented Reality Smart Glasses (ARSGs) in order-picking tasks in warehouses and explores their impact on workers' well-being. While previous research has primarily focused on the performance advantages of ARSGs, this study provides a new perspective by investigating the human factor and perceptions of worker well-being in two empirical studies. The results corroborate previous findings that ARSGs enhance order-picking efficiency by improving workers' speed and reducing error rates. Additionally, the results reveal that using ARSGs directly increases perceptions of well-being compared to traditional support tools. However, contrary to expectations, ARSG use does not generally lead to higher job satisfaction or perceived productivity enhancements for all workers. Technology savviness is identified as a crucial individual factor that shapes these effects, with tech-savvy workers benefiting more from ARSG use. Overall, this research underscores the importance of tailoring ARSG implementation to the specific needs and characteristics of the workforce and highlights the need for further investigation into the beneficiaries of ARSG technology.}
}
@article{CHEN2020518,
title = {A multichannel human-swarm robot interaction system in augmented reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {6},
pages = {518-533},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300905},
author = {Mingxuan Chen and Ping Zhang and Zebo Wu and Xiaodan Chen},
keywords = {Human-swarm interaction, Augmented reality, Multichannel integration},
abstract = {Background
A large number of robots have put forward the new requirements for humanrobot interaction. One of the problems in human-swarm robot interaction is how to naturally achieve an efficient and accurate interaction between humans and swarm robot systems. To address this, this paper proposes a new type of human-swarm natural interaction system.
Methods
Through the cooperation between three-dimensional (3D) gesture interaction channel and natural language instruction channel, a natural and efficient interaction between a human and swarm robots is achieved.
Results
First, A 3D lasso technology realizes a batch-picking interaction of swarm robots through oriented bounding boxes. Second, control instruction labels for swarm-oriented robots are defined. The instruction label is integrated with the 3D gesture and natural language through instruction label filling. Finally, the understanding of natural language instructions is realized through a text classifier based on the maximum entropy model. A head-mounted augmented reality display device is used as a visual feedback channel.
Conclusions
The experiments on selecting robots verify the feasibility and availability of the system.}
}
@article{CHEN2020101948,
title = {A virtual-physical collision detection interface for AR-based interactive teaching of robot},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {64},
pages = {101948},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101948},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300481},
author = {Chengjun Chen and Yong Pan and Dongnian Li and Shilei Zhang and Zhengxu Zhao and Jun Hong},
keywords = {Robot path planning, Teaching programming, Augmented reality, Virtual-physical collision detection, Depth image},
abstract = {At present, online lead-through and offline programming methods are widely used in programming of industrial robots. However, both methods have some drawbacks for unskilled shopworkers. This paper presents an Augmented Reality (AR)-based interactive robot teaching programming system, which virtually projected the robot onto the physical industrial environment. The unskilled shopworkers can use Handheld Teaching Device (HTD) to move end-effector of virtual robot to follow endpoint of the HTD. In this way, the path of the virtual robot can be planned or tested interactively. In addition, collisions detection between virtual robot and physical environment is key to test the feasibility of robot path. So, a method for detecting virtual-physical collisions is presented in this paper by comparing the depth values of corresponding pixels in depth image acquired by Kinect and computer-generated image in order to get collision-free paths of the virtual robot. The Quadtree model is used to accelerate the collision detection process and get distance between virtual model and physical environment. Using the AR-based interactive robot teaching programming system presented in this paper, all workers even unskilled ones in robot programming, can quickly and effectively get the collision-free robot path.}
}
@article{VENKATA2019253,
title = {A novel mixed reality in breast and constructive jaw surgical tele-presence},
journal = {Computer Methods and Programs in Biomedicine},
volume = {177},
pages = {253-268},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719300744},
author = {Haritha Sallepalli Venkata and Abeer Alsadoon and P.W.C. Prasad and Omar Hisham Alsadoon and Sami Haddad and Anand Deva and Jeremy Hsu},
keywords = {Augmented reality, Virtual reality, Mixed reality, Surgical telepresence, Visualization},
abstract = {Background and aim
Surgical telepresence has been implemented using Mixed reality (MR) but, MR is theory based and only used for investigating research. The Aim of this paper is to propose and implement a new solution by merging augmented video (generating in local site) and virtual expertise surgeon hand (remote site). This system is to improve the visualization of surgical area, overlay accuracy in the merged video without having any discoloured patterns on hand, smudging artefacts on surgeon hand boundary and occluded areas of surgical area.
Methodology
The Proposed system consists of an Enhanced Multi-Layer Mean Value Cloning (EMLMV) algorithm that improves the overlay accuracy, visualization accuracy and the processing time. This proposed algorithm includes trimap and alpha matting as a pre-processing stage of merging process, which helps to remove the smudging and discoloured artefacts surrounded by remote surgeon hand.
Results
Results showing that the proposed system improved the accuracy by reducing the overlay error of merging image from 1.3 mm (Millimeter) to 0.9 mm. Furthermore, it improves the visibility of surgeon hand in the final merged image from 98.4% (visibility of pixels) to 99.1% (visibility of pixels). Similarly, the processing time in our proposed solution is reduced, which is computed as 10 s to produce 50 frames, whilst, the state of art solution computes 11 s for the same number of frames.
Conclusion
The proposed system focuses on the merging of augmented reality video (local site), and the virtual reality video (remote site) with the accurate visualization. we consider discoloured areas, smudging artefacts and occlusion as the main aspects to improve the accuracy of merged video in terms of overlay error and visualization error. So, the proposed system would produce the merged video with the removal of artefacts around the expert surgeon hand.}
}
@article{ZAOUI2001451,
title = {A 6 D.O.F. opto-inertial tracker for virtual reality experiments in microgravity},
journal = {Acta Astronautica},
volume = {49},
number = {3},
pages = {451-462},
year = {2001},
issn = {0094-5765},
doi = {https://doi.org/10.1016/S0094-5765(01)00121-7},
url = {https://www.sciencedirect.com/science/article/pii/S0094576501001217},
author = {Mohamed Zaoui and Dean Wormell and Yury Altshuler and Eric Foxlin and Joseph McIntyre},
abstract = {Gravity plays a role in many different levels of human motor behavior. It dictates the laws of motion of our body and limbs, as well as of the objects in the external world with which we wish to interact. The dynamic interaction of our body with the world is molded within gravity's constraints. The role played by gravity in the perception of visual stimuli and the elaboration of human movement is an active research theme in the field of Neurophysiology. Conditions of microgravity, coupled with techniques from the world of virtual reality, provide a unique opportunity to address these questions concerning the function of the human sensorimotor system [1]. The ability to measure movements of the head and to update in real time the visual scene presented to the subject based on these measurements is a key element in producing a realistic virtual environment. A variety of head-tracking hardware exists on the market today [2–4], but none seem particularly well suited to the constraints of working with a space station environment. Nor can any of the existing commercial systems meet the more stringent requirements for physiological experimentation (high accuracy, high resolution, low jitter, low lag) in a wireless configuration. To this end, we have developed and tested a hybrid opto-inertial 6 degree-of-freedom tracker based on existing inertial technology [5–8]. To confirm that the inertial components and algorithms will function properly, this system was tested in the microgravity conditions of parabolic flight. Here we present the design goals of this tracker, the system configuration and the results of 0g and 1g testing.}
}
@article{AYAZ2024108115,
title = {Brain MR image simulation for deep learning based medical image analysis networks},
journal = {Computer Methods and Programs in Biomedicine},
volume = {248},
pages = {108115},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108115},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001111},
author = {Aymen Ayaz and Yasmina {Al Khalil} and Sina Amirrajab and Cristian Lorenz and Jürgen Weese and Josien Pluim and Marcel Breeuwer},
keywords = {Brain MRI simulation, Large synthetic population, Brain MRI segmentation, WM/GM/CSF segmentation},
abstract = {Background and Objective
As large sets of annotated MRI data are needed for training and validating deep learning based medical image analysis algorithms, the lack of sufficient annotated data is a critical problem. A possible solution is the generation of artificial data by means of physics-based simulations. Existing brain simulation data is limited in terms of anatomical models, tissue classes, fixed tissue characteristics, MR sequences and overall realism.
Methods
We propose a realistic simulation framework by incorporating patient-specific phantoms and Bloch equations-based analytical solutions for fast and accurate MRI simulations. A large number of labels are derived from open-source high-resolution T1w MRI data using a fully automated brain classification tool. The brain labels are taken as ground truth (GT) on which MR images are simulated using our framework. Moreover, we demonstrate that the T1w MR images generated from our framework along with GT annotations can be utilized directly to train a 3D brain segmentation network. To evaluate our model further on larger set of real multi-source MRI data without GT, we compared our model to existing brain segmentation tools, FSL-FAST and SynthSeg.
Results
Our framework generates 3D brain MRI for variable anatomy, sequence, contrast, SNR and resolution. The brain segmentation network for WM/GM/CSF trained only on T1w simulated data shows promising results on real MRI data from MRBrainS18 challenge dataset with a Dice scores of 0.818/0.832/0.828. On OASIS data, our model exhibits a close performance to FSL, both qualitatively and quantitatively with a Dice scores of 0.901/0.939/0.937.
Conclusions
Our proposed simulation framework is the initial step towards achieving truly physics-based MRI image generation, providing flexibility to generate large sets of variable MRI data for desired anatomy, sequence, contrast, SNR, and resolution. Furthermore, the generated images can effectively train 3D brain segmentation networks, mitigating the reliance on real 3D annotated data.}
}
@article{VOGT201946,
title = {Virtual reality interventions for balance prevention and rehabilitation after musculoskeletal lower limb impairments in young up to middle-aged adults: A comprehensive review on used technology, balance outcome measures and observed effects},
journal = {International Journal of Medical Informatics},
volume = {126},
pages = {46-58},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618303010},
author = {Sarah Vogt and Nina Skjæret-Maroni and Dorothee Neuhaus and Jochen Baumeister},
keywords = {Virtual reality, Balance training, Musculoskeletal disorders, Lower limb, Exergames},
abstract = {Background
Balance training is an important aspect in prevention and rehabilitation of musculoskeletal lower limb injuries. Virtual reality (VR) is a promising addition or alternative to traditional training. This review aims to provide a comprehensive overview of VR technology and games employed for balance prevention and rehabilitation, balance outcome measures, and effects for both balance prevention and balance rehabilitation following musculoskeletal lower limb impairments.
Methods
A systematic literature search was conducted in electronic databases to identify all related articles with a longitudinal study design on VR, balance, and prevention or musculoskeletal rehabilitation of the lower limbs in adult subjects between 19 and 65 years.
Results
Eleven articles concerning balance prevention and five articles regarding balance rehabilitation were included. All studies used screen-based VR and off-the-shelf gaming consoles with accompanying games. The Star Excursion Balance Test (SEBT) was the most frequently used outcome measure. Two studies found positive effects of VR balance training in healthy adults, while none reported negative effects. None of the included studies showed a significant difference in balance performance after a VR balance rehabilitation intervention compared to traditional balance training.
Conclusion
Few studies have been published concerning musculoskeletal balance rehabilitation and balance prevention in healthy adult subjects. However, the studies published have shown that VR exercises are equally effective compared to traditional balance training for both domains of application. As there is large variability between studies, recommendations for future research are given to prospectively investigate the use of VR technology for balance training.}
}
@article{HOU2024111060,
title = {Virtual reality space in architectural design education: Learning effect of scale feeling},
journal = {Building and Environment},
volume = {248},
pages = {111060},
year = {2024},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.111060},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323010879},
author = {Ning Hou and Daisaku Nishina and So Sugita and Rui Jiang and Sayaka Kindaichi and Hiroshi Oishi and Akihiro Shimizu},
keywords = {VR space, Scale feeling, Design education, Spatial recognition, Architectural education, HMD},
abstract = {In teaching architectural design, accurately visualizing a space's realistic size through the dimensional figures on the architectural drawings (scale feeling) is considered a crucial design competency. Typically, students learn this through field surveys and measurements of numerous buildings and rooms. However, this process might be time-consuming. Therefore, to expedite the improvement of students' scale feeling, we used virtual reality (VR) instead of real space and verified whether VR training can also develop students' scale feeling and achieve similar training effects as in real space. In this study, we used undergraduate architectural students as subjects and observed the change in verbal responses to the length of some objects before and after training using VR or real space to confirm the learning effect of scale feeling. Consequently, younger students who lacked architectural knowledge demonstrated a 19.4 % (p < 0.01) decrease in inaccuracy after VR training and a 31.4 % (p < 0.01) decrease in real space, while older students demonstrated a small decrease: 8.2 % (p < 0.1) with VR and 3.1 % (n.s.) with real space. No significant difference occurred between students trained in VR and real space. In summary, we conclude that utilizing VR in architecture education can improve students' scale feelings to a similar extent as in real space. When VR space is used for teaching scale feeling, the focus should be on students without architectural knowledge, which is more meaningful than teaching those with architectural knowledge. This study provides theoretical support for the researcher's further use of VR as a tool in teaching architectural design.}
}
@article{ARSHAD2018440,
title = {Correlation between resilient modulus (MR) and constrained modulus (MC) values of granular materials},
journal = {Construction and Building Materials},
volume = {159},
pages = {440-450},
year = {2018},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2017.10.047},
url = {https://www.sciencedirect.com/science/article/pii/S0950061817320871},
author = {Muhammad Arshad},
keywords = {Pavement structure, Recycled material, Material properties, Empirical correlation, Laboratory experimentation},
abstract = {The resilient modulus (MR) value of unbound aggregates has been widely accepted as the principal mechanical property required in the mechanistic-empirical design/analysis of pavement structures. Resilient modulus measured through repeated triaxial load test is highly desirable to characterise granular materials. However, because of the complexities encountered with this test, other laboratory tests would be desirable if a reliable correlation could be established. Reclaimed asphalt pavement (RAP) and recycled concrete aggregate (RCA) have potential to be used as base and subbase material in substantial percentage to achieve economy and to minimise the undesirable environmental effects linked to the use of virgin aggregates. This paper presents a correlation developed between MR value and constrained modulus (MC) value on the basis of experimental results obtained for 32 reconstituted granular samples containing virgin aggregates, RAP and RCA materials. Proposed model considers the effect of stress state through the experimentally determined MC value while model parameters are correlated to the percentage of RAP materials in the reconstituted blends. Statistical analysis of the suggested correlation by t-test demonstrates encouraging results in terms of its strength and significance.}
}
@article{JEUNG2023107323,
title = {Augmented reality-based surgical guidance for wrist arthroscopy with bone-shift compensation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {230},
pages = {107323},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107323},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722007040},
author = {Deokgi Jeung and Kyunghwa Jung and Hyun-Joo Lee and Jaesung Hong},
keywords = {Wrist arthroscopy, Augmented reality, Bone-shift compensation, In vivo computed tomography},
abstract = {Background and objectives
Intraoperative joint condition is different from preoperative CT/MR due to the motion applied during surgery, inducing an inaccurate approach to surgical targets. This study aims to provide real-time augmented reality (AR)-based surgical guidance for wrist arthroscopy based on a bone-shift model through an in vivo computed tomography (CT) study.
Methods
To accurately visualize concealed wrist bones on the intra-articular arthroscopic image, we propose a surgical guidance system with a novel bone-shift compensation method using noninvasive fiducial markers. First, to measure the effect of traction during surgery, two noninvasive fiducial markers were attached before surgery. In addition, two virtual link models connecting the wrist bones were implemented. When wrist traction occurs during the operation, the movement of the fiducial marker is measured, and bone-shift compensation is applied to move the virtual links in the direction of the traction. The proposed bone-shift compensation method was verified with the in vivo CT data of 10 participants. Finally, to introduce AR, camera calibration for the arthroscope parameters was performed, and a patient-specific template was used for registration between the patient and the wrist bone model. As a result, a virtual bone model with three-dimensional information could be accurately projected on a two-dimensional arthroscopic image plane.
Results
The proposed method was possible to estimate the position of wrist bone in the traction state with an accuracy of 1.4 mm margin. After bone-shift compensation was applied, the target point error was reduced by 33.6% in lunate, 63.3% in capitate, 55.0% in scaphoid, and 74.8% in trapezoid than those in preoperative wrist CT. In addition, a phantom experiment was introduced simulating the real surgical environment. AR display allowed to expand the field of view (FOV) of the arthroscope and helped in visualizing the anatomical structures around the bones.
Conclusions
This study demonstrated the successful handling of AR error caused by wrist traction using the proposed method. In addition, the method allowed accurate AR visualization of the concealed bones and expansion of the limited FOV of the arthroscope. The proposed bone-shift compensation can also be applied to other joints, such as the knees or shoulders, by representing their bone movements using corresponding virtual links. In addition, the movement of the joint skin during surgery can be measured using noninvasive fiducial markers in the same manner as that used for the wrist joint.}
}
@article{AVAN2022390,
title = {Enhancing VR experience with RBF interpolation based dynamic tuning of stereoscopic rendering},
journal = {Computers & Graphics},
volume = {102},
pages = {390-401},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321002107},
author = {Emre Avan and Tolga K. Capin and Hasmet Gurcay and Ufuk Celikcan},
keywords = {Virtual reality, Stereoscopic rendering, Depth perception, RBF interpolation, Depth authoring},
abstract = {Providing a depth-rich Virtual Reality (VR) experience to users without causing discomfort remains to be a challenge with today’s commercially available head-mounted displays (HMDs), which enforce strict measures on stereoscopic rendering parameters for the sake of keeping visual discomfort to a minimum. However, these measures often lead to an unimpressive VR experience with shallow depth feeling. Using radial basis function interpolation and projection matrix manipulations, we propose a novel method for dynamic tuning of stereoscopic rendering parameters toward enhanced VR experience and demonstrate that it is ready to be used with existing consumer HMDs for automated stereoscopic camera control. We present our method with an approach that is primarily intended for guided VR experiences such as VR walkthroughs or VR rides, assuming a depth layout defined by a path of depth-authored key points in the virtual environment. For this purpose, we also introduce a novel immersive interface for authoring unique 3D stereoscopic cinematographies for VR HMD experiences. The results of the user study indicate that our approach, with proper depth authoring, is able to enhance user experience in terms of overall perceived depth and picture quality while maintaining visual comfort on a par with the HMD’s default arrangement. We also investigate the effects of using depth-of-field blurring in combination with our approach and observe that the quality of the outcome varies based on the overall scene depth.}
}
@article{ZHANG2023389,
title = {A novel MR remote collaboration system using 3D spatial area cue and visual notification},
journal = {Journal of Manufacturing Systems},
volume = {67},
pages = {389-409},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000365},
author = {Xiangyu Zhang and Xiaoliang Bai and Shusheng Zhang and Weiping He and Shuxia Wang and Yuxiang Yan and Quan Yu and Liwei Liu},
keywords = {Remote collaboration, Mixed reality, 3D sketch cue, 3D spatial area cue, Visual notification, Hand gesture, MR assembly},
abstract = {In remote collaboration for emergency maintenance or training, since local workers may lack relevant experience and knowledge, it is very important to support remote experts to annotate 3D spatial areas for local workers in their workspace, such as risky spatial areas that need to be avoided for safe operation when assembling replacement parts. Recently, many researchers have studied various visual communication cues for expressing spatial information in Mixed Reality (MR) remote collaboration systems for physical tasks. However, the expression and transmission of 3D spatial area information have not been well explored, especially in tasks that need to annotate specific spatial areas in 3D space (such as risky 3D spatial areas). In the study, we developed a novel MR remote collaboration system that supports remote experts using 3D spatial area cues and visual notification to create Augmented Reality (AR) instructions for local workers on the shared 3D stereoscopic scene of the local workspace in the Virtual Reality (VR) space. We performed a user study to explore the effect of three interfaces on remote collaborative physical assembly tasks in the workspace with risky 3D spatial areas: 3D sketch cue (3DS) which is the baseline solution, 3D sketch cue combines the novel 3D spatial area cue (3DSA), and 3D sketch cue combines 3D spatial area cue and visual notification (3DSAN). We found the 3DSAN can significantly improve collaboration efficiency, reduce operation errors, and improve the usability and collaboration experience (especially the level of awareness), among industrial remote collaborative physical tasks requiring 3D spatial area reference.}
}
@article{SHOSHANI2023107546,
title = {From virtual to prosocial reality: The effects of prosocial virtual reality games on preschool Children's prosocial tendencies in real life environments},
journal = {Computers in Human Behavior},
volume = {139},
pages = {107546},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107546},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222003661},
author = {Anat Shoshani},
keywords = {Virtual reality, Children, Prosocial, Games, Positive emotions, Preschool},
abstract = {Virtual Reality (VR) technology can provide new opportunities to promote prosocial learning in young children. However, little empirical research has examined how this technology can impact preschool children's prosocial behavior. To respond to this need, two experiments investigated how VR prosocial games affect preschool children's prosocial behavior in real-life settings. Positive affect and sense of competence were examined as potential mediators between the VR prosocial play and real-life prosocial behavior. In the first experiment, 4-to 6-year-olds (N = 166) were randomly assigned to play a prosocial, violent, or neutral VR game. After the game, helping behaviors towards the experimenter were tested on a behavioral task. In the second experiment, 4-to 6-year-olds (N = 173) were randomly assigned to a prosocial, positive affect, or neutral VR game condition, and their sharing behavior with peers was examined on a dictator game. Across experiments, children in the pro-social game condition exhibited more helping and sharing behaviors than children in the violent, positive affect, or neutral conditions. Positive affect mediated the effect of VR prosocial play on prosocial behavior; the effect of competence was not significant. The contribution of gamified VR environments to facilitating prosocial development during the preschool age is discussed.}
}
@article{QIAO2022102651,
title = {Numerical simulation methods for motion-induced eddy current testing signals based on Ar formulation and edge finite elements},
journal = {NDT & E International},
volume = {129},
pages = {102651},
year = {2022},
issn = {0963-8695},
doi = {https://doi.org/10.1016/j.ndteint.2022.102651},
url = {https://www.sciencedirect.com/science/article/pii/S0963869522000500},
author = {Liang Qiao and Hong-En Chen and Meng Wang and Yulong Zhu and Xudong Li and Zongfei Tong and Shejuan Xie and Hui Chen and Zhenmao Chen},
keywords = {Numerical simulation method, Motion-induced eddy current, ECT, Reduced vector potential method},
abstract = {In this paper, numerical methods for simulation of the motion-induced eddy current testing (MIECT) signals were developed based on the reduced magnetic vector potential formulation (Ar method) and the edge finite element. According to the principle of the MIECT problem, numerical methods with reference frame referring to the transducer or the inspection object were developed respectively to solve the eddy current and pickup magnetic field signals. In addition, a quasi-static numerical scheme was also adopted to simulate the MIECT problem for inspecting crack along the moving direction. Based on the proposed numerical schemes and the conventional Ar code, numerical programs of the 3 methods were developed and validated for simulating MIECT signals. Through comparing the numerical results and efficiencies of the three simulation schemes each other, the validity of the proposed methods and the corresponding numerical codes were theoretically verified at first. Then, a MIECT experimental system and a prototype probe were developed and the validity of the proposed numerical methods were further proved experimentally. It is found that the moving probe-fixed object method is the most efficient scheme for MIECT signal simulation among the three proposed numerical methods. The numerical methods and codes developed in this paper provide numerical tools for design and optimization of the MIECT probes and testing system used in inspection of rails or wheels system etc, as well as the quantitative defect evaluation based on the MIECT signals through model based inverse analyses.}
}
@article{CHALHOUB20181,
title = {Using Mixed Reality for electrical construction design communication},
journal = {Automation in Construction},
volume = {86},
pages = {1-10},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517304296},
author = {Jad Chalhoub and Steven K. Ayer},
keywords = {Mixed Reality, Prefabrication, Productivity, BIM},
abstract = {Building Information Modeling (BIM) techniques have enabled the construction industry to realize various benefits. However, most projects still rely on 2D drawings to communicate the 3D BIM content to construction personnel. While Mixed Reality (MR) could theoretically be the primary means of communicating BIM content to onsite personnel in 3D, there is not currently a thorough understanding of how this might impact the construction performance of industry practitioners. This paper explores this topic by examining the field of electrical construction. It addresses research questions related to: MR's influence on the productivity and quality of electrical conduit construction; and the effects of an industry practitioner's background on his or her performance using MR. To address these topics, a quasi-experiment was conducted that compares the performance of eighteen electrical construction personnel who were tasked with building similar conduit assemblies using traditional paper and MR. Participants completed pre- and post-activity questionnaires to provide their perceptions of the experience. The results suggest that MR enabled: a significantly higher productivity rate; reduced the time required to understand the design; led to fewer errors during the assembly process; and increased the number of accurately constructed conduits as compared to the conduits constructed using traditional paper. Additionally, nearly all participants agreed that MR is easy to use, but most still felt that they would prefer to use paper plans for design communication. The findings of this work were noteworthy because many of the participants had substantial prior experience constructing conduit using paper plans, yet they still performed the task better and faster using MR. While the small sample size limits the extent to which these findings can be generalized, the contribution of this work is in demonstrating, as a proof-of-concept, that MR can be a viable option for communicating existing BIM content to current industry practitioners and that it can offer advantages that are not currently observed through the use of a paper-based communication methods.}
}
@article{TERNIFI2018219,
title = {Improvements of Liver MR Imaging Clinical Protocols to Simultaneously Quantify Steatosis and Iron Overload},
journal = {IRBM},
volume = {39},
number = {3},
pages = {219-225},
year = {2018},
issn = {1959-0318},
doi = {https://doi.org/10.1016/j.irbm.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1959031818300071},
author = {R. Ternifi and P. Pouletaut and M. Sasso and V. Miette and F. Charleux and S.F. Bensamoun},
keywords = {Liver MRI/MRE, Clinical protocols, Fat fraction, Iron overload},
abstract = {Purpose
Fat accumulation and iron overload are important cofactors in chronic liver disease. Clinical quantifications of fat fraction and iron are currently assessed using MRI protocols. The purpose is to improve these measurements to simultaneously provide iron and fat maps from a single acquisition.
Methods
Ten healthy volunteers and ten patients with steatosis underwent MRI for fat fraction (FF: IDEAL-IQ®), iron overload concentration (IOC: Gandon, Starmap®) and viscoelastic characterization (MR-Touch®). IDEAL-IQ® data, the clinical FF reference, were compared to the advanced Gandon protocol, post-treated with a 3pt Dixon method. The originality was to use IDEAL-IQ® fat sequence to quantify iron volumetrically using the Wood equation. To validate the iron data, the reference Gandon protocol was applied and improved to provide map of IOC. Then, IOC data were also compared to another clinical sequence (Starmap®) which was also improved (scale, number of ROI). The estimated error associated with each method was evaluated with the coefficient of variation.
Results
IDEAL-IQ® and Gandon protocols were modified to provide simultaneously FF and IOC maps (2D, volume). Healthy FF were in the same range with all protocols (≈3%). For patients with steatosis, Gandon protocols underestimated the FF value (≈7%) compared to IDEAL-IQ®. Healthy and fibrosis patients were correctly diagnosed (no hemochromatosis) with all the protocols and viscoelastic properties were in the same range.
Conclusion
Manufacturer's tools were improved to simultaneously quantify liver markers saving time for the patient and the clinical setting. These parameters are of great value for clinical diagnostics and novel therapeutics to treat liver diseases.}
}
@article{FASTBERGLUND201831,
title = {Testing and validating Extended Reality (xR) technologies in manufacturing},
journal = {Procedia Manufacturing},
volume = {25},
pages = {31-38},
year = {2018},
note = {Proceedings of the 8th Swedish Production Symposium (SPS 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.054},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305730},
author = {Åsa Fast-Berglund and Liang Gong and Dan Li},
keywords = {AR, VR, cyber-physical, information levels, utilization, CPPS-testbed, learning factories},
abstract = {xR technologies such as Augmented Reality (AR) and Virtual Reality (VR) are increasing at shop-floors but are still in need for validation in order to make good strategic decisions regarding implementation. The areas where most industries apply AR is within remote guidance and complex tasks such as maintenance. VR is mostly used for layout planning and more and more for virtual training. This paper aims to present results from six case studies with over two hundred responders from academy and industry about usage and strategies on where and when to implement what xR technology..}
}
@article{AUGUSTIN2022103736,
title = {An Improved Deep Persistent Memory Network for Rician Noise Reduction in MR Images},
journal = {Biomedical Signal Processing and Control},
volume = {77},
pages = {103736},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103736},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422002580},
author = {Anate Mary Augustin and Chandrasekharan Kesavadas and P.V. Sudeep},
keywords = {Deep learning, Denoising, MemNet, MRI, Parallel imaging, Rician, Quantum ReLU},
abstract = {Magnetic Resonance Imaging (MRI) is extensively employed in medical, scientific and investigative contexts today. Noise on the other hand, restricts the diagnostic utility of MR images by deteriorating their quality during acquisition. The noise in single coil magnitude MRI has stationary Rician distribution and images reconstructed with parallel MR-imaging techniques have non-stationary noise levels. Recently, deep learning models are finding ubiquitous employment in image restoration tasks, owing to their powerful capabilities in learning and solving inverse-problems. Nonetheless, only a few such techniques have been reported to suppress noise in MRI. In this paper, we propose a robust MR image denoising approach based on the concept of memory persistence. Accordingly, we improvised and optimized the deep model of memory networks by introducing a data sensitive activation function and a robust cost function, resulting in a compact design with improved noise filtering, feature preservation and enhanced performance. Experiments on real and synthetic data reveal that the proposed method outperformed state-of-the-art (SoTA) methods.}
}
@article{JIANG2021103657,
title = {RETRACTED: Research on 3D simulation of swimming technique training based on FPGA and virtual reality technology},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103657},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103657},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308036},
author = {Luchuan Jiang},
abstract = {This article has been retracted: please see Elsevier Policy on Article Withdrawal (http://www.elsevier.com/locate/withdrawalpolicy). This article has been retracted at the request of the Editor-in-Chief. Significant similarities were noticed post-publication between this special issue article and other published sources. There is an indication that attempts have been made to disguise the copying using automated paraphrasing. Subsequent to acceptance of these special issue papers by the responsible guest editor, Hameem Shanavas, the integrity and rigor of the peer-review process of the Special Issue were investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. Due to a configuration error in the editorial system, unfortunately neither the Editor in Chief nor the designated Handling Editors received these papers for approval as per the journal’s standard workflow.}
}
@article{TREBBI2022103888,
title = {MR-based quantitative measurement of human soft tissue internal strains for pressure ulcer prevention},
journal = {Medical Engineering & Physics},
volume = {108},
pages = {103888},
year = {2022},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2022.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1350453322001369},
author = {Alessio Trebbi and Ekaterina Mukhina and Pierre-Yves Rohan and Nathanaël Connesson and Mathieu Bailet and Antoine Perrier and Yohan Payan},
keywords = {DVC, FEM, Heel, Sacrum},
abstract = {Pressure ulcers are a severe disease affecting patients that are bedridden or in a wheelchair bound for long periods of time. These wounds can develop in the deep layers of the skin of specific parts of the body, mostly on heels or sacrum, making them hard to detect in their early stages. Strain levels have been identified as a direct danger indicator for triggering pressure ulcers. Prevention could be possible with the implementation of subject-specific Finite Element (FE) models. However, generation and validation of such FE models is a complex task, and the current implemented techniques offer only a partial solution of the entire problem considering only external displacements and pressures, or cadaveric samples. In this paper, we propose an in vivo solution based on the 3D non-rigid registration between two Magnetic Resonance (MR) images, one in an unloaded configuration and the other deformed by means of a plate or an indenter. From the results of the image registration, the displacement field and subsequent strain maps for the soft tissues were computed. An extensive study, considering different cases (on heel pad and sacrum regions) was performed to evaluate the reproducibility and accuracy of the results obtained with this methodology. The implemented technique can give insight for several applications. It adds a useful tool for better understanding the propagation of deformations in the heel soft tissues that could generate pressure ulcers. This methodology can be used to obtain data on the material properties of the soft tissues to define constitutive laws for FE simulations and finally it offers a promising technique for validating FE models.}
}
@article{GULAMHUSSENE2022102122,
title = {Predicting 4D liver MRI for MR-guided interventions},
journal = {Computerized Medical Imaging and Graphics},
volume = {101},
pages = {102122},
year = {2022},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2022.102122},
url = {https://www.sciencedirect.com/science/article/pii/S0895611122000921},
author = {Gino Gulamhussene and Anneke Meyer and Marko Rak and Oleksii Bashkanov and Jazan Omari and Maciej Pech and Christian Hansen},
keywords = {4D MRI, Reconstruction, End-to-end, Deep learning, Respiration},
abstract = {Organ motion poses an unresolved challenge in image-guided interventions like radiation therapy, biopsies or tumor ablation. In the pursuit of solving this problem, the research field of time-resolved volumetric magnetic resonance imaging (4D MRI) has evolved. However, current techniques are unsuitable for most interventional settings because they lack sufficient temporal and/or spatial resolution or have long acquisition times. In this work, we propose a novel approach for real-time, high-resolution 4D MRI with large fields of view for MR-guided interventions. To this end, we propose a network-agnostic, end-to-end trainable, deep learning formulation that enables the prediction of a 4D liver MRI with respiratory states from a live 2D navigator MRI. Our method can be used in two ways: First, it can reconstruct high quality fast (near real-time) 4D MRI with high resolution (209×128×128 matrix size with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic interventional 2D navigator slice for guidance during an intervention. Second, it can be used for retrospective 4D reconstruction with a temporal resolution of below 0.2s/volume for motion analysis and use in radiation therapy. We report a mean target registration error (TRE) of 1.19±0.74mm, which is below voxel size. We compare our results with a state-of-the-art retrospective 4D MRI reconstruction. Visual evaluation shows comparable quality. We compare different network architectures within our formulation. We show that small training sizes with short acquisition times down to 2 min can already achieve promising results and 24 min are sufficient for high quality results. Because our method can be readily combined with earlier time reducing methods, acquisition time can be further decreased while also limiting quality loss. We show that an end-to-end, deep learning formulation is highly promising for 4D MRI reconstruction.}
}
@article{CHEKHOVSKOY2022223,
title = {The Use of Virtual Reality Technologies in the Specialists’ Training in the Field of Information Security},
journal = {Procedia Computer Science},
volume = {213},
pages = {223-231},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017513},
author = {Yuriy Chekhovskoy and Kirill Plaksiy and Andrey Nikiforov and Natalia Miloslavskaya},
keywords = {virtual reality, specialists’ training, information security, virtual reality testbed, network security},
abstract = {The paper presents a study of ready-made solutions and ways to use virtual reality (VR) technologies in training specialists. Due to the lack of tools that meet the needs of such preparation in the field of information security (IS), when students study network attacks and ways of protection against them, a training and laboratory complex using the Unity development environment for educational purposes were developed. It is intended for a detailed study of current network attacks implementation and development of protection measures against them. The educational and laboratory bench includes a VR testbed and its algorithmic support successfully tested in the educational process of the the National Research Nuclear University MEPhI (Moscow Engineering Physics Institute) within the framework of the discipline "Secure Information Systems".}
}
@article{ARROYAVETOBON201573,
title = {AIR-MODELLING: A tool for gesture-based solid modelling in context during early design stages in AR environments},
journal = {Computers in Industry},
volume = {66},
pages = {73-81},
year = {2015},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361514001857},
author = {Santiago Arroyave-Tobón and Gilberto Osorio-Gómez and Juan F. Cardona-McCormick},
keywords = {Augmented reality, Modelling in context, Solid modelling, Conceptual design, Hand gestures, Natural interfaces},
abstract = {Augmented reality (AR) technologies are just being used as interface in CAD tools allowing the user to perceive 3D models over a real environment. The influence of the use of AR in the conceptualization of products whose configuration, shape and dimensions depend mainly on the context remains unexplored. We aimed to prove that modelling in AR environments allows to use the context in real-time as an information input for making the iterative design process more efficient. In order to prove that, we developed a tool called AIR-MODELLING in which the designer is able to create virtual conceptual products by hand gestures meanwhile he/she is interacting directly with the real scenario. We conducted a test for comparing designers’ performance using AIR-MODELLING and a traditional CAD system. We obtained an average reduction of 44% on the modeling time in 76% of the cases. We found that modelling in AR environments using the hands as interface allows the designer to quickly and efficiently conceptualize potential solutions using the spatial restrictions of the context as an information input in real-time. Additionally, modelling in a natural scale, directly over the real scene, prevents the designer from drawing his/her attention on dimensional details and allows him/her to focus on the product itself and its relation with the environment.}
}
@article{ASISH202275,
title = {Detecting distracted students in educational VR environments using machine learning on eye gaze data},
journal = {Computers & Graphics},
volume = {109},
pages = {75-87},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322001856},
author = {Sarker Monojit Asish and Arun K. Kulshreshth and Christoph W. Borst},
keywords = {Education, Virtual Reality, Eye Tracking, Machine Learning, Deep Learning, Distraction Detection},
abstract = {Virtual Reality (VR) has been found useful to improve engagement and retention level of students, for some topics, compared to traditional learning tools such as books, and videos. However, a student could still get distracted and disengaged due to a variety of factors including stress, mind-wandering, unwanted noise, and external alerts. Student eye gaze data could be useful for detecting these distracted students. Gaze data-based visualizations have been proposed in the past to help a teacher monitor distracted students. However, it is not practical for a teacher to monitor a large number of student indicators while teaching. To help filter students based on distraction level, we propose an automated system based on machine learning to classify students based on their distraction level. The key aspects are: (1) we created a labeled eye gaze dataset from an educational VR environment, (2) we propose an automatic system to gauge a student’s distraction level from gaze data, and (3) we apply and compare several classifiers for this purpose. Each classifier classifies distraction, per educational activity section, into one of three levels (low, mid or high). Our results show that Random Forest (RF) classifier had the best accuracy (98.88%) compared to the other models we tested. Additionally, a personalized machine learning model using either RF, kNN, or Extreme Gradient Boosting (XGBoost) model was found to improve the classification accuracy significantly.}
}
@article{ZHANG20181278,
title = {Design and Interaction Interface using Augmented Reality for Smart Manufacturing},
journal = {Procedia Manufacturing},
volume = {26},
pages = {1278-1286},
year = {2018},
note = {46th SME North American Manufacturing Research Conference, NAMRC 46, Texas, USA},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.07.140},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918308163},
author = {Yunbo Zhang and Tsz-Ho Kwok},
keywords = {computer-aided design, smart manufacturing, industry 4.0, augmented reality, additive manufacturing, 3D printing},
abstract = {In this paper, we apply Augmented Reality (AR) technologies to develop a design and interaction interface for Smart Manufacturing (SmartMFG). This work is motivated by the lack of appropriate human-machine-interaction (HMI) tools to support interaction and customization in SmartMFG environment. Trying to address this research problem, we hypothesize that AR-based design interfaces that communicate with Machine Control Unit (MCU) directly will increase the degree of interaction and the complexity of instructions performed in Manual Data Input (MDI) systems. To test this hypothesis, we developed a prototyping system consisting of an AR-tablet device as the input interface and an Ultimaker 3 printer as the machine tool. Firstly, this AR-based system has sensing, design and control capabilities to interact and communicate with the machine tool via Wifi. Secondly, a set of sketch-based computational tools is developed for users to design shapes on existing objects easily and efficiently within the AR environment. Finally, The customized design is converted to machine code, which is also customized based on the machine tool and the registration of the virtual model and the existing object. We tested our system by designing two customized shapes onto an existing shape in the AR environment and generating the G-code to control the printer to fabricate them onto the physical object.}
}
@article{WANG201496,
title = {Integrating Augmented Reality with Building Information Modeling: Onsite construction process controlling for liquefied natural gas industry},
journal = {Automation in Construction},
volume = {40},
pages = {96-105},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S092658051300215X},
author = {Xiangyu Wang and Martijn Truijens and Lei Hou and Ying Wang and Ying Zhou},
keywords = {Building Information Modeling (BIM), Augmented Reality (AR), BIM+AR, Liquefied Natural Gas (LNG) industry},
abstract = {The extent of effectiveness of real-time communication within BIM environment is somehow restrained due to the limited sense of immersion into virtual environments. The objective of this paper highlights the need for a structured methodology of fully integrating Augmented Reality (AR) technology in BIM. Based on the generic review of BIM in construction, this paper forms the rationales for the onsite information system for construction site activities, and then formulates the methods of configuring BIM+AR prototypes. It is demonstrated that, extended to the site via the “hand” of AR, the BIM solution can address more real problems, such as low productivity in retrieving information, tendency of committing error in assembly, and low efficiency of communication and problem solving.}
}
@article{HERNANDEZMELGAREJO2020336,
title = {Mechatronic design and implementation of a bicycle virtual reality system},
journal = {ISA Transactions},
volume = {97},
pages = {336-351},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0019057819303386},
author = {G. Hernández–Melgarejo and D.A. Flores–Hernández and A. Luviano–Juárez and L.A. Castañeda and I. Chairez and S. {Di Gennaro}},
keywords = {Virtual reality, Haptic feedback, Active disturbance rejection control, Mechatronic system, Fast prototyping},
abstract = {The aim of this study is to design and implement a virtual reality bicycle system based on a functional-based mechatronic design approach. The development of virtual reality technologies with haptic systems demands a proper integration of the involved disciplines to provide immerse experiences for users. The proposed design approach provides a formal manner to gather the subsystems in the mechatronic device. The developed system is divided in a Virtual Reality System (VRS) and a Physical System (PS) for the design process. The former includes an interactive virtual environment in which an Avatar is animated using a simple kinematic bicycle model. The latter includes an adapted mountain bicycle with haptic feedback mechanisms to interact with the user and to produce the corresponding inputs for the bicycle model. Both systems are integrated by a control behavior system that works under two operation modes, where the user carries out virtual tours and gets feedbacks from a stereoscopic display system, audio cues, and haptic mechanisms. A multibody simulation validates the consistency and the integration of the physical system. In addition, a set of experimental results show the performance of instrumentation elements, control strategies, and feedback mechanisms, to provide the user with an immersive experience in the virtual environment. A brief survey was carried out to assess the opinion of users about the virtual bicycle tours, providing feedback for future improvements. The different designed modules and sub-systems allow modifying and enhancing the VRS without major modifications of the PS, or allow enhancing the physical platform without affecting the functionality of the virtual environment.}
}
@article{ZHANG2012117,
title = {Construction of parallel and distributed static simulation system based on augmented reality},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {19},
number = {4},
pages = {117-121},
year = {2012},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(11)60291-4},
url = {https://www.sciencedirect.com/science/article/pii/S1005888511602914},
author = {Jin-ling ZHANG},
keywords = {augmented reality, registration, parallel and distributed structure, synchronization control},
abstract = {This article puts forward a kind of parallel and distributed static augmented scene system structure to improve the performance of real time augmented simulation system. Based on static registration technique, several groups of processing nodes do parallel scene pictures taking, 3D registration and virtual-real merging. Process on different nodes is controlled by uniform synchronization mechanism and network transmitting. Wide field of view image can be obtained from image mosaic operation and displayed by wide view display system. Detailed system architecture, registration algorithm, method how to determine camera position and synchronization mechanism between each process node are introduced. The experiment result can validate the good performance of the designed system.}
}
@article{CHUANG2002189,
title = {A virtual reality-based system for hand function analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {69},
number = {3},
pages = {189-196},
year = {2002},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(01)00190-0},
url = {https://www.sciencedirect.com/science/article/pii/S0169260701001900},
author = {Tien-Yow Chuang and Wei-Shin Huang and Shu-Chiung Chiang and Yun-An Tsai and Ji-Liang Doong and Henrich Cheng},
keywords = {Virtual realities, Hand function, Motor retraining, Spatial relation task, Dataglove, Root mean square},
abstract = {The goal of this study was to demonstrate the usability and usefulness of virtual reality technology in assessing hand functions. Ten healthy, non-disabled right-handed adult volunteers were recruited. Each volunteer used a dataglove to insert three-dimensional virtual representations of a cylinder and a prism into the target holes. To verify the reliability of the tests, each subject was retested twice. The performance testing assessed the visual-motor coordination a person needs to achieve a task accurately and within a set time. For each trial, the root mean square (RMS) value of the hand movement trajectory was projected onto the X, Y, and Z axes. This projection enabled us to measure the extent of the genuine, summative displacement of the manipulating hand. The reproducibility of the virtual reality assessment was analyzed using the intraclass correlation (ICC) approach. The total ICC values of 10 subjects demonstrated a high task completion time and RMS on the X and Z axes for the transferring of the prism. However, the values were low for the transferring of the cylinder. Because the individual coefficients of variations (CVs) varied widely in the moving of both the cylinder and the prism, the total (CVs) showed a high reading for the task completion time. Although rehabilitation clinics routinely carry out peg-moving exercises for disabled patients, our model provides a valuable quantitative real time and off-line measure of whole hand functions.}
}
@article{WU2022104252,
title = {Real-time mixed reality-based visual warning for construction workforce safety},
journal = {Automation in Construction},
volume = {139},
pages = {104252},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104252},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200125X},
author = {Shaoze Wu and Lei Hou and Guomin (Kevin) Zhang and Haosen Chen},
keywords = {Construction safety, Mixed reality, Digital twin, Deep learning, Wearable device, Visualisation},
abstract = {Spatial locations of personnel, equipment, and materials are constantly changing as construction projects progress. The dynamic nature of the construction industry affects workers' performance of identifying hazards. Even though a great deal of effort has been made to improve construction safety, the construction industry still witnesses a high accident rate. In order to complement the existing body of knowledge relating to construction safety, this paper integrates Digital Twin (DT), Deep Learning (DL), and Mixed Reality (MR) technologies into a newly developed real-time visual warning system, which enables construction workers to proactively determine their safety status and avoid accidents. Next, system tests were conducted under three quasi-on-site scenarios, and the feasibility was proven in terms of synchronising construction activities over a large area and visually representing hazard information to its users. These evidenced merits of the development testing scenarios can improve workers' risk assessment accuracy, reinforce workers' safety behaviour, and provide a new perspective for construction safety managers to analyse construction safety status.}
}
@article{FLOR2021110554,
title = {Virtual reality as a tool for evaluating user acceptance of view clarity through ETFE double-skin façades},
journal = {Energy and Buildings},
volume = {231},
pages = {110554},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110554},
url = {https://www.sciencedirect.com/science/article/pii/S037877882032925X},
author = {Jan-F. Flor and Marina Aburas and Fedaa Abd-AlHamid and Yupeng Wu},
keywords = {View perception, Virtual reality, ETFE, Double-skin Façades},
abstract = {Equipping building envelopes with an additional layer is an effective measure for improving the overall thermal and daylighting performance and reducing the energy consumption of buildings. This study investigated the user acceptance of energy-saving retrofitting measures in office buildings, reporting on the view perception and emotional response towards ETFE double-skin façades (DSF). Virtual Reality (VR) and physics-based imaging techniques were used to evaluate the user experience of a window view in an office space equipped with a pneumatic ETFE cushion as a second building skin. Three DSF scenarios with different ETFE cushions, including a clear, fritted and switchable sample, were evaluated and compared to the original single-skin façade with double-glazed windows. The physical and luminous conditions of the office space were replicated in a virtual environment with a validated physically-based imaging technique and presented to a group of volunteers (N = 22) using a virtual reality headset. While immersed in the virtual environment, participants responded to a questionnaire enquiring into their view perception and emotional states. The results revealed a preference for view clarity of clear ETFE in double-skin façades (Mdn = 5) and less satisfaction for fritted (Mdn = 4) and switchable foil cushions (Mdn = 1.75), yet double glazing was preferred in all measured parameters (Mdn = 6). Statistical significance was found for fritted in comparison to switchable ETFE in terms of spatial pleasure and control. The highest ratings were given to clear glazing across all investigated parameters of view perception and emotional response. The lowest ranking in all questions was given to the sample with the switchable ETFE cushion. The study concluded that view clarity is a major aspect for the user acceptance of ETFE double-skin façades. Overall, this study provides a better understanding of the visual and emotional implications of viewing through ETFE foil and contributes to forming criteria for the design of next-generation ETFE building envelopes.}
}
@article{DANIELSSON201789,
title = {Assessing Instructions in Augmented Reality for Human-robot Collaborative Assembly by Using Demonstrators},
journal = {Procedia CIRP},
volume = {63},
pages = {89-94},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.02.038},
url = {https://www.sciencedirect.com/science/article/pii/S221282711730135X},
author = {Oscar Danielsson and Anna Syberfeldt and Rodney Brewster and Lihui Wang},
keywords = {Augmented Reality, Human Robot Collaboration, Assembly},
abstract = {Robots are becoming more adaptive and aware of their surroundings. This has opened up the research area of tight human-robot collaboration, where humans and robots work directly interconnected rather than in separate cells. The manufacturing industry is in constant need of developing new products. This means that operators are in constant need of learning new ways of manufacturing. If instructions to operators and interaction between operators and robots can be virtualized this has the potential of being more modifiable and available to the operators. Augmented Reality has previously shown to be effective in giving operators instructions in assembly, but there are still knowledge gaps regarding evaluation and general design guidelines. This paper has two aims. Firstly it aims to assess if demonstrators can be used to simulate human-robot collaboration. Secondly it aims to assess if Augmented Reality-based interfaces can be used to guide test-persons through a previously unknown assembly procedure. The long-term goal of the demonstrator is to function as a test-module for how to efficiently instruct operators collaborating with a robot. Pilot-tests have shown that Augmented Reality instructions can give enough information for untrained workers to perform simple assembly-tasks where parts of the steps are done with direct collaboration with a robot. Misunderstandings of the instructions from the test-persons led to multiple errors during assembly so future research is needed in how to efficiently design instructions.}
}
@article{ALSABBAG2022101473,
title = {Interactive defect quantification through extended reality},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101473},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101473},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002238},
author = {Zaid Abbas Al-Sabbag and Chul Min Yeum and Sriram Narasimhan},
keywords = {Visual inspection, Extended reality, Augmented reality, Damage detection},
abstract = {In this study, a new visual inspection method that can interactively detect and quantify structural defects using an Extended Reality (XR) device (headset) is proposed. The XR device, which is at the core of this method, supports an interactive environment using a holographic overlay of graphical information on the spatial environment and physical objects being inspected. By leveraging this capability, a novel XR-supported inspection pipeline, called eXtended Reality-based Inspection and Visualization (XRIV), is developed. Key tasks supported by this method include detecting visual damage from sensory data acquired by the XR device, estimating its size, and visualizing (overlaying) information on the spatial environment. The crucial step of real-time interactive segmentation—detection and pixel-wise damage boundary refinement—is achieved using a feature Back-propagating Refinement Scheme (f-BRS) algorithm. Then, a ray-casting algorithm is applied to back-project the 2D image pixel coordinates of the damage region to their 3D world coordinates for damage area quantification in real-world (physical) units. Finally, the area information is overlaid and anchored to the scene containing damage for visualization and documentation. The performance of XRIV is experimentally demonstrated by measuring surface structural damage of an in-service concrete bridge with less than 10% errors for two different test cases, and image processing latency of 2–3 s (or 0.5 s per seed point) from f-BRS. The proposed XRIV pipeline underscores the advantages of real-time interaction between expert users and the XR device through immersive visualization so that a human–machine collaborative workflow can be established to obtain better inspection outcomes in terms of accuracy and robustness.}
}
@article{WU2014869,
title = {Real-time advanced spinal surgery via visible patient model and augmented reality system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {113},
number = {3},
pages = {869-881},
year = {2014},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2013.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0169260713004112},
author = {Jing-Ren Wu and Min-Liang Wang and Kai-Che Liu and Ming-Hsien Hu and Pei-Yuan Lee},
keywords = {Visible patient, Augmented reality, Camera-projector system, Digital spinal surgery},
abstract = {This paper presents an advanced augmented reality system for spinal surgery assistance, and develops entry-point guidance prior to vertebroplasty spinal surgery. Based on image-based marker detection and tracking, the proposed camera-projector system superimposes pre-operative 3-D images onto patients. The patients’ preoperative 3-D image model is registered by projecting it onto the patient such that the synthetic 3-D model merges with the real patient image, enabling the surgeon to see through the patients’ anatomy. The proposed method is much simpler than heavy and computationally challenging navigation systems, and also reduces radiation exposure. The system is experimentally tested on a preoperative 3D model, dummy patient model and animal cadaver model. The feasibility and accuracy of the proposed system is verified on three patients undergoing spinal surgery in the operating theater. The results of these clinical trials are extremely promising, with surgeons reporting favorably on the reduced time of finding a suitable entry point and reduced radiation dose to patients.}
}
@article{DUAN2024117707,
title = {Vibration absorption based on MR with synchronous controlled stiffness and damping for propulsion shafting},
journal = {Ocean Engineering},
volume = {304},
pages = {117707},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117707},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824010448},
author = {Weishi Duan and Hui Huang and Baizhou Ma and Yuanliang Wu and Shumei Chen},
keywords = {Dynamic vibration absorber, Magnetorheological, Propulsion shafting system, Optimal resonance frequency},
abstract = {A dynamic vibration absorber (DVA) using magnetorheological (MR) for synchronous stiffness and damping control is detailed in this study. The variable longitudinal vibration frequency of propulsion shafting, which is attributed to a broad spectrum of propeller speed fluctuations, necessitates effective vibration mitigation. Conventional DVAs suffer from limited tunability, which can lead to the failure of optimal resonance, narrowed vibration suppression bandwidth, and potential vibration exacerbation. Thus, a control criterion for optimizing the stiffness and damping parameters is proposed in this study. Firstly, a longitudinal vibration dynamic model for the propulsion shafting system is developed, to determine the acceleration transmissibility and resonance frequency points. Next, a magnetorheological vibration absorber (MRVA) is designed to encompass synchronous controlled stiffness and damping. The mechanical property evaluations and frequency shift tests confirm the absorber's capacity for variable stiffness and damping. A coupled dynamic model that integrates the propulsion shafting system and MRVA is established. A control criterion for the stiffness and damping has been formulated to accommodate the diverse working conditions. The results of the vibration reduction experiments substantiate the efficacy of the proposed control criterion, thus demonstrating the attenuation of acceleration amplitude above 37%, besides, up to 90% has been achieved at specific frequencies.}
}
@article{SUAREZWARDEN2015306,
title = {Test of Voltage for Electrical Diagnosis Aided by AR in Equipment Adaptation or Predictive Maintenance},
journal = {Procedia Computer Science},
volume = {75},
pages = {306-315},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037138},
author = {Fernando Suárez-Warden and Eduardo González Mendívil},
keywords = {voltage test, Augmented Reality, diagnostic and functional tests, predictive maintenance, equipment adaptation.},
abstract = {Electrical flops, as typical problems of machines which components suffer unsatisfactory maintenance or which parts, subject to wear or diagnostic errors or with design mistakes, occur due to inefficient adaptation. So when a flaw appears or a maintenance program suggests monitoring for an electrical part, a mandatory diagnosis process (to determine components condition) must be executed. Similarly if an investigation tries to undertake an adaptation of machine to the process, various functional tests are commanded among which the voltage test is distinguished. A diagram of electrical test for being aided by Augmented Reality and a related study case are developed.}
}
@article{ANGHEL201563,
title = {Single electrode Ar bubbled plasma source for methylene blue degradation and concurrent synthesis of carbon based nanoparticles},
journal = {Journal of Electrostatics},
volume = {75},
pages = {63-71},
year = {2015},
issn = {0304-3886},
doi = {https://doi.org/10.1016/j.elstat.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304388615000285},
author = {S.D. Anghel and D. Zaharie-Butucel and I.E. Vlad},
keywords = {Plasma in gas bubbles, Single electrode, Dye degradation, Solid particles},
abstract = {This work presents a new possibility of generating in-liquids Ar bubbled plasma by using a single electrode placed in a quartz tube. When the electrode is powered with ac high voltage the plasma is generated between the electrode's free end and the wall of Ar bubble. The ignition mechanism of plasma is analyzed and the generated active species are identified. The plasma was tested for the decomposition of methylene blue aqueous solutions and for the concurrent synthesis of very small solid particles. The influence of the methylene blue initial concentration and of the treatment time on the processes was studied.}
}
@article{BAGASSI2020103291,
title = {Human-in-the-loop evaluation of an augmented reality based interface for the airport control tower},
journal = {Computers in Industry},
volume = {123},
pages = {103291},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103291},
url = {https://www.sciencedirect.com/science/article/pii/S016636152030525X},
author = {Sara Bagassi and Francesca {De Crescenzio} and Sergio Piastra and Carlo A. Persiani and Mohamed Ellejmi and Alan R. Groskreutz and Jorge Higuera},
abstract = {An innovative airport control tower concept based on the use of modern augmented reality technologies has been developed and validated by means of human-in-the-loop experiments in a simulated environment. An optical-based augmented reality interface underpins the proposed concept that consists in providing air traffic control operators in the airport control tower with complete head-up information, as opposed to the current mix of information retrieval through both head-up real view and head-down interfaces. Specific measurement of the time spent by the operator working in either head-up or head-down position, show that the proposal has a clear effect in stimulating the air traffic control operator to work in a head-up position more than in a head-down position, with positive effects on his/her situational awareness and perceived workload, especially when dealing with low visibility conditions operational scenarios.}
}
@article{SCHUSTER2021660,
title = {Human acceptance evaluation of AR-assisted assembly scenarios},
journal = {Journal of Manufacturing Systems},
volume = {61},
pages = {660-672},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520302260},
author = {Florian Schuster and Bastian Engelmann and Uwe Sponholz and Jan Schmitt},
keywords = {Augmented reality, Acceptance, Assembly},
abstract = {The contribution examines the acceptance of Augmented Reality (AR) in assembly scenarios by a model-based approach for acceptance evaluation. After a critical literature research and analysis, a proprietary model for acceptance measurement is developed, which includes and synthesizes previous models and simplifies them considerably for the purpose of industrial assembly. Consequently, a structural model is derived which is based on the basic concepts of the Unified Theory of Acceptance and Use of Technology (UTAUT) and Technology Acceptance Model (TAM). This is built up as a path model to explain user acceptance. Following, a laboratory experiment is set up to collect data. At the final assembly workstation of the FHWS c-Factory the study participants assemble a toy truck once without and once with AR support. The c-Factory is a concept factory for smart production in an IoT environment. Afterwards, relevant data are collected by means of a survey. The evaluation shows that the acceptance of AR is given and the model is resilient. The results show also, that AR is accepted by the participants supporting their work.}
}
@article{WEI2024107983,
title = {CT synthesis from MR images using frequency attention conditional generative adversarial network},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107983},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107983},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000672},
author = {Kexin Wei and Weipeng Kong and Liheng Liu and Jian Wang and Baosheng Li and Bo Zhao and Zhenjiang Li and Jian Zhu and Gang Yu},
keywords = {MR, Synthetic CT, Deep learning, Generative adversarial networks, Attention mechanism},
abstract = {Magnetic resonance (MR) image-guided radiotherapy is widely used in the treatment planning of malignant tumors, and MR-only radiotherapy, a representative of this technique, requires synthetic computed tomography (sCT) images for effective radiotherapy planning. Convolutional neural networks (CNN) have shown remarkable performance in generating sCT images. However, CNN-based models tend to synthesize more low-frequency components and the pixel-wise loss function usually used to optimize the model can result in blurred images. To address these problems, a frequency attention conditional generative adversarial network (FACGAN) is proposed in this paper. Specifically, a frequency cycle generative model (FCGM) is designed to enhance the inter-mapping between MR and CT and extract more rich tissue structure information. Additionally, a residual frequency channel attention (RFCA) module is proposed and incorporated into the generator to enhance its ability in perceiving the high-frequency image features. Finally, high-frequency loss (HFL) and cycle consistency high-frequency loss (CHFL) are added to the objective function to optimize the model training. The effectiveness of the proposed model is validated on pelvic and brain datasets and compared with state-of-the-art deep learning models. The results show that FACGAN produces higher-quality sCT images while retaining clearer and richer high-frequency texture information.}
}
@article{CUNHA2022426,
title = {Using Virtual Reality in the Development of an Index-Engine of Physical and Emotional Sustainability},
journal = {Procedia Computer Science},
volume = {196},
pages = {426-433},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022559},
author = {Carlos R. Cunha and Alcina Nunes and Paula O. Fernandes and José Bragada and Luís Pires and Maria José and Pedro Magalhães},
keywords = {Virtual Reality, Machine Learning, Model, Emotional Sustainability, Well-being, Health},
abstract = {Nowadays, there is a growing need for rehabilitation associated with demographic changes and health trends, with an increase in the prevalence of non-communicable diseases and an aging population. Although well-being is classically associated with a set of physical activities, the reality of today’s society often shows that there is a lack of time available to develop activities that promote well-being. This fact is aggravated in aging populations by less mobility and greater isolation. In this domain, information and communication technologies have greatly contributed to helping healthcare providers to develop new understanding, measurement and action strategies that promote the physical and emotional well-being of their patients. Accordingly, the role of Virtual Reality has been shown to be promising for the generation of well-being. This article, as part of a broader funding project, reviews the state of the art of the role that Virtual Reality has played in the promotion of well-being, being an exercise of conceptualization that converges in the proposal of a conceptual model - an Index-Engine of Physical and Emotional Sustainability, which will be prototyped and field tested in the future.}
}
@article{TROMBETTA201715,
title = {Motion Rehab AVE 3D: A VR-based exergame for post-stroke rehabilitation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {151},
pages = {15-20},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S016926071730113X},
author = {Mateus Trombetta and Patrícia Paula {Bazzanello Henrique} and Manoela Rogofski Brum and Eliane Lucia Colussi and Ana Carolina Bertoletti {De Marchi} and Rafael Rieder},
keywords = {Stroke, Serious game, Rehabilitation, Unity},
abstract = {Background and objective
Recent researches about games for post-stroke rehabilitation have been increasing, focusing in upper limb, lower limb and balance situations, and showing good experiences and results. With this in mind, this paper presents Motion Rehab AVE 3D, a serious game for post-stroke rehabilitation of patients with mild stroke. The aim is offer a new technology in order to assist the traditional therapy and motivate the patient to execute his/her rehabilitation program, under health professional supervision.
Methods
The game was developed with Unity game engine, supporting Kinect motion sensing input device and display devices like Smart TV 3D and Oculus Rift. It contemplates six activities considering exercises in a tridimensional space: flexion, abduction, shoulder adduction, horizontal shoulder adduction and abduction, elbow extension, wrist extension, knee flexion, and hip flexion and abduction. Motion Rehab AVE 3D also report about hits and errors to the physiotherapist evaluate the patient's progress.
Results
A pilot study with 10 healthy participants (61–75 years old) tested one of the game levels. They experienced the 3D user interface in third-person. Our initial goal was to map a basic and comfortable setup of equipment in order to adopt later. All the participants (100%) classified the interaction process as interesting and amazing for the age, presenting a good acceptance.
Conclusions
Our evaluation showed that the game could be used as a useful tool to motivate the patients during rehabilitation sessions. Next step is to evaluate its effectiveness for stroke patients, in order to verify if the interface and game exercises contribute into the motor rehabilitation treatment progress.}
}
@article{CHEN2020103135,
title = {Transfer learning enhanced AR spatial registration for facility maintenance management},
journal = {Automation in Construction},
volume = {113},
pages = {103135},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103135},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519308842},
author = {Keyu Chen and Jianfei Yang and Jack C.P. Cheng and Weiwei Chen and Chun Ting Li},
keywords = {AR spatial registration, Facility maintenance management, Transferable CNN-LSTM, Wi-Fi fingerprinting},
abstract = {Augmented reality (AR), which requires a spatial registration technique, has proved to greatly improve the efficiency of facility maintenance management (FMM) activities. Being one of the most promising techniques for indoor localization, Wi-Fi fingerprinting has been widely used for AR spatial registration. However, localization accuracy of Wi-Fi fingerprinting decreases over time due to dynamics of environmental factors. Readings from different mobile devices can also affect the accuracy negatively. In this paper, a transfer learning technique named transferable CNN-LSTM is proposed for improving the robustness of Wi-Fi fingerprinting while implementing AR in FMM activities. Convolutional neural network (CNN), embedded with long short term memory (LSTM) networks, is utilized to predict the location of unlabeled fingerprints. Multiple kernel variant of maximum mean discrepancy (MK-MMD) is adopted to reduce the distribution difference between the source domain and the target domain, so that the location of the newly collected unlabeled fingerprints can be predicted accurately. As shown in the experimental validation, the transferable CNN-LSTM can achieve an accuracy of 97.1% in short-term (without significant environmental changes) spatial registration, 87.8% in long-term (with significant environmental changes) spatial registration, and around 90% in multi-device spatial registration, indicating a higher accuracy and better robustness over other conventional approaches.}
}
@article{YUAN2005980,
title = {A generalized registration method for augmented reality systems},
journal = {Computers & Graphics},
volume = {29},
number = {6},
pages = {980-997},
year = {2005},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2005.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0097849305001688},
author = {M.L. Yuan and S.K. Ong and A.Y.C. Nee},
keywords = {Augmented reality, Registration, Projective reconstruction, Tracking},
abstract = {In augmented reality systems, registration is one of the most difficult problems currently limiting their applications. In this paper, we propose a generalized registration method using projective reconstruction technique in computer vision. This registration method is composed of embedding and tracking. Embedding involves specifying four points to build the world coordinate system on which a virtual object will be superimposed. In this stage, any arbitrary two unrelated images or any 3×4 projective matrices with rank 3 can be used to calculate the 3D pseudo-projective coordinates of the four specified points. In the tracking process, these 3D pseudo-projective coordinates are used to track the four specified points to compute the registration matrix for augmentation. The proposed method is simple, as only four points need to be specified at the embedding stage, and the virtual object can then be easily augmented onto a real scene from a video sequence. One advantage is that the virtual objects can still be superimposed on the specified regions even when the regions are occluded in the video sequence. Another advantage of the proposed method is that the registration errors can be adjusted in real-time to ensure that they are less than certain thresholds that have been specified at the initial embedding stage. Several experiments have been conducted to validate the performance of the proposed generalized method.}
}
@article{MUNOZ201975,
title = {Mixed reality-based user interface for quality control inspection of car body surfaces},
journal = {Journal of Manufacturing Systems},
volume = {53},
pages = {75-92},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S027861251930072X},
author = {Adolfo Muñoz and Xavier Mahiques and J. Ernesto Solanes and Ana Martí and Luis Gracia and Josep Tornero},
keywords = {Car body surface quality control, Mixed reality, Augmented reality},
abstract = {In recent years, the quality control of car body surfaces production lines have been put in the context of Industry 4.0. The emergence of automatic defect detection systems have helped to standardize the brand quality and gather information about all quality control tasks performed by workers. However, current worker interfaces used to indicate the location and other characteristics of the defects found by these systems have overcome the ergonomics of workers and increased their stress at work. This paper presents a novel mixed reality-based user interface for quality control inspection which is more intuitive, in order to improve the ergonomics of workers, reduce their stress at work and improve the productivity of current quality control production lines. An experimental prototype is shown in the paper in order to demonstrate the benefits of the proposed interface. In addition, the paper shows the results of several usability tests that compare the proposed mixed reality-based user interface with current interfaces used in important factories such as Mercedes-Benz, analyzing the benefits and drawbacks of each interface.}
}
@article{YIM20104096,
title = {Heuristic guidelines and experimental evaluation of effective augmented-reality based instructions for maintenance in nuclear power plants},
journal = {Nuclear Engineering and Design},
volume = {240},
number = {12},
pages = {4096-4102},
year = {2010},
note = {The 6th Japan-Korea Symposium on Nuclear Thermal Hydraulics and Safety - NTHAS6 Special Section},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2010.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0029549310006266},
author = {Ho Bin Yim and Poong Hyun Seong},
abstract = {As industrial plants and factories age, their maintenance requirements increase. Because maintenance mistakes directly increase the operating costs of a power plant, maintenance quality is significant concern to plant management. By law, all personnel working with nuclear technology must be re-trained every three years in Korea; however, as the statistical data show, the number of shutdown accidents at nuclear power plants (NPPs) due to maintenance failure is still high and needs to be reduced. Industries have started to adopt various technologies to increase the speed and accuracy of maintenance. Among those technologies, augmented reality (AR) is the latest multimedia presentation technology to be applied to plant maintenance, and it offers superior intuitiveness and user interactivity over other conventional multimedia. This empirical study aims to measure the optimum amounts of information to be delivered at a time and to identify what types of information enhance the learning ability of novices and to suggest heuristic guidelines by which to make effective AR training instructions. In the first experiment, the optimum amount of information in an AR learning environment for novices was found to be 4–5 pieces of information in a chunk by comparing results between a pre-test and an after-test. This result implies that intentionally made chunks help novices learn more effectively. In the second experiment, the AR training instruction based on the suggested heuristic guidelines was slightly more effective than other AR training instructions. Maintenance in nuclear power plants can be more reliable and accurate by training through AR training instruction based on the suggested heuristic guidelines.}
}
@article{ALARCONYAQUETTO2021104404,
title = {Effect of augmented reality books in salivary cortisol levels in hospitalized pediatric patients: A randomized cross-over trial},
journal = {International Journal of Medical Informatics},
volume = {148},
pages = {104404},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104404},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000307},
author = {Dulce E. Alarcón-Yaquetto and Jean P. Tincopa and Daniel Guillén-Pinto and Nataly Bailon and César P. Cárcamo},
keywords = {Cortisol, Pediatrics, Hospitalization, Stress, Augmented reality},
abstract = {Objective
This study sought to assess the effect of reading augmented reality (AR) books on salivary cortisol levels in hospitalized pediatric patients compared to reading a standard children’s book.
Methods
This was a randomized, two-period, cross-over trial in hospitalized children aged 7–11 years. AR books currently in the market were used as intervention. Complete block randomization was used to randomize the order of the intervention. Children allocated to the ‘AR-first’ group received the book, a tablet and were left to interact independently with the technology for an hour. After a 48 -h wash-out period, children received a standard book. ‘Standard-book-first’ group received only the standard book and after wash-out received the tablet and the AR book. Salivary cortisol and a validated visual analogue scale (VAS) for psychological stress were assessed at the beginning and at the end of each intervention.
Results
A total of 29 children were recruited in the study. One was lost during follow up. Cortisol levels decreased after the AR intervention (P = 0.019). Nevertheless, the decrease was not greater than the one associated to reading the standard book. VAS scores increased after the AR intervention (P < 0.001).
Discussion
There is evidence of order and sequence effects that might explain results. First assessment of AR-based interventions on stress. Results justify further research.
Conclusions
There was no evidence that reading AR books diminished cortisol levels more than reading a standard book. AR-books improved VAS score for psychological stress compared to a standard book.}
}
@article{WINKES2015152,
title = {Method for an Enhanced Assembly Planning Process with Systematic Virtual Reality Inclusion},
journal = {Procedia CIRP},
volume = {37},
pages = {152-157},
year = {2015},
note = {CIRPe 2015 - Understanding the life cycle implications of manufacturing},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115008446},
author = {Pascal A. Winkes and Jan C. Aurich},
keywords = {Assembly planning, Virtual Reality, improvement},
abstract = {The use of Virtual Reality (VR) technology is a promising approach to eliminate crucial problems in assembly planning. This approach presents an enhanced assembly planning process with valuable integrated VR assistance for a definite structured detection of planning failures. It integrates the VR validation of the assembly in a workshop after the specification step of the assembly planning process. Detailed holistic instructions for the workshop preparation and the VR use are given. This includes a data flow model, hardware requirements and the workshop team structure as well as testing the single assembly steps and the detection and evaluating of planning failures. Further, a procedure for improving those imperfections during the VR workshop is developed. Changes will be written back into the specifications book and the planning will be implemented in the physical factory. This leads to an enhanced detection and improvement of planning failures before the realization of a new assembly station.}
}
@article{SUN20203306,
title = {Team effectiveness evaluation and virtual reality scenario mapping model for helicopter emergency rescue},
journal = {Chinese Journal of Aeronautics},
volume = {33},
number = {12},
pages = {3306-3317},
year = {2020},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2020.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1000936120302211},
author = {Xue SUN and Hu LIU and Yongliang TIAN and Guanghui WU and Yuan GAO},
keywords = {Helicopter emergency rescue, Scenario mapping model, Team effectiveness evaluation, Virtual reality (VR), Virtual simulation-based training},
abstract = {The application of helicopter emergency rescue is becoming increasingly widespread, but the flight crew training in this area is still difficult due to high cost and risk. Nevertheless, with the development of Virtual Reality (VR) technology, virtual simulation has become a significant role in crew training of helicopter rescue. During the implementation of VR-based training, how to transform complex real tasks into VR scenarios and how to evaluate the performance of crew are of great importance. To address these issues, a novel VR-based R-E-A-D (Report, Evaluate, Agree, Do) evaluation model for training is proposed, which is suitable for complex missions with multiple tasks, multiple scenarios, and multiple people. Then, a mapping method of VR scenarios is put forward, which can transform the real tasks into virtual scenarios to serve the virtual simulation training. Finally, an experiment is carried out to verify the feasibility of the evaluation method and virtual scenario mapping method.}
}
@article{ZHU2021103908,
title = {RETRACTED: Three-dimensional simulation of swimming training based on Android mobile system and virtual reality technology},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103908},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103908},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121000879},
author = {Xionghao Zhu and Fan Kou},
abstract = {This article has been retracted: please see Elsevier Policy on Article Withdrawal(http://www.elsevier.com/locate/withdrawalpolicy) This article has been retracted at the request of the Editor-in-Chief. Significant similarities were noticed post-publication between this special issue article and other published sources. There is an indication that attempts have been made to disguise the copying using automated paraphrasing. Subsequent to acceptance of these special issue papers by the responsible guest editor, Hameem Shanavas, the integrity and rigor of the peer-review process of the Special Issue were investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. Due to a configuration error in the editorial system, unfortunately neither the Editor in Chief nor the designated Handling Editors received these papers for approval as per the journals standard workflow.}
}
@article{ROUPE201442,
title = {Interactive navigation interface for Virtual Reality using the human body},
journal = {Computers, Environment and Urban Systems},
volume = {43},
pages = {42-50},
year = {2014},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971513000884},
author = {Mattias Roupé and Petra Bosch-Sijtsema and Mikael Johansson},
keywords = {Virtual Reality, Natural User Interface, Navigation, Interaction, Urban planning},
abstract = {The use of Virtual Reality (VR) and interactive real-time rendering in urban planning and building design is becoming more and more common. However, the integration of desktop-VR in the urban planning process suffers from complicated navigation interfaces. In particular, people unfamiliar to gaming environments and computers are less prone to interact with a VR visualization using keyboard and mouse as controlling devices. This paper addresses this issue by presenting an implementation of the XBOX 360 Kinect sensor system, which uses the human body to interact with the virtual environment. This type of interaction interface enables a more natural and user-friendly way of interacting with the virtual environment. The validation of the system was conducted with 60 participants using quantitative and qualitative methods. The result showed that participants perceived the interface as non-demanding and easy to use and the interface was perceived better in relation to mouse/keyboard interaction. The implemented interface supported users to switch between different architecture proposals of an urban plan and the switching positively affected learning, understanding and spatial reasoning of the participants. The study also shows that females perceived the system as less demanding than males. Furthermore, the users associated and related their body (human interaction interface) to VR, which could indicate that they used their body during spatial reasoning. This type of spatial reasoning has been argued to enhance the spatial-perception.}
}
@incollection{AUCCAHUASI2021271,
title = {Chapter 12 - Low-cost system in the analysis of the recovery of mobility through inertial navigation techniques and virtual reality},
editor = {Valentina E. Balas and Souvik Pal},
booktitle = {Healthcare Paradigms in the Internet of Things Ecosystem},
publisher = {Academic Press},
pages = {271-292},
year = {2021},
isbn = {978-0-12-819664-9},
doi = {https://doi.org/10.1016/B978-0-12-819664-9.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196649000120},
author = {Wilver Auccahuasi and Mónica Diaz and Fernando Sernaque and Edward Flores and Justiniano Aybar and Elizabeth Oré},
keywords = {Acceleration, Gyroscope, Pathologies, The march, Video game, Virtual reality},
abstract = {Abstract
People can develop fully, when they can make use of all their faculties and abilities, the ability to mobilize is one of the most important factors; when this capacity fails to develop or is limited generates a deficit in mental and personal development, and when this problematic situation is taken to children, it becomes more critical because children are in a stage of physical and emotional growth and development, present some deficiency that makes their mobilization impossible to have harmful effects; these deficiencies in most cases are caused by diseases and accidents. Their recovery depends in most cases on surgical interventions and their subsequent treatment to recover mobility, based on physiotherapy and rehabilitation exercises. Rehabilitation treatments in adults are carried out with minimal problems due to the degree of awareness and the needs of the patient; the problem grows considerably when dealing with children, who in the development of rehabilitation exercises are required without considering the environment, which in most cases are not conditioned for them, so that its realization is in some traumatic cases and generates a negative reaction to these exercises; the proposed system manages to improve the environment for children to make their exercises and in particular at the moment of being able to evaluate the progress, with the use of recreational environments, the proposal is based on the use of video game technologies with a virtual reality approach based on the video game “Minecraft,” as mechanism of registration of the displacement at the moment of the march; an inertial navigation circuit is used to register the direction, displacement, deviation at the time the child walks interacting with the game, giving the feeling that the child is inside the video game; interaction and control is done wirelessly using a Bluetooth connection or a Wi-Fi connection depending on the distance between the child and the workstation. The results in the tests show that the child has better acceptance at the time he performs the rehabilitation exercises and at the time he moves to analyze the progress because it is part of the video game, as well as you can record the movements made and can graph it to assess its center of gravity and linearity in the displacement.}
}
@article{SCHIOPU2021101575,
title = {Virus tinged? Exploring the facets of virtual reality use in tourism as a result of the COVID-19 pandemic},
journal = {Telematics and Informatics},
volume = {60},
pages = {101575},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101575},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321000149},
author = {Andreea F. Schiopu and Remus I. Hornoiu and Mihaela A. Padurean and Ana-Maria Nica},
keywords = {VR use in tourism, Technology Acceptance Model (TAM), Perceived substitutability of VR},
abstract = {Several studies have investigated the use of virtual reality (VR) in tourism, but none has taken an epidemiological outlook. This research examined the use of VR in tourism through the lenses of an extended TAM model in times of COVID-19 pandemic. The premise was that, in this context, people would prefer less risky experiences and would see VR as a substitute for traditional travel. The data used was collected through a within-subjects experiment, which proved that intention to use VR in tourism increased under the COVID-19 effect. This study tested a conceptual model that showed this intention was influenced by the perceived ease of use, perceived usefulness, and perceived substitutability of VR, all mediated by people’s interest in VR use in tourism. The perceived authenticity of VR experience determined the perceived substitutability of VR. This paper has theoretical and practical implications. In the long term, promoting tourism-related VR activities might reduce the risk of virus spreading, lessen the pressure imposed on this sector by such epidemic episodes, and increase its sustainability.}
}
@article{FUSCO2023102212,
title = {Enhancing hurricane risk perception and mitigation behavior through customized virtual reality},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102212},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102212},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003403},
author = {Giovanna Fusco and Jin Zhu},
keywords = {Customized Virtual Reality, Hurricane, Residential Building, Risk Perception, Mitigation},
abstract = {Hurricanes are one of the most destructive natural disasters, and proper preparation and mitigation are essential to enhance resilience. However, homeowners may have inaccurate risk perceptions, resulting in inadequate mitigation decision-making. Virtual reality (VR) damage visualization has the potential to communicate hurricane risk effectively and improve disaster resilience. Previous studies utilizing VR for hurricanes have primarily focused on disaster response such as evacuation, and often presented generalized visualization consistent across all users. This study presents a methodology for offering individual users a customized tool to experience the impacts of hypothetical hurricane events on their residential structures and explore the effects of various building retrofit actions in customized 3D virtual environments. The methodology was tested with 25 participants. Participants experienced structural damage and utility disruptions caused by hurricanes in their own residential homes in a customized, immersive virtual environment. Participants’ pre- and post-VR risk perceptions, as well as their choices of mitigation actions were surveyed. The results indicate that customized VR was effective in enhancing risk awareness and that participants were able to reduce their structural damages in hypothetical hurricane events by adopting various mitigation measures after the customized VR experience. The outcomes suggest that customized visualization of potential damages in virtual environments can effectively help disaster preparation and mitigation.}
}
@article{GINNIS20101045,
title = {VELOS: A VR platform for ship-evacuation analysis},
journal = {Computer-Aided Design},
volume = {42},
number = {11},
pages = {1045-1058},
year = {2010},
note = {Computer aided ship design: Some recent results and steps ahead in theory, methodology and practice},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2009.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010448509002322},
author = {A.I. Ginnis and K.V. Kostas and C.G. Politis and P.D. Kaklis},
keywords = {Design for safety, Evacuation analysis, Ship-passenger safety, Virtual reality, Ship design},
abstract = {“Virtual Environment for Life On Ships” (VELOS) is a multi-user Virtual Reality (VR) system that aims to support designers to assess (early in the design process) passenger and crew activities on a ship for both normal and hectic conditions of operations and to improve ship design accordingly. This article focuses on presenting the novel features of VELOS related to both its VR and evacuation-specific functionalities. These features include: (i) capability of multiple users’ immersion and active participation in the evacuation process, (ii) real-time interactivity and capability for making on-the-fly alterations of environment events and crowd-behavior parameters, (iii) capability of agents and avatars to move continuously on decks, (iv) integrated framework for both the simplified and advanced method of analysis according to the IMO/MSC 1033 Circular, (v) enrichment of the ship geometrical model with a topological model suitable for evacuation analysis, (vi) efficient interfaces for the dynamic specification and handling of the required heterogeneous input data, and (vii) post-processing of the calculated agent trajectories for extracting useful information for the evacuation process. VELOS evacuation functionality is illustrated using three evacuation test cases for a ro–ro passenger ship.}
}
@article{JEON2019468,
title = {Three-dimensional virtual reality-based subjective evaluation of road traffic noise heard in urban high-rise residential buildings},
journal = {Building and Environment},
volume = {148},
pages = {468-477},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360132318306929},
author = {Jin Yong Jeon and Hyun In Jo},
keywords = {Road traffic noise, Annoyance, Head-related transfer function, Head-mounted display, Auditory test, Audio-visual factors},
abstract = {In this study, a new noise evaluation method is proposed in which virtual reality (VR) technology in the form of a head-related transfer function (HRTF) and a head-mounted display (HMD) is applied to assess the noise experienced inside residential buildings. Firstly, the sound pressure level and frequency characteristics of road traffic noise recorded in a living room were identified before and after applying the HRTF. Secondly, the subjective response was evaluated in terms of loudness, annoyance, disturbance, and allowance for road traffic noise levels LAeq of 40–65 dB in four different test environments, namely, without the HRTF or HMD, with the HRTF, with the HMD, and with both the HRTF and HMD. The impacts of the directional information provided by the HRTF and the visual information provided by the HMD were 77% and 23%, respectively. Furthermore, based on an evaluation of the source- and environment-related spatial attributes, the recognition of road traffic noise was the highest with the HRTF, and the recognition of space was the highest with the HMD. When both the HRTF and HMD were applied, the influence of sound externalization in the VR environment increased along with the sense of immersion and realism. Moreover, the recognition of the noise direction and noise width played a significant role in the psycho-acoustic impact, including the annoyance level of the road traffic noise. The findings of this study provide new insights on the influence of external noise on indoor environments.}
}
@article{VORRABER2020139,
title = {Assessing augmented reality in production: remote-assisted maintenance with HoloLens},
journal = {Procedia CIRP},
volume = {88},
pages = {139-144},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303401},
author = {Wolfgang Vorraber and Johannes Gasser and Helena Webb and Dietmar Neubacher and Philipp Url},
keywords = {Augmented reality, Industry 4.0, Information systems, Service design, Digitalisation},
abstract = {Digitalisation provides many opportunities for process improvements by using smart devices such as Augmented-reality (AR) displays in production. Nevertheless, in-depth studies about the effects of these applications on key performance process indicators are sparse. The goal of this study is to evaluate AR-based remote maintenance processes supported by cutting edge optical head-mounted display technology. A total of 12 test runs conducted with real-world maintenance engineers in a real-world industry setting were analysed based on video recordings and semi-structured interviews. The study identified both case-specific and general issues, and the inherent potential of the technology.}
}
@article{URL20194,
title = {Practical Insights On Augmented Reality Support for Shop-Floor Tasks},
journal = {Procedia Manufacturing},
volume = {39},
pages = {4-12},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.222},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920302663},
author = {Philipp Url and Wolfgang Vorraber and Johannes Gasser},
keywords = {Augmented Reality, Information Systems, Production, Socio-Technical Systems, Service Design},
abstract = {The use of Augmented Reality (AR) to improve processes in production has a long history and there are various established use cases in industry and research. Nevertheless, barriers to its seamless integration in everyday shop-floor processes still remain. A holistic approach grounded on socio-technical systems theory and a practical use case in a real-world production environment serve to identify the barriers and to provide recommendations for implementing AR in shop-floor processes. As a starting point, an overview of use cases in industry and research is provided focusing on how AR is used to support shop-floor tasks. Based on this, a practical use case, where maintenance engineers were guided by a head-mounted AR device (Microsoft HoloLens), was implemented to gain insights into the current AR-technology. An experiment was carried out consisting of twelve test runs with six maintenance engineers in a real-world production setting in the automotive industry. Semi-structured interviews with the test persons and video recordings of the tests serve as a basis for data analysis. The results consist primarily of specific use case feedback from test persons and video-based observation findings. Based on these findings, we derived recommendations to seamlessly integrate AR into production processes. These results are structured around enterprise IT infrastructure, AR-technology, shop-floor process related and human-centered aspects.}
}
@article{OUALI2020602,
title = {A New Architecture based AR for Detection and Recognition of Objects and Text to Enhance Navigation of Visually Impaired People},
journal = {Procedia Computer Science},
volume = {176},
pages = {602-611},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031886X},
author = {Imene OUALI and Mohamed Saifeddine {HADJ SASSI} and Mohamed BEN HALIMA and Ali WALI},
keywords = {Augmented reality, Vuforia, Usability, Visually impaired, Objects, Text, Detection, Recognition},
abstract = {Navigation in unfamiliar places is a big challenge for partially sighted and visually impaired people. Improving visual information on the location and content of objects such as drugs can help navigation in unfamiliar environments. There are several existing navigation solutions capable of helping these people. However, navigation solutions are rarely adopted and implemented in reality. In order to optimize the perception of digital information of objects from sensors and to naturally interact with the pervasive computing landscape, Augmented Reality (AR) equipment has to be seamlessly integrated into the user’s environment. For this purpose, we develop an architecture of text and objects recognition based on AR in order to assist partially sighted and visually impaired people. Such architecture makes navigation easier in the environment by helping users to find drugs and to verify the number of pills using speech. The proposed architecture uses context-aware mobile computing and shows great potential to integrate AR for object recognition through AR engines.}
}
@article{OJADOSGONZALEZ2017111,
title = {Development and assessment of a tractor driving simulator with immersive virtual reality for training to avoid occupational hazards},
journal = {Computers and Electronics in Agriculture},
volume = {143},
pages = {111-118},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917308219},
author = {D. {Ojados Gonzalez} and B. Martin-Gorriz and I. {Ibarra Berrocal} and A. {Macian Morales} and G. {Adolfo Salcedo} and B. {Miguel Hernandez}},
keywords = {Tractor safety, Overturn, ROPS, Injury, Safety devices},
abstract = {Tractor overturns are the leading cause of fatalities in the agricultural sector. When drivers misuse the foldable roll over protective structure (ROPS) in tractors, it becomes highly inefficient as a rollover protection system. To solve this problem, the purpose of the present paper is to detail the development and assessment of a tractor driving simulator with immersive virtual reality for training to minimize this risk. In the agricultural sector, tractor driving simulators make it possible to train drivers in risk situations that are not feasible in the real field due to the high risk of roll over. The simulator includes a motion platform for this particular application. The findings of this study suggest that participants with safety knowledge make fewer errors in deploying the ROPS. To reduce the consequences of tractor accidents in the agricultural sector, the promotion of training courses is essential to avoid the misuse of the ROPS. On the contrary, the perception of risk and safety increased after the tractor driving simulator experience for all of the participants but increased significantly more so for non-frequent users of tractors. All of the groups of participants reported that the use of the tractor driving simulator was a positive experience because it can help them to drive more safely, and they feel that they need more training programmes in occupational safety.}
}
@article{HEEMSKERK20112082,
title = {Extending Virtual Reality simulation of ITER maintenance operations with dynamic effects},
journal = {Fusion Engineering and Design},
volume = {86},
number = {9},
pages = {2082-2086},
year = {2011},
note = {Proceedings of the 26th Symposium of Fusion Technology (SOFT-26)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2011.04.066},
url = {https://www.sciencedirect.com/science/article/pii/S0920379611004625},
author = {C.J.M. Heemskerk and M.R. {de Baar} and H. Boessenkool and B. Graafland and M.J. Haye and J.F. Koning and M. Vahedi and M. Visser},
keywords = {Maintenance, Remote Handling, Virtual Reality, ITER},
abstract = {Virtual Reality (VR) simulation can be used to study, improve and verify ITER maintenance operations during preparation. VR can also improve the situational awareness of human operators during actual Remote Handling (RH) operations. Until now, VR systems use geometric models of the environment and the objects being handled and kinematic models of the manipulation systems. The addition of dynamic effects into the VR simulation was investigated. Important dynamic effects are forces due to contact transitions and the bending of beams under heavy loads. A novel dynamics simulation module was developed and introduced as an add-on to the VR4Robots VR software. Tests were performed under simplified test conditions and in the context of realistic ITER maintenance tasks on a benchmark product and on the ECRH Upper Port Launcher Plug (UPL). The introduction of dynamic effects into VR simulations was found to add realism and provide new insights in procedure development. The quality of the haptic feedback depends strongly on the haptic device used to “display” haptic feedback to the operator. Dynamic effect simulation can also form the basis for real-time guidance support to operators during the execution of maintenance tasks (augmented reality).}
}
@article{MEDELLINCASTILLO201646,
title = {The evaluation of a novel haptic-enabled virtual reality approach for computer-aided cephalometry},
journal = {Computer Methods and Programs in Biomedicine},
volume = {130},
pages = {46-53},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715300055},
author = {H.I. Medellín-Castillo and E.H. Govea-Valladares and C.N. Pérez-Guerrero and J. Gil-Valladares and Theodore Lim and James M. Ritchie},
keywords = {Cephalometry, Haptic technologies, Cephalometric values, Landmark selection, Task completion time (TCT)},
abstract = {Background and objective
In oral and maxillofacial surgery, conventional radiographic cephalometry is one of the standard auxiliary tools for diagnosis and surgical planning. While contemporary computer-assisted cephalometric systems and methodologies support cephalometric analysis, they tend neither to be practical nor intuitive for practitioners. This is particularly the case for 3D methods since the associated landmarking process is difficult and time consuming. In addition to this, there are no 3D cephalometry norms or standards defined; therefore new landmark selection methods are required which will help facilitate their establishment. This paper presents and evaluates a novel haptic-enabled landmarking approach to overcome some of the difficulties and disadvantages of the current landmarking processes used in 2D and 3D cephalometry.
Method
In order to evaluate this new system's feasibility and performance, 21 dental surgeons (comprising 7 Novices, 7 Semi-experts and 7 Experts) performed a range of case studies using a haptic-enabled 2D, 2½D and 3D digital cephalometric analyses.
Results
The results compared the 2D, 2½D and 3D cephalometric values, errors and standard deviations for each case study and associated group of participants and revealed that 3D cephalometry significantly reduced landmarking errors and variability compared to 2D methods.
Conclusions
Through enhancing the process by providing a sense of touch, the haptic-enabled 3D digital cephalometric approach was found to be feasible and more intuitive than its counterparts as well effective at reducing errors, the variability of the measurements taken and associated task completion times.}
}
@article{CHOI2023107644,
title = {A single stage knowledge distillation network for brain tumor segmentation on limited MR image modalities},
journal = {Computer Methods and Programs in Biomedicine},
volume = {240},
pages = {107644},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107644},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723003097},
author = {Yoonseok Choi and Mohammed A. Al-masni and Kyu-Jin Jung and Roh-Eul Yoo and Seong-Yeong Lee and Dong-Hyun Kim},
keywords = {Brain tumor segmentation, Missing modality, Knowledge distillation, Barlow twins, nnU-Net, Glioblastoma},
abstract = {Background and objective
Precisely segmenting brain tumors using multimodal Magnetic Resonance Imaging (MRI) is an essential task for early diagnosis, disease monitoring, and surgical planning. Unfortunately, the complete four image modalities utilized in the well-known BraTS benchmark dataset: T1, T2, Fluid-Attenuated Inversion Recovery (FLAIR), and T1 Contrast-Enhanced (T1CE) are not regularly acquired in clinical practice due to the high cost and long acquisition time. Rather, it is common to utilize limited image modalities for brain tumor segmentation.
Methods
In this paper, we propose a single stage learning of knowledge distillation algorithm that derives information from the missing modalities for better segmentation of brain tumors. Unlike the previous works that adopted a two-stage framework to distill the knowledge from a pre-trained network into a student network, where the latter network is trained on limited image modality, we train both models simultaneously using a single-stage knowledge distillation algorithm. We transfer the information by reducing the redundancy from a teacher network trained on full image modalities to the student network using Barlow Twins loss on a latent-space level. To distill the knowledge on the pixel level, we further employ a deep supervision idea that trains the backbone networks of both teacher and student paths using Cross-Entropy loss.
Results
We demonstrate that the proposed single-stage knowledge distillation approach enables improving the performance of the student network in each tumor category with overall dice scores of 91.11% for Tumor Core, 89.70% for Enhancing Tumor, and 92.20% for Whole Tumor in the case of only using the FLAIR and T1CE images, outperforming the state-of-the-art segmentation methods.
Conclusions
The outcomes of this work prove the feasibility of exploiting the knowledge distillation in segmenting brain tumors using limited image modalities and hence make it closer to clinical practices.}
}
@article{ZHENG2017327,
title = {Evaluation and comparison of 3D intervertebral disc localization and segmentation methods for 3D T2 MR data: A grand challenge},
journal = {Medical Image Analysis},
volume = {35},
pages = {327-344},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2016.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1361841516301530},
author = {Guoyan Zheng and Chengwen Chu and Daniel L. Belavý and Bulat Ibragimov and Robert Korez and Tomaž Vrtovec and Hugo Hutt and Richard Everson and Judith Meakin and Isabel Lŏpez Andrade and Ben Glocker and Hao Chen and Qi Dou and Pheng-Ann Heng and Chunliang Wang and Daniel Forsberg and Aleš Neubert and Jurgen Fripp and Martin Urschler and Darko Stern and Maria Wimmer and Alexey A. Novikov and Hui Cheng and Gabriele Armbrecht and Dieter Felsenberg and Shuo Li},
keywords = {Intervertebral disc, MRI, Localization, Segmentation, Challenge, Evaluation},
abstract = {The evaluation of changes in Intervertebral Discs (IVDs) with 3D Magnetic Resonance (MR) Imaging (MRI) can be of interest for many clinical applications. This paper presents the evaluation of both IVD localization and IVD segmentation methods submitted to the Automatic 3D MRI IVD Localization and Segmentation challenge, held at the 2015 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI2015) with an on-site competition. With the construction of a manually annotated reference data set composed of 25 3D T2-weighted MR images acquired from two different studies and the establishment of a standard validation framework, quantitative evaluation was performed to compare the results of methods submitted to the challenge. Experimental results show that overall the best localization method achieves a mean localization distance of 0.8 mm and the best segmentation method achieves a mean Dice of 91.8%, a mean average absolute distance of 1.1 mm and a mean Hausdorff distance of 4.3 mm, respectively. The strengths and drawbacks of each method are discussed, which provides insights into the performance of different IVD localization and segmentation methods.}
}
@article{GONG2017336,
title = {A Novel VR Tool for Collaborative Planning of Manufacturing Process Change using Point Cloud Data},
journal = {Procedia CIRP},
volume = {63},
pages = {336-341},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117302354},
author = {Liang Gong and Jonatan Berglund and Dennis Saluäär and Björn Johansson},
keywords = {Point Cloud, Virtual Reality, Manufacturing Process Change},
abstract = {Today, manufacturing industry is facing increasing demands of customized products and global competition. Companies need constant and efficient changes in manufacturing process to meet the challenges and to stay competitive. However, a successful manufacturing process change is not easy to accomplish due to the fact that any change in the manufacturing system will affect various actors involved. Previous research has shown that active engagement of all involved actors in the planning phase improves the quality and success rate of the manufacturing process change outcome. The conventional supporting tools used mostly in this process such as documentation tools for text, numbers and static pictures typically requires an experienced user to be fully understood. Thus, some of the involved actors are not able to participate in the manufacturing process change (MPC) on equal terms. Over the last decade, the advancement of virtual technologies has shown the potential to improve the quality and efficiency of the planning of MPCs. Specifically, 3D laser scanning technologies can produce realistic virtual representation of factory environment with rapid point cloud data capturing. Immersive experience with rich context in combination with e.g. the virtual reality head-mounted display (VR HMD) could be provided, which is beneficial in the assessment and usable for several actors. This paper presents a novel supporting tool for the MPC design and planning, which incorporates point cloud data of real-world truck factories visualized using VR HMD technologies. It provides a collaborative and immersive environment for all involved actors to actively contribute in the MPC process. Tests and interviews have been conducted with various actors from industry for evaluation and validation of the proposed tool and its findings. The test data were analyzed and discussed with regard to the benefits and problems found as well as potential for future research studies and development.}
}
@article{CHOI2023102220,
title = {Intracranial steno-occlusive lesion detection on time-of-flight MR angiography using multi-task learning},
journal = {Computerized Medical Imaging and Graphics},
volume = {107},
pages = {102220},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102220},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000381},
author = {Dongjun Choi and Tackeun Kim and Jinhee Jang and Leonard Sunwoo and Kyong Joon Lee},
keywords = {Intracranial artery, Magnetic resonance angiography, Lesion detection, Multi-task learning},
abstract = {Steno-occlusive lesions in intracranial arteries refer to segments of narrowed or occluded blood vessels that increase the risk of ischemic strokes. Steno-occlusive lesion detection is crucial in clinical settings; however, automatic detection methods have hardly been studied. Therefore, we propose a novel automatic method to detect steno-occlusive lesions in sequential transverse slices on time-of-flight magnetic resonance angiography. Our method simultaneously detects lesions while segmenting blood vessels based on end-to-end multi-task learning, reflecting that the lesions are closely related to the connectivity of blood vessels. We design classification and localization modules that can be attached to arbitrary segmentation network. As blood vessels are segmented, both modules simultaneously predict the presence and location of lesions for each transverse slice. By combining outputs from the two modules, we devise a simple operation that boosts the performance of lesion localization. Experimental results show that lesion prediction and localization performance is improved by incorporating blood vessel extraction. Our ablation study demonstrates that the proposed operation enhances lesion localization accuracy. We also verify the effectiveness of multi-task learning by comparing our approach with those that individually detect lesions with extracted blood vessels.}
}
@article{LIN2002333,
title = {An eye behavior measuring device for VR system},
journal = {Optics and Lasers in Engineering},
volume = {38},
number = {6},
pages = {333-359},
year = {2002},
issn = {0143-8166},
doi = {https://doi.org/10.1016/S0143-8166(02)00034-9},
url = {https://www.sciencedirect.com/science/article/pii/S0143816602000349},
author = {Chern-Sheng Lin},
keywords = {Eye-tracking device, Pattern recognition, The position of the pupil},
abstract = {This work presents an eye-tracking and pupil size-measuring device that interfaces with a computer for applications useful in psychometry, ophthalmology, physiology and virtual reality (VR) systems. This system utilizes a change-coupled device (CCD) camera, appropriate lenses, PC with frame grabber and a DSP unit with various types of VR equipment, i.e., HMD, simulator or LCD projection device. The digital signal processing unit is used to calculate the average brightness and contrast of the VR video image. A CCD camera with various attachments can be mounted on various VR systems to capture the human eye image for testing. An image capture card and a personal computer are used to analyze the test image. From the eye digital image, the computer obtains data on the pupil size and a trace of the tested eye. A pattern recognition computer program and five measurement parameters are used to distinguish the position of the pupil, calculate the pupil location coordinate and analyze the physical conditions of the user. These data can be plotted against the average brightness and contrast of the VR video image in real time. This information is shown on the screen of a personal computer and used for cross-link analysis. This eye-tracking interface can determine the position of a subject's pupil and map that position into a display point on a computer screen. The pupil size and location data versus the average brightness and contrast of a VR video image are computed in real time.}
}
@article{SUN2023107814,
title = {Like being there: How the soft VR technology could change consumers’ initial decision-making},
journal = {Computers in Human Behavior},
volume = {146},
pages = {107814},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107814},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001656},
author = {Yi Sun and Yaobin Lu and Bin Wang and Weiguo (Patrick) Fan},
keywords = {Virtual reality, Soft VR, Metaverse, Initial decision, Decision-making},
abstract = {With the sweeping wave of digital transformation, Virtual Reality (VR) technology has become one of the key technologies for building the metaverse with its characteristic of lifting the physical interaction restrictions between people and things, which has attracted the extensive attention of scholars and practitioners. However, the VR industry is gradually evolving from traditional device-based hard VR to soft VR, where the market is larger but less researched. Additionally, although most businesses have high expectations for the potential of VR technology, little is known about whether a huge investment in VR will deliver the expected returns. The objective of this study is to explore whether and how soft VR technology can play a significant positive role in promoting consumers' initial decision-making. This study collected and empirically analyzed the data of 51,495 online second-hand condominiums on an online platform for housing transactions and services. The results show that the soft VR technology can significantly increase consumers' attention to the product and prompt their initial decision-making. Moreover, it is interesting that the enhancement effect of soft VR technology will show an equilibrating trend due to the different features of the search products. This study contributes to scholarship and practice by initiating the study of the emerging phenomenon of soft VR and empirically testing its effect on consumers' initial decision-making.}
}
@article{CHEN2021103631,
title = {Development of BIM, IoT and AR/VR technologies for fire safety and upskilling},
journal = {Automation in Construction},
volume = {125},
pages = {103631},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000820},
author = {Haosen Chen and Lei Hou and Guomin (Kevin) Zhang and Sungkon Moon},
keywords = {Building fires, Situational awareness, Fire safety, IoT, BIM, VR, AR},
abstract = {High-rise building fires can pose a significant threat to occupants and firefighters. The state-of-the-art technologies such as sensor-based Internet of Things (IoT), Building Information Modelling (BIM), Virtual Reality (VR) and Augmented Reality (AR) may offer great potential to improve building fire safety and rescue efficiency, primarily through ameliorating the level of situational awareness. This study proposes an innovative technology integrated framework for prototyping a proof-of-concept BIM, IoT and AR/VR system based upon the rationale of situational awareness. A pilot test based on a simulated fire scenario is conducted to evaluate the functionality of the framework. The outcomes reveal that the data generated by the system can be leveraged by the firefighting department to quickly locate the whereabouts of the indoor fires, and the VR gamification scenarios can expedite the development of situational awareness for the trainees. The limitations and future works are also discussed at the end of this paper.}
}
@article{SEGOVIA2015195,
title = {Machining and Dimensional Validation Training Using Augmented Reality for a Lean Process},
journal = {Procedia Computer Science},
volume = {75},
pages = {195-204},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036996},
author = {Daniel Segovia and Hector Ramírez and Miguel Mendoza and Manuel Mendoza and Eloy Mendoza and Eduardo González},
keywords = {Augmented Reality, FARO Gage, Workshop Training, Dimensional Validation, Mobile Device, HMD Plant Managers},
abstract = {Quality control does not concern only to a finished product, nowadays measuring technologies control all the fabrication process in an active way. If product quality does not meet the customer specifications exists the risk of even lose projects. When multiple or complex operations are required in the fabrication process of a part and its dimensional validation is primordial and also happens that the operator has doubts about which step to follow or that the expert supervisor is not at the workshop for some reason, that is why comes the opportunity of implementing Augmented Reality (A.R.) technology in education and in a lean manufacturing process, as support for technician operators of machine-tools and coordinate-measuring machines, with the objective of helping the technician to perform, in a proper and timely manner, the step sequence of the process. It was proved that having A.R. as ally in the technician's education, the mistakes and time required to fulfill the machining and dimensioning of a part can be considerably reduced, as well as eliminate operator's dependency to the experts. At the end of the implementation, savings were found in the three stages analyzed, generating 27.36% savings in lathe process, 26.54% in milling and 45.16% in dimensional validation.}
}
@article{HUANG2021121878,
title = {Anti-rutting performance of the damping asphalt mixtures (DAMs) made with a high content of asphalt rubber (AR)},
journal = {Construction and Building Materials},
volume = {271},
pages = {121878},
year = {2021},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2020.121878},
url = {https://www.sciencedirect.com/science/article/pii/S0950061820338824},
author = {Jiandong Huang and Jia Zhang and Jiaolong Ren and Haiwei Chen},
keywords = {Damping asphalt mixtures, Hamburg wheel tracking, Rutting, Dynamic modulus, Indirect tensile strength},
abstract = {Damping asphalt mixtures (DAMs) have been developed to resist vibration and noise caused by traffic loads, and the ultimate design goal in this process is to increase damping. However, while optimizing its damping characteristics, its ability to resist rutting may face challenges. In the present study, two DAMs are designed based on the open-graded (OG) aggregate structure, and the mechanical properties and resistance to rutting are evaluated. Considering that DAMs are often laid as an intermediate part of the asphalt concrete (AC) layer, a new sample-preparing method based on Superpave Gyration Compactor (SGC) is proposed, and Hamburg wheel tracking (HWT) experimental tests are performed on the prepared samples. Digital image processing (DIP) is used before and after the HWT tests to analyze the mechanism of DAMs in rutting resistance. Results show that DAMs meet the requirements of conventional mechanical properties and have high damping characteristics. Working as an interlayer in the HWT sample, DAMs have better rutting resistance than traditional mixtures due to its denser characteristics, sufficient mechanical strength, and lower water sensitivity.}
}
@article{LONGO2022594,
title = {An ontology-based, general-purpose and Industry 4.0-ready architecture for supporting the smart operator (Part I – Mixed reality case)},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {594-612},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001303},
author = {Francesco Longo and Giovanni Mirabelli and Letizia Nicoletti and Vittorio Solina},
keywords = {Mixed reality, Ontology, Internet of things, Smart operator, Smart factory},
abstract = {The advent of novel industry 4.0-driven technologies is offering significant opportunities to manufacturing systems, but at the same time it is posing new great challenges. The growing number of connected and interconnected devices is enormously increasing the amount of data generated, which must be properly organized to give value to the business. Basically, the need for approaches that are able to guarantee compliance with FAIR data principles is significantly emerging. Recently, the KNOW4I platform has been proposed in the literature to support the smart operator through a suite of Smart Utilities and Objects (Longo et al., 2022). The main purpose of this paper is to extend such platform, in the form of an ontology-based, general-purpose and industry 4.0-ready architecture, capable of improving the capabilities of the smart operator, with a focus on mixed reality. The novel proposal is based on two fundamental aspects: (1) a new general ontology, developed through the ontology engineering methodology; (2) the adoption of FIWARE, an open-source infrastructure, capable of enabling interoperability between different systems. The proposed architecture is implemented and validated on two case studies belonging to the manufacturing sector, which respectively concern (1) scheduled maintenance and alarm management and (2) customer order management. The experimental phase shows that the architecture is able to effectively and efficiently support the smart operator.}
}
@article{SOOD2023706,
title = {Classification and Pathologic Diagnosis of Gliomas in MR Brain Images},
journal = {Procedia Computer Science},
volume = {218},
pages = {706-717},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000510},
author = {Meenakshi Sood and Shruti Jain and Jyotsna Dogra},
keywords = {Glioma, Accuracy, High Grade Glioma, Low Grade Glioma, MRI, Graph-Cut},
abstract = {Detection and classification of a brain tumour in medical images is always a challenging task, as the treatment may lead to Radiosurgery depending upon the exact shape, size, and position of a tumour. To provide a complete characterization of glioma and the degree of malignancy, classification and grading are vital. In some complicated cases, localization of the tumour, comparison of tumour tissues with adjacent regions, to make it clear for detection, and finally classification without human intervention is need of the hour. Out of the available numerous imaging techniques to detect and classify brain tumours, MRI is the most suitable non-invasive technique and has superior image quality. The edge over other techniques is its superior soft-tissue resolution and the ability to acquire different images employing contrast-enhanced agents. This research work classifies Low-Grade Glioma (LGG) and High-Grade Glioma(HGG) tumours on the basis of features obtained from extracted tumor region. The results of the classification of HGG and LGG tumours using the most prominent features are validated by five-fold cross-validation. Satisfactory and encouraging classification results are obtained using large and publicly available clinical datasets. Using the extracted features, a computer-assisted algorithm is developed that uses a machine-learning algorithm along with Gradient-Based Kernel Selection Graph Cut to provide binary classification with an accuracy of 94.6% with T1ce sequence of MRI. The proposed models can be employed to assist physicians and radiologists in the early pathological detection and classification of gliomas. The proposed framework also can be a model for validating brain tumours and their initial screening for their grading classification.}
}
@article{ZIAEI20112033,
title = {Real-time markerless Augmented Reality for Remote Handling system in bad viewing conditions},
journal = {Fusion Engineering and Design},
volume = {86},
number = {9},
pages = {2033-2038},
year = {2011},
note = {Proceedings of the 26th Symposium of Fusion Technology (SOFT-26)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2010.12.082},
url = {https://www.sciencedirect.com/science/article/pii/S0920379611000160},
author = {Z. Ziaei and A. Hahto and J. Mattila and M. Siuko and L. Semeraro},
keywords = {Augmented Reality, Remote Handling, Template based matching, CAD-model, Position, Orientation},
abstract = {Remote Handling (RH) in harsh environments usually has to tackle the lack of sufficient visual feedback for the human operator due to the limited number of on-site cameras, the not optimized position of the cameras, the poor viewing angles, occlusion, failure, etc. Augmented Reality (AR) enables the user to perceive virtual computer-generated objects in a real scene. The most common goals usually include visibility enhancement and provision of extra information, such as positional data of various objects. The proposed AR system first recognizes and locates the markerless object by using a template based matching algorithm, and then augments the virtual model on top of the recognized item. The tracking algorithm is exploited for locating the object in a continuous sequence of frames. Conceptually, the template is found by computing the similarity between the template and the image frame, for all the relevant template poses (rotation and translation). As a case study, AR interface was displaying measured orientation and transformation of the Water Hydraulic Manipulator (WHMAN) Divertor preloading tool, in near real-time tracking. The bad viewing condition implies on the case when the view angle is such that the interesting features of the object are not in the field of view. The method in this paper was validated in concrete operational context at DTP2. The developed method proved to deliver robust positional and orientation information while augmenting and tracking the moving tool object.}
}
@article{LIU2023102966,
title = {A novel MR-conditional cam-based automatic prostate biopsy device},
journal = {Mechatronics},
volume = {91},
pages = {102966},
year = {2023},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2023.102966},
url = {https://www.sciencedirect.com/science/article/pii/S0957415823000223},
author = {Rongrong Liu and Farrukh Anique and Rongwan Chen and Sung Suk Oh and Jung Ki Jo and Seong Young Ko},
keywords = {MR conditional, Prostate biopsy, Medical robotics},
abstract = {Biopsy is the most commonly used method for diagnosing prostate cancer, in which a biopsy gun is used to obtain the tissue samples. The outstanding performance of magnetic resonance imaging (MRI) for soft tissue can provide better medical guidance for prostate biopsy. This paper reports an automatic biopsy device that can be used in the MRI environment and that is expected to be used together with the MR conditional robotic prostate biopsy system. This paper proposes a compact cam-based automatic biopsy device that can perform all actions required for prostate biopsy sampling procedure using a single actuator. Its design concept, implementation, required force analysis, sampling performance evaluation, and MR compatibility evaluation are also provided. The experimental evaluation showed that the tissue sampling process can be easily and completely performed using the proposed automatic biopsy device, and that its sampling performance is comparable to that of the existing commercially available manual biopsy gun. MR compatibility test showed that an 18.7% reduction in the signal-to-noise ratio when the device was operating.}
}
@article{ZULFIQAR2023104777,
title = {Multi-class classification of brain tumor types from MR images using EfficientNets},
journal = {Biomedical Signal Processing and Control},
volume = {84},
pages = {104777},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.104777},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423002100},
author = {Fatima Zulfiqar and Usama {Ijaz Bajwa} and Yasar Mehmood},
keywords = {Brain tumor classification, Magnetic resonance imaging (MRI), Convolutional neural networks, Transfer learning, Fine-tuning, EfficientNets},
abstract = {Accurate classification of the type of brain tumor plays an important role in the early diagnosis of the tumor which can be the difference between life and death. Magnetic Resonance Imaging (MRI) is commonly used to capture high-contrast grayscale images of the brain and is a non-invasive method for brain tumor diagnosis. Deep Learning (DL) using Convolutional Neural Networks (CNN) has revolutionized Computer Aided Diagnostic (CAD) systems by producing remarkable results for various medical imaging analysis tasks including brain tumor detection. In this research, a transfer learning-based fine-tuning approach for the classification of brain tumors into three classes i.e. glioma, meningioma, and pituitary tumor using EfficientNets is carried out. Five variants of pre-trained models from the EfficientNets family i.e. EfficientNetB0 – EfficientNetB4 are fine-tuned on publically available CE-MRI “Figshare – Brain Tumor Dataset”. The proposed method of fine-tuning pre-trained EfficientNet is carried out by first loading ImageNet weights to the EfficientNet model followed by the addition of several top layers and a fully connected layer for the classification of brain tumor types. Several experiments are carried out to analyze the robustness of the proposed fine-tuned EfficientNets in comparison to other pre-trained models. Moreover, the effect of data augmentation on the model’s test accuracy is also studied. Finally, the Grad-CAM visualization of the attention maps of the best model is presented that successfully highlights the tumorous region of the brain cell. The proposed method of fine-tuning pre-trained EfficientNet showed significant performance using EfficientNetB2 as a backbone achieving overall test accuracy, precision, recall/sensitivity, and F1-score of 98.86%, 98.65%, 98.77%, and 98.71% respectively. The proposed fine-tuned EfficientNetB2 is lightweight, computationally inexpensive (during training and inference), and generalizes well. The source code is available at https://github.com/FatimaZulfiqar/multi-class-brain-tumor-classification.}
}
@article{YABUKI2011228,
title = {An invisible height evaluation system for building height regulation to preserve good landscapes using augmented reality},
journal = {Automation in Construction},
volume = {20},
number = {3},
pages = {228-235},
year = {2011},
note = {Augmented and Virtual Reality in Architecture, Engineering and Construction (CONVR2009)},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2010.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0926580510002062},
author = {Nobuyoshi Yabuki and Kyoko Miyashita and Tomohiro Fukuda},
keywords = {Augmented reality, Landscape preservation, Invisible height, Building height regulation},
abstract = {Preserving good landscapes such as historical buildings, temples, shrines, churches, bridges, and natural sceneries is important. Recently, destruction of good landscapes from multiple viewpoints due to construction of high rise buildings has been reported. To regulate structures such as high rise buildings and high voltage transmission towers, the government or public agencies have established or are going to constitute height regulations for building and structures surrounding the location of interest. In order to check whether some portions of high structures are visible or not behind the locations of interest from multiple viewpoints, it is necessary to make a 3D model representing geography, existing structures and natural objects using 3D CAD or Virtual Reality (VR) software. However, it usually takes much time and cost to make such a 3D model. Thus, in this research, we propose a new method using Augmented Reality (AR). In this method, a number of 3D virtual rectangular objects with a scale are located on the grid of 3D geographical model. And then, the CG models are displayed in an overlapping manner with the actual landscape from multiple viewpoints using the AR technology. The user measures the maximum invisible height for each rectangular object at a grid point. Using the measured data, the government or public agencies can establish appropriate height regulation for all surrounding areas of the locations of interest. To verify the proposed method, we developed a system deploying ARToolKit and applied it to the Convention Center of Osaka University, deemed as a scenic building. We checked the performance of the system and evaluated the error of the obtained data. In conclusion, the proposed method was evaluated feasible and effective.}
}
@article{PARK201361,
title = {A framework for proactive construction defect management using BIM, augmented reality and ontology-based data collection template},
journal = {Automation in Construction},
volume = {33},
pages = {61-71},
year = {2013},
note = {Augmented Reality in Architecture, Engineering, and Construction},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2012.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0926580512001598},
author = {Chan-Sik Park and Do-Yeop Lee and Oh-Seong Kwon and Xiangyu Wang},
keywords = {Construction defect management, Augmented reality, Ontology, Data collection template, BIM},
abstract = {In construction process, defects occur inevitably and repeatedly. It is one of the primary causes of project schedule and cost overruns. Many studies on defect causation analysis and management system have been conducted to facilitate defect measures and rectifications as well as to reduce the reoccurrence of the defect. However, those studies did not sufficiently consider the relationship of defect information flow in the construction process, which resulted in reactive rather than proactive defect management plan. This paper investigates the issues and needs of current defect management practices in the construction industry. It also presents a conceptual system framework for construction defect management that integrates ontology and augmented reality (AR) with building information modeling (BIM). The following three main technical solutions are proposed in the system framework: 1) defect data collection template to assure data quality and accuracy; 2) defect domain ontology to search and retrieve project or work-specific defect information; and 3) AR-based Defect Inspection System to support field defect management. It is envisaged that the system framework and solutions could enable proactive reduction of the defect occurrence during the construction process and that could greatly improve current defect management practices in the construction industry.}
}
@article{MASAD20191027,
title = {Automated measurements of lumbar lordosis in T2-MR images using decision tree classifier and morphological image processing},
journal = {Engineering Science and Technology, an International Journal},
volume = {22},
number = {4},
pages = {1027-1034},
year = {2019},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2215098618321414},
author = {Ihssan S. Masad and Amjed Al-Fahoum and Isam Abu-Qasmieh},
keywords = {Spine, Lumbar lordosis curvature, T2-MRI, Data mining, Decision trees, Texture features, Image segmentation, Morphological image processing, Cobb angle},
abstract = {Lumbar spine’s lordosis is a very important parameter functionally and clinically; it is a key feature in maintaining the sagittal balance, in addition to its crucial role in evaluating the spinal deformities. The main objective of the current study is to present a fully-automated measurement of the lumbar spine’s lordotic curve angle in T2-MR images. This goal has been achieved by the automatic measurement of lordosis radius at the lumbar spine level by computer-aided methods utilizing data mining classification and image segmentation followed by morphological image processing. The spine has been segmented from the entire image using a machine-learning technique that is based on texture features for recognizing the lumbar-spine pattern. The extracted features were fed to C4.5 decision tree classifier for designing the lumbar-spine recognition system. The resultant classifier’s “if-then” rules have been employed for segmenting the spine region from the entire image. Multiple morphological image processes have been applied to the raw segmentation result to enhance the true positive rate and suppressing the false positive rate. The mean radius of lumbar spine’s curvature has been evaluated by fitting the contours average to the closest circle using least-square fitting algorithm which was followed by calculating the lumbar lordosis curvature angle. The proposed approach has been tested and validated on normal and pathological T2-MR spine images and found to perform effectively. The calculation of lumbar lordosis angles showed a strong correlation with the Cobb angle measurements (R = 93.2%).}
}
@article{MARTIN2024117164,
title = {The examination of operator performance when controlling a shipboard crane anti-sway control system within a virtual-reality simulator},
journal = {Ocean Engineering},
volume = {298},
pages = {117164},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117164},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824005018},
author = {Iain A. Martin and Rishad A. Irani},
keywords = {Anti-sway control, Shipboard crane, Simulator, Human factors, Operator-in-the-loop, Virtual reality},
abstract = {Anti-sway control systems are valuable tools for cranes to prevent unexpected payload sway and undesired motion. However, for shipboard cranes, where the operator moves with the ship, anti-sway control systems can result in significant relative motion between the payload and operator, particularly in rough seas while attempting to align the payload with an ocean-frame target. Therefore, an important question to ask is, do operators actually find anti-sway systems intuitive, or do they feel they have to “fight” the system to achieve their desired performance? To address the question, this paper presents a human factors study designed to evaluate the effectiveness of a shipboard crane anti-sway system with an operator-in-the-loop. Participants completed a series of tests in a virtual-reality simulator, in which they attempted to align the payload of a nine degree-of-freedom shipboard knuckle boom crane with targets in both the ocean/world coordinate frame and ship deck coordinate frame, using an anti-sway system that provided complete motion compensation in both coordinate frames. The study found that there was a statistically significant improvement in the participant’s ability to track a desired payload target with the use of the anti-sway system of up to 49.1%. In addition, as the participant had no knowledge of how the anti-sway system operated, or even if it was active, the results indicate anti-sway control systems can be intuitive for operators to use.}
}
@article{DACOSTA2004173,
title = {The acceptance of virtual reality devices for cognitive rehabilitation: a report of positive results with schizophrenia},
journal = {Computer Methods and Programs in Biomedicine},
volume = {73},
number = {3},
pages = {173-182},
year = {2004},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(03)00066-X},
url = {https://www.sciencedirect.com/science/article/pii/S016926070300066X},
author = {Rosa Maria Esteves Moreira {da Costa} and Luı́s Alfredo Vidal {de Carvalho}},
keywords = {Virtual reality, Virtual environment, Cognitive rehabilitation, Schizophrenia},
abstract = {This study presents a process of virtual environment development supported by a cognitive model that is specific to cognitive deficits of diverse disorders or traumatic brain injury, and evaluates the acceptance of computer devices by a group of schizophrenic patients. The subjects that participated in this experiment accepted to work with computers and immersive glasses and demonstrated a high level of interest in the proposed tasks. No problems of illness have been observed. This experiment indicated that further research projects must be carried out to verify the value of virtual reality technology for cognitive rehabilitation of psychiatric patients. The results of the current study represent a small but necessary step in the realization of that potential.}
}
@article{LIU2019116779,
title = {Pullout properties of AR-glass textile embedded in cement matrix under different velocities and temperatures},
journal = {Construction and Building Materials},
volume = {228},
pages = {116779},
year = {2019},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2019.116779},
url = {https://www.sciencedirect.com/science/article/pii/S0950061819322093},
author = {Sai Liu and Deju Zhu and Xuan Wang and Caijun Shi},
keywords = {TRC materials, AR glass, Pullout properties, Dynamic loading, Temperatures},
abstract = {New class of building materials which are also known as ‘textile reinforced cementitious (TRC) materials’ offer superior mechanical properties under different loadings like tension and compression. With the increase in applications, TRCs have gained scientists attention for exploring its future possibilities. Therefore, the mechanical characterization for basic mechanical properties is continuously going-on for past few decades. This paper investigates, the pullout response of alkali-resistant glass: AR glass (in two forms i.e. single yarn and textile) from cement matrix under extreme conditions. These conditions are varying pullout velocities (from quasi-static to dynamic loading) at four temperatures from −25 °C to 50 °C. The pullout tests are performed on two mechanical setups (i) an electro-mechanical load frame (for static loading at 5 × 10−5 m/s) and (ii) drop weight impact test set-up (for dynamic loading at four velocities from 1 to 4 m/s). The testing results are calculated and compared with quasi-static loading results for peak pullout load and work done in fracturing the samples under different loading rates and temperatures. The testing and comparative results highlighted the significant variation in mechanical properties for cementitious concretes (single yarn and textile based). In addition, the analysis of mechanical properties also revealed that at a constant velocity with temperature variations play an important role (variation in bonding performance) which cannot be neglected for using these materials in extreme environmental conditions. However, the displacement at peak force point and failure morphologies (i.e. FESEM scanning of fractured surface) does not show any remarkable variations at different loading velocities and temperatures. Overall, the pullout properties of textile reinforced samples are higher than those of single-yarn reinforced ones. Whereas, the bond-strength of textile reinforced samples is lower than that of single-yarn reinforced ones.}
}
@article{NEVEN2018335,
title = {SOULMATE - Secure Old people’s Ultimate Lifestyle Mobility by offering Augmented reality Training Experiences},
journal = {Procedia Computer Science},
volume = {141},
pages = {335-342},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.192},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318428},
author = {An Neven and Tom Bellemans and Astrid Kemperman and Pauline van den Berg and Martijn Kiers and Lex van Velsen and Judith Urlings and Davy Janssens and Yves Vanrompay},
keywords = {Virtual trip training, Mobile routing, GPS-based monitoring, Virtual, real-life user experiences, Customizable guardian angel},
abstract = {With ageing, travel becomes more and more complex. Physical and cognitive abilities may decline, and thus, elderly people may experience several problems that hinder them from travelling independently. One of the most difficult issues when addressing the problem of age-related travel restrictions is the heterogeneity of the ageing population. SOULMATE offers a non-intrusive and personalized mobility package that evolves with the end-user across his/her different life stages to ensure him/her to make secure trips. SOULMATE aggregates three types of mobility support: Indoor virtual training of a route, active routing during trips and monitoring by a coach at a distance during trips. For each individual, the desired or necessary functionalities can be chosen, based on his/her specific abilities and travel needs. The SOULMATE solution will be developed iteratively, in co-creation with end-users and stakeholders, and the package will be tested intensively, and evaluated in three countries (Belgium, The Netherlands and Austria) based on usability, technical and business aspects.}
}
@article{ZHANG2023104848,
title = {Virtual reality enhanced multi-role collaboration in crane-lift training for modular construction},
journal = {Automation in Construction},
volume = {150},
pages = {104848},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104848},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523001085},
author = {Zhiqian Zhang and Mun On Wong and Wei Pan},
keywords = {Crane-lift training, Multi-role collaboration, Virtual reality, Multi-user system, Modular construction},
abstract = {Crane lifts for bulky module installation greatly affect site safety and efficiency. However, few previous studies addressed collaborative training for a lifting crew in modular construction. Therefore, this paper aims to develop a virtual reality (VR) system for enhancing multi-role collaboration in crane-lift training. The system is developed by integrating full-, semi-, and non-immersive VR devices with role-specific functions designed for crane operators, riggers, and signalers in modular building projects. The system is objectively assessed by six groups of researchers with memory tests and proved more effective in group-based crane-lift training than the lecture-based approach, resulting in a 33% improvement in the trainees' memory stability. The system is also evaluated by the researchers and six industry practitioners who affirm that their understanding of the crane-lift process is enhanced through multi-role collaboration using the system. The developed multi-user VR system should facilitate safe and efficient delivery of modular buildings.}
}
@article{ZHU2023106420,
title = {Effects of volume fraction and surface coating of textile yarns on the tensile performance of AR-glass textile reinforced concrete},
journal = {Journal of Building Engineering},
volume = {71},
pages = {106420},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106420},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223005995},
author = {Deju Zhu and Xiayang Bai and Qiyao Yao and Md Zillur Rahman and Xinliang Li and Ting Yang and Shuaicheng Guo},
keywords = {Textile reinforced concrete, Alkali-resistant glass, Tensile performance, Constitutive model},
abstract = {This study investigates the effects of volume fraction and epoxy coating of textile yarns on the failure morphologies and tensile properties of alkali-resistant glass textile reinforced concrete (ARG-TRC). The quasi-static tensile tests were first carried out on the single yarn and fiber strips with different numbers of warp and weft yarns to evaluate the effect of the epoxy coating on their tensile performance. The ARG-TRC specimens with various volume fractions of warp yarn (0.24, 0.49, 0.73, and 1.09 vol%) and weft yarn (0, 0.20, 0.48, and 0.96 vol%) were subsequently tested, and their crack evolution and strain distribution were monitored and captured by the digital image correlation (DIC) method. It was found that the failure morphologies and tensile properties of ARG-TRC mainly depend on the volume fraction of warp yarn and the epoxy coating, while the effect of weft yarn is minimal. Furthermore, the effect of epoxy coating on the tensile properties of ARG-TRC is discussed with variations in the volume fraction of textile yarn. A simplified experimental fitting model is obtained by trilinear fitting and compared with the ACK theoretical model. The findings from this study can serve as a valuable reference for optimizing fabric configurations and designing TRC components.}
}
@article{EXNER201596,
title = {Validation of Product-service Systems in Virtual Reality},
journal = {Procedia CIRP},
volume = {30},
pages = {96-101},
year = {2015},
note = {7th Industrial Product-Service Systems Conference - PSS, industry transformation for sustainability and business},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.02.092},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115001687},
author = {Konrad Exner and Rainer Stark},
keywords = {Product-Service Systems, validation, evaluation, Virtual Reality, experiencing},
abstract = {Research in the area of the integrated development of products and services, designated as Product-Service Systems (PSS), is maturing and a transition in industrial practices is noticeable. Nevertheless, PSS development methodologies lack consistent approaches regarding the integrated validation of different PSS elements rather than a separate development and validation. To prevent an expensive roll-out and testing in late development stages new methodologies and techniques need to be developed and applied. The challenge is theenablement of experiencing and thus testing of PSS in early stages, like planning and concept phase.In order to address these challenges for an integrated validation of PSS a prototyping approach named SHP4PSS has been introduced on a conceptual level [1], integrating virtual and physical prototypes in a Virtual Reality (VR). To complete the methodology a matrix is presented to derive test cases out of early PSS concepts. Furthermore, the evaluation matrix regarding the test phase and the current version of the demonstrator is introduced.}
}
@article{SINGH2021103190,
title = {A comparative evaluation of the wearable augmented reality-based data presentation interface and traditional methods for data entry tasks},
journal = {International Journal of Industrial Ergonomics},
volume = {86},
pages = {103190},
year = {2021},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2021.103190},
url = {https://www.sciencedirect.com/science/article/pii/S0169814121001086},
author = {Ankit Singh and Yalda Ghasemi and Heejin Jeong and Myunghee Kim and Andrew Johnson},
keywords = {Augmented reality, Data entry, Interface design, User testing},
abstract = {Data entry is a ubiquitous task performed in today's offices. Persistent data entry is linked with high workload and fatigue due to poor ergonomic workplace design and poor posture. This study aims to alleviate data entry operators' workload and improve data entry performance by applying wearable augmented reality (AR) technology to data entry tasks. An AR-based interface was developed and used to present data to the participants, who entered the data in web-based data entry forms. A total of eighteen participants performed data entry tasks to evaluate the AR interface with traditional methods – extra desktop monitor and paper-based data presentation methods. Each method's performance was judged on the task completion time, typing error rate, workload, and usability. The usability and overall perceived workload while using an AR interface for data entry were similar to the traditional way of using paper, despite the additional burden due to the weight of the AR headset. AR interface did not perform better than the extra desktop monitor interface for usability and overall perceived workload. The results from this study can be utilized to design AR devices that are suited for data entry tasks.}
}
@article{SUAREZWARDEN2015281,
title = {Assembly Operations Aided by Augmented Reality: An Endeavour toward a Comparative Analysis},
journal = {Procedia Computer Science},
volume = {75},
pages = {281-290},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.249},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037102},
author = {Fernando Suárez-Warden and Eduardo González Mendívil and Ciro A. Rodríguez and Salvador Garcia-Lumbreras},
keywords = {augmented reality (AR), complex assembly, experimental conditions, maintenance, aeronautical assembly, small sample.},
abstract = {Costly complex assembly operations supported by augmented reality demands endeavors to achieve a comparative and experimental depiction of different circumstances for assays to analyze fluctuating conclusions where the situation with limited number of tries has compelled to recognize that variations exist depending on test conditions. Then developments in emergent technologies, for training in Maintenance and Repair Operations (MRO), must be evaluated. Scenarios are focused on assessing an AR request, by stating a confrontation of declarations about assembly time and comparisons respect to outstanding authors who conduct assembly operations. An assembly operations case study establishes a small sample size during experimentation.}
}
@article{SAPINSKI2008379,
title = {Autonomous control system for a 3 DOF pitch-plane suspension model with MR shock absorbers},
journal = {Computers & Structures},
volume = {86},
number = {3},
pages = {379-385},
year = {2008},
note = {Smart Structures},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2007.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0045794907000855},
author = {Bogdan Sapiński and Maciej Rosół},
keywords = {MR shock absorber, Pitch-plane suspension model, Autonomous control system, Embedded controller, Rapid prototyping, Vibration control},
abstract = {The study is concerned with an autonomous control system (ACS) for a 3 DOF pitch-plane suspension model comprising magnetorheological shock absorbers (MRAs). The model simulating a pitch-plane vehicle suspension with two independent spring–MRA sets in front and rear sections and a driver’s seat suspension with the third spring–MRA set is presented. The development of the ACS based on phyCORE-MPC555 single board computer and rapid prototyping of real-time controllers are described. Results of experiments in the open loop and feedback systems for the tested application are provided.}
}
@article{KIDO2021101281,
title = {Assessing future landscapes using enhanced mixed reality with semantic segmentation by deep learning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101281},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101281},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000367},
author = {Daiki Kido and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Mixed reality, Dynamic occlusion handling, Landscape index estimation, Landscape design, Video communication, Deep learning},
abstract = {Architecture, engineering, and construction projects need to be promoted in harmony with the natural environment and with the aim of preserving people’s living environment. At the planning and design stage, decision-makers and stakeholders share and assess landscape images during and after construction in order to avoid as much uncertainty as possible when performing environmental impact assessment. Given the lack of a standard visualization method for future landscapes that do not yet exist, mixed reality (MR), which overlays virtual content onto a real scene, has attracted attention in the field of landscape design. One challenge in MR is occlusion, which occurs when virtual objects obscure physical objects that should be rendered in the foreground. In MR-based landscape visualization, the distance between the MR camera and real objects located in front of the virtual objects might vary and might be large, causing difficulty for existing occlusion handling methods. In the process of landscape design, an evidence-based approach has also become important. Landscape index estimation using semantic segmentation by deep learning, which can recognize the surrounding environment, has been actively studied for landscape assessment. In this study, semantic segmentation by deep learning was integrated into an MR system to enable dynamic occlusion handling and landscape index estimation for both existing and designed landscape assessment. This system can be operated on a mobile device with video communication over the internet by connecting to real-time semantic segmentation on a high-performance personal computer. The applicability of the developed system is demonstrated through accuracy verification and case studies.}
}
@article{DALLASEGA202049,
title = {BIM, Augmented and Virtual Reality empowering Lean Construction Management: a project simulation game},
journal = {Procedia Manufacturing},
volume = {45},
pages = {49-54},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.059},
url = {https://www.sciencedirect.com/science/article/pii/S235197892031101X},
author = {Patrick Dallasega and Andrea Revolti and Philipp Christopher Sauer and Felix Schulze and Erwin Rauch},
keywords = {Virtual Reality, Augmented Reality, Building Information Modeling, business simulation game, learning factory},
abstract = {During the last decades, Lean Management methodologies established in the manufacturing environment have been applied and adapted to the construction industry under the term “Lean Construction”. Currently, concepts and technologies from Industry 4.0 are mainly transforming the manufacturing industry and only few applications have been implemented to construction and its connected supply chains. This paper shows how new technologies like Building Information Modeling connected with Virtual and Augmented Reality could empower Lean Construction methodologies to increase efficiency during the building execution process. The approach was tested by using the project simulation game Villego® with students from the course “Project Management” of the master degree LM-33 “Industrial Mechanical Engineering” of the Free University of Bozen-Bolzano.}
}
@article{GATTULLO2019276,
title = {Towards augmented reality manuals for industry 4.0: A methodology},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {56},
pages = {276-286},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518301236},
author = {Michele Gattullo and Giulia Wally Scurati and Michele Fiorentino and Antonio Emmanuele Uva and Francesco Ferrise and Monica Bordegoni},
keywords = {Augmented reality, Industry 4.0, Maintenance support, Technical documentation},
abstract = {Augmented Reality (AR), is one of the most promising technology for technical manuals in the context of Industry 4.0. However, the implementation of AR documentation in industry is still challenging because specific standards and guidelines are missing. In this work, we propose a novel methodology for the conversion of existing “traditional” documentation, and for the authoring of new manuals in AR in compliance to Industry 4.0 principles. The methodology is based on the optimization of text usage with the ASD Simplified Technical English, the conversion of text instructions into 2D graphic symbols, and the structuring of the content through the combination of Darwin Information Typing Architecture (DITA) and Information Mapping (IM). We tested the proposed approach with a case study of a maintenance manual of hydraulic breakers. We validated it with a user test collecting subjective feedbacks of 22 users. The results of this experiment confirm that the manual obtained using our methodology is clearer than other templates.}
}
@article{ESCHEN2018156,
title = {Augmented and Virtual Reality for Inspection and Maintenance Processes in the Aviation Industry},
journal = {Procedia Manufacturing},
volume = {19},
pages = {156-163},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300222},
author = {Henrik Eschen and Tobias Kötter and Rebecca Rodeck and Martin Harnisch and Thorsten Schüppstuhl},
keywords = {Augmented Reality, Virtual Reality, Mixed Reality, Inspection, Maintenance, Aviation Industry},
abstract = {Maturity of augmented and virtual reality devices has considerably grown recently. As processes in the aviation industry are error prone and time consuming, efforts are made to implement these technologies to support human workers during inspection and maintenance. Nevertheless, varying process and device characteristics impede the selection of a suitable technology. A concept is presented to evaluate the potential of inspection and maintenance processes in the aviation industry regarding the use of mixed reality systems. Four different use cases are discussed applying augmented or virtual reality devices in an industrial context.}
}
@article{SHARMA20221,
title = {The design and evaluation of an AR-based serious game to teach programming},
journal = {Computers & Graphics},
volume = {103},
pages = {1-18},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322000024},
author = {Vandit Sharma and Kaushal Kumar Bhagat and Huai-Hsuan Huang and Nian-Shing Chen},
keywords = {Augmented Reality, Computational thinking, Learning Analytics, Gamification, Feedback design, System usability},
abstract = {The ubiquity of smartphone and tablet devices, combined with the increasing availability of serious games, has enabled students to learn various abstract concepts in an appealing and convenient manner. While several researchers have explored the use of Augmented Reality (AR) in serious games, many of these games have not been critically explained or evaluated. To that end, we employed game-based learning methodologies and Game Learning Analytics (GLA) to systematize the design and evaluation of an AR-based serious game to teach programming. We evaluated our game for usability and effectiveness by conducting a user study on twenty-seven undergraduate students. The evaluation primarily consisted of a learning test conducted twice – before and after playing the game – along with a usability questionnaire that players completed after playing the game. Our results showed that players made significant progress after playing the game. The game helped players improve their basic programming skills, especially for the group having lower prior programming skills. The results highlighted various ways in which GLA can be used to benefit different stakeholders in the game. Based on players’ qualitative responses, we also identified several areas of improvement, most prominently the trade-off between ease of use and game complexity. We provide suggestions and discuss implications for future work.}
}
@article{WANG2008399,
title = {User perspectives on mixed reality tabletop visualization for face-to-face collaborative design review},
journal = {Automation in Construction},
volume = {17},
number = {4},
pages = {399-412},
year = {2008},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2007.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0926580507000933},
author = {Xiangyu Wang and Phillip S. Dunston},
keywords = {Mixed reality, Usability, Computer supported cooperative work (CSCW), Design review, 3D models},
abstract = {Mixed Reality (MR) technologies create an environment where virtual objects and real objects can be presented together on a single display. An MR-based face-to-face design review prototype was created for the purpose of improving design review collaboration. The work presented in this paper investigated how users experienced virtual models in the MR system and assessed the users' experience and attitude towards the effectiveness of the MR tool for design review as compared with a 3D paper-based method. The findings from user feedback suggest that the MR tool may facilitate problem-solving and the quantity of work in a given amount of time and that virtual design displayed in the mixed scene was a useful aid in the design error detection task. Experimental usability evaluation of the MR system was also implemented and suggestions for improvements of the human–computer interface of future system versions were produced.}
}
@article{FARES20181821,
title = {Achieving public participation in inaccessible areas using virtual reality a case study of Beit Hanoun – Gaza – Palestine},
journal = {Alexandria Engineering Journal},
volume = {57},
number = {3},
pages = {1821-1828},
year = {2018},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1110016817302387},
author = {Fekry Fares and Dina Sameh Taha and Zeyad Tarek {EL Sayad}},
keywords = {Public participation, Virtual reality, Urban design},
abstract = {The responsible authorities depend on public participation in decision-making. However, there are many obstacles facing them because of their reliance on traditional tools. These obstacles include that a large number of participants do not understand the tools of participation, as well as the inconsistency of time and place in which the participation process is taking place. This creates a gap between decision-makers and participants, all of which reduce the efficiency of participation. This research paper will study the use of virtual reality, using programs such as Vizard in the conduct of a participatory process by linking virtual reality and internet without restricting the participants with a particular place and time to carry out the process of participation, as well as the provision of tools to facilitate decision-making tools. Beit Hanoun (Gaza-Palestine) was selected to test the effectiveness of the tool used because it suffers from the efficiency of the tools available to participate in the city. The results show that the tool used to effectively integrate citizens into decision-making in a more interactive way can enhance communication between decision-makers and citizens. Virtual reality tools and the Internet are reliable tools in any study of the integration of citizens into decision-making.}
}
@article{JOTHI2018236,
title = {Simultaneous determination of ascorbic acid, dopamine and uric acid by a novel electrochemical sensor based on N2/Ar RF plasma assisted graphene nanosheets/graphene nanoribbons},
journal = {Biosensors and Bioelectronics},
volume = {105},
pages = {236-242},
year = {2018},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2018.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0956566318300538},
author = {Lavanya Jothi and Sudarsan Neogi and Saravana kumar Jaganathan and Gomathi Nageswaran},
keywords = {N/Ar RF plasma, Graphenenanosheets/graphene nanoribbons, Ascorbic acid, Dopamine, Uric acid, Differential pulse voltammetry},
abstract = {A novel nitrogen/argon (N2/Ar) radio frequency (RF) plasma functionalized graphene nanosheet/graphene nanoribbon (GS/GNR) hybrid material (N2/Ar/GS/GNR) was developed for simultaneous determination of ascorbic acid (AA), dopamine (DA) and uric acid (UA). Various nitrogen mites introduced into GS/GNR hybrid structure was evidenced by a detailed microscopic, spectroscopic and surface area analysis. Owing to the unique structure and properties originating from the enhanced surface area, nitrogen functional groups and defects introduced on both the basal and edges, N2/Ar/GS/GNR/GCE showed high electrocatalytic activity for the electrochemical oxidations of AA, DA, and UA with the respective lowest detection limits of 5.3, 2.5 and 5.7 nM and peak-to-peak separation potential (ΔEP) (vs Ag/AgCl) in DPV of 220, 152 and 372 mV for AA/DA, DA/UA and AA/UA respectively. Moreover, the selectivity, stability, repeatability and excellent performance in real time application of the fabricated N2/Ar/GS/GNR/GCE electrode suggests that it can be considered as a potential electrode material for simultaneous detection of AA, DA, and UA.}
}
@article{IMBERT2013364,
title = {Adding Physical Properties to 3D Models in Augmented Reality for Realistic Interactions Experiments},
journal = {Procedia Computer Science},
volume = {25},
pages = {364-369},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012490},
author = {Nicolas Imbert and Frederic Vignat and Charlee Kaewrat and Poonpong Boonbrahm},
keywords = {Augmented Reality, Physical Properties, Realistic Interaction},
abstract = {Augmented Reality is the combination of virtual objects (created by computer i.e. video, texts or 3D computer models) overlay on top of real world image. Applications of Augmented Reality can be ranged from advertising, edutainment, education, engineering, medicine to industrial manufacturing. In basic applications, like in advertisement or games, users only see the actions or interact with part of the screen designed for initiate some actions. In order to make users have realistic experiences, the interaction amongst virtual objects in Augmented Reality must be restricted to the law of Physics. Virtual objects can have their own dimensions, volumes or weights. When interaction between virtual objects occurred, the collision for example, they should not penetrate each other. The objects will react to each other by the law of Physics. With this concept, all kinds of experiments can be tested or practiced without spending a lot of fortunes with the real setup ranging from simple science experiment, medical training or even assembly process of equipment. In this research, Unity 3D game engine is used on Vuforia platform. Unity is a fully integrated development engine for creating games and other interactive 3D content and Vuforia platform make it possible to write a single native application that runs on almost all smartphones and tablets. To test the concept, 8 pieces of virtual 3D puzzle modules were created using 8 markers. Each virtual module was assigned with physical properties such dimensions, shapes and positions. When assemble the puzzle, each piece of the marker must be able to move around so that the virtual modules can fit to each other. By lifting and rotating the markers, the virtual module will snap with the other proper virtual part, forming virtual 3D puzzle. The virtual module will not penetrate each other because they have their own territory due to their dimensions. With this experiment, users will have a realistic feeling on assembling the virtual model. The concept can be implemented for experiments that are dangerous or expensive to setup. Experiments related to interaction between objects such as physics and chemistry experiments, engineering and medical training are the main targets for using this kind of technology.}
}
@article{UNLU201037,
title = {Computerized method for nonrigid MR-to-PET breast-image registration},
journal = {Computers in Biology and Medicine},
volume = {40},
number = {1},
pages = {37-53},
year = {2010},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2009.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0010482509001942},
author = {M.Z. Unlu and A. Krol and A. Magri and J.A. Mandel and W. Lee and K.G. Baum and E.D. Lipson and I.L. Coman and D.H. Feiglin},
keywords = {MR-to-PET nonrigid breast-image registration, FEM-based soft tissue multimodality nonrigid image registration},
abstract = {We have developed and tested a new simple computerized finite element method (FEM) approach to MR-to-PET nonrigid breast-image registration. The method requires five–nine fiducial skin markers (FSMs) visible in MRI and PET that need to be located in the same spots on the breast and two on the flanks during both scans. Patients need to be similarly positioned prone during MRI and PET scans. This is accomplished by means of a low gamma-ray attenuation breast coil replica used as the breast support during the PET scan. We demonstrate that, under such conditions, the observed FSM displacement vectors between MR and PET images, distributed piecewise linearly over the breast volume, produce a deformed FEM mesh that reasonably approximates nonrigid deformation of the breast tissue between the MRI and PET scans. This method, which does not require a biomechanical breast tissue model, is robust and fast. Contrary to other approaches utilizing voxel intensity-based similarity measures or surface matching, our method works for matching MR with pure molecular images (i.e. PET or SPECT only). Our method does not require a good initialization and would not be trapped by local minima during registration process. All processing including FSMs detection and matching, and mesh generation can be fully automated. We tested our method on MR and PET breast images acquired for 15 subjects. The procedure yielded good quality images with an average target registration error below 4mm (i.e. well below PET spatial resolution of 6–7mm). Based on the results obtained for 15 subjects studied to date, we conclude that this is a very fast and a well-performing method for MR-to-PET breast-image nonrigid registration. Therefore, it is a promising approach in clinical practice. This method can be easily applied to nonrigid registration of MRI or CT of any type of soft-tissue images to their molecular counterparts such as obtained using PET and SPECT.}
}