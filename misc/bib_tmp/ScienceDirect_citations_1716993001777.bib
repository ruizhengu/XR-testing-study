@article{MORENO201448,
title = {Interactive fire spread simulations with extinguishment support for Virtual Reality training tools},
journal = {Fire Safety Journal},
volume = {64},
pages = {48-60},
year = {2014},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2014.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0379711214000162},
author = {Aitor Moreno and Jorge Posada and Álvaro Segura and Ander Arbelaiz and Alejandro García-Alonso},
keywords = {Fire spread simulation, Wildfires, Urban fires, Virtual Reality, Extinguishment support, Spotting fires},
abstract = {Virtual Reality training for fire fighters and managers has two main advantages. On one hand, it supports the simulation of complex scenarios like big cities, where a fire cannot be simulated in the real world. On the other hand, fire fighting VR simulators allow trainees to experience situations as similar as possible to real fire, reducing the probability of accidents when they are practising exercises with real fire. The success of the Virtual Reality training tools also depends on how close to reality the simulation process is. This work provides fire spread algorithms for forest and urban environments, which can be used at interactive rates. Due to the interactive nature of the algorithms, the users are able to fight the fire by throwing extinguishing agents. Although the algorithms assume many simplifications of the problem, their behaviour is satisfactory. This is due to the efficient management of the cell states in a 3m×3m cell grid. Also the variables that have more influence on fire propagation constitute the core of the algorithms. The overall system deals with user extinguishment actions, natural and artificial firebreaks, variable wind conditions (even at a cell level) and non-contiguous fire propagation (embers and spotting fires). The unified forest/urban model leads to an object oriented architecture which supports the fire propagation algorithms. This also allows the system to compute efficiently mixed forest–urban environments.}
}
@article{TAVARES2019102825,
title = {Collaborative Welding System using BIM for Robotic Reprogramming and Spatial Augmented Reality},
journal = {Automation in Construction},
volume = {106},
pages = {102825},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518311191},
author = {Pedro Tavares and Carlos M. Costa and Luís Rocha and Pedro Malaca and Pedro Costa and António P. Moreira and Armando Sousa and Germano Veiga},
keywords = {Robotic Welding, BIM, Spatial Augmented Reality, Structural Steel Construction Industry},
abstract = {The optimization of the information flow from the initial design and through the several production stages plays a critical role in ensuring product quality while also reducing the manufacturing costs. As such, in this article we present a cooperative welding cell for structural steel fabrication that is capable of leveraging the Building Information Modeling (BIM) standards to automatically orchestrate the necessary tasks to be allocated to a human operator and a welding robot moving on a linear track. We propose a spatial augmented reality system that projects alignment information into the environment for helping the operator tack weld the beam attachments that will be later on seam welded by the industrial robot. This way we ensure maximum flexibility during the beam assembly stage while also improving the overall productivity and product quality since the operator no longer needs to rely on error prone measurement procedures and he receives his tasks through an immersive interface, relieving him from the burden of analyzing complex manufacturing design specifications. Moreover, no expert robotics knowledge is required to operate our welding cell because all the necessary information is extracted from the Industry Foundation Classes (IFC), namely the CAD models and welding sections, allowing our 3D beam perception systems to correct placement errors or beam bending, which coupled with our motion planning and welding pose optimization system ensures that the robot performs its tasks without collisions and as efficiently as possible while maximizing the welding quality.}
}
@article{YI2021102388,
title = {Process monitoring of economic and environmental performance of a material extrusion printer using an augmented reality-based digital twin},
journal = {Additive Manufacturing},
volume = {48},
pages = {102388},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102388},
url = {https://www.sciencedirect.com/science/article/pii/S221486042100542X},
author = {Li Yi and Moritz Glatt and Svenja Ehmsen and Wentao Duan and Jan C. Aurich},
keywords = {Additive manufacturing, Material extrusion, Digital twin, Augmented reality, Process monitoring},
abstract = {The concept of digital twin is based on the linkage of a physical system with a digital model-based representation and shows numerous potentials in additive manufacturing (AM). The existing approaches related to digital twins of AM mainly focus on the process, machine, and factory levels to monitor and optimize the process and system performance of AM. This study aims at the machine level and adopts augmented reality (AR) to develop a digital twin for a material extrusion printer. To provide a digital representation of the printing process of a component, this study has proposed an approach using cumulated small cylinders to approximate the geometry of the component, which is called “Volume Approximation by Cumulated Cylinders (VACCY)”. To monitor the economic and environmental performance, this study has modeled four process indicators (electricity use, manufacturing cost, greenhouse gas emission, and primary energy consumption) and integrated them in the AR-based digital twin. Through the test of the developed digital twin for printing three components, the feasibility and performance of the developed digital twin are well demonstrated.}
}
@article{BRAN20201487,
title = {In-vehicle Visualization of Data by means of Augmented Reality},
journal = {Procedia Computer Science},
volume = {176},
pages = {1487-1496},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.159},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320597},
author = {Emanuela Bran and Dragos Florin Sburlan and Dorin Mircea Popovici and Crenguta Mădălina Puchianu and Elena Băutu},
keywords = {In-vehicle System, Augmented Reality, Visualization, Multi-modal Interaction},
abstract = {Recently, in-vehicle systems have considered integrating Augmented Reality (AR) to serve various purposes. The information offered to the driver by an in-vehicle system should be presented such that it can be easily interpreted without negatively affecting the driver’s attention for the driving task. The interface must be accessible, visible and easily legible, while also being symbolically relevant. In this paper, we explain the requirements and design decisions that lead to the proposal of our current AR-based visualization component of our in-vehicle system. The 3D AR visualization scheme we propose enhances the automatic adaptive notification filtering module of the in-vehicle system previously proposed. We integrated voice and gesture commands in order to manage common actions during driving, such as navigation system functions, control of the radio or of the air conditioner, or generic actions regarding incoming notifications. The system is tested in an artificial setting in the laboratory, using a simulated prototype vehicle deck and windshield.}
}
@article{WITHER2011810,
title = {Indirect augmented reality},
journal = {Computers & Graphics},
volume = {35},
number = {4},
pages = {810-822},
year = {2011},
note = {Semantic 3D Media and Content},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2011.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0097849311001130},
author = {Jason Wither and Yun-Ta Tsai and Ronald Azuma},
keywords = {Mixed Reality, Information Presentation, User Interface, Evaluation},
abstract = {Developing augmented reality (AR) applications for mobile devices and outdoor environments has historically required a number of technical trade-offs related to tracking. One approach is to rely on computer vision which provides very accurate tracking, but can be brittle, and limits the generality of the application. Another approach is to rely on sensor-based tracking which enables widespread use, but at the cost of generally poor tracking performance. In this paper we present and evaluate a new approach, which we call Indirect AR, that enables perfect alignment of virtual content in a much greater number of application scenarios. To achieve this improved performance we replace the live camera view used in video see through AR with a previously captured panoramic image. By doing this we improve the perceived quality of the tracking while still maintaining a similar overall experience. There are some limitations of this technique, however, related to the use of panoramas. We evaluate these boundaries conditions on both a performance and experiential basis through two user studies. The result of these studies indicates that users preferred Indirect AR over traditional AR in most conditions, and when conditions do degrade to the point the experience changes, Indirect AR can still be a very useful tool in many outdoor application scenarios.}
}
@article{DORILEO2022102840,
title = {CT/MR-compatible physical human-robotized needle interactions: From modeling to percutaneous steering},
journal = {Mechatronics},
volume = {85},
pages = {102840},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102840},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822000721},
author = {Éderson Dorileo and Nabil Zemiti and Sébastien Krut and Philippe Poignet},
keywords = {pHRI, Needle steering, Robotized needles, CT/MRI-compatible, Percutaneous insertion, Rigid needles, Interventional radiology},
abstract = {In recent years a number of robotic steering systems have been proposed that are geared towards improving interventional radiology procedures such as tumor ablation and biopsy. These solutions have introduced new safety challenges in the physical human–robot interaction domain. This study presents a new 3D robotized needle steering algorithm compatible with CT and MR-imaging guidance. The steering algorithm is featured with an adaptive self-correction mechanism that works as a failure contingency tool that could be adapted online at each insertion step. The developed pHRI solution was designed to be compatible to ferro-magnetic issues and a reduced workspace inside the scanner bore. As far as we know, this is the first approach designed to steer rigid needles free of force sensors and which meets the challenges that prevail in our context. Our proposed approach helps overcome safety issues regarding the physical interaction between robotized needles and patients. Validation testing highlighted the feasibility of the new needle steering algorithm, while its accuracy revealed the potential of the approach under the proposed scope of application.}
}
@article{CHU2018305,
title = {Integrating mobile Building Information Modelling and Augmented Reality systems: An experimental study},
journal = {Automation in Construction},
volume = {85},
pages = {305-316},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517301218},
author = {Michael Chu and Jane Matthews and Peter E.D. Love},
keywords = {AR, BIM, Cloud-based, Experiment, Design science, Task efficiency},
abstract = {The benefits of Building Information Modelling (BIM) have typically been tied to its capability to support information structuring and exchange through the centralization of information. Its increasing adoption and the associated ease of data acquisition has created information intensive work environments, which can result in information overload and thus negatively impact workers task efficiency during construction. Augmented Reality (AR) has been proposed as a mechanism to enhance the process of information extraction from building information models to improve the efficiency and effectiveness of workers' tasks. Yet, there is limited research that has evaluated the effectiveness and usability of AR in this domain. This research aims to address this gap and evaluate the effectiveness of BIM and AR system integration to enhance task efficiency through improving the information retrieval process during construction. To achieve this, a design science research approach was adopted that enabled the development and performance of a mobile BIM AR system (artefact) with cloud-based storage capabilities to be tested and evaluated using a portable desktop experiment. A total of 20 participants compared existing manual information retrieval methods (control group), with information retrieval through the artefact (non-control group). The results revealed that the participants using the artefact were approximately 50% faster in completing their experiment tasks, and committed less errors, when compared to the control group. This research demonstrates that a minor modification to existing information formats (2D plans) with the inclusion of Quick Response markers can significantly improve the information retrieval process and that BIM and AR integration has the potential to enhance task efficiency.}
}
@article{YIN20231035,
title = {A Mixed Reality-enhanced Rapid Prototyping Approach for Industrial Articulated Products},
journal = {Procedia CIRP},
volume = {119},
pages = {1035-1040},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.178},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123006121},
author = {Yue Yin and Pai Zheng and Chengxi Li and Yat Ming Pang},
keywords = {Mixed reality, Rapid prototyping, Articulated objects, Kinematics, Augmented reality},
abstract = {The design of industrial articulated products is challenging due to the consideration of multiple poses, movements, and especially in-field constraints, such as the design of a personalized robot arm or the layout of articulated hoists. Traditionally, depicting 2D sketches from multiple perspectives and poses of the product is tedious. Meanwhile, on-site evaluation after fabricating the physical prototype is costly and time-consuming. In order to assist in context-aware decision-making at the early design stage, this research proposes a mixed reality-enhanced rapid prototyping approach for industrial articulated products. In a 3D mixed reality environment that incorporates the surroundings, designers can depict key joints and rough shapes through intuitive gesture-based interaction. After a quick prototype creation, the kinematics preview is provided to designers for preliminary functional verification. Finally, a user study shows that the proposed approach can help designers rapidly complete the MR prototype of articulated products at the early design stage, provide a better understanding of the product movement, assist in-field solution generation and facilitate communication. It is hoped this explorative study can offer insightful ideas with effective toolkits for assisting the new product design process more intuitively and interactively.}
}
@article{LI2024108045,
title = {Cooperative-Net: An end-to-end multi-task interaction network for unified reconstruction and segmentation of MR image},
journal = {Computer Methods and Programs in Biomedicine},
volume = {245},
pages = {108045},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108045},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724000415},
author = {Xiaodi Li and Yue Hu},
keywords = {MRI, Multi-task interaction, MR imaging acceleration, Multi-class tissue segmentation, Structural prior},
abstract = {Background and objective
In clinical applications, there is an increasing demand for rapid acquisition and automated analysis of magnetic resonance imaging (MRI) data. However, most existing methods focus on either MR image reconstruction from undersampled data or segmentation using fully sampled data, hardly considering MR image segmentation in fast imaging scenarios. Consequently, it is imperative to investigate a multi-task approach that can simultaneously achieve high scanning acceleration and accurate segmentation results.
Methods
In this paper, we propose a novel end-to-end multi-task interaction network, termed as the Cooperative-Net, which integrates accelerated MR imaging and multi-class tissue segmentation into a unified framework. The Cooperative-Net consists of alternating reconstruction modules and segmentation modules. To facilitate effective interaction between the two tasks, we introduce the spatial-adaptive semantic guidance module, which leverages the semantic map as a structural prior to guide MR image reconstruction. Furthermore, we propose a novel unrolling network with a multi-path shrinkage structure for MR image reconstruction. This network consists of parallel learnable shrinkage paths to handle varying degrees of degradation across different frequency components in the undersampled MR image, effectively improving the quality of the recovered image.
Results
We use two publicly available datasets, including the cardiac and knee MR datasets, to validate the efficacy of our proposed Cooperative-Net. Through qualitative and quantitative analysis, we demonstrate that our method outperforms existing state-of-the-art multi-task approaches for joint MR image reconstruction and segmentation.
Conclusions
The proposed Cooperative-Net is capable of achieving both high accelerated MR imaging and accurate multi-class tissue segmentation.}
}
@article{XIANG2021103704,
title = {Mobile projective augmented reality for collaborative robots in construction},
journal = {Automation in Construction},
volume = {127},
pages = {103704},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001552},
author = {Siyuan Xiang and Ruoyu Wang and Chen Feng},
keywords = {Mobile projective AR (MPAR), Camera-projector system, Pose estimation, Construction co-robots},
abstract = {Augmenting virtual construction information directly in a physical environment is promising to increase onsite productivity and safety. However, it has been found that using off-the-self augmented reality (AR) devices, such as goggles or helmets, could potentially cause more health, safety, and efficiency concerns in complex real-world construction projects, due to the restricted field of view and the non-negligible weight of those devices. To address these issues, we propose a mobile projective AR (MPAR) framework in which the AR device is detached from human workers and carried by one or more mobile collaborative robots (co-robots). MPAR achieves glassless AR that is visible to the naked eye using a camera-projector system to superimpose virtual 3D information onto planar or non-planar physical surfaces. Since co-robots often need to move during the operation, we design algorithms to ensure consistent mobile projection with two major components: projector pose estimation and projection image generation. For planar surfaces, MPAR is achieved by a homography-based pose estimation and image warping. For non-planar surfaces, MPAR uses iterative closest point (ICP) for pose estimation and common graphics pipelines to generate projection images. We conducted both qualitative and quantitative experiments to validate the feasibility of MPAR in a laboratory setting, by projecting 1) as-planned building information onto a planar surface, and 2) as-built 3D information onto a piece-wise planar surface. Our evaluation demonstrated centimeter-level projection accuracy of MPAR from different distances and angles to the two types of surfaces.}
}
@article{VOSNIAKOS2019524,
title = {Exploration of two safety strategies in human-robot collaborative manufacturing using Virtual Reality},
journal = {Procedia Manufacturing},
volume = {38},
pages = {524-531},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.066},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300676},
author = {George-Christopher Vosniakos and Lucas Ouillon and Elias Matsas},
keywords = {human-robot collaboration, virtual reality, safety, fabric laying},
abstract = {“beWare of the Robot” is a custom-designed Virtual Reality application and implemented as a platform for simulating in real-time Human-Robot collaboration in manufacturing tasks. This work reports on its use for investigating two alternative safe collaboration strategies, namely ‘Reduce Speed’ and ‘Move Back’, in the task of handling fabric when building aerospace composite material parts. Both methods are triggered when the distance of the human to the robot falls below a certain threshold. Experimentation was conducted in live tests, taking into account pressure and force thresholds regarding painful transient contact of the robot with the human following ISO/TS 15066:2016. Indices such as mean robot speed, time of collaboration, number and severity of collisions, were recorded in order to assess and ‘tweak’ these methods. Tweaking involved speed reduction factor in ‘Reduce Speed’ and waiting time after completion of stationary tasks in both strategies. Visual cues in terms of graded coloured robot motion volume to indicate varying interference of the human proved ineffective. Beyond statistical assessment of safe collaboration strategies, qualitative assessment was provided by the experiment participants according to acquired experience.}
}
@article{RODRIGUES2022106831,
title = {Usability, acceptance, and educational usefulness study of a new haptic operative dentistry virtual reality simulator},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106831},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106831},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722002139},
author = {Pedro Rodrigues and Artur Esteves and João Botelho and Vanessa Machado and Carlos Zagalo and Ezequiel Roberto Zorzal and José João Mendes and Daniel Simões Lopes},
keywords = {Operative dentistry, Virtual reality, Tooth drilling, Haptic feedback, 3D Sound, Simulator},
abstract = {Background
Dental preclinical training has been traditionally centered onverbal instructions and subsequent execution on phantom heads and plastic training models. However, these present present limitations. Virtual Reality (VR) and haptic simulators have been proposed with promising results and advantages and have showed usefullness in the preclinical training environment. We designed DENTIFY, a multimodal immersive simulator to assist Operative Dentistry learning, which exposes the user to different virtual clinical scenarios while operating a haptic pen to simulate dental drilling.
Objective
The main objective is to assess DENTIFY’s usability, acceptance, and educational usefulness to dentists, in order to make the proper changes and, subsequently, to test DENTIFY with undergraduate preclinical dental students.
Methods
DENTIFY combines an immersive head mounted VR display, a haptic pen in which the pen itself has been replaced by a 3D printed model of a dental turbine and a controller with buttons to adjust and select the scenario of the simulation, along with 3D sounds of real dental drilling. The user’s dominant hand operated the virtual turbine on the VR-created scenario, while the non-dominant hand is used to activate the simulator and case selection. The simulation sessions occurred in a controlled virtual environment. We evaluated DENTIFY’s usability and acceptance over the course of 13 training sessions with dental professionals, after the users performed a drilling task in virtual dental tissues.
Results
The conducted user acceptance indicates that DENTIFY shows potencial enhancing learning in operative dentistry as it promotes self-evaluation and multimodal immersion on the dental drilling experience.
Conclusions
DENTIFY presented significant usability and acceptance from trained dentists. This tool showed to have teaching and learning (hence, pedagogical) potential in operative dentistry.}
}
@article{WANG2020189,
title = {Multi-Channel Augmented Reality Interactive Framework Design for Ship Outfitting Guidance},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {189-196},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.098},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321001877},
author = {Jinge Wang and Minghua Zhu and Xiumin Fan and Xuyue Yin and Zelin Zhou},
keywords = {Augmented reality, Shipbuilding guidance, Human-machine interface, Multichannel interaction, HoloLens 2},
abstract = {Industrial Augmented Reality (IAR) technique has been bloomed in recent years, but the interaction schemes of existing applications are relatively sole and inconvenient. To improve the interaction efficiency for complex scenes like the ship outfitting guidance, this paper proposes a natural multi-channel fusion interaction framework that integrates four information sensing channels including bar code scanning, free gesture, eye movement, and voice control to catch the user’s interaction intention comprehensively. A communication storage architecture and an asynchronous mechanism are introduced to address the multi-channel sensing signals in real-time. A prototype application was developed on the Microsoft HoloLens 2 to facilitate the product designers to browse the parts assembly sequences for process validation. Experimental results verify that the proposed method can realize the interaction intention of the subjects more intuitively and takes less responding time than the single-channel interaction scheme.}
}
@incollection{COURTECUISSE2023455,
title = {Chapter 18 - Augmented reality biomechanical simulations for pelvic conditions diagnoses},
editor = {Mathias Brieu and Michel Cosson and Poul Nielsen},
booktitle = {Biomechanics of the Female Reproductive System: Breast and Pelvic Organs},
publisher = {Academic Press},
pages = {455-479},
year = {2023},
series = {Biomechanics of Living Organs},
issn = {25890999},
doi = {https://doi.org/10.1016/B978-0-12-823403-7.00032-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234037000324},
author = {Hadrien Courtecuisse and Jérémie Dequidt},
keywords = {Real-time computation, training simulations, augmented reality},
abstract = {Digital tools are increasingly used for intraoperative assistance. Initially designed for training and learning, finite element simulations are now used and considered essential in the operating room and can be used to display through augmented reality (AR) the internal structures (vessels, tumors, etc.) on top of the intraoperative images. An essential advantage of biomechanical models lies in their ability to predict structures' behavior, providing a physics-based extrapolation, not just geometric, in areas where few or no intraoperative data are available. Yet, a significant difficulty for training simulations and biomechanical AR for medical purposes concerns the need for real-time computing without sacrificing accuracy. This chapter shows a generic constraint-based biomechanical framework that allows for the simulations of complex interactions (such as friction, contacts, etc.) but also imposes displacements and correct modeling errors using intraoperative images. As an example of possible clinical outcomes, this modeling and simulation are used to build a decision-making software that helps diagnosing pelvic pathologies.}
}
@article{ZOGOPOULOS20211113,
title = {Image-based state tracking in Augmented Reality supported assembly operations},
journal = {Procedia CIRP},
volume = {104},
pages = {1113-1118},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.187},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010854},
author = {Vasilios Zogopoulos and Merwan Birem and Roeland {De Geest} and Robbert Hofman and Lode Jorissen and Bram Vanherle and Dorothy Gors},
keywords = {Augmented reality, Assembly, State tracking, Neural network, Object recognition},
abstract = {Visual tracking and holographic information representation techniques have become robust enough to support operators in complex tasks on the shop floor. This paper presents an approach for coupling AR-supported assembly task instructions with image-based state tracking, so as to assist the operators in product assembly operations. The developed system consists of a visualization platform for AR-supported assembly instructions, a state tracker that includes object recognition, localization and hand tracking, using deep neural networks, and a server that handles the data exchange between the two. The developed framework is applied and validated in an industrial use case.}
}
@article{FLETCHER20131045,
title = {The development of an integrated haptic VR machining environment for the automatic generation of process plans},
journal = {Computers in Industry},
volume = {64},
number = {8},
pages = {1045-1060},
year = {2013},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513001395},
author = {Craig Fletcher and James Ritchie and Theo Lim and Raymond Sung},
keywords = {CAPP, Haptics, Virtual reality, Virtual machining, Prototyping, Simulation},
abstract = {In modern manufacturing it is crucial to be able to produce a product right first time, as efficiently as possible. Virtual prototyping can play a key part in fulfilling this requirement, allowing the simulation of production in advance without the need for unnecessary downtime, material waste or increased lead times. This makes virtual reality (VR) potentially an ideal planning tool since, with each simulation, a better understanding of requirements can be reached and a more effective solution found. This application of haptic virtual reality is particularly relevant in the field of machining where both material and resource costs are high. Planning for the machining of a product requires an intricate knowledge of materials, processes and machining methods. Intuitive virtual reality interfaces can provide new, engineer-friendly means of generating such manufacturing sequences. Current research in haptic virtual reality focuses on specific individual aspects of machining such as path planning or simulation for training; however, in order to simulate multi-operation machining with a view to generating practical and usable process plans, in which there are several processes involved in bringing a product to fruition; an approach is required which allows this type of functionality. This paper presents such a haptic virtual process planning system that allows an operator to load and set up a billet for machining, set up, sequence and tear down any combination of milling, drilling or turning operations then produce a time-estimated process plan. All of the human expertise captured during this process is logged and parsed to generate the final instructions including operation times, tool lists, operation lists and route sheets in easy-to-read format. Via three planning examples, the generation of process plans is demonstrated; with these subsequently validated through their use by a shop floor machinist who utilised them to produce the parts, and a comparison to previously existing plans for the same parts.}
}
@article{ROHACZ20211209,
title = {The Acceptance of Augmented Reality as a Determining Factor in Intralogistics Planning},
journal = {Procedia CIRP},
volume = {104},
pages = {1209-1214},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.203},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101101X},
author = {Anke Rohacz and Steffen Strassburger},
keywords = {Augmented Reality, Technology Acceptance Model, Intralogistics Planning, Automotive Industry},
abstract = {In the automotive industry, an innovative tool to support the intralogistics planning is essential. One possibility is the use of augmented reality (AR). AR can be a suitable tool for improving the planning of intralogistics processes. An improvement of the intralogistics planning processes can be realized by applying this technology. The application thereby dependents on the acceptance of AR. In this paper, the Technology Acceptance Model and a survey are used to verify the acceptance of AR in intralogistics planning. In addition, relevant factors are identified having an impact on the acceptance using AR in intralogistics.}
}
@article{YIN2024676,
title = {Enhancing human-guided robotic assembly: AR-assisted DT for skill-based and low-code programming},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {676-689},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000827},
author = {Yue Yin and Pai Zheng and Chengxi Li and Ke Wan},
keywords = {Augmented reality, Digital twin, Robot programming, Robotic assembly, Human–robot interaction},
abstract = {Efficient and natural programming strategies play a crucial role in enabling human-guided robotic assembly to adapt quickly to dynamic tasks. The combination of Augmented Reality (AR) and Digital Twins (DT) has shown promising potential in enhancing the intuitiveness of human–robot interaction while leveraging digital representations of human intelligence to empower robots in manufacturing tasks. However, traditional programming methods lack intuitive interaction and rely heavily on simulation environments or pre-set CAD models, leading to high costs for both initial setup and sim-to-real deployment. On the other hand, existing AR-based robot control methods have primarily focused on the basic movements of robots, overlooking higher-level skills necessary for complex tasks. To address these limitations, this study introduces a four-layer system architecture that integrates AR-assisted DT into skill-based robotic assembly scenarios. Additionally, a skill-based and low-code programming system for human-guided robotic assembly is designed and implemented, which incorporates natural human guidance and robot autonomous intelligence to generate adaptive and feasible action plans. The feasibility and efficiency of the proposed system are verified by two case studies and a quantitative experiment comparing to traditional programming methods. The results demonstrate the usability of our AR-assisted DT approach in improving programming efficiency, intuitiveness, and safety for human-guided robotic assembly while reducing cognitive load.}
}
@article{CHALILMADATHIL2017501,
title = {An investigation of the efficacy of collaborative virtual reality systems for moderated remote usability testing},
journal = {Applied Ergonomics},
volume = {65},
pages = {501-514},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0003687017300376},
author = {Kapil {Chalil Madathil} and Joel S. Greenstein},
keywords = {Collaborative virtual reality systems, Moderated usability testing, Remote testing},
abstract = {Collaborative virtual reality-based systems have integrated high fidelity voice-based communication, immersive audio and screen-sharing tools into virtual environments. Such three-dimensional collaborative virtual environments can mirror the collaboration among usability test participants and facilitators when they are physically collocated, potentially enabling moderated usability tests to be conducted effectively when the facilitator and participant are located in different places. We developed a virtual collaborative three-dimensional remote moderated usability testing laboratory and employed it in a controlled study to evaluate the effectiveness of moderated usability testing in a collaborative virtual reality-based environment with two other moderated usability testing methods: the traditional lab approach and Cisco WebEx, a web-based conferencing and screen sharing approach. Using a mixed methods experimental design, 36 test participants and 12 test facilitators were asked to complete representative tasks on a simulated online shopping website. The dependent variables included the time taken to complete the tasks; the usability defects identified and their severity; and the subjective ratings on the workload index, presence and satisfaction questionnaires. Remote moderated usability testing methodology using a collaborative virtual reality system performed similarly in terms of the total number of defects identified, the number of high severity defects identified and the time taken to complete the tasks with the other two methodologies. The overall workload experienced by the test participants and facilitators was the least with the traditional lab condition. No significant differences were identified for the workload experienced with the virtual reality and the WebEx conditions. However, test participants experienced greater involvement and a more immersive experience in the virtual environment than in the WebEx condition. The ratings for the virtual environment condition were not significantly different from those for the traditional lab condition. The results of this study suggest that participants were productive and enjoyed the virtual lab condition, indicating the potential of a virtual world based approach as an alternative to conventional approaches for synchronous usability testing.}
}
@article{SEONG2022102994,
title = {Corroborating the effect of positive technology readiness on the intention to use the virtual reality sports game “Screen Golf”: Focusing on the technology readiness and acceptance model},
journal = {Information Processing & Management},
volume = {59},
number = {4},
pages = {102994},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001091},
author = {Bo-Hyun Seong and Chang-Yu Hong},
keywords = {Technology readiness, Technology readiness and acceptance model, Screen golf, Perceived usefulness, Perceived ease of use, Intentions to use},
abstract = {This study investigates the positive and behavioral topic of screen golf, which is often regarded as the most commercially successful virtual reality sports game. Our team analyzed the decision-making process related to screen golf through the widely used the technology readiness and acceptance model to explain the relationships among technology readiness, belief in technology acceptance, and use intentions. The proposed model fit the data satisfactorily, and several of our hypotheses were supported. Structural equation modeling tested the nine hypotheses established based on a literature review, analyzing 350 valid responses obtained through online surveys. Perceived utility (ß = .519**) was the most influential factor in individuals’ plans to participate in the virtual sport. This means that practical considerations such as correcting individuals’ posture and improving their abilities should be prioritized when creating screen golf programs.}
}
@article{CHANNARONG2019677,
title = {Virtual reality barrel shaft design and assembly planning accompany with CAM},
journal = {Procedia Manufacturing},
volume = {30},
pages = {677-684},
year = {2019},
note = {Digital Manufacturing Transforming Industry Towards Sustainable Growth},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.02.063},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919300940},
author = {T. Channarong and B. Suthep},
keywords = {Virtual Reality Design, Assembly Planning, Barrel Shaft, Digital Verification, Optimization, CAM},
abstract = {This paper proposes the virtual reality concept to design and assembly planning for a barrel shaft. The shaft model was created on CAM and digital verification for effective functional design together with simulation as traditional method. Presently, virtual reality technology is sophisticated to mechanical components design and assembly animation which can help a design engineer in order to show possibility alternative assembly planning and achieve optimization. Collaborative virtual environment system was used to create the assembly plan animation.}
}
@article{KASAPAKIS2023100020,
title = {Virtual reality in education: The impact of high-fidelity nonverbal cues on the learning experience},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100020},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000144},
author = {Vlasios Kasapakis and Elena Dzardanova and Androniki Agelada},
keywords = {Virtual reality, Avatars, Motion capture, Non-verbal cues, Education},
abstract = {Integrating variant Information and Communication Technologies (ICTs) into the learning process provides students and teachers alike with specialized tools which eliminate the distance between them and create a classroom-like experience. Virtual Reality (VR) is bound to not only match the qualities of interpersonal communication for distance-learning but reconfigure the learning process altogether, providing students and teachers with novel hyper-tools and methods for presentation and interaction. Virtual Reality Learning Environments (VRLEs) are already being designed, developed, and tested out as an educational tool. Among the less investigated aspects of VRLEs is the impact of avatars and characters Nonverbal Cues (NVCs) on the students' learning experience. This study presents the development of a prototype which uses off-the-shelf technologies commonly used in Social Virtual Reality (SVR) platforms to capture a real professor's body motion and gaze, along with his facial expressions, in real-time, during the delivery of a real lecture. The recorded data are later solved onto a high-fidelity avatar delivering the same lecture in a VRLE. A between-groups study including ninety-six (96) participants, all university students, revealed no correlation between the professor's avatar NVCs fidelity and perceived usability, realism, usefulness, and social presence, and no differences in knowledge acquisition as well.}
}
@article{BASORI201993,
title = {iMars: Intelligent Municipality Augmented Reality Service for Efficient Information Dissemination based on Deep Learning algorithm in Smart City of Jeddah},
journal = {Procedia Computer Science},
volume = {163},
pages = {93-108},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321301},
author = {Ahmad Hoirul Basori and Ahmad Luqman bin {Abdul Hamid} and Andi Besse {Firdausiah Mansur} and Norazah Yusof},
keywords = {Smart City, Deep Learning, Augmented Reality, Municipality Service},
abstract = {A smart city is one of a huge transformation in these millennia, where it combines latest and advanced technology to overcome the daily basis problem of humans. Artificial intelligence with augmented reality has been highlighted as the potential solution along with IOT and another system to support the smart city system. The smarter Jeddah is part of Saudi Vision 2030 where Saudi government tends aims to transform this second biggest city in the kingdom to be a smart city with a ton of applications that connected with IOT and they can monitor people inside the city in real time. The proposed system focused on providing the resident of the Jeddah city with smart apps that can help them on conducting their daily basis activities (tourism) or even in an emergency situation. The geo-location data and other information will be used as a feeder for deep learning to make the system learn and getting smarter to disseminate information to the user based on their needs, location, emergency factors, etc. Resident can access the municipality public facilities such as Jeddah waterfront location and see what the event happens inside. They also can find the location of other attractive location such as Al-Fakieh aquarium and observe the content of aquarium facilities with virtual reality technology. In term of emergency cases: accident, car stopping and another incident; the proposed apps will be able to inform the police patrol nearby or nearest workshop for tyre or car workshop. This app is capable for wide range utilization to support program of the Ministry of Municipal and Rural Affairs to apprehend the smart city integrated application. The simulation test has confidently proved that deep learning algorithm overcome other classifier Naïve Bayes and Random forest with 67.5% recognition rate. The future work can be integrated with IOT sensor based system to enhance the municipality service in smart city.}
}
@article{MEZA20151,
title = {Measuring the potential of augmented reality in civil engineering},
journal = {Advances in Engineering Software},
volume = {90},
pages = {1-10},
year = {2015},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2015.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997815000915},
author = {Sebastjan Meža and Žiga Turk and Matevž Dolenc},
keywords = {Augmented reality, Mobile computing, Computer integrated design, Computer integrated engineering, Civil engineering, Project documentation, Building information modelling, BIM},
abstract = {Recently building information models have substantially improved the explicit semantic content of design information. Information models are used to integrate the initial phases of project development. On the construction site, however, the designs are still mostly represented as line-based paper drawings or projections on portable displays. A generic technology that can integrate information and situate it in time, place and context is augmented reality. The specific research issues addressed are (1) does augmented reality have a potential use in civil engineering, (2) how big – in comparison to other technologies - is this potential and (3) what are the main barriers to its adoption. The generic research issue was to develop a methodology for evaluation of potentials of technology. A prototype was built. It was tested on a real construction site to evaluate the potential of its use using the action-research method. A set of structured interviews with potential users was then conducted to compare the prototype to conventional presentation methods. Using this methodology it has been found out that augmented reality is expected to be as big a step as the transition from 2D line drawings to photorealistic 3D projections. The main barrier to the adoption is immature core virtual reality technology, conservative nature of construction businesses and size of building information models.}
}
@article{PIZZOLANTE2023107876,
title = {Awe in the metaverse: Designing and validating a novel online virtual-reality awe-inspiring training},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107876},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107876},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002273},
author = {Marta Pizzolante and Francesca Borghesi and Eleonora Sarcinella and Sabrina Bartolotta and Carola Salvi and Pietro Cipresso and Andrea Gaggioli and Alice Chirico},
keywords = {Creativity, Metaverse, Complex emotions, Virtual reality, Awe-inspiring training},
abstract = {An increasing number of studies have unveiled the nuanced nature of awe - a complex emotion stemming from stimuli so perceptually and conceptually vast to impact individuals’ current mental frames. Its positive impact on human wellbeing and health have been reported even after a single short exposure to awe-inspiring stimuli. Recently, Virtual Reality (VR) has emerged as a suitable technique for eliciting brief moments of intense awe. Moreover, nowadays, the Metaverse has increased the opportunity to access even complex experiences, such as awe, for a prolonged period. However, the impact of a prolonged exposure to an awe-inspiring simulated experience still must be investigated. Here, in the first study, we designed and tested usability, user experience and preliminary effectiveness of the first VR awe-inspiring training vs. an equivalent neutral training in VR. We relied on an immersive virtual reality online social platform- Altspace VR - for designing the training. In the second study, we investigated whether a prolonged exposure to awe could hold the same effect on creative thinking as it was demonstrated for brief exposure to awe in VR. Specifically, we tested the impact of a long-lasting exposure to awe on creative thinking in the short and on the long run (after the training and in a one-week follow-up) and vs. an equivalent neutral condition (the same as the first study). Creativity thinking, was assessed through Alternative Uses Task (AUT). Additionally, measures related to the disposition to feel positive emotions, social desirability and level of curiosity were collected. The first study supported the feasibility of the training together with the usability of the platform as well as its effectiveness in eliciting awe (vs. neutral condition). Moreover, for the second study, there was a main effect of time on some of the dimensions of creative thinking. Participants scored higher in fluency, originality, and flexibility one week after the training compared to the pre-training phase. These results suggested preliminary design guidelines for creating awe experiences in the Metaverse, unveiling the role of time exposure and duration of their effects on individuals.}
}
@article{CARDENASSAINZ2023100036,
title = {Evaluation of eXtended reality (XR) technology on motivation for learning physics among students in mexican schools},
journal = {Computers & Education: X Reality},
volume = {3},
pages = {100036},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100036},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000302},
author = {Brandon Antonio Cárdenas-Sainz and María Lucía Barrón-Estrada and Ramón Zatarain-Cabada and Maria Elena Chavez-Echeagaray},
keywords = {Extended reality, Virtual reality, Augmented reality, Learning motivation, Physics education, ARCS motivation theory},
abstract = {Developing countries, including Mexico, face the challenge of integrating technology to enhance education and improve learning outcomes. Despite evidence in many settings of the benefits of using virtual reality (VR) and augmented reality (AR) as learning tools, their potential use is still understudied in many developing regions. The objective of the present study is to evaluate the impact of a web-based eXtended Reality (XR) learning tool, PhysXR, among college-level students enrolled in a Mexican University. PhysXR is a web-based learning application designed to present users with information focused on Newtonian mechanics. This tool presents users with interactive experiences ranging from VR to AR environments and supports a physics simulator for experiments on physical phenomena of dynamics and kinematics. Overall, learning methodologies implemented using PhysXR follow the competency-based learning model implemented in Mexican Education Institutions, and include Learn by Doing and Problem Based Learning (PBL). In order to evaluate the PhysXR tool, 99 students were recruited and randomized to either experimental (VR and AR conditions using PhysXR) or control groups. Outcomes included student's learning and motivation, assessed using the John Keller's Attention, Relevance, Confidence and Satisfaction (ARCS) learning motivation model. Results from this study indicate that the use of the PhysXR tool, both VR and AR approaches, generates a significant improvement in learning gains and motivation compared with traditional methods, highlighting the potential of cross-platform capabilities that web-based XR technology could offer, as well as the use of real time physics simulations for learning.}
}
@article{AMIN2024101725,
title = {Maximizing stroke recovery with advanced technologies: A comprehensive assessment of robot-assisted, EMG-Controlled robotics, virtual reality, and mirror therapy interventions},
journal = {Results in Engineering},
volume = {21},
pages = {101725},
year = {2024},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.101725},
url = {https://www.sciencedirect.com/science/article/pii/S2590123023008526},
author = {Faisal Amin and Asim Waris and Javaid Iqbal and Syed Omer Gilani and M. Zia {Ur Rehman} and Shafaq Mushtaq and Niaz Bahadur Khan and M. Ijaz Khan and Mohammed Jameel and Nissren Tamam},
keywords = {Stroke rehabilitation, Robot-assisted therapy, EMG-Controlled robotics, Virtual reality interventions, EMG biofeedback, Intelligent rehabilitation interventions},
abstract = {Inconvenience caused by stroke brings physical disability found in many people which restricts their daily life activities. Globally, it was predicted that the prevalence of stroke will rise to 21.9 % by 2030. To improve the sensory and motor recovery of the stroke survivors, they required high attention for their controllability and adaptability. About 80 % of stroke victims have upper-limb motor deficits which affects their daily living activities. Due to technological advancement in rehabilitation engineering with new interventions, traditional physical therapy for stroke patients now translates into new rehabilitation strategies which help in fast recovery of motor tasks with intention motions. Electromyography (EMG) based stroke rehabilitation now being developed for enhancement and assessment of motion control in clinical settings. The integration of EMG-based intervention provides the feasibility to use the concept of myoelectric control with therapeutic settings. In this paper, the use of EMG-based robot aided therapy has been discussed and highlights the contribution of interventions for stroke rehabilitation. Furthermore, discussion also emphasis on virtual reality and mirror therapy and their latest interventions and approaches carried out by different investigators for the evaluations and assessment of stroke rehabilitation in lieu of considerations for improved functional motor task. The most widely used functional outcome measures for conducting clinical assessment of stroke patients in randomized control trials (RCTs) are Fugl-meyer assessment-upper extremity (FMA-UE), a score range from 0 to 66points, Fugl-meyer assessment for lower extremity (FMA-LE) has a score from 0 to 34 points, the Action Research Arm Test (ARAT), a scale from 0 to 57 points, the Functional independence measure (FIM) has a scale of 18–126 points, Modified Ashworth Scale (MAS), a scale from 0 to 5 points and Box and block test (BBT). Furthermore, in most of the RCTs the statistical significance was set at p < 0.05 (95 % confidence interval). Moreover, future directions suggested that for real time autonomous detection of motion intentions, the EMG-based machine learning must be involved with robots, virtual reality and mirror therapy-based interventions to achieve 95 % accuracy that could leads to the development of intelligent rehabilitation interventions for stroke survivors.}
}
@article{KIA2023104107,
title = {Effects of error rates and target sizes on neck and shoulder biomechanical loads during augmented reality interactions},
journal = {Applied Ergonomics},
volume = {113},
pages = {104107},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104107},
url = {https://www.sciencedirect.com/science/article/pii/S000368702300145X},
author = {Kiana Kia and Jaejin Hwang and Jeong Ho Kim},
keywords = {Human-computer interaction, Head-mounted display, Wearable device, Biomechanics, Musculoskeletal problems},
abstract = {Augmented reality (AR) interactions have been associated with increased biomechanical loads on the neck and shoulders. To provide a better understanding of the factors that may impact such biomechanical loads, this repeated-measures laboratory study evaluated the effects of error rates and target sizes on neck and shoulder biomechanical loads during two standardized AR tasks (omni-directional pointing and cube placing). Twenty participants performed the two AR tasks with different error rates and target sizes. During the tasks, angles, moments, and muscle activity in the neck and shoulders were measured. The results showed that the target sizes and error rates significantly affected angles, moments, and muscle activity in the neck and shoulder regions. Specifically, the presence of errors increased neck extension, shoulder flexion angles and associated moments. Muscle activity in the neck (splenius capitis) and shoulder (anterior and medial deltoids) also increased when the errors were introduced. Moreover, interacting with larger targets resulted in greater neck extension moments and shoulder abduction angles along with higher muscle activity in the splenius capitis and upper trapezius muscles. These findings indicate the importance of reducing errors and incorporating appropriate target sizes in the AR interfaces to minimize risks of musculoskeletal discomfort and injuries in the neck and shoulders.}
}
@article{OYEWUNMI2016263,
title = {On the use of SAFT-VR Mie for assessing large-glide fluorocarbon working-fluid mixtures in organic Rankine cycles},
journal = {Applied Energy},
volume = {163},
pages = {263-282},
year = {2016},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0306261915012672},
author = {Oyeniyi A. Oyewunmi and Aly I. Taleb and Andrew J. Haslam and Christos N. Markides},
keywords = {Organic Rankine cycle (ORC), Power generation, Waste heat recovery, CHP and energy integration, Working fluid mixtures, SAFT},
abstract = {By employing the SAFT-VR Mie equation of state, molecular-based models are developed from which the thermodynamic properties of pure (i.e., single-component) organic fluids and their mixtures are calculated. This approach can enable the selection of optimal working fluids in organic Rankine cycle (ORC) applications, even in cases for which experimental data relating to mixture properties are not available. After developing models for perfluoroalkane (n-C4F10+n-C10F22) mixtures, and validating these against available experimental data, SAFT-VR Mie is shown to predict accurately both the single-phase and saturation properties of these fluids. In particular, second-derivative properties (e.g., specific heat capacities), which are less reliably calculated by cubic equations of state (EoS), are accurately described using SAFT-VR Mie, thereby enabling an accurate prediction of important working-fluid properties such as the specific entropy. The property data are then used in thermodynamic cycle analyses for the evaluation of ORC performance and cost. The approach is applied to a specific case study in which a sub-critical, non-regenerative ORC system recovers and converts waste heat from a refinery flue-gas stream with fixed, predefined conditions. Results are compared with those obtained when employing analogue alkane mixtures (n-C4H10+n-C10H22) for which sufficient thermodynamic property data exist. When unlimited quantities of cooling water are utilized, pure perfluorobutane (and pure butane) cycles exhibit higher power outputs and higher thermal efficiencies compared to mixtures with perfluorodecane (or decane), respectively. The effect of the composition of a working-fluid mixture in the aforementioned performance indicators is non-trivial. Only at low evaporator pressures (<10bar) do the investigated mixtures perform better than the pure fluids. A basic cost analysis reveals that systems with pure perfluorobutane (and butane) fluids are associated with relatively high total costs, but are nevertheless more cost effective per unit power output than the fluid mixtures (due to the higher generated power). When the quantity of cooling water is constrained by the application, overall performance deteriorates, and mixtures emerge as the optimal working fluids.}
}
@article{ZHOU2017112,
title = {Implementation of augmented reality for segment displacement inspection during tunneling construction},
journal = {Automation in Construction},
volume = {82},
pages = {112-121},
year = {2017},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517301796},
author = {Ying Zhou and Hanbin Luo and Yiheng Yang},
keywords = {Tunneling construction, Construction application of augmented reality, Tunnel segment displacement inspection},
abstract = {This paper discusses the feasibility of using Augmented Reality (AR) to rapidly inspect segment displacement during tunneling construction. This technology will allow on-site quality inspectors to retrieve the virtual quality control baseline model which can be established according to the quality standard, and overlay that model onto the real segment displacement in AR. Therefore, the structural safety can be automatically evaluated by measuring the differences between the baseline model and the real facility view. Additionally, the feedback can be potentially used to explore how to enhance the tunnel's stability to avoid significant accidents. A case study has been conducted to evaluate the benefits of the inspection using a prototype AR system over a conventional method. The result shows that all inspections and analyses can be conducted on-site, in real-time, and at a very low cost. The proposed approach might enable AR to be applied in on-site inspections more practically and effectively.}
}
@article{TU2021104402,
title = {Augmented reality based navigation for distal interlocking of intramedullary nails utilizing Microsoft HoloLens 2},
journal = {Computers in Biology and Medicine},
volume = {133},
pages = {104402},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104402},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001967},
author = {Puxun Tu and Yao Gao and Abel J Lungu and Dongyuan Li and Huixiang Wang and Xiaojun Chen},
keywords = {Augmented reality, Intramedullary nailing, Optical see-through head-mounted display, Surgical navigation, Orthopedic and trauma surgery},
abstract = {Background and objective
The distal interlocking of intramedullary nail remains a technically demanding procedure. Existing augmented reality based solutions still suffer from hand-eye coordination problem, prolonged operation time, and inadequate resolution. In this study, an augmented reality based navigation system for distal interlocking of intramedullary nail is developed using Microsoft HoloLens 2, the state-of-the-art optical see-through head-mounted display.
Methods
A customized registration cube is designed to assist surgeons with better depth perception when performing registration procedures. During drilling, surgeons can obtain accurate and in-situ visualization of intramedullary nail and drilling path, and dynamic navigation is enabled. An intraoperative warning system is proposed to provide intuitive feedback of real-time deviations and electromagnetic disturbances.
Results
The preclinical phantom experiment showed that the reprojection errors along the X, Y, and Z axes were 1.55 ± 0.27 mm, 1.71 ± 0.40 mm, and 2.84 ± 0.78 mm, respectively. The end-to-end evaluation method indicated the distance error was 1.61 ± 0.44 mm, and the 3D angle error was 1.46 ± 0.46°. A cadaver experiment was also conducted to evaluate the feasibility of the system.
Conclusion
Our system has potential advantages over the 2D-screen based navigation system and the pointing device based navigation system in terms of accuracy and time consumption, and has tremendous application prospects.}
}
@article{DAWOOD2023102861,
title = {Uncertainty aware training to improve deep learning model calibration for classification of cardiac MR images},
journal = {Medical Image Analysis},
volume = {88},
pages = {102861},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102861},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001214},
author = {Tareen Dawood and Chen Chen and Baldeep S. Sidhu and Bram Ruijsink and Justin Gould and Bradley Porter and Mark K. Elliott and Vishal Mehta and Christopher A. Rinaldi and Esther Puyol-Antón and Reza Razavi and Andrew P. King},
keywords = {Uncertainty, Calibration, AI, Reliability, Trust, CRT, CAD},
abstract = {Quantifying uncertainty of predictions has been identified as one way to develop more trustworthy artificial intelligence (AI) models beyond conventional reporting of performance metrics. When considering their role in a clinical decision support setting, AI classification models should ideally avoid confident wrong predictions and maximise the confidence of correct predictions. Models that do this are said to be well calibrated with regard to confidence. However, relatively little attention has been paid to how to improve calibration when training these models, i.e. to make the training strategy uncertainty-aware. In this work we: (i) evaluate three novel uncertainty-aware training strategies with regard to a range of accuracy and calibration performance measures, comparing against two state-of-the-art approaches, (ii) quantify the data (aleatoric) and model (epistemic) uncertainty of all models and (iii) evaluate the impact of using a model calibration measure for model selection in uncertainty-aware training, in contrast to the normal accuracy-based measures. We perform our analysis using two different clinical applications: cardiac resynchronisation therapy (CRT) response prediction and coronary artery disease (CAD) diagnosis from cardiac magnetic resonance (CMR) images. The best-performing model in terms of both classification accuracy and the most common calibration measure, expected calibration error (ECE) was the Confidence Weight method, a novel approach that weights the loss of samples to explicitly penalise confident incorrect predictions. The method reduced the ECE by 17% for CRT response prediction and by 22% for CAD diagnosis when compared to a baseline classifier in which no uncertainty-aware strategy was included. In both applications, as well as reducing the ECE there was a slight increase in accuracy from 69% to 70% and 70% to 72% for CRT response prediction and CAD diagnosis respectively. However, our analysis showed a lack of consistency in terms of optimal models when using different calibration measures. This indicates the need for careful consideration of performance metrics when training and selecting models for complex high risk applications in healthcare.}
}
@article{URBAS2019832,
title = {Displaying Product Manufacturing Information in Augmented Reality for Inspection},
journal = {Procedia CIRP},
volume = {81},
pages = {832-837},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.208},
url = {https://www.sciencedirect.com/science/article/pii/S221282711930513X},
author = {Uroš Urbas and Rok Vrabič and Nikola Vukašinović},
keywords = {model-based definition, product manufacturing information, augmented reality, graphical presentation},
abstract = {Model-based definition and augmented reality are two emerging technologies, which enable a faster product development cycle and better dimensional control. They are an enabler of industry 4.0, and a better way to transfer the data to the user. Although each of the technologies is well researched, their combination presents a challenge from a research perspective. Model-based definition enables the product manufacturing data to be used in downstream processes, which include quality control and inspection. We developed and presented a method that enables the transfer of product manufacturing information from a 3D model and displays the graphical presentation to the user in augmented reality. The presented data is expected to aid the user with performing quality control more effectively, which needs to be tested.}
}
@article{SHI201750,
title = {Individual refinement of attenuation correction maps for hybrid PET/MR based on multi-resolution regional learning},
journal = {Computerized Medical Imaging and Graphics},
volume = {60},
pages = {50-57},
year = {2017},
note = {Computational Methods for Molecular Imaging},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895611116301033},
author = {Kuangyu Shi and Sebastian Fürst and Liang Sun and Mathias Lukas and Nassir Navab and Stefan Förster and Sibylle I. Ziegler},
keywords = {PET/MR, Attenuation correction, Multi-resolution, Machine learning},
abstract = {PET/MR is an emerging hybrid imaging modality. However, attenuation correction (AC) remains challenging for hybrid PET/MR in generating accurate PET images. Segmentation-based methods on special MR sequences are most widely recommended by vendors. However, their accuracy is usually not high. Individual refinement of available certified attenuation maps may be helpful for further clinical applications. In this study, we proposed a multi-resolution regional learning (MRRL) scheme to utilize the internal consistency of the patient data. The anatomical and AC MR sequences of the same subject were employed to guide the refinement of the provided AC maps. The developed algorithm was tested on 9 patients scanned consecutively with PET/MR and PET/CT (7 [18F]FDG and 2 [18F]FET). The preliminary results showed that MRRL can improve the accuracy of segmented attenuation maps and consequently the accuracy of PET reconstructions.}
}
@article{AKCAYIR2016334,
title = {Augmented reality in science laboratories: The effects of augmented reality on university students’ laboratory skills and attitudes toward science laboratories},
journal = {Computers in Human Behavior},
volume = {57},
pages = {334-342},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215303253},
author = {Murat Akçayır and Gökçe Akçayır and Hüseyin Miraç Pektaş and Mehmet Akif Ocak},
keywords = {Augmented reality, Science laboratory, Higher education, Laboratory skills, Attitude},
abstract = {This study investigated the effects of the use of augmented reality (AR) technologies in science laboratories on university students' laboratory skills and attitudes towards laboratories. A quasi-experimental pre-test/post-test control group design was employed. The participants were 76 first-year university students, aged 18–20 years old. They were assigned to either an experimental or a control group. Qualitative and quantitative data collection tools were used. The experimental results obtained following the 5-week application revealed that the AR technology significantly enhanced the development of the university students' laboratory skills. AR technology both improved the students’ laboratory skills and helped them to build positive attitudes towards physics laboratories. The statements of the students and the instructor regarding other effects of AR technology on science laboratories, both negative and positive, are also discussed.}
}
@article{MANDAL2020111518,
title = {Synchrotron GIXRD and slow positron beam characterisation of Ar ion irradiated pure V and V-4Cr-4Ti alloy: Candidate structural material for Fusion reactor application},
journal = {Fusion Engineering and Design},
volume = {154},
pages = {111518},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111518},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620300661},
author = {Sudipta Mandal and S.K. Sharma and N. Gayathri and K. Sudarshan and P. Mukherjee and P.K. Pujari and Ranjini Menon and P.Y. Nabhiraj and Archna Sagdeo},
keywords = {Irradiation, Synchrotron X-ray diffraction, XRDLPA, Positron annihilation spectroscopy},
abstract = {Pure V and V-4Cr-4Ti has been irradiated with Ar9+ion at 0.4, 2.3, 8.4 and 23 dpa. GIXRD was carried at two different incident angles i.e. at 0.2° and 0.45° using Synchrotron radiation. The XRD data were analyzed to determine the variation of microstructural parameters like domain size and microstrain as a function of dose by modified Rietveld technique. The dislocation density calculated from these parameters show contrasting behaviour in pure V and V-4Cr-4Ti as a function of dose. In pure V the dislocation density increases at the highest dose, whereas in V-4Cr-4Ti it saturates after 8.4 dpa. However, in V-4Cr-4Ti at the highest dpa clear evidence of segregation of the alloying elements is observed. Slow positron annihilation spectroscopy has been used to characterize the irradiation induced depth dependent changes in the microstructure with dose. The positron diffusion lengths in pure V (202 nm) and V-4Cr-4Ti (370 nm) indicate a defect free bulk state. An increase in S-parameter (∼1.07Sbulk) confirms the creation of vacancy type open volume defects in the damaged region (0−300 nm from surface) of the ion-irradiated samples. It was observed that that there is no appreciable change in open volume defect density and type of defects up to 8.4 dpa for pure V except at the highest dose. However, in contrary to pure V, the increase in S-parameter at highest dpa clearly indicates that the type of defects at highest dose is significantly different from the other samples.}
}
@article{GORS2021714,
title = {An adaptable framework to provide AR-based work instructions and assembly state tracking using an ISA-95 ontology},
journal = {Procedia CIRP},
volume = {104},
pages = {714-719},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.120},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010180},
author = {Dorothy Gors and Merwan Birem and Roeland De Geest and Corentin Domken and Vasilios Zogopoulos and Steven Kauffmann and Maarten Witters},
keywords = {Assembly, ISA-95, Digtal Instructions, Augmented Reality},
abstract = {The high degree of digitalization in modern manufacturing systems and the increase in the systems that need to exchange data so that the production information reaches the target employee has pushed forward the need to apply standardized ontologies. This article presents how a modular framework to create and provide AR-based work instructions coupled with image-based state tracking can be modelled in an ontology based on the industrial ISA-95 standard to represent the data exchange among the different modules. The proposed modelling of the data exchanged in such a framework is validated in a use case from the agriculture machinery industry.}
}
@article{DALLEL2023105655,
title = {Digital twin of an industrial workstation: A novel method of an auto-labeled data generator using virtual reality for human action recognition in the context of human–robot collaboration},
journal = {Engineering Applications of Artificial Intelligence},
volume = {118},
pages = {105655},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105655},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006455},
author = {Mejdi Dallel and Vincent Havard and Yohan Dupuis and David Baudry},
keywords = {Digital Twins (DTs), Industry 4.0, Auto-labeled data generation, Virtual Reality (VR), Human Action Recognition (HAR), Human–Robot Collaboration (HRC)},
abstract = {The recognition of human actions based on artificial intelligence methods to enable Human–Robot Collaboration (HRC) inside working environments remains a challenge, especially because of the necessary huge training datasets needed. Meanwhile, Digital Twins (DTs) of human centered productions are increasingly developed and used in the design and operation phases. As instance, DTs are already helping industries to design, visualize, monitor, manage, and maintain their assets more effectively. However, few works are dealing with using DTs as a dataset generator tool. Therefore, this paper explores the use of a DT of a real industrial workstation involving assembly tasks with a robotic arm interfaced with Virtual Reality (VR) to extract a digital human model. The DT simulates assembly operations performed by humans aiming to generate self-labeled data. Thereby, a Human Action Recognition dataset named InHARD-DT was created to validate a real use case in which we use the acquired auto-labeled DT data of the virtual representation of the InHARD dataset to train a Spatial–Temporal Graph Convolutional Neural Network with skeletal data on one hand. On the other hand, the Physical Twin (PT) data of the InHARD dataset was used for testing. Obtained results show the effectiveness of the proposed method.}
}
@article{MASULLO2022205,
title = {Virtual Reality Overhead Crane Simulator},
journal = {Procedia Computer Science},
volume = {200},
pages = {205-215},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002289},
author = {Massimiliano Masullo and Aniello Pascale and Roxana Adina Toma and Gennaro Ruggiero and Luigi Maffei},
keywords = {Virtual reality, crane, training, simulator, safety},
abstract = {In recent decades, government investments in occupational safety and health programs, as well as the development and training of workers’ skills have taken place. Furthermore, the demand for training methods tailored to the needs of workers led the researchers to question the effectiveness of conventional training methods and investigate new ones. Innovative training systems such as virtual reality-based ones allow certain constraints to be overcome and researchers to investigate the psychophysiological factors and environmental settings that can influence workers’ performances. Virtual reality-based training systems are used in various fields. Heavy load handling is predominant in different industrial sectors, and false use of cranes can cause serious accidents with fatal consequences. After a preliminary investigation of the most hazardous scenarios in Italy, a Virtual Reality Overhead Crane Simulator has been built. In previous research, the simulator was used to investigate different typologies of noise (low frequency, high frequency and modulated) on the overhead crane operators’ performance and physiological responses. The preliminary results of this investigation revealed that high-frequency noise influences more than others the errors of oblique manoeuvres and excessive oscillation. The same results, although not conclusive, showed that modulated noise seems to highly affect the variations of the GSR and the LF/HF ratio of heart rate. This paper aims to illustrate the workflow used to build up the Virtual Reality Overhead Crane Simulator from the 3d modelling to the construction of the virtual scenario and the interacting programming used to investigate some critical aspects of the handling of heavy loads in industrial environments highlighting the criticalities found during the studies carried until now and the future approaches}
}
@article{BLANKEMEYER2018155,
title = {Intuitive Robot Programming Using Augmented Reality},
journal = {Procedia CIRP},
volume = {76},
pages = {155-160},
year = {2018},
note = {7th CIRP Conference on Assembly Technologies and Systems (CATS 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300933},
author = {Sebastian Blankemeyer and Rolf Wiemann and Lukas Posniak and Christoph Pregizer and Annika Raatz},
keywords = {augmented reality, intuitive robot programming, human-machine-interaction},
abstract = {The demands on companies caused by the markets are becoming more and more fast-moving and complex. Reasons are the increasing number of variants of products as well as reduced product life cycle times. This is particularly relevant for assembly tasks, since a very high flexibility and adaptability to varying ambient condition must be ensured in this area. Therefore, a lot of steps are currently being carried out manually. However, if an automation is attempted in the field of assembly, companies are often facing a trade-off between a high degree of automation and flexibility of the production system. A concept to increase the automation rate in assembly tasks can be seen in the use of collaborative robots, so that the benefits of both, humans and robots, can be combined to accomplish the task. In doing so, one key issue is to simplify and thus to accelerate the programming process so that the necessary programming skills of employees can be reduced. Although some of today’s collaborative robots already offer good programming approaches like kinesthetic teaching, this article introduces a new and more intuitive programming method which is based on Augmented Reality. For this purpose, components of an assembly group are virtually linked with CAD models by using optical markers. The operator can then virtually assemble the components according to the assembly sequence. Results of first tests indicate, that once the assembly process is recorded, the robot can accomplish the assembly in reality. Finally, possibilities for future developments are presented.}
}
@article{FAN2024108156,
title = {A hybrid robotic system for zygomatic implant placement based on mixed reality navigation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {249},
pages = {108156},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108156},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001524},
author = {Xingqi Fan and Yuan Feng and Baoxin Tao and Yihan Shen and Yiqun Wu and Xiaojun Chen},
keywords = {Zygomatic implant surgery, Mixed reality navigation, Hybrid robotic system},
abstract = {Backgrounds
Zygomatic implant (ZI) placement surgery is a viable surgical option for patients with severe maxillary atrophy and insufficient residual maxillary bone. Still, it is difficult and risky due to the long path of ZI placement and the narrow field of vision. Dynamic navigation is a superior solution, but it presents challenges such as requiring operators to have advanced skills and experience. Moreover, the precision and stability of manual implantation remain inadequate. These issues are anticipated to be addressed by implementing robot-assisted surgery and achieved by introducing a mixed reality (MR) navigation-guided hybrid robotic system for ZI placement surgery.
Methods
This study utilized a hybrid robotic system to perform the ZI placement surgery. Our first step was to reconstruct a virtual 3D model from preoperative cone-beam CT (CBCT) images. We proposed a series of algorithms based on coordinate transformation, which includes image-phantom registration, HoloLens-tracker registration, drill-phantom calibration, and robot-implant calibration, to unify all objects within the same coordinate system. These algorithms enable real-time tracking of the surgical drill's position and orientation relative to the patient phantom. Subsequently, the surgical drill is directed to the entry position, and the planned implantation paths are superimposed on the patient phantom using HoloLens 2 for visualization. Finally, the hybrid robot system performs the processed of drilling, expansion, and placement of ZIs under the guidance of the MR navigation system.
Results
Phantom experiments of ZI placement were conducted using 10 patient phantoms, with a total of 40 ZIs inserted. Out of these, 20 were manually implanted, and the remaining 20 were robotically implanted. Comparisons between the actual implanted ZI paths and the preoperatively planned ZI paths showed that our MR navigation-guided hybrid robotic system achieved a coronal deviation of 0.887 ± 0.213 mm, an apical deviation of 1.201 ± 0.318 mm, and an angular deviation of 3.468 ± 0.339° This demonstrates significantly better accuracy and stability than manual implantation.
Conclusion
Our proposed hybrid robotic system enables automated ZI placement surgery guided by MR navigation, achieving greater accuracy and stability compared to manual operations in phantom experiments. Furthermore, this system is expected to apply to animal and cadaveric experiments, to get a good ready for clinical studies.}
}
@article{ZABANRAN2020163718,
title = {The effects of gold nanoparticles characteristics and laser irradiation conditions on spatiotemporal temperature pattern of an agar phantom: A simulation and MR thermometry study},
journal = {Optik},
volume = {202},
pages = {163718},
year = {2020},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2019.163718},
url = {https://www.sciencedirect.com/science/article/pii/S003040261931616X},
author = {Mohammad Zabanran and Mohamadreza Asadi and Arash Zare-Sadeghi and Ali {Abbasian Ardakani} and Ali Shakeri-Zadeh and Ali Komeili and S. Kamran Kamrava and Behafarid Ghalandari},
keywords = {Cancer, Nanotechnology, Laser, Photothermal effects, Thermometry},
abstract = {In this paper, the effects of parameters related to gold nanoparticles (type, size, and concentration) and the laser parameters on spatiotemporal temperature pattern of an agar phantom during a photothermal therapy (PTT) procedure were modeled and then experimentally verified. Eight agar phantoms loaded by gold nanoparticles were made. An agar phantom without any nanoparticles was also considered as the control. Different sizes of two types of gold nanoparticles (spherical and silica-gold core shell) at various concentrations were studied. The phantoms were irradiated by various laser powers for 5 min. The temperature changes in each phantom was firstly calculated using COMSOL Multiphysics software. Also, each phantom was irradiated by laser and MR thermometry was performed to validate the simulation results. A reasonable correlation between simulation and MR thermometry was obtained (R = 0.92). The error interval between calculations and experiments was ranged from ±3% to ±6%. It was clearly evident that laser irradiation conditions and nanoparticle characteristics affected the temperature rise profile. Spherical 20 nm gold nanoparticles had better thermal efficiency and generated higher level of heat. The protocol suggested in this study may be appropriate to make a pre-clinical calculation and effect visualization for any nanoparticles-based PTT procedure before entrance into the clinics.}
}
@article{WANG2024102402,
title = {A 3D framework for segmentation of carotid artery vessel wall and identification of plaque compositions in multi-sequence MR images},
journal = {Computerized Medical Imaging and Graphics},
volume = {116},
pages = {102402},
year = {2024},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102402},
url = {https://www.sciencedirect.com/science/article/pii/S089561112400079X},
author = {Jian Wang and Fan Yu and Mengze Zhang and Jie Lu and Zhen Qian},
keywords = {Atherosclerosis, Carotid artery, Multi-sequence MR images, Plaque components, Segmentation},
abstract = {Accurately assessing carotid artery wall thickening and identifying risky plaque components are critical for early diagnosis and risk management of carotid atherosclerosis. In this paper, we present a 3D framework for automated segmentation of the carotid artery vessel wall and identification of the compositions of carotid plaque in multi-sequence magnetic resonance (MR) images under the challenge of imperfect manual labeling. Manual labeling is commonly done in 2D slices of these multi-sequence MR images and often lacks perfect alignment across 2D slices and the multiple MR sequences, leading to labeling inaccuracies. To address such challenges, our framework is split into two parts: a segmentation subnetwork and a plaque component identification subnetwork. Initially, a 2D localization network pinpoints the carotid artery’s position, extracting the region of interest (ROI) from the input images. Following that, a signed-distance-map-enabled 3D U-net (Çiçek etal, 2016)an adaptation of the nnU-net (Ronneberger and Fischer, 2015) segments the carotid artery vessel wall. This method allows for the concurrent segmentation of the vessel wall area using the signed distance map (SDM) loss (Xue et al., 2020) which regularizes the segmentation surfaces in 3D and reduces erroneous segmentation caused by imperfect manual labels. Subsequently, the ROI of the input images and the obtained vessel wall masks are extracted and combined to obtain the identification results of plaque components in the identification subnetwork. Tailored data augmentation operations are introduced into the framework to reduce the false positive rate of calcification and hemorrhage identification. We trained and tested our proposed method on a dataset consisting of 115 patients, and it achieves an accurate segmentation result of carotid artery wall (0.8459 Dice), which is superior to the best result in published studies (0.7885 Dice). Our approach yielded accuracies of 0.82, 0.73 and 0.88 for the identification of calcification, lipid-rich core and hemorrhage components. Our proposed framework can be potentially used in clinical and research settings to help radiologists perform cumbersome reading tasks and evaluate the risk of carotid plaques.}
}
@article{MAHMOOD2022114333,
title = {VR-enabled portable brain-computer interfaces via wireless soft bioelectronics},
journal = {Biosensors and Bioelectronics},
volume = {210},
pages = {114333},
year = {2022},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2022.114333},
url = {https://www.sciencedirect.com/science/article/pii/S0956566322003736},
author = {Musa Mahmood and Noah Kim and Muhammad Mahmood and Hojoong Kim and Hyeonseok Kim and Nathan Rodeheaver and Mingyu Sang and Ki Jun Yu and Woon-Hong Yeo},
abstract = {Noninvasive, wearable brain-computer interfaces (BCI) find limited use due to their obtrusive nature and low information. Currently available portable BCI systems are limited by device rigidity, bulky form factors, and gel-based skin-contact electrodes – and therefore more prone to noise and motion artifacts. Here, we introduce virtual reality (VR)-enabled split-eye asynchronous stimulus (SEAS) allowing a target to present different stimuli to either eye. This results in unique asynchronous stimulus patterns measurable with as few as four EEG electrodes, as demonstrated with improved wireless soft electronics for portable BCI. This VR-embedded SEAS paradigm demonstrates potential for improved throughput with a greater number of unique stimuli. A wearable soft platform featuring dry needle electrodes and shielded stretchable interconnects enables high throughput decoding of steady-state visually evoked potentials (SSVEP) for a text spelling interface. A combination of skin-conformal electrodes and soft materials offers high-quality recordings of SSVEP with minimal motion artifacts, validated by comparing the performance with a conventional wearable system. A deep-learning algorithm provides real-time classification, with an accuracy of 78.93% for 0.8 s and 91.73% for 2 s with 33 classes from nine human subjects, allowing for a successful demonstration of VR text spelling and navigation of a real-world environment. With as few as only four data recording channels, the system demonstrates a highly competitive information transfer rate (243.6 bit/min). Collectively, the VR-enabled soft system offers unique advantages in wireless, real-time monitoring of brain signals for portable BCI, neurological rehabilitation, and disease diagnosis.}
}
@article{ROHM2021536,
title = {AR based Assistance for the Tool Change of Cyber-Physical Systems},
journal = {Procedia CIRP},
volume = {104},
pages = {536-541},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.090},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009884},
author = {Benjamin Röhm and Johannes Olbort and Reiner Anderl},
keywords = {Augmented Reality, Cyber-Physical System, CNC Milling Machine, HMI},
abstract = {The Department of Computer Integrated Design at the TU Darmstadt deals with new innovative interaction possibilities between workers and Cyber-Physical Systems on the shop floor. In this paper, technologies of Augmented Reality play an essential role. Thus, a digital workflow between product development, planning and production can be established. For this purpose, a concept for AR-based assistance for the tool change of a CNC machine was developed. Based on the processed instructions for a workpiece, information for a complete tool change is progressively provided to the operator. The concept was implemented and validated using a Microsoft HoloLens and a 3-axis CNC milling machine.}
}
@article{KIM201628,
title = {Predicting the use of smartphone-based Augmented Reality (AR): Does telepresence really help?},
journal = {Computers in Human Behavior},
volume = {59},
pages = {28-38},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216300012},
author = {Hyeon-Cheol Kim and Martin Yongho Hyun},
keywords = {Revised TAM model, TMH model, Usefulness, Telepresence, Mediation effects},
abstract = {The purpose of this study was to determine what predicts the use of smartphone-based Augmented Reality (AR). The study proposes two models to determine whether telepresence can substitute for usefulness and whether both usefulness in the revised technology acceptance model and telepresence in the telepresence mediation hypothesis model can mediate the relationship among three types of AR quality and the intention to reuse AR. Two models were tested with 134 undergraduates who experienced the smartphone-based AR application OVJET. Two competing models demonstrate that all hypothesized paths in the revised technology acceptance model and the telepresence mediation hypothesis model are significant except for one path from service quality to telepresence. A path difference test shows that three pairs of paths (system quality → usefulness vs. system quality → telepresence; service quality → usefulness vs. service quality → telepresence; and usefulness → AR reuse intention vs. telepresence → AR reuse intention) are significantly different, while one pair of the paths (i.e., information system → elepresence vs. information quality → usefulness) is not. In addition, usefulness performs both partial and complete mediating roles, while telepresence has only a complete mediation effect.}
}
@article{AYACHIOMAR2022100696,
title = {Effect of cooling system design on the heat dissipation of the magnetron sensitive components with rectangular target during sputtering by Ar+},
journal = {Results in Engineering},
volume = {16},
pages = {100696},
year = {2022},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2022.100696},
url = {https://www.sciencedirect.com/science/article/pii/S2590123022003668},
author = {Ali {Ayachi Omar} and Nail Faikovich Kashapov and Alexander Grigoryvich Luchkin and Asma {Ayachi Amor} and Abdelouahed {Ayachi Amar}},
keywords = {Magnetron, Monte Carlo simulation, Computational fluid dynamics, Curie-temperature, Heat dissipation, Cooling system},
abstract = {The temperature of the magnetron-sensitive components should be kept below the threshold limit during sputtering. Overheating over the Curie-temperature of the magnets can occur if the heat generated in the magnetron is not sufficiently dissipated, causing the collapse of the entire sputtering process can occur. To ensure proper dissipation of the heat generated in the magnetron and understand the process, we proposed 3D model, through which we were able to model the process and determine the heat source on the surface of the target using Monte Carlo simulation. With the target being a heat source in the magnetron, using Computational fluid dynamics model with Fourier's-law, we simulated the heat transfer inside a magnetron and validated the simulation results by measuring the temperature using a thermal camera. Then we analyzed the heat fluxes and heat exchange with different cooling-water paths, and we explained their effect on the heat transfer inside a magnetron. The results indicate that according to the calculation criteria, an appropriate design must be selected that ensures that the cooling-water reaches the heat source to maximize heat dissipation and keep the temperature below the Curie-temperature level of the magnets. Engineers and researchers can use this model to optimize process efficiency and explore new designs, while at the same time reducing the need for costly experimental trials.}
}
@incollection{JAVAID202395,
title = {Chapter Six - Role of virtual reality for healthcare education},
editor = {Samiya Khan and Mansaf Alam and Shoaib Amin Banday and Mohammed Shaukat Usta},
booktitle = {Extended Reality for Healthcare Systems},
publisher = {Academic Press},
pages = {95-113},
year = {2023},
isbn = {978-0-323-98381-5},
doi = {https://doi.org/10.1016/B978-0-323-98381-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983815000167},
author = {Mohd Javaid and Ibrahim Haleem Khan and Rajiv Suman and Shahbaz Khan},
keywords = {Education, Healthcare, Learning efficiency, Training, Virtual reality (VR)},
abstract = {In the healthcare field, virtual reality (VR) is emerging as a high-tech approach to improve the learning and training of medical practitioners with a focus on healthcare workers. Medical practitioners need to have up-to-date teaching and learning methods and processes in healthcare education and treatment. Health educators can focus on VR to store patients' data in 3D data points and provide the same virtually to assistants, students, and tutors. Very few are using this technique to evaluate the success of medical students and examine them. VR is used to analyze the status of patient bones, skin organs, blood vessels, and muscles. It enables virtual observation at any angle and enhances the comprehension of anatomy greatly. In advance of authentic medical practice, it allows students to establish contactless processes with diverse patients and improve clinical outcomes. VR inpatient treatment is used to boost medical education quality and medical standards. It helps medical students view a patient situation from various angles with diverse controls and variable evaluation techniques. VR technology allows broader and more flexible access to medical training. Medical practitioners can use VR to image the human body's interior and learn about human anatomy more effectively. This chapter provides various benefits and advancements of VR in healthcare. This chapter further identifies and discusses the significant role of VR in healthcare education. Clinical practitioners use this technology to communicate the effects of unpleasant lifestyles, such as harmful medicaments, obesity, tumor formation, lung, cardiovascular problems, liver, and consequences of smoking. It provides valuable exploration and reduced error while performing the actual treatment process. In the future, VR will become a way forward developed technology for learning and evaluation, such as rehearsing complicated operations, and other often seen situations at a hospital. Specialists will also be best trained to deal with complicated and dynamic medical conditions using the 3D data points visualized.}
}
@article{OH201621,
title = {Advanced Navigation Aids System based on Augmented Reality},
journal = {International Journal of e-Navigation and Maritime Economy},
volume = {5},
pages = {21-31},
year = {2016},
issn = {2405-5352},
doi = {https://doi.org/10.1016/j.enavi.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405535216300183},
author = {Jaeyong OH and Sekil Park and Oh-Seok Kwon},
keywords = {navigational aids system, augmented reality, IBS(Integrated Bridge System)},
abstract = {Many maritime accidents have been caused by human-error including such things as inadequate watch keeping and/or mistakes in ship handling. Also, new navigational equipment has been developed using Information Technology (IT) technology to provide various kinds of information for safe navigation. Despite these efforts, the reduction of maritime accidents has not occurred to the degree expected because, navigational equipment provides too much information, and this information is not well organized, such that users feel it to be complicated rather than helpful. In this point of view, the method of representation of navigational information is more important than the quantity of that information and research is required on the representation of information to make that information more easily understood and to allow decisions to be made correctly and promptly. In this paper, we adopt Augmented Reality (AR) technologies for the representation of information. AR is a 3D computer graphics technology that blends virtual reality and the real world. Recently, this technology has been widely applied in our daily lives because it can provide information more effectively to users. Therefore, we propose a new concept, a navigational system based on AR technology; we review experimental results from a ship-handling simulator and from an open sea test to verify the efficiency of the proposed system.}
}
@article{SCOROLLI2023107910,
title = {Would you rather come to a tango concert in theater or in VR? Aesthetic emotions & social presence in musical experiences, either live, 2D or 3D},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107910},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107910},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002613},
author = {Claudia Scorolli and Eduardo {Naddei Grasso} and Lorenzo Stacchio and Vincenzo Armandi and Giovanni Matteucci and Gustavo Marfia},
keywords = {Musical aesthetic experience, Immersive concert, Virtual reality, Aesthetic emotions, Social presence, Embodied experience, Psychology of art, Empirical aesthetics},
abstract = {This work shows the preliminary results of a pioneering project aimed at comparing the aesthetic experience of a musical concert experienced in different contexts for which the audience's perceived presence is modulated in a continuum ranging from a live concert to a music video, passing through immersive artificial environments. In contrast to previous qualitative investigations of various immersive contexts, our study is unique in both the use of validated scales and the structured comparison of four experimental conditions: 1.live concert (LC), 2.the same concert through a traditional non-immersive music video (MV, analogous to fruition on YouTube), and finally in a virtual reality environment (VR), provided by two different devices, 3. a google cardboard (CVR) and 4. an HTC vive (HVR), allowing respectively for a basic and easily accessible experience, or for a less affordable but more immersive one. Through these manipulations we presumably affected not just the subjective aesthetic experience, but also the perceived presence of the Other/s. Consistently we measured both through the administration of the Aesthetic Emotions Scale (Aesthemos) and the Networked Minds Measure of Social Presence (NMMSP). The NMMSP showed no notable differences between conditions, which instead emerged from the analyses on the Aesthemos. The most liked experience was the Live one. Results also showed that LC experience had a stronger emotional impact only when compared to MV and CVR, but not to HTC since this last manipulation was the one eliciting the greatest interest. Theoretical implications are critically discussed, suggesting novel applications of the proposed approach.}
}
@article{LIU201879,
title = {Precision study on augmented reality-based visual guidance for facility management tasks},
journal = {Automation in Construction},
volume = {90},
pages = {79-90},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518301225},
author = {Fei Liu and Stefan Seipel},
keywords = {Augmented reality “X-ray vision”, Facility management, Positioning task, Precision study, Spatial judgment, Experiment},
abstract = {One unique capability of augmented reality (AR) is to visualize hidden objects as a virtual overlay on real occluding objects. This “X-ray vision” visualization metaphor has proved to be invaluable for operation and maintenance tasks such as locating utilities behind a wall. Locating virtual occluded objects requires users to estimate the closest projected positions of the virtual objects upon their real occluders, which is generally under the influence of a parallax effect. In this paper we studied the task of locating virtual pipes behind a real wall with “X-ray vision” and the goal is to establish relationships between task performance and spatial factors causing parallax through different forms of visual augmentation. We introduced and validated a laser-based target designation method which is generally useful for AR-based interaction with augmented objects beyond arm's reach. The main findings include that people can mentally compensate for the parallax error when extrapolating positions of virtual objects on the real surface given traditional 3D depth cues for spatial understanding. This capability is, however, unreliable especially in the presence of the increasing viewing offset between the users and the virtual objects as well as the increasing distance between the virtual objects and their occluders. Experiment results also show that positioning performance is greatly increased and unaffected by those factors if the AR support provides visual guides indicating the closest projected positions of virtual objects on the surfaces of their real occluders.}
}
@article{KNEISSL202017564,
title = {Mixed-Reality Testing of Multi-Vehicle Coordination in an Automated Valet Parking Environment},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17564-17571},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2669},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334297},
author = {Maximilian Kneissl and Sebastian {vom Dorff} and Adam Molin and Maxime Denniel and Tong Duy Son and Nicolas Ochoa Lleras and Hasan Esen and Sandra Hirche},
keywords = {Mixed-reality, Simulation, Interactive Vehicle Control, Automated Valet Parking, AVP, Virtual Test},
abstract = {The development of highly automated driving functions requires rigorous testing to demonstrate the safety and functionality of the automated vehicle. One open question is how to perform such tests to sufficiently prove the vehicle’s capabilities. Designing a proper testing platform is particularly important for multi-vehicle scenarios, because test setups with actual real vehicles are not scalable. This paper proposes a mixed-reality testing framework which seamlessly combines virtual and real testing. The authors conducted a mixed-reality experiment of an automated valet parking (AVP) scenario, where virtual vehicles interact with a real vehicle-in-the-loop system in a simulated world. The experiment was developed to evaluate the algorithmic design of a distributed control scheme for an AVP system. Comparing the mixed-reality results to those of a pure virtual simulation allowed the authors to demonstrate the robustness of the algorithmic design against certain disturbances, while also revealing which parts of the system were sufficiently or inadequately modeled during the development process.}
}
@article{LI2021101379,
title = {The impact of CCT on driving safety in the normal and accident situation: A VR-based experimental study},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101379},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101379},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001324},
author = {Xiaojun Li and Jiaxin Ling and Yi Shen and Tong Lu and Shouzhong Feng and Hehua Zhu},
keywords = {CCT, Virtual reality, Reaction time, Driving fatigue, Rear-end accident, Road tunnel},
abstract = {Correlated color temperature (CCT) of the light source inside the road tunnel plays a crucial role in ensuring driving safety, which is demonstrated by previous studies that CCT influences not only visual effects but also non-visual effects. Although conventional laboratory experiments could simulate the CCT environment inside the tunnel to some extent, they fail to restore driving experience, let alone simulate driving behavior in the accident situation. It has largely remained unclear whether and how CCT would influence visual/non-visual performance of the subjects who are performing driving tasks, especially in the accident situation. Motivated by this gap, a virtual-reality-based framework for assessing the influence of CCT on the visual and non/visual performance in normal driving situation and in accident situation was proposed. In this study, tunnel models under seven different CCTs were created and a rear-end accident was designed in the tunnel. By integrating analog driving equipment, all participants were required to perform virtual driving tasks both in the normal situation and in the accident situation. The non-visual performance (driving fatigue) and the visual performance (reaction time) of the participants were collected and analyzed. Results show that the CCT of light source inside the tunnel was significant on the driving fatigue of the driver who was performing driving task, and it also had a significant impact on the visual performance when the driver was faced with a rear-end accident. Detailed experimental methodology, behavioral explanations underlying these findings, validity of results and practical implications are also discussed in the paper.}
}
@article{CASINI2018353,
title = {Analysis of a Moon outpost for Mars enabling technologies through a Virtual Reality environment},
journal = {Acta Astronautica},
volume = {143},
pages = {353-361},
year = {2018},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2017.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0094576517301947},
author = {Andrea E.M. Casini and Paolo Maggiore and Nicole Viola and Valter Basso and Marinella Ferrino and Jeffrey A. Hoffman and Aidan Cowley},
keywords = {Virtual Reality, Moon outpost, Illumination analysis, Incremental exploration architecture},
abstract = {The Moon is now being considered as the starting point for human exploration of the Solar System beyond low-Earth orbit. Many national space agencies are actively advocating to build up a lunar surface habitat capability starting from 2030 or earlier: according to ESA Technology Roadmaps for Exploration this should be the result of a broad international cooperation. Taking into account an incremental approach to reduce risks and costs of space missions, a lunar outpost can be considered as a test bed towards Mars, allowing to validate enabling technologies, such as water processing, waste management, power generation and storage, automation, robotics and human factors. Our natural satellite is rich in resources that could be used to pursue such a goal through a necessary assessment of ISRU techniques. The aim of this research is the analysis of a Moon outpost dedicated to the validation of enabling technologies for human space exploration. The main building blocks of the outpost are identified and feasible evolutionary scenarios are depicted, to highlight the incremental steps to build up the outpost. Main aspects that are dealt with include outpost location and architecture, as well as ISRU facilities, which in a far term future can help reduce the mass at launch, by producing hydrogen and oxygen for consumables, ECLSS, and propellant for Earth-Moon sorties and Mars journeys. A test outpost is implemented in a Virtual Reality (VR) environment as a first proof-of-concepts, where the elements are computer-based mock-ups. The VR facility has a first-person interactive perspective, allowing for specific in-depth analyses of ergonomics and operations. The feedbacks of these analyses are crucial to highlight requirements that might otherwise be overlooked, while their general outputs are fundamental to write down procedures. Moreover, the mimic of astronauts' EVAs is useful for pre-flight training, but can also represent an additional tool for failures troubleshooting during the flight controllers' nominal operations. Additionally, illumination maps have been obtained to study the light conditions, which are essential parameters to assess the base elements location. This unique simulation environment may offer the largest suite of benefits during the design and development phase, as it allows to design future systems to optimize operations, thus maximizing the mission's scientific return, and to enhance the astronauts training, by saving time and cost. The paper describes how a virtual environment could help to design a Moon outpost for an incremental architecture strategy towards Mars missions.}
}
@article{POULIN2024104023,
title = {Investigating the effects of spatial augmented reality on user participation in co-design sessions: A case study},
journal = {Computers in Industry},
volume = {154},
pages = {104023},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104023},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001732},
author = {Maud Poulin and Cédric Masclet and Jean-François Boujut},
keywords = {Collaborative design, Design cognition, Multimodal analysis, Virtual reality, Prototypes},
abstract = {New technologies such as Spatial Augmented Reality (SAR) have created new opportunities for including end users in the early phases of the design process when prototypes are not always available for functional testing. This study investigates the impact of introducing SAR technology on designers and end users working together in co-design sessions. To this end, a multi-modal analysis focusing on cognitive activities and gestural behaviour was conducted. After comparing the traditional setting with the SAR setting (over 44 sessions), the session activity profiles proved to be quite similar. However, the study highlights specific correlations between gestures and cognitive activities, in both environments. Finally, contrary to expectations, free gestures continue to be made in the digitally mediated situation.}
}
@article{MARINO2021103412,
title = {An Augmented Reality inspection tool to support workers in Industry 4.0 environments},
journal = {Computers in Industry},
volume = {127},
pages = {103412},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103412},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000191},
author = {Emanuele Marino and Loris Barbieri and Biagio Colacino and Anna Kum Fleri and Fabio Bruno},
keywords = {Augmented Reality, Industry 4.0, Operator 4.0, Google ARCore, Marker-based tracking, Usability studies},
abstract = {Among the key technologies of Industry 4.0, Augmented Reality (AR) is one of the most promising and enabling technologies for supporting factory workers and engineers at the workplace. To this end, the paper proposes a novel AR tool to assist operators during the inspection activities for the detection of production and assembly errors. In fact, thanks to the superimposition of the 3D models, as designed by the technical office, a worker can easily detect the presence of design discrepancies on the final physical assembled product and report them by adding 3D annotations directly on virtual models. This AR tool has been developed by using ARCore™ libraries to ensure, in the first place, its compatibility with commonly used devices for which workers are already trained and, secondly, to take advantage of the hybrid-tracking techniques that combine vision- and sensor-based methods to improve the reliability of the AR visualization. Nevertheless, the proposed AR tool adopts multiple markers to minimize tracking errors and therefore to provide greater freedom of movement to the user, who can use the tool also for the assessment of large-size products. Field experimentations have been carried out on a real case study with end-users in order to assess its usability and perceived mental workload through the SUS (System Usability Scale) and NASA-TLX (Task Load Index) standard questionnaires, respectively. The usability study was performed taking into account also objective metrics, i.e., by analysing user performance in target acquisition tasks while interacting with the AR tool. Statistical analysis proved that the adoption of this AR tool requires low mental demand, and its usability has reached a high level of satisfaction both by the factory workers and engineers involved in the user study.}
}
@article{DAGAMA2016105,
title = {MirrARbilitation: A clinically-related gesture recognition interactive tool for an AR rehabilitation system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {135},
pages = {105-114},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716300542},
author = {Alana Elza Fontes {Da Gama} and Thiago Menezes Chaves and Lucas Silva Figueiredo and Adriana Baltar and Ma Meng and Nassir Navab and Veronica Teichrieb and Pascal Fallavollita},
keywords = {Rehabilitation, Augmented reality, Interaction, Kinect, Movement analysis, Biomechanics},
abstract = {Background and Objective
Interactive systems for rehabilitation have been widely investigated for motivational purposes. However, more attention should be given to the manner in which user movements are recognized and categorized. This paper aims to evaluate the efficacy of using a clinically-related gesture recognition tool, based on the international biomechanical standards (ISB) for the reporting of human joint motion, for the development of an interactive augmented reality (AR) rehabilitation system —mirrARbilitation.
Methods
This work presents an AR rehabilitation system based on ISB standards, which enables the system to interact and to be configured according to therapeutic needs. The KinectTM skeleton tracking technology was exploited and a new movement recognition method was developed to recognize and classify biomechanical movements. Further, our mirrARbilitation system provides exercise instructions while simultaneously motivating the patient. The system was evaluated on a cohort of 33 patients, physiotherapists, and software developers when performing shoulder abduction therapy exercises. Tests were performed in three moments: (i) users performed the exercise until they feel tired without the help of the system, (ii) the same however using the mirrARbilitation for motivation and guidance, and (iii) users performed the exercise again without the system. Users performing the movement without the help of the system worked as baseline reference.
Results
We demonstrated that the percentage of correct exercises, measured by the movement analysis method we developed, improved from 69.02% to 93.73% when users interacted with the mirrARbilitation. The number of exercise repetitions also improved from 34.06 to 66.09 signifying that our system increased motivation of the users. The system also prevented the users from performing the exercises in a completely wrong manner. Finally, with the help of our system the users' worst result was performing 73.68% of the rehabilitation movements correctly. Besides the engagement, these results suggest that the use of biomechanical standards to recognize movements is valuable in guiding users during rehabilitation exercises.
Conclusion
The proposed system proved to be efficient by improving the user engagement and exercise performance outcomes. The results also suggest that the use of biomechanical standards to recognize movements is valuable in guiding users during rehabilitation exercises.}
}
@article{SZAJNA20203618,
title = {The Production Quality Control Process, Enhanced with Augmented Reality Glasses and the New Generation Computing Support System},
journal = {Procedia Computer Science},
volume = {176},
pages = {3618-3625},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319177},
author = {Andrzej Szajna and Roman Stryjski and Waldemar Woźniak and Norbert Chamier-Gliszczyński and Tomasz Królikowski},
keywords = {Augmented Reality, industry, quality control, AR glasses},
abstract = {The world is accelerating the digitisation of many branches of the economy. In production, it is often called Industry 4.0 - the term invented by Prof. Wahlster in Germany. One branch of ‘Production Digitisation’ is Augmented Reality (AR)- a technology that gives a very convenient and natural human-machine interface. Users are presented with the information requested, right in front of their eyes, and can communicate with the system through simple gestures and speech. The key advantage of AR is having one’s handsfree so that the user can perform his/her regular tasks, unhindered. In the first part of this paper, the concept of using AR glasses, in the quality control process, is presented. The present authors describe a proprietary support system. A specific example is then given in a real production environment on a real production process, comparing the old way with the new AR way of conducting test procedures. An evaluation of its usability is characterised using important factors that can affect the final outcome or features of the product and the manner in which it is applied in real life. In concluding, the authors focus on its advantages, the disadvantages, the technological imperfections, and the limitations and go on to deliberate on the conclusions and further research topics.}
}
@article{CHAKRABORTY2021101252,
title = {Time-series data optimized AR/ARMA model for frugal spectrum estimation in Cognitive Radio},
journal = {Physical Communication},
volume = {44},
pages = {101252},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2020.101252},
url = {https://www.sciencedirect.com/science/article/pii/S1874490720303293},
author = {Debashis Chakraborty and Salil Kr. Sanyal},
keywords = {Spectrum Estimation, Goodness of fit, Convex optimization, AR/ARMA, AIC–BIC, WARP board},
abstract = {Wideband and agile Spectrum Estimation (SE) is a fundamental component of the Cognitive Radio (CR) system. However, CR systems generally utilize the classical sensing techniques for SE due to heteroscedasticity of the available spectrum. Unfortunately, analysis of the Time-series data for SE using a testbed is rare to find out. A novel Goodness-of-Fit (GoF) based accurate SE technique for CR system has been proposed in this work involving Time-series data samples generated from Field Programmable Gate Array (FPGA) based Wireless open Access Radio Protocol (WARP) testbed having a sampling frequency of 40 MHz. Anderson–Darling (AD) rejection based Null-Hypothesis testing has been employed to implement the CR system within a frequency range of 9 kHz to 10 MHz. 1 MHz sinusoidal signal has been generated by the testbed for digital transmission/reception through Radio Board 1 and 3. Statistical parameters like Mean Square Error (MSE), Final Prediction Error (FPE), Loss Function and Fit(%) of the received samples adjudicate the Convex optimization of the data length. Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC) are responsible for the selection of the Auto Regressive Moving Average (ARMA) (3,2) model for optimal signal processing. Finally, the Power Spectral Density (PSD) confirms the superiority of the proposed work with the most optimized data length and lag order in real-time. Computation of complexities of the proposed algorithms also indicates a parsimonious choice of the model.}
}
@article{NAPIER2023107744,
title = {Using mobile-based augmented reality and object detection for real-time Abalone growth monitoring},
journal = {Computers and Electronics in Agriculture},
volume = {207},
pages = {107744},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.107744},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923001321},
author = {Thomas Napier and Ickjai Lee},
keywords = {Deep learning, Greenlip Abalone, Augmented reality, Object detection, Aquaculture},
abstract = {Abalone are becoming increasingly popular for human consumption. Whilst their popularity has risen, measuring the number and size distribution of Abalone at various stages of growth in existing farms remains a significant challenge. Current Abalone stock management techniques rely on manual inspection which is time consuming, causes stress to the animal, and results in mediocre data quality. To rectify this, we propose a novel mobile-based tool which combines object detection and augmented reality for the real-time counting and measuring of Abalone, that is both network and location independent. We applied our portable handset tool to both measure and count Abalone at various growth stages, and performed extended measuring evaluation to assess the robustness of our proposed approach. Our experimental results revealed that the proposed tool greatly outperforms traditional approaches and was able to successfully count up to 15 Abalone at various life stages with above 95% accuracy, as well as significantly decrease the time taken to measure Abalone while still maintaining an accuracy within a maximum error range of 2.5% of the Abalone’s actual size.}
}
@article{LIU200521,
title = {A system for brain tumor volume estimation via MR imaging and fuzzy connectedness},
journal = {Computerized Medical Imaging and Graphics},
volume = {29},
number = {1},
pages = {21-34},
year = {2005},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2004.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0895611104000989},
author = {Jianguo Liu and Jayaram K. Udupa and Dewey Odhner and David Hackney and Gul Moonis},
keywords = {Brain tumor, Image segmentation, Magnetic resonance imaging, Fuzzy connectedness, Volume measurement, Visualization},
abstract = {This paper presents a method for the precise, accurate and efficient quantification of brain tumor (glioblastomas) via MRI that can be used routinely in the clinic. Tumor volume is considered useful in evaluating disease progression and response to therapy, and in assessing the need for changes in treatment plans. We use multiple MRI protocols including FLAIR, T1, and T1 with Gd enhancement to gather information about different aspects of the tumor and its vicinity. These include enhancing tissue, nonenhancing tumor, edema, and combinations of edema and tumor. We have adapted the fuzzy connectedness framework for tumor segmentation in this work and the method requires only limited user interaction in routine clinical use. The system has been tested for its precision, accuracy, and efficiency, utilizing 10 patient studies. The percent coefficient of variation (% CV) in volume due to operator subjectivity in specifying seeds for fuzzy connectedness segmentation is less than 1%. The mean operator and computer time required per study for estimating the volumes of both edema and enhancing tumor is about 16min. The software package is designed to run under operator supervision. Delineation has been found to agree with the operators' visual inspection most of the time except in some cases when the tumor is close to the boundary of the brain. In the latter case, the scalp, surgical scar, or orbital contents are included in the delineation, and an operator has to exclude this manually. The methodology is rapid, robust, consistent, yielding highly reproducible measurements, and is likely to become part of the routine evaluation of brain tumor patients in our health system.}
}
@article{FREITAG2018114,
title = {Agile Product-Service Design with VR-technology: A use case in the furniture industry},
journal = {Procedia CIRP},
volume = {73},
pages = {114-119},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.305},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304864},
author = {Mike Freitag and Phil Westner and Christian Schiller and Maria Jose Nunez and Fernando Gigante and Soledad Berbegal},
keywords = {Product-Service, Product-Service Design, Service Engineering, Virtual Reality, Service Lifecycle Management},
abstract = {Agile development methods and the use of the VR-technology are becoming more and more important for manufacturing companies nowadays. Agile methods help to avoid mistakes and unnecessary costs. Beside the development methods, the test phase of a Product-Service Design has often been neglected in business practice, but more and more its value is being fully recognised. The focus of this paper is the validation of agile approach of a Product-Service Design in the furniture industry with the help of VR/AR-Technology in the use case in the furniture industry with the companies AIDIMME and ACTIU. The use case enables employees to select their new office furniture with the help of VR/AR-technology. It shows the old and three new alternatives of the office furniture with the help of 3D-models. How to use the innovative technology and how to handle the employee’s integration is the topic of the agile approach. This paper shows that an integrated and agile approach of a Product-Service Design. It helps to increase the customer integration and reduce the market risk for new product-service systems.}
}
@article{JO201921,
title = {Downstairs resident classification characteristics for upstairs walking vibration noise in an apartment building under virtual reality environment},
journal = {Building and Environment},
volume = {150},
pages = {21-32},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2018.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S036013231830814X},
author = {Hyun In Jo and Jin Yong Jeon},
keywords = {Floor impact sound, Head-related transfer function, Head mounted device, Annoyance, Allowance limit, Classification},
abstract = {Psychoacoustic experiments were performed to investigate head-related transfer function (HRTF) and head-mounted display (HMD) effects, to develop a new virtual-reality (VR)-based noise assessment methodology and classification method for heavy-weight impact sounds in an apartment indoor environment. Sound pressure levels and frequency characteristics before and after HRTF application were examined to reproduce the “upstairs condition” (overhead movement), for a heavy-weight impact source recorded in an apartment living room. Floor impact noise was assessed using the HMD by modeling a typical living room. For four environments (NONE, HRTF, HMD, and HRTF + HMD), subjective responses (annoyance, allowance limit, semantic expression) were examined for 38–62-dB maximum sound pressure levels with an A-weighted frequency response and fast time constant. Directional information through the HRTF significantly increased annoyance and decreased the allowance limit, and HMD visual information partially (1.2 times) affected the allowance limit in a high inter-floor noise environment owing to increased visual involvement (56 dBA). The semantic expression assessment revealed comprehensive inter-floor noise responses of dissatisfaction, irregularity, and discontinuity, and confirmed greater annoyance due to walking vibration noise in an environment with directional information, because of increased perception of heavy-impact sound-induced discomfort. Regarding annoyance- and allowance-limit-based classification, the satisfaction standards for low-noise environments were assessed as 6–7 dB lower than those of previous studies when both HRTF and HMD were applied, confirming that test subjects respond to noise more sensitively in a virtual environment where audiovisual information is provided. Therefore, a new indoor noise assessment methodology for environmental factors is proposed herein.}
}
@article{OTTO2019785,
title = {A Virtual Reality Assembly Assessment Benchmark for Measuring VR Performance & Limitations},
journal = {Procedia CIRP},
volume = {81},
pages = {785-790},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.195},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305001},
author = {Michael Otto and Eva Lampen and Philipp Agethen and Mareike Langohr and Gabriel Zachmann and Enrico Rukzio},
keywords = {virtual reality, benchmark, assembly, assessment, usabitity, score, production balidation},
abstract = {With an increasing product complexity in manufacturing industry, virtual reality (VR) offers the possibility to immersively assess assembly processes already in early product development stages. Within production validation phases, engineers visually assess product part assembly and interactively validate corresponding production processes. Nevertheless, by now research does not give answers on how VR assembly system’s performance can be measured with respect to its technical limitations. The proposed Virtual Reality Assembly Assessment (VR2A) benchmark is an open, standardized experiment design for evaluating the overall VR assembly assessment performance in terms of sizes and clearances instead of measuring single technical impact factors within the interaction cycle, such as tracking, rendering and visualization limitations. VR2A benchmark focusses on the overall production engineer’s assessment objective generating quantifiable metrics. Using VR2A, users gain practical insights on their overall VR assessment system’s performance and limitations. An in-depth evaluation with production engineers (N=32) revealed, that negative clearances can be detected more easily than positive ones, part sizes directly correlate with the assessment performance. Additionally, the evaluation showed that VR2A is easy to use, universally usable and generates objective insights on the applied VR system.}
}
@article{BAO2024169,
title = {Effects of virtual agents on interaction efficiency and environmental immersion in MR environments},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {169-179},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000761},
author = {Yihua Bao and Jie Guo and Dongdong Weng and Yue Liu and Zeyu Tian},
keywords = {Mixed reality, Virtual agents, Interaction performance, Environmental immersion, Virtual environments},
abstract = {Background
Physical entity interactions in mixed reality (MR) environments aim to harness human capabilities in manipulating physical objects, thereby enhancing virtual environment (VEs) functionality. In MR, a common strategy is to use virtual agents as substitutes for physical entities, balancing interaction efficiency with environmental immersion. However, the impact of virtual agent size and form on interaction performance remains unclear.
Methods
Two experiments were conducted to explore how virtual agent size and form affect interaction performance, immersion, and preference in MR environments. The first experiment assessed five virtual agent sizes (25%, 50%, 75%, 100%, and 125% of physical size). The second experiment tested four types of frames (no frame, consistent frame, half frame, and surrounding frame) across all agent sizes. Participants, utilizing a head-mounted display, performed tasks involving moving cups, typing words, and using a mouse. They completed questionnaires assessing aspects such as the virtual environment effects, interaction effects, collision concerns, and preferences.
Results
Results from the first experiment revealed that agents matching physical object size produced the best overall performance. The second experiment demonstrated that consistent framing notably enhances interaction accuracy and speed but reduces immersion. To balance efficiency and immersion, frameless agents matching physical object sizes were deemed optimal.
Conclusions
Virtual agents matching physical entity sizes enhance user experience and interaction performance. Conversely, familiar frames from 2D interfaces detrimentally affect interaction and immersion in virtual spaces. This study provides valuable insights for the future development of MR systems.}
}
@article{SOLOMON200676,
title = {Segmentation of brain tumors in 4D MR images using the hidden Markov model},
journal = {Computer Methods and Programs in Biomedicine},
volume = {84},
number = {2},
pages = {76-85},
year = {2006},
note = {Medical Image Segmentation Special Issue},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2006.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169260706001970},
author = {Jeffrey Solomon and John A. Butman and Arun Sood},
keywords = {Brain tumor, Expectation-maximization, Hidden Markov model, Software},
abstract = {Tumor size is an objective measure that is used to evaluate the effectiveness of anticancer agents. Responses to therapy are categorized as complete response, partial response, stable disease and progressive disease. Implicit in this scheme is the change in the tumor over time; however, most tumor segmentation algorithms do not use temporal information. Here we introduce an automated method using probabilistic reasoning over both space and time to segment brain tumors from 4D spatio-temporal MRI data. The 3D expectation-maximization method is extended using the hidden Markov model to infer tumor classification based on previous and subsequent segmentation results. Spatial coherence via a Markov Random Field was included in the 3D spatial model. Simulated images as well as patient images from three independent sources were used to validate this method. The sensitivity and specificity of tumor segmentation using this spatio-temporal model is improved over commonly used spatial or temporal models alone.}
}
@article{ZHANG2020368,
title = {Cloud-to-end Rendering and Storage Management for Virtual Reality in Experimental Education},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {368-380},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300541},
author = {Hongxin Zhang and Jin Zhang and Xue Yin and Kan Zhou and Zhigeng Pan and Abdennour {El Rhalibi}},
keywords = {Clout-to-end render, Cloud storage, Virtual reality, Experimental education},
abstract = {Background
Real-time 3D rendering and interaction is important for virtual reality (VR) experimental education. Unfortunately, standard end-computing methods prohibitively escalate computational costs. Thus, reducing or distributing these requirements needs urgent attention, especially in light of the COVID-19 pandemic
Methods
In this study, we design a cloud-to-end rendering and storage system for VR experimental education comprising two models: background and interactive. The cloud server renders items in the background and sends the results to an end terminal in a video stream. Interactive models are then lightweight-rendered and blended at the end terminal. An improved 3D warping and hole-filling algorithm is also proposed to improve image quality when the user’s viewpoint changes.
Results
We build three scenes to test image quality and network latency. The results show that our system can render 3D experimental education scenes with higher image quality and lower latency than any other cloud rendering systems.
Conclusions
Our study is the first to use cloud and lightweight rendering for VR experimental education. The results demonstrate that our system provides good rendering experience without exceeding computation costs.}
}
@incollection{GOMEZTONE20237,
title = {Chapter 2 - Augmented and immersive virtual reality to train spatial skills in STEAM university students},
editor = {Surbhi {Bhatia Khan} and Suyel Namasudra and Swati Chandna and Arwa Mashat and Fatos Xhafa},
booktitle = {Innovations in Artificial Intelligence and Human-Computer Interaction in the Digital Era},
publisher = {Academic Press},
pages = {7-30},
year = {2023},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-99891-8},
doi = {https://doi.org/10.1016/B978-0-323-99891-8.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998918000024},
author = {Hugo C. Gomez-Tone and Jorge Martin-Gutierrez and Betty K. Valencia-Anci},
keywords = {Augmented reality, Immersive virtual reality, Spatial skills, STEAM, Training},
abstract = {A mandatory requirement for all people who want to go into the STEAM field (science, technology, engineering, arts and math) is to have developed levels of spatial skills because these skills allow the adequate performance of students, especially in the first year of university studies. One hundred years of studies of these skills have shown that it is possible to train them, and countless forms of training have been developed. However, new technologies and human–computer interaction are proving to be more effective. We will show in this chapter the effectiveness of two short training courses using augmented reality and immersive virtual reality in engineering and architecture students from two universities in different countries. For the measurement of these skills, we considered three subcomponents measured with the Mental Rotation Test (MRT) for the mental rotations component, the Differential Aptitude Test (DAT5-SR) for the spatial visualization component, and Spatial Orientation Test (SOT) for the spatial orientation component. The data obtained from experimental and control groups were analyzed using parametric and non-parametric tests: one-factor ANOVA, Student’s t test, and Kruskal–Wallis. We conclude that augmented reality and immersive virtual reality are very effective as short spatial skills training in engineering and architecture students.}
}
@article{ROGALA2022118939,
title = {Design and experimental study on precooled MR JT cryocooler for LNG recondensation purposes},
journal = {Applied Thermal Engineering},
volume = {215},
pages = {118939},
year = {2022},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2022.118939},
url = {https://www.sciencedirect.com/science/article/pii/S1359431122008754},
author = {Zbigniew Rogala and Rafał Siemasz and Błażej Baran and Adrian Kwiatkowski},
keywords = {Test stand, Mixed refrigerant, Experimental study, Precooled MR JT, Cascaded MR JT},
abstract = {The paper presents a preliminary experimental study of the precooled MR JT cryocooler, which was designed for LNG recondensation and potentially liquefaction purposes. The paper also includes the most vital aspects of the design: compressor selection and limitations, selection and size of expansion devices, effect of MR composition on the stability of the load temperature, and minimum approach during recuperation. In addition, a detailed description of the features of the test stand is provided. A series of experimental tests has been performed with varied heat load, suction pressure, discharge pressure, and precooling temperature. The cryocooler provided cooling power of 270 W at a temperature of −158 °C, which is the largest MR JT cryocooler reported in the literature. Experiments indicate an optimum precooling temperature similar to that derived from the theoretical study (approximately −35 °C).}
}
@article{DANG2023105810,
title = {A 3D-Panoramic fusion flood enhanced visualization method for VR},
journal = {Environmental Modelling & Software},
volume = {169},
pages = {105810},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105810},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223001962},
author = {Pei Dang and Jun Zhu and Yuxuan Zhou and Yuting Rao and Jigang You and Jianlin Wu and Mengting Zhang and Weilian Li},
keywords = {Flood simulation, Panoramas, Fusion visualization, CA-PBD, Immersive virtual reality},
abstract = {Flood visualization plays a pivotal role in understanding and predicting flood disasters, as it aids in the development of effective disaster mitigation measures and emergency response strategies. However, traditional methods of flood computation and visualization often suffer from poor timeliness and subpar visualization effects, particularly in virtual reality (VR) scenarios. Panoramic data, inherently suitable for VR visualization, offers a low data volume attribute that significantly reduces the time cost of data processing. This allows for the real-time overlay of flood simulation results with the actual environment. We proposed a multi-scale flood simulation method which is subsequently integrated with panoramic images or videos through a dimensionality reduction process, enhancing visualization effects. We developed a prototype system to validate the proposed methodology, and the results demonstrated improved visualization outcomes, rendering efficiency, and user experience in VR panoramic flood simulations.}
}
@article{AOKI201547,
title = {AR based ornament design system for 3D printing},
journal = {Journal of Computational Design and Engineering},
volume = {2},
number = {1},
pages = {47-54},
year = {2015},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2014.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2288430014000062},
author = {Hiroshi Aoki and Jun Mitani and Yoshihiro Kanamori and Yukio Fukui},
keywords = {3DCG, Modeling, Augmented reality, 3D printing, Voxel, Octet truss},
abstract = {In recent years, 3D printers have become popular as a means of outputting geometries designed on CAD or 3D graphics systems. However, the complex user interfaces of standard 3D software can make it difficult for ordinary consumers to design their own objects. Furthermore, models designed on 3D graphics software often have geometrical problems that make them impossible to output on a 3D printer. We propose a novel AR (augmented reality) 3D modeling system with an air-spray like interface. We also propose a new data structure (octet voxel) for representing designed models in such a way that the model is guaranteed to be a complete solid. The target shape is based on a regular polyhedron, and the octet voxel representation is suitable for designing geometrical objects having the same symmetries as the base regular polyhedron. Finally, we conducted a user test and confirmed that users can intuitively design their own ornaments in a short time with a simple user interface.}
}
@article{ZOROOFI2004267,
title = {Automated segmentation of necrotic femoral head from 3D MR data},
journal = {Computerized Medical Imaging and Graphics},
volume = {28},
number = {5},
pages = {267-278},
year = {2004},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0895611104000503},
author = {Reza A. Zoroofi and Yoshinobu Sato and Takashi Nishii and Nobuhiko Sugano and Hideki Yoshikawa and Shinichi Tamura},
keywords = {Bone segmentation, Femoral head estimation, Necrosis quantification, Femur modeling, 3D ellipse fitting, MRI necrosis segmentation},
abstract = {Segmentation of diseased organs is an important topic in computer assisted medical image analysis. In particular, automatic segmentation of necrotic femoral head is of importance for various corresponding clinical tasks including visualization, quantitative assessment, early diagnosis and adequate management of patients suffering from avascular necrosis of the femoral head (ANFH). Early diagnosis and treatment of ANFH is crucial since the disease occurs in relatively young individuals with an average age of 20–50, and since treatment options for more advanced disease are frequently unsuccessful. The present paper describes several new techniques and software for automatic segmentation of necrotic femoral head based on clinically obtained multi-slice T1-weighted MR data. In vivo MR data sets of 50 actual patients are used in the study. An automatic method built up to manage the segmentation task according to image intensity of bone tissues, shape of the femoral head, and other characters. The processing scheme consisted of the following five steps. (1) Rough segmentation of non-necrotic lesions of the femur by applying a 3D gray morphological operation and a 3D region growing technique. (2) Fitting a 3D ellipse to the femoral head by a new approach utilizing the constraint of the shape of the femur, and employing a principle component analysis and a simulated annealing technique. (3) Estimating the femoral neck location, and also femoral head axis by integrating anatomical information of the femur and boundary of estimated 3D ellipse. (4) Removal of non-bony tissues around the femoral neck and femoral head ligament by utilizing the estimated femoral neck axis. (5) Classification of necrotic lesions inside the estimated femoral head by a k-means technique. The above method was implemented in a Microsoft Windows software package. The feasibility of this method was tested on the data sets of 50 clinical cases (3000 MR images).}
}
@article{FUKUDA2019179,
title = {An indoor thermal environment design system for renovation using augmented reality},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {2},
pages = {179-188},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S228843001830068X},
author = {Tomohiro Fukuda and Kazuki Yokoi and Nobuyoshi Yabuki and Ali Motamedi},
keywords = {Environmental design, Indoor thermal environment, Intuitive visualization, Interactive environment, Computational Fluid Dynamics (CFD), Augmented Reality (AR)},
abstract = {The renovation projects of buildings and living spaces, which aim to improve the thermal environment, are gaining importance because of energy saving effects and occupants' health considerations. However, the indoor thermal design is not usually performed in a very efficient manner by stakeholders, due to the limitations of a sequential waterfall design process model, and due to the difficulty in comprehending the CFD simulation results for stakeholders. On the other hand, indoor greenery has been introduced to buildings as a method for adjusting the thermal condition. Creating a VR environment, which can realistically and intuitively visualize a thermal simulation model is very time consuming and the resulting VR environment created by 3D computer graphics objects is disconnected from the reality and does not allow design stakeholders to experience the feelings of the real world. Therefore, the objective of this research is to develop a new AR-based methodology for intuitively visualizing indoor thermal environment for building renovation projects. In our proposed system, easy-to-comprehend visualization of CFD results augment the real scenes to provide users with information about thermal effects of their renovation design alternatives interactively. Case studies to assess the effect of indoor greenery alternatives on the thermal environment are performed. In conclusion, integrating CFD and AR provides users with a more natural feeling of the future thermal environment. The proposed method was evaluated feasible and effective.}
}
@article{SHAO2022105826,
title = {Augmented reality calibration using feature triangulation iteration-based registration for surgical navigation},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105826},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105826},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005856},
author = {Long Shao and Shuo Yang and Tianyu Fu and Yucong Lin and Haixiao Geng and Danni Ai and Jingfan Fan and Hong Song and Tao Zhang and Jian Yang},
keywords = {Augmented reality, Surgical navigation, Shape feature matching, Feature triangulation iteration registration},
abstract = {Background
Marker-based augmented reality (AR) calibration methods for surgical navigation often require a second computed tomography scan of the patient, and their clinical application is limited due to high manufacturing costs and low accuracy.
Methods
This work introduces a novel type of AR calibration framework that combines a Microsoft HoloLens device with a single camera registration module for surgical navigation. A camera is used to gather multi-view images of a patient for reconstruction in this framework. A shape feature matching-based search method is proposed to adjust the size of the reconstructed model. The double clustering-based 3D point cloud segmentation method and 3D line segment detection method are also proposed to extract the corner points of the image marker. The corner points are the registration data of the image marker. A feature triangulation iteration-based registration method is proposed to quickly and accurately calibrate the pose relationship between the image marker and the patient in the virtual and real space. The patient model after registration is wirelessly transmitted to the HoloLens device to display the AR scene.
Results
The proposed approach was used to conduct accuracy verification experiments on the phantoms and volunteers, which were compared with six advanced AR calibration methods. The proposed method obtained average fusion errors of 0.70 ± 0.16 and 0.91 ± 0.13 mm in phantom and volunteer experiments, respectively. The fusion accuracy of the proposed method is the highest among all comparison methods. A volunteer liver puncture clinical simulation experiment was also conducted to show the clinical feasibility.
Conclusions
Our experiments proved the effectiveness of the proposed AR calibration method, and revealed a considerable potential for improving surgical performance.}
}
@article{SEELIGER2022570,
title = {Augmented Reality for Machine Setups: Task Performance and Usability Evaluation in a Field Test},
journal = {Procedia CIRP},
volume = {107},
pages = {570-575},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003110},
author = {Arne Seeliger and TorbjØrn Netland and Stefan Feuerriegel},
keywords = {Augmented Reality, HoloLens, User Guidance, Usability, Field Study, Injection Molding},
abstract = {Head-mounted displays (HMDs) for augmented reality (AR) have received much scholarly attention. Most studies on AR HMDs, however, rely on lab-based environments or simplistic tasks that do not reflect the complexity of industrial work processes. In this work, we conducted a field study in which professional machine operators performed a setup task for an injection molding machine. We assessed and compared task performance, perceived workload, and usability of an AR HMD versus a tablet computer. We find that the use of an AR HMD is perceived as positive overall and not mentally demanding. However, task completion times were not reduced, although participants felt they were.}
}
@article{LIU2022102357,
title = {Probing an intelligent predictive maintenance approach with deep learning and augmented reality for machine tools in IoT-enabled manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {77},
pages = {102357},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102357},
url = {https://www.sciencedirect.com/science/article/pii/S073658452200045X},
author = {Changchun Liu and Haihua Zhu and Dunbing Tang and Qingwei Nie and Tong Zhou and Liping Wang and Yejia Song},
keywords = {Machine tools, Intelligent predictive maintenance, CNN-LSTM, Deep reinforcement learning, Augmented reality, IoT-enabled manufacturing},
abstract = {In the Industry 4.0 era, the number and complexity of machine tools are both increased, which is prone to cause malfunctions and downtime in the manufacturing process. Predictive Maintenance (PdM), as a pivotal part of Prognostics and Health Management (PHM), plays a vital role in enhancing the reliability of machine tools in the Internet of Things (IoT)-enabled manufacturing. In order to realize a highly reliable maintenance plan integrated with the fault prediction, the maintenance decision-making, and the Augmented Reality (AR)-enabled auxiliary maintenance, an intelligent predictive maintenance approach for machine tools is proposed in this paper via multiple services cooperating within a single framework. The fault prediction service is supported by the combination of Convolutional Neural Network and Long Short-Term Memory (CNN-LSTM). Specified features from massive production data acquired by IoT can be comprehensively extracted using CNN, and their nonlinear relationship can be fitted by LSTM. Based on the fault prediction result, deep reinforcement learning is adopted to achieve the production control and schedule maintenance personnel if a fault code appears. On top of this, the guidance information from the maintenance experience database can be integrated into the faulty machine tools in the form of visibility through AR, which can guide the maintenance personnel to complete maintenance tasks more efficiently. Moreover, the remote expert service is also integrated in the AR-supported auxiliary maintenance, which is activated to solve unexpected faults that are not stored in the maintenance experience database. Comparative experiments are conducted in the IoT-enabled manufacturing workshop with real-world case studies, and the results demonstrate that the proposed predictive maintenance approach is both effective and practical.}
}
@article{SMITH2019100010,
title = {Response activation and inhibition after exposure to virtual reality},
journal = {Array},
volume = {3-4},
pages = {100010},
year = {2019},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2019.100010},
url = {https://www.sciencedirect.com/science/article/pii/S2590005619300104},
author = {Shamus P. Smith and Elizabeth L. Burd},
keywords = {Virtual reality, Head mounted display, Exposure impact, Reaction time, Human factors},
abstract = {The widespread availability of affordable head-mounted displays and easy access to virtual reality (VR) applications and games has significantly increased the use of such technology by the general public. Thus there is increasing interest in determining any risks of using such technology and any aftereffect from exposure. Head-mounted display manufacturers provide general usage guidance but this is ad hoc and there is limited recent evidence comparing early virtual environment studies with experiences from modern head-mounted displays. The primary objective of this study was to explore response activation and inhibition after participants experienced a typical virtual environment in a head-mounted display. Reaction times were collected with a robust cued go/nogo test as pre- and post-tests. Participants (n ​= ​22, female ​= ​11) played Minecraft VR for 15 ​minutes using an Oculus Rift headset. In contrast to other studies, the results showed no significant impact on reaction times across response activation or inhibition. However, evidence of participant fatigue in the reaction time tests was found. This work confirms safe use of virtual reality experiences in modern head-mounted displays for short duration exposures and identifies issues with reaction time testing that are in need of further investigation.}
}
@article{ZHENG202012,
title = {A smart assistance system for cable assembly by combining wearable augmented reality with portable visual inspection},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {1},
pages = {12-27},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300036},
author = {Lianyu Zheng and Xinyu Liu and Zewu An and Shufei Li and Renjie Zhang},
keywords = {Cable assembly, Visual inspection, Text reading, Wearable AR, Deep learning},
abstract = {Background
Assembly guided by paper documents is the most widespread type used in the process of aircraft cable assembly. This process is very complicated and requires assembly workers with high-level skills. The technologies of wearable Augmented Reality (AR) and portable visual inspection can be exploited to improve the efficiency and the quality of cable assembly.
Methods
In this study, we propose a smart assistance system for cable assembly that combines wearable AR with portable visual inspection. Specifically, a portable visual device based on binocular vision and deep learning is developed to realize fast detection and recognition of cable brackets that are installed on aircraft airframes. A Convolutional Neural Network (CNN) is then developed to read the texts on cables after images are acquired from the camera of the wearable AR device. An authoring tool that was developed to create and manage the assembly process is proposed to realize visual guidance of the cable assembly process based on a wearable AR device. The system is applied to cable assembly on an aircraft bulkhead prototype.
Results
The results show that this system can recognize the number, types, and locations of brackets, and can correctly read the text of aircraft cables. The authoring tool can assist users who lack professional programming experience in establishing a process plan, i. e., assembly outline based on AR for cable assembly.
Conclusions
The system can provide quick assembly guidance for aircraft cable with texts, images, and a 3D model. It is beneficial for reducing the dependency on paper documents, labor intensity, and the error rate.}
}
@article{KIM2016162,
title = {Robust semi-automated quantification of cardiac MR perfusion using level set: Application to hypertrophic cardiomyopathy patient data},
journal = {Computers in Biology and Medicine},
volume = {71},
pages = {162-173},
year = {2016},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2016.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0010482516300439},
author = {Yoon-Chul Kim and Sung Mok Kim and Yeon Hyeon Choe},
keywords = {MRI, Dynamic contrast enhanced imaging, Myocardial perfusion, Image segmentation},
abstract = {Background
Recently there have been several clinical MR perfusion studies in patients with hypertrophic cardiomyopathy (HCM) who may suffer from myocardial ischemia due to coronary microvascular dysfunction. In these studies, data analysis relied on a manual procedure of tracing epicardial and endocardial borders. The goal of this work is to develop and validate a robust semi-automated analysis method for myocardial perfusion quantification in clinical HCM data.
Method
Dynamic multi-slice stress perfusion MRI data were acquired from 18 HCM patients. The proposed semi-automated method required user input of two landmark selections: LV center point and RV insertion point. Automated segmentations of the endocardial and epicardial borders were performed in three short-axis slices using distance regularized level set evolution on RV, LV, and myocardial enhancement frames.
Results
The proposed automated epicardial border detection method resulted in average radial distance errors of 7.5%, 9.5%, and 11.6% in basal, mid, and apical slices, respectively, when compared to manual tracing of the borders as a reference. In linear regression analysis, the highest correlation of myocardial upslope measurements was observed between the manual method and the proposed method in the anterolateral section (r=0.964), and the lowest correlation was observed in the inferoseptal section (r=0.866).
Conclusion
The proposed semi-automated method for myocardial MR perfusion quantification is feasible in HCM patients who typically show (1) irregular myocardial shape and (2) low image contrast between the myocardium and its surrounding regions due to coronary microvascular disease.}
}
@article{TREBBI2021125,
title = {MR-compatible loading device for assessment of heel pad internal tissue displacements under shearing load.},
journal = {Medical Engineering & Physics},
volume = {98},
pages = {125-132},
year = {2021},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1350453321001302},
author = {Alessio Trebbi and Antoine Perrier and Mathieu Bailet and Yohan Payan},
keywords = {Soft tissue deformations, Plantar pressure, Pressure ulcers, Polymeric springs},
abstract = {In the last decade, the role of shearing loads has been increasingly suspected to play a determinant impact in the formation of deep pressure ulcers. In vivo observations of such deformations are complex to obtain. Previous studies only provide global measurements of such deformations without getting the quantitative values of the loads that generate these deformations. To study the role that shearing loads have in the etiology of heel pressure ulcers, an MR-compatible device for the application of shearing and normal loads was designed. Magnetic resonance imaging is a key feature that allows to monitor deformations of soft tissues after loading in a non-invasive way. Measuring applied forces in an MR-environment is challenging due to the impossibility to use magnetic materials. In our device, forces are applied through the compression of springs made of polylactide. Shearing and normal loads were applied on the plantar skin of the human heel through a flat plate while acquiring MR images. The device materials did not introduce any imaging artifact and allowed for high quality MR deformation measurements of the internal components of the heel. The obtained subject-specific results are an original data set that can be used in validations for Finite Element analysis and therefore contribute to a better understanding of the factors involved in pressure ulcer development.}
}
@article{LENG2024107940,
title = {Development of a virtual reality-based zygomatic implant surgery training system with global collision detection and optimized finite element method model},
journal = {Computer Methods and Programs in Biomedicine},
volume = {243},
pages = {107940},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107940},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723006065},
author = {Ao Leng and Bolun Zeng and Yizhou Chen and Puxun Tu and Baoxin Tao and Xiaojun Chen},
keywords = {Computer-assisted surgery, Virtual reality, Zygomatic implant surgery, Surgical training},
abstract = {Background and objective
Zygomatic implant surgery is challenging due to the complex structure of the zygomatic bone, limited visual range during surgery, and lengthy implant path. Moreover, traditional training methods are costly, and experimental subjects are scarce.
Methods
To overcome these challenges, we propose a novel training system that integrates visual, haptic, and auditory feedback to create a more immersive surgical experience. The system uses dynamic bounding volume hierarchy (BVH) and Symplectic Euler to detect global collisions between surgical tools and models, while an optimized finite element method (FEM) model simulates soft tissue and detects collisions. Compared to previous works, our system achieves global rigid-body collisions between surgical tools and patient models, while also providing stable and realistic simulation and collisions of soft tissues. This advancement offers a more realistic simulation for zygomatic implant surgery.
Results
We conducted three experiments and evaluations. The first experiment measured the axial force generated during the zygomatic implant simulation process and compared it with actual surgery, demonstrating the realistic force rendering feedback of our system. The second evaluation involved 15 novice surgeons who experienced the system and completed a questionnaire survey focusing on five aspects. The results showed satisfactory evaluations. The third experiment involved six surgeons who underwent in-depth training for two hours daily and were tested on the first, third, and fifth days. We collected data and combined it with the doctors' feedback to prove that our system can improve surgeons' proficiency in zygomatic implant surgery and provide a novel training solution for this procedure.
Conclusion
We have innovatively integrated global collision detection and optimized soft tissue simulation into our system. Furthermore, we have conducted experimental validation to demonstrate the effectiveness of this implementation. We present a novel solution for zygomatic implant surgery training.}
}
@article{HA201210,
title = {An interactive 3D movement path manipulation method in an augmented reality environment},
journal = {Interacting with Computers},
volume = {24},
number = {1},
pages = {10-24},
year = {2012},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2011.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0953543811000968},
author = {Taejin Ha and Mark Billinghurst and Woontack Woo},
keywords = {Immersive augmented reality, Augmented reality authoring, Movement path editing, Tangible user interface, 3D object selection and manipulation},
abstract = {In this paper, we evaluate a path editing method using a tangible user interface to generate and manipulate the movement path of a 3D object in an Augmented Reality (AR) scene. To generate the movement path, each translation point of a real 3D manipulation prop is examined to determine which point should be used as a control point for the path. Interpolation using splines is then used to reconstruct the path with a smooth line. A dynamic score-based selection method is also used to effectively select small and dense control points of the path. In an experimental evaluation, our method took the same time and generated a similar amount of errors as a more traditional approach, however the number of control points needed was significantly reduced. For control manipulation, the task completion time was quicker and there was less hand movement needed. Our method can be applied to drawing or curve editing methods in AR educational, gaming, and simulation applications.}
}
@article{HINCAPIE2021107281,
title = {Augmented reality mobile apps for cultural heritage reactivation},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107281},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107281},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002639},
author = {Mauricio Hincapié and Christian Díaz and Maria-Isabel Zapata-Cárdenas and Henry de Jesus Toro Rios and Dalia Valencia and David Güemes-Castorena},
keywords = {Educational innovation, Cultural heritage, Augmented reality, GPS-based applications, Mobile Apps, Higher education},
abstract = {The use of augmented reality and GPS-guided applications has proven successful for reviving cultural heritage. However, the impact of using these technologies on the learning process of cultural heritage has not been studied in depth. This article reports the findings of how using GPS and augmented reality with mobile apps impacted learning during a project to reactivate cultural heritage around Medellin's Cisnero Square and its surroundings (also called the "Park of Lights"). An experimental test comparing two groups was performed. The experimental design goal was to determine the differences in learning and user perception when the proposed Mobile App was and was not used in guiding the cultural heritage tour as part of Cisnero Square's reactivation. The results showed that the experimental group achieved better learning and cultural heritage learning outcomes. This study offers valuable insights into technological applications for reactivating cultural heritage, which can be applied almost anywhere.}
}
@article{QESHMY201924,
title = {Managing Human Errors: Augmented Reality systems as a tool in the quality journey},
journal = {Procedia Manufacturing},
volume = {28},
pages = {24-30},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313477},
author = {Danial Etemady Qeshmy and Jacob Makdisi and Elias Hans Dener {Ribeiro da Silva} and Jannis Angelis},
keywords = {Industry 4.0, smart factory, Augmented Reality, human error, artificial intelligence},
abstract = {The manufacturing industry is shifting, entering a new era with smart and connected devices. The fourth industrial revolution is promising increased growth and productivity by the Smart Factory and within the enabling technologies is Augmented Reality (AR). At the same time as the technology is introduced, errors in manufacturing are a problem which is affecting the productivity and the quality. This research aims to find the main causes of human errors in assembly lines and thereafter explores whether augmented reality is an appropriate tool to be used in order to address those issues. Based on a literature review that identified and characterized a preliminary set of root causes for human errors in assembly lines, these causes were empirically studied in an exercise that covered an in-depth case study in a multinational automotive company. Data in form of interviews and deviation- reports have been used to identify the causing factors and the result showed that the main causes of human errors are the amount of thinking, deciding and searching for information which affected the cognitive load of the operator and in result their performance. Several interviews with experts in augmented reality allowed to verify if AR technology would be feasible to solve or mitigate the found causes. Besides that, in repetitive manual assembly operations, AR is better used showing the process in order to train new operators. At the same time for experienced operators, AR should show information only when an error occurs and when there is a need of taking an active choice. Nevertheless, while theoretically able to managing human error when fully developed, the desired application makes the augmentation of visual objects redundant and increasingly complex for solving the identified causes of errors which questions the appropriateness of using AR systems. Furthermore, the empirical findings showed that for managing human errors, the main bottleneck of an AR system is the systems artificial intelligence capabilities.}
}
@article{HERBERT2022566,
title = {Cognitive load considerations for Augmented Reality in network security training},
journal = {Computers & Graphics},
volume = {102},
pages = {566-591},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001874},
author = {Bradley Herbert and Grant Wigley and Barrett Ens and Mark Billinghurst},
keywords = {Intelligent Tutoring Systems, Augmented Reality, Cognitive Load Theory, Instructional design, Training and education},
abstract = {This paper presents an Augmented Reality (AR)-based network cabling tutoring system that trains users how to interconnect cables within a Virtual Local Area Network (VLAN) on a physical switching rack. AR arrows combined with text-based instruction and a checklist provided assistance during practical learning. When learners made a mistake, an Intelligent Tutoring System (ITS) identified the source of the mistake and provided real-time feedback using text, AR and text-to-speech mechanisms. The design was motivated by the human-cognitive architecture and its five evolutionary principles (proposed by Sweller and Sweller (2006)). Users performed four consecutive network cabling training tasks with assistance from our ITS. We found that users made fewer errors when the AR cues, text-based instruction and checklist solutions were replaced with summarised information and then removed completely in the final task compared to those who used an identical system with fixed instruction. Cognitive Load Theory (CLT) explains our results by suggesting that the instructional mechanisms become redundant as knowledge increases. Implications of the study are discussed as well as how AR can help facilitate knowledge transfer in the network cabling domain.}
}
@article{DROUOT2022103793,
title = {Augmented reality on industrial assembly line: Impact on effectiveness and mental workload},
journal = {Applied Ergonomics},
volume = {103},
pages = {103793},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103793},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001168},
author = {Mathilde Drouot and Nathalie {Le Bigot} and Emmanuel Bricard and Jean-Louis de Bougrenet and Vincent Nourrit},
keywords = {Augmented reality, Industrial assembly, Mental workload},
abstract = {Studies examining the potential of augmented reality (AR) to improve assembly tasks are often unrepresentative of real assembly line conditions and assess mental workload only through subjective measurements and leads to conflicting results. We proposed a study directly carried out in industrial settings, to compare the impact of AR-based instructions to computerized instructions, on assembly effectiveness (completion time and errors) and mental workload using objective (eye tracking), subjective (NASA-TLX) and behavioral measurements (dual task paradigm). According to our results, AR did not improve effectiveness (increased assembly times and no decrease in assembly errors). Two out of three measurements indicated that AR led to more mental workload for simple assembly workstation, but equated computer instructions for complex workstation. Our data also suggest that, AR users were less able to detect external events (danger, alert), which may play an important role in the occurrence of work accidents.}
}
@article{GRAJEWSKI2015348,
title = {Improving the Skills and Knowledge of Future Designers in the Field of Ecodesign Using Virtual Reality Technologies},
journal = {Procedia Computer Science},
volume = {75},
pages = {348-358},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.257},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037187},
author = {Damian Grajewski and Jacek Diakun and Radosław Wichniarek and Ewa Dostatni and Paweł Buń and Filip Górski and Anna Karwasz},
keywords = {ecodesign, design for recycling, virtual reality, immersive training ;},
abstract = {Nowadays future designers are required to explore different fields of engineering (e.g. basis of design and operation of machines, methodology of process design, characteristics of phases of a product life cycle, ergonomics, etc.) in order to obtain sufficient knowledge about the product, the problems of its operation and related services. Currently, there is a strong tendency to take into account the impact of a product on the natural environment. The handling of products that are withdrawn from usage is one of the big problems emerging in a modern society. One of the possibilities to solve these problems is taking into account the environmental issues at the very early stage of the product design. Authors created the methodology to include recycling requirements in the phase of design of the product with support of the CAD 3D systems14,15. In details, the method of ecological-oriented product assessment during the design process was implemented in CAD 3D environment in order to improve the skills and knowledge of future designers about environmental aspects of the designed product12. In order to enhance the effectiveness of the training of designers, authors decided to use immersive Virtual Reality (VR) technologies that are more and more often used as advanced training systems. The user/trained person can explore Virtual Environment (VE) for educational and exercise purposes5. Immersive VR technologies are gaining wider use as engineering design tools to support the product design phase because of their ability to deliver an immersive and user friendly environment that can be used as digital test-bed for prototypes of the product30. Paper briefly describes the analysis of the recycling of selected product designed in the CAD 3D system with the support of Virtual Reality technologies that were based on the recycling product model (RpM) and the agent technology12,14,15. The RpM, developed during the geometric modelling phase, includes the data necessary for a comprehensive product recyclability evaluation already at the design stage. This approach allows designer to select appropriate solutions that facilitate future disassembly and to choose materials most suitable in terms of future recycling. The analysis allows to make an assessment of the susceptibility of the product for recycling using the agent system. The agent system, in accordance with the recycling product model (RpM) that was implemented in the 3D CAD system, is an innovative tool for the designer to enable a comprehensive assessment of the product in terms of its susceptibility to recycling. Future designers (students of mechanical engineering fields) gained possibility to improve their skills and knowledge in the field of ecodesign through the immersive trainings of virtual product design for recycling. Applying the Virtual Reality technologies for training of the future designers was aimed at enhancing the effectiveness of training mainly due to immersion and interaction with the virtual environment and to explore the product before it is constructed, as well as provide learning of the environments in which the finished product will be operated. As the example of the recycling-oriented virtual product design, case study of the design of small household appliance is presented. Participants of the immersive training (students) have a possibility to design the product in the Virtual Reality and select the optimal variant, which meets all environmental requirements. Short description of the recycling modeling (creation of connections, defining the extended materials and disassembly attributes) as well as an example of recycling assessment such as calculation of recycling assessment measures or providing tips and suggestions generated by agent system is also presented.}
}
@article{YANG2023111286,
title = {Mechanical properties and failure patterns of sandwich panels with AR-glass textile reinforced concrete face sheets subjected to quasi-static load},
journal = {Thin-Walled Structures},
volume = {193},
pages = {111286},
year = {2023},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2023.111286},
url = {https://www.sciencedirect.com/science/article/pii/S0263823123007644},
author = {Ting Yang and Deju Zhu and Chao Chen},
keywords = {TRC sandwich panel (TRC-SP), Static bending loading, Bending properties, Failure},
abstract = {A sandwich panel was prepared with Alkali Resistant-glass textile reinforced concrete (ARG-TRC) as the face sheet and Expanded Polystyrene (EPS) granular mortar as the core in this paper. Afterward, a quasi-static bending test was conducted to investigate the influences of the number of reinforcement layer, core thickness, and interface rib on the bending behaviors and failure of ARG-TRC sandwich panels (ARG-TRC-SPs). On this basis, the elastic bending deformation and ultimate bending capacity were theoretically predicted, and the failure mechanism was analyzed. The results show that ARG-TRC-SPs exhibited three common failure modes: face sheet bending fracture, core shear, and core crushing. Increasing the reinforcement layers significantly improved the properties of ARG-TRC-SPs but decreased textile utilization. Although the enhancement of the interface rib was dependent on the number of reinforcement layers, it could improve the integrity of ARG-TRC-SPs. There was a suitable core thickness for the ARG-TRC-SPs, and a thicker core did not indicate better mechanical properties. The theoretically predicted elastic deformation and ultimate bending capacity were in good agreement with the test results. However, the contribution of the core and interface ribs was underestimated. These results can provide a valuable reference for researchers and engineers in designing and applying TRC-SPs.}
}
@article{PELEGADLER201852,
title = {The effects of aging on the use of handheld augmented reality in a route planning task},
journal = {Computers in Human Behavior},
volume = {81},
pages = {52-62},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217306830},
author = {Rinat Peleg-Adler and Joel Lanir and Maria Korman},
keywords = {Augmented reality, Older adults, Aging, Sensory and perceptual processes, Planning and navigation, User experience},
abstract = {Technology may offer benefits for the older population in keeping their independence and connection to society. However, adopting new technologies, such as augmented reality (AR) by older adults may be difficult due to decline of cognitive and physical abilities as well as due to hostility and apprehension regarding the use of technology. In the current study, we compared performance of older and younger participants in a route planning task of public transportation, implemented using both a handheld see-through AR interface, and a standard non-AR application on a mobile phone. Faster task completion times but also higher error rates were associated with the use of the AR interface, regardless of the age of the participants. Older adults showed significantly slower performance compared to younger participants while using the AR application, however, error rates were not significantly different. No trial-to-trial learning was observed for the AR interface, indicating that the AR see-through technology is intuitive and easily adopted. Finally, elderly participants reported on average better user experience for the AR interface compared to younger participants, and preferred the AR over the non-AR application. Our findings highlight the potential of AR technology for older adults, as a possible aid tool to manage everyday tasks, such as navigation and planning.}
}
@article{PORTALES2023125,
title = {Mixed Reality Annotation of Robotic-Assisted Surgery videos with real- time tracking and stereo matching},
journal = {Computers & Graphics},
volume = {110},
pages = {125-140},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322002291},
author = {Cristina Portalés and Jesús Gimeno and Antonio Salvador and Alfonso García-Fadrique and Sergio Casas-Yrurzum},
keywords = {Mixed Reality, Annotation, Robotic-Assisted Surgery, Tracking, Stereo matching},
abstract = {Robotic-Assisted Surgery (RAS) is beginning to unlock its potential. However, despite the latest advances in RAS, the steep learning curve of RAS devices remains a problem. A common teaching resource in surgery is the use of videos of previous procedures, which in RAS are almost always stereoscopic. It is important to be able to add virtual annotations onto these videos so that certain elements of the surgical process are tracked and highlighted during the teaching session. Including virtual annotations in stereoscopic videos turns them into Mixed Reality (MR) experiences, in which tissues, tools and procedures are better observed. However, an MR-based annotation of objects requires tracking and some kind of depth estimation. For this reason, this paper proposes a real-time hybrid tracking–matching method for performing virtual annotations on RAS videos. The proposed method is hybrid because it combines tracking and stereo matching, avoiding the need to calculate the real depth of the pixels. The method was tested with six different state-of-the-art trackers and assessed with videos of a sigmoidectomy of a sigma neoplasia, performed with a Da Vinci® X surgical system. Objective assessment metrics are proposed, presented and calculated for the different solutions. The results show that the method can successfully annotate RAS videos in real-time. Of all the trackers tested for the presented method, the CSRT (Channel and Spatial Reliability Tracking) tracker seems to be the most reliable and robust in terms of tracking capabilities. In addition, in the absence of an absolute ground truth, an assessment with a domain expert using a novel continuous-rating method with an Oculus Quest 2 Virtual Reality device was performed, showing that the depth perception of the virtual annotations is good, despite the fact that no absolute depth values are calculated.}
}
@article{MASOOD2020103112,
title = {Adopting augmented reality in the age of industrial digitalisation},
journal = {Computers in Industry},
volume = {115},
pages = {103112},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519301186},
author = {Tariq Masood and Johannes Egger},
keywords = {Augmented reality, Manufacturing, Technology, Organisation, Environment, TOE, Challenges, Implementation, Critical success factors, Industry 4.0, Industrial experiments, Digital},
abstract = {Industrial augmented reality (IAR) is one of the key pillars of the industrial digitalisation concepts, which connects workers with the physical world through overlaying digital information. Augmented reality (AR) market is increasing but still its adoption levels are low in industry. While companies strive to learn and adopt AR, there are chances that they fail in such endeavours due to lack of understanding key challenges and success factors in this space. This study identifies critical success factors and challenges for IAR implementation projects based on field experiments. The broadly used technology, organisation, environment (TOE) framework was used as a theoretical basis for the study, while 22 experiments were conducted for validation. It is found that, while technological aspects are of importance, organisational issues are more relevant for industry, which has not been reflected to the same extent in the literature.}
}
@article{YANG2014690,
title = {A virtual try-on system in augmented reality using RGB-D cameras for footwear personalization},
journal = {Journal of Manufacturing Systems},
volume = {33},
number = {4},
pages = {690-698},
year = {2014},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612514000594},
author = {Yu-I Yang and Chih-Kai Yang and Chih-Hsing Chu},
keywords = {Mixed reality, Object tracking, Virtual try-on, Design evaluation},
abstract = {This paper presents a system for design evaluation of footwear using commercial depth-sensing technologies. In a mixed reality environment, the system allows users to virtually try on 3D shoe models in a live video stream. A two-stage object tracking algorithm was developed to correctly align shoe models to moving feet during the try-on process. Color markers on the user's foot enabled markerless tracking. Tracking was driven by an iterative closest point (ICP) algorithm that superimposed the captured depth data and predefined reference foot models. Test data showed that the two-stage approach resulted in increased positional accuracy compared with tracking using only surface registration. Trimming the reference model using the instant view angle increased the computational efficiency of the ICP algorithm. The proposed virtual try-on function is an effective tool for realizing human-centered design. This study also demonstrated a new application of RGB-D cameras to product design.}
}
@article{KUO2021103502,
title = {Development of an immersive SLAM-based VR system for teleoperation of a mobile manipulator in an unknown environment},
journal = {Computers in Industry},
volume = {132},
pages = {103502},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103502},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001093},
author = {Chen-Yu Kuo and Chun-Chi Huang and Chih-Hsuan Tsai and Yun-Shuo Shi and Shana Smith},
keywords = {Virtual reality, SLAM, Immersive, Visual aids, Remote mobile manipulator},
abstract = {Operating a remote robot in an unknown environment is a challenge. Usually, a camera is attached to the robot to capture the images of the remote scenes. However, it is difficult to perceive a 3D environment using a 2D display. In this study, a real-time simultaneous localization and mapping (SLAM)-based virtual reality (VR) 3D human-machine interaction system is developed to provide users with immersive telepresence to better operate a remote mobile manipulator in an unknown environment. An RGB-depth camera is mounted on the mobile platform to scan and reconstruct the remote environment in real time. A head mounted display is used to display the 3D reconstructed environment, with a virtual copy of the actual robot in it, to provide users the feeling of immersion at the remote site. Visual aids are added in the 3D reconstructed environment to guide users with the direction and distance information. Users can control the remote robot by manipulating the virtual copy of the actual robot in the VR environment, without being constrained in the robot first person view. The system is compared with a video-based teleoperation and a direct on-site operation. Experimental results show that the grasping error and placing threshold of the SLAM-based VR teleoperation are 0.5 cm and 1.5 cm, respectively. The average error of the reconstructed environment is less than 4%. Within the accuracy of the reconstructed environment, the SLAM-based VR teleoperation has a higher grasping success rate and lower operation time, compared to the video-based teleoperation. This study shows a promising application of immersive SLAM-based VR in interactive telerobotic operations in a remote unknown environment.}
}
@article{ADWERNAT2020710,
title = {Optimizing the Design Review Process for Cyber-Physical Systems using Virtual Reality},
journal = {Procedia CIRP},
volume = {91},
pages = {710-715},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.115},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120308891},
author = {Stefan Adwernat and Mario Wolf and Detlef Gerhard},
keywords = {Virtual Reality, Design Review, Cyber-Physical Systems, Usability, Engineering Collaboration},
abstract = {The radical shift from traditional products towards intelligent, connected cyber-physical Systems (CPS) leads to a growing accumulation of physical products with large amounts of directly related digital contents. A core challenge of research associated with Industrie 4.0 and CPS is to bridge the gap between the physical and digital world. Virtual Reality (VR) may contribute to this endeavor, since this technology is inherently suitable to visualize digital contents and allows intuitive user interactions within a highly immersive virtual environment. This contribution addresses the design review process for CPS by introducing a VR-driven concept, taking CPS characteristics into account, like the use data of (previous) product instances in the field as an additional source of information. The concept was prototypically implemented utilizing modern VR devices to validate the overall-viability of the proposed concept and to improve on typical weak points of existing VR approaches. A non-formal usability study under laboratory conditions indicated the necessity for an initial training to familiarize with the hardware, the design review specific tools in VR and the adapted workflows of the proposed concept.}
}
@article{ZHOU2022105585,
title = {Personalized virtual reality simulation training system for percutaneous needle insertion and comparison of zSpace and vive},
journal = {Computers in Biology and Medicine},
volume = {146},
pages = {105585},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105585},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522003778},
author = {Zeyang Zhou and Zhiyong Yang and Shan Jiang and Bowen Jiang and Bin Xu and Tao Zhu and Shixing Ma},
keywords = {Percutaneous needle insertion, Virtual reality training, Simulated surgery},
abstract = {Purpose
In this research, we present a personalized simulation training system for percutaneous needle insertion based on virtual reality (VR). Within this system, surgeons can become more familiar with real surgical procedures, thereby reducing errors that may occur in real surgery. Additionally, different VR technologies, i.e., zSpace and Vive, were compared to provide surgeons with a better surgical training environment.
Methods
and Methods A VR system combined with the treatment planning system was developed to create personalized patient training environment. An evaluation study recruiting twenty novices was performed to demonstrate the system. Each participant performed six independent needle placements using the VR system. Placement time was recorded. Placement error was defined as the distance from the needle tip to the target center. The participants completed a seven-point Likert scale questionnaire after the simulation.
Results
Compared with Vive, using zspace reduces the placement time (from 90.32 s to 68.94 s). The placement error using zSpace and Vive was similar (1.27 ± 0.68 mm, 1.56 ± 0.81 mm, respectively). The questionnaire survey results show that most participants are highly satisfied with the training effect of the VR system. Participants prefer the operation mode and convenience of zSpace but think that the immersion of Vive is better.
Conclusions
The personalized virtual reality surgical training system was effective as a training system for percutaneous needle insertion. The system based on zSpace had a shorter placement time while maintaining placement accuracy compared to the system based on Vive. The system based on zSpace achieved higher satisfaction in most aspects except for immersion.}
}
@article{GUO2020106227,
title = {A hybrid method for evaluation of maintainability towards a design process using virtual reality},
journal = {Computers & Industrial Engineering},
volume = {140},
pages = {106227},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106227},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219306965},
author = {Ziyue Guo and Dong Zhou and Qidi Zhou and Shunfeng Mei and Shengkui Zeng and Dequan Yu and Jiayu Chen},
keywords = {Product design, Maintenance, Virtual reality, Human factor},
abstract = {Modern society has a high demand for manufactured products in almost all fields. To achieve high availability and reduce costs, products require good maintainability, which is an intrinsic property that is decided mostly in the design and manufacturing stages. Design evaluation plays an important role in improving the maintainability of products. At present, though designers can verify maintainability by conducting virtual maintenance simulations on digital mockups (DMUs) to test design and manufacturing effects, this approach requires substantial time and specialist knowledge. In this paper, a hybrid maintainability evaluation method based on virtual reality (VR) and fuzzy comprehensive evaluation (FCE) is proposed to solve the problem. First, an evaluation index system is established to launch the design requirements. A VR-based method is proposed to help conduct the immersive maintenance simulation in a virtual environment. A maintenance simulation monitoring platform is developed to help designers evaluate maintainability and make decisions. Finally, FCE is leveraged to give a comprehensive evaluation result for maintainability status. A case of comparative experiments was conducted to verify the effectiveness of the proposed method. The participants of two groups were encouraged to evaluate the maintainability of a fluid physical experiment rack for a space station based on a traditional method and the proposed method. After the physical verification and result analysis, the proposed method was demonstrated to have better abilities to help predict design flaw and make design decision.}
}
@article{RUNJI2020101957,
title = {Markerless cooperative augmented reality-based smart manufacturing double-check system: Case of safe PCBA inspection following automatic optical inspection},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {64},
pages = {101957},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101957},
url = {https://www.sciencedirect.com/science/article/pii/S073658451930064X},
author = {Joel Murithi Runji and Chyi-Yeu Lin},
keywords = {Augmented reality, Inspection, Cooperative, Printed circuit board assembly, Smart manufacturing, Optical see-through},
abstract = {Augmented reality (AR) is a key technology anchored towards realizing Industry 4.0 smart manufacturing aims. In manufacturing inspection, AR has previously been employed to support operators in assessing thickness of manufactured parts, ship building, aircraft subassemblies and printed circuit board assemblies (PCBAs) with or without external markers in standalone head-mounted (HMD) or handheld systems. These AR systems often use optical see-through (OST) or video see-through (VST) technologies. Currently, cyber physical integration of processes in an industrial system synergizes production through increased efficiency and improved quality while facilitating customization. An operator manually double-checking/inspecting a product that has previously been automatically inspected can better rely on this existing defect location information once it is contextually and spatially overlaid within their field of view to intuitively and efficiently execute the task. Additional interactive information provided can quickly reorient a user, provide necessary contextual reference, and monitor progress of the task while alerting them of their safety. In this study, adopting an automatic optical inspection (AOI) wirelessly aided HMD-OST AR-based manual inspection system requiring no external markers for contextual registration is a promising direction towards smart and safe PCBA manufacturing in line with Industry 4.0. Animated rectangular bracket of high color contrast is employed to localize the spawned AOI defect point, an arrow overlaid with distance text provides user guidance, a contextual resizable image facilitates user double-checking and a progress bar tracks the inspection progress while monitoring tracking state. We evaluate the developed system's robustness, effectiveness of display technology used, suitability of contextual against static display modes, and display technology influence on various inspection attributes in a user study. Results demonstrate the system to be robust, OST-HMD outperforms handheld VST devices, contextual display mode to be significantly preferred to static mode, and display technology employed has no significant influence on the inspection attributes. Finally, registration precision results demonstrate usability of the system while superposition of distance to orientation information raises the inspection rate of PCBAs.}
}
@article{KOUROUTHANASSIS201571,
title = {Tourists responses to mobile augmented reality travel guides: The role of emotions on adoption behavior},
journal = {Pervasive and Mobile Computing},
volume = {18},
pages = {71-87},
year = {2015},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2014.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1574119214001527},
author = {Panos Kourouthanassis and Costas Boletsis and Cleopatra Bardaki and Dimitra Chasanidou},
keywords = {Mobile augmented reality, Tourist guide, Personalization, Adoption study, Emotional design},
abstract = {This research presents a mobile augmented reality (MAR) travel guide, named CorfuAR, which supports personalized recommendations. We report the development process and devise a theoretical model that explores the adoption of MAR applications through their emotional impact. A field study on Corfu visitors (n=105) shows that the functional properties of CorfuAR evoke feelings of pleasure and arousal, which, in turn, influence the behavioral intention of using it. This is the first study that empirically validates the relation between functional system properties, user emotions, and adoption behavior. The paper discusses also the theoretical and managerial implications of our study.}
}
@article{IZAGUIRRE2024e00318,
title = {Exploring cultural heritage and archaeological research from a VR-based approach},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00318},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00318},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000031},
author = {Joaquín Ignacio Izaguirre and Alejandro Andrés Ferrari and Félix Alejandro Acuto},
keywords = {Virtual reality, Video games, Cultural heritage, South Andean archaeology},
abstract = {In this paper, we approach an archaeological site in the Southern Andes using Virtual Reality (VR) environments to enhance cultural heritage and academic research. We will detail how we used a video game engine to model the light, landscape, sound dispersion, and architecture and create VR experiences. We will also discuss the results of pilot tests and interviews that we did to evaluate the success of such VR experiences. Even though we believe these experiences can positively impact the field of research and the protection of tangible and intangible heritage, we must carefully consider how to design VR environments and experiences.}
}
@article{NOON2012500,
title = {A system for rapid creation and assessment of conceptual large vehicle designs using immersive virtual reality},
journal = {Computers in Industry},
volume = {63},
number = {5},
pages = {500-512},
year = {2012},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2012.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166361512000371},
author = {Christian Noon and Ruqin Zhang and Eliot Winer and James Oliver and Brian Gilmore and Jerry Duncan},
keywords = {Product configuration, Conceptual design, Virtual reality},
abstract = {Currently, new product concepts are often evaluated by developing detailed virtual part and assembly models with traditional computer aided design (CAD) tools followed by appropriate analyses (e.g., finite element analysis, computational fluid dynamics, etc.). The creation of these models and analyses are tremendously time consuming. If a number of different conceptual configurations have been determined, it may not be possible to model and analyze each of them due to the complexity of these evaluation processes. Thus, promising concepts might be eliminated based solely on insufficient time and resources for assessment. In addition, the virtual models and analyses performed are usually of much higher detail and accuracy than what is needed for such early assessment. By eliminating the time-consuming complexity of a CAD environment and incorporating qualitative assessment tools, engineers could spend more time evaluating concepts that may have been previously abandoned due to time constraints. To address these issues, the Advanced Systems Design Suite (ASDS), was created. The ASDS incorporates a PC user interface with an immersive virtual reality (VR) environment to ease the creation and assessment of conceptual design prototypes individually or collaboratively in an immersive VR environment. Assessment tools incorporate metamodeling approximations and immersive visualization to evaluate the feasibility of each concept. In this paper, the ASDS system and interface along with specifically designed immersive VR assessment tools such as state saving and dynamic viewpoint creation are presented for conceptual large vehicle design. A test case example of redesigning an airplane is presented to explore the feasibility of the proposed system.}
}