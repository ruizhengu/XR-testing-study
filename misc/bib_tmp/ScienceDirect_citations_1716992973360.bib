@article{PAN2024100662,
title = {Sports game teaching and high precision sports training system based on virtual reality technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100662},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100662},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000302},
author = {Yang Pan},
keywords = {Posture recognition, Virtual reality, Digital filtering technology, Sports training, Teaching games},
abstract = {Sports can combine virtual reality technology with high-precision training to enhance the training effectiveness of users. In view of the current problems of insufficient physical quality and lack of exercise motivation faced by college students, this paper aims to explore a new type of college sports game teaching model through the integration of virtual reality technology. This is to improve students 'physical exercise efficiency and subjective initiative. This study constructed an experiential sports game model by applying posture recognition technology and fuzzy comprehensive evaluation method. The research indicated that the error accuracy with residual structure added will quickly decrease. When calculating the error, the overall error decreased by 6 mm. The proposed method can accurately distinguish human behavior in motion and trajectories in the motion area. The motion capture accuracy was high, and the deformation amount met the teaching needs of higher education institutions for sports game teaching. The contribution of this study is the significant improvement in the accuracy of motion capture, resulting in a total error reduction to 6 mm. This improvement ensured that the model can accurately distinguish between motion behavior and trajectory. The cumulative variance explanation rate of the scale reached 712.407 %, which was far more than that of the traditional model, and fully covered and explained the questionnaire information. Through the comparative test with four knowledge tracking models, the effectiveness and superiority of the sports game teaching mode integrated with virtual reality technology were verified.}
}
@article{SCHEFFER2021816,
title = {Augmented reality for IT/OT failures in maintenance operations of digitized trains: current status, research challenges and future directions},
journal = {Procedia CIRP},
volume = {100},
pages = {816-821},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S221282712100500X},
author = {Sara Scheffer and Alberto Martinetti and Roy Damgrave and Leo {van Dongen}},
keywords = {Augmented Reality, Maintenance, Digitized trains, IT/OT failures},
abstract = {The railway industry is moving towards a complex socio-technological system that relies on computer control and human-machine interfaces. Digitization and convergence of information technology (IT) with operational technology (OT) becomes increasingly important. Therefore, enabling innovations, lowering costs, increasing safety systems, enhancing performance, and increasing flexibility gains. In this scenario, opportunities arise for Augmented Reality (AR) capabilities to enhance maintenance operations of digitized trains. Initial research and tests have enabled training technicians and assistance of train drivers during specific activities by facilitating interactions with machines that go beyond traditional training manuals. Despite the positive results AR has shown, there are no specific indications available regarding the ‘deployability’ of the technology related to maintenance tasks. This position paper presents arguable boundaries and challenges for a structural appraisal of AR on maintenance operations of IT/OT converged trains. Potential AR solutions and specifications were based on current maintenance operations. Semi-structured interviews and surveys were conducted with maintenance providers to determine existing challenges. Moreover, the paper comprises an analysis to reveal the potential application AR has in maintenance procedures of digitized trains. Finally, this paper provides future research directions of AR technologies related to maintenance procedures of trains.}
}
@article{BUN2017445,
title = {Low – Cost Devices Used in Virtual Reality Exposure Therapy},
journal = {Procedia Computer Science},
volume = {104},
pages = {445-451},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.158},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730159X},
author = {Pawel Bun and Filip Gorski and Damian Grajewski and Radoslaw Wichniarek and Przemyslaw Zawadzki},
keywords = {Virtual reality, Low – cost devices, Virtual reality exposure therapy},
abstract = {The Virtual Reality technology is nowadays dynamically developed, thanks to appearance of low-cost devices and interest by large companies related to entertainment, communication and visualization. It is especially important in medicine, as it allows a much wider access to tools such as Virtual Reality Exposure Therapy. The paper presents possibilities of using low-cost VR devices in curing phobias by exposure to a stress-generating factor in an immersive virtual environment. Several use scenarios are presented for a simple application aimed at exposing a patient to fear of heights. A test group consisted of healthy individuals. To evaluate level of immersion and fear caused by the application, a heart rate monitor was used, to record heartbeat in real time.}
}
@article{ROMERO2022103678,
title = {A user-centric computer-aided verification process in a virtuality-reality continuum},
journal = {Computers in Industry},
volume = {140},
pages = {103678},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103678},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000756},
author = {Victor Romero and Romain Pinquié and Frédéric Noël},
keywords = {Model-based systems engineering, Validation, Verification, User-centred design, Virtual reality, Human-in-the-loop simulation, Immersion, Realism},
abstract = {Although companies systematically strive for a full digitalisation of their products and their processes, the design phase shows that the quality of models is very unequal. Indeed, detailed design benefits from much more sophisticated methods and tools than the specification and architecture activities. Although, we should note the recent paradigm shift from document-based to model-based systems engineering, these models, which are mainly static 2D diagrams, remain poor to facilitate design verification early on. Thus, to detect most errors during the design phase, companies have no other alternative than to wait up to the testing phase which occurs after several years of development for complex systems. Thus, we propose a user-centric computer-aided verification process to ensure that the design meets the requirements under realistic operational conditions. The verification process provides a progressive immersion into the virtual system before seamlessly transitioning to the real system. Our work is built upon state-of-the-art MBSE methods such as the Property Model Methodology, which enables systems engineers to co-simulate specification models and design models. We improve such MBSE methods by increasing the level of realism that experiences the end-user during the verification of a design by the original combination of Model-In-the-Loop, Immersive Model-In-the-Loop, Human-In-the-Loop, and Hardware-In-the-Loop simulation strategies. A robot arm is used as a use case to illustrate the verification process.}
}
@article{GUDIGAR2019359,
title = {Application of multiresolution analysis for automated detection of brain abnormality using MR images: A comparative study},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {359-367},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314560},
author = {Anjan Gudigar and U. Raghavendra and Tan Ru San and Edward J. Ciaccio and U. Rajendra Acharya},
keywords = {Brain MR images, Classification, Particle swarm optimization, Shearlet transform, Support vector machine, Texture features},
abstract = {Neurological disorders are abnormalities related to the human nervous system, and comprise electrical, biochemical, or structural changes in the spinal cord, brain, or central nervous system that induce various symptoms. These symptoms may be in the form of muscle weakness, paralysis, and poor coordination, among other factors. Early diagnosis of these changes is important for treatment, to limit disease progression. Magnetic resonance (MR) imaging is a widely used modality for diagnosing brain abnormality. Expert reading and interpretation of MR images is time-consuming, tedious, and subject to interobserver variability. Hence, various automated computer aided diagnosis (CAD) tools have been developed to detect brain abnormalities from MR imaging. Multiresolution analysis involves the transformation of images to capture obscure signatures. In this paper, we compare the performance of three different multi-resolution analysis techniques – the discrete wavelet transform, curvelet transform and shearlet transform – for detecting brain abnormality. Further, textural features extracted from the transformed image are optimally selected using particle swarm optimization (PSO), and classified using a support vector machine (SVM). The proposed method is applied on 83 control images, as well as 529 abnormal images from patients with cerebrovascular, neoplastic, degenerative and inflammatory diseases. For quantitative analysis, a cross validation scheme is implemented to improve system generality. Among the three techniques, the shearlet transform achieves a highest classification accuracy of 97.38% using only fifteen optimally selected features. The proposed system requires testing on a large data set prior to implementation as a standalone system to assist neurologists and radiologists in the early detection of brain abnormality.}
}
@article{LAYONA2018457,
title = {Web based Augmented Reality for Human Body Anatomy Learning},
journal = {Procedia Computer Science},
volume = {135},
pages = {457-464},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.197},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831487X},
author = {Rita Layona and Budi Yulianto and Yovita Tunardi},
keywords = {human anatomy, augmented reality, 3D object, human organ},
abstract = {Human body anatomy becomes an important topic in Biology subject that must be understood since junior high school. Learning materials are mostly available in form of book and anatomy mannequin (puppet), but it is still insufficient enough to help students in understanding human body anatomy. Augmented Reality (AR) is a technology that combines a real thing into virtual environment interactively. This research purpose is to develop an AR application for human body anatomy learning to be more interesting and easier for student to understand. This application enables student to learn human body anatomy with 3D object interaction while previously using textbook and mannequin. Research method in for this study is by using quantitative method that collects data and then develops the prototype to prove the impact. Application development method is done by using waterfall method that includes planning (collect data and analysis), design (user interface and diagram), implementation, and testing. Research result is AR application for human body anatomy learning that contains 3D object, organ explanation and position that can be accessible on web.}
}
@article{GIMENO20131263,
title = {A new AR authoring tool using depth maps for industrial procedures},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1263-1271},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513001267},
author = {J. Gimeno and P. Morillo and J.M. Orduña and M. Fernández},
keywords = {Augmented reality, Authoring tools, Non-immersive desktop, Kinect, Occlusion},
abstract = {Several augmented reality systems have been proposed for different target fields such as medical, cultural heritage and military. However, most of the current AR authoring tools are actually programming interfaces that are exclusively suitable for programmers. In this paper, we propose an AR authoring tool which provides advanced visual effect, such as occlusion or media contents. This tool allows non-programming users to develop low-cost AR applications, specially oriented to on-site assembly and maintenance/repair tasks. A new 3D edition interface is proposed, using photos and Kinect depth information to improve 3D scenes composition. In order to validate our AR authoring tool, two evaluations have been performed, to test the authoring process and the task execution using AR. The evaluation results show that overlaying 3D instructions on the actual work pieces reduces the error rate for an assembly task by more than a 75%, particularly diminishing cumulative errors common in sequential procedures. Also, the results show how the new edition interface proposed, improves the 3D authoring process making possible create more accurate AR scenarios and 70% faster.}
}
@article{RAGNI2018462,
title = {ARTool Zero: Programming trajectory of touching probes using augmented reality},
journal = {Computers & Industrial Engineering},
volume = {124},
pages = {462-473},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218303498},
author = {Matteo Ragni and Matteo Perini and Amedeo Setti and Paolo Bosetti},
keywords = {Touching probe, Augmented reality, Human machine interface, Part program generation},
abstract = {In manufacturing applications, process preparation is a time consuming and an error prone operation, and costs are especially relevant when dealing with small batches. One mandatory operation for machining preparation is the raw material alignment with respect to machine reference frame, commonly done through an electronic touching probe. Machine manufacturers implement in-controller preparatory codes for the identification of a range of geometric features (simple contacts, plane normals, circles, etc.) in order to make alignments faster, but those procedures rely on human-input parameters, and an error may have catastrophic consequences for the probe itself. ARTool Zero supports the operators in programming touching probe trajectories, generating and simulating on-the-fly the part-program that guides the probe in the identification of a geometric features. The interface of ARTool Zero is an augmented reality application based on the ARTool Framework. The application runs on a mobile device that communicates with the machine controller. ARTool Zero projects coordinates from the mobile device screen to the machine active reference frame through the means of markers. One of the markers acts as an anchor for a virtual plane, i.e. the plane in the real world on which the mobile screen coordinates are projected, allowing to convert a 2D screen tap in a 3D point in space. Ego-localization accuracy is one of the critical aspects of the application, thus the validation of the core ARTool library is discussed. A video that presents the application in a real-case scenario is available at https://youtu.be/wFF89pqfm6c.}
}
@article{GUO201841,
title = {Using virtual reality to support the product’s maintainability design: Immersive maintainability verification and evaluation system},
journal = {Computers in Industry},
volume = {101},
pages = {41-50},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517303950},
author = {Ziyue Guo and Dong Zhou and Jiayu Chen and Jie Geng and Chuan Lv and Shengkui Zeng},
keywords = {Virtual reality, Maintainability, Product Design},
abstract = {Maintainability is an important characteristic of complex products. Good maintainability design can save substantial costs and reduce incidents and accidents throughout the product life cycle. Therefore, maintainability should be given full consideration in the early design stages. However, the current desktop tools are not effective to aid in maintainability design. Virtual reality (VR), a state-of-the-art of computer science, generates a virtual environment in which the user can have intuitive feelings and interact with virtual objects. In this paper, an immersive maintainability verification and evaluation system (IMVES) based on virtual reality is proposed. The goal of the system is to develop a cost-effective, rapid and precise method to improve the maintainability design in the early design stages. IMVES enables the user to interact with maintenance objects and conduct immersive simulations. Based on the developed methods, the data generated during simulation can be gathered to analyze the maintainability status. A case study applying IMVES into an aero-engine project is presented to demonstrate the effectiveness and feasibility of the system. Compared with desktop-based methods, the IMVES could provide a more efficient way to conduct a maintenance simulation. Furthermore, the credibility and objectivity of the maintainability evaluation are also improved.}
}
@article{MOURTZIS2022215,
title = {Challenges and Opportunities for Integrating Augmented Reality and Computational Fluid Dynamics Modeling under the Framework of Industry 4.0},
journal = {Procedia CIRP},
volume = {106},
pages = {215-220},
year = {2022},
note = {9th CIRP Conference on Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001822},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Simulation, Augmented Reality, CFD, Visualization, Industrial Internet of Things},
abstract = {The industrial requirements change rapidly due to the technological advancements, and the need to immediately investigate prospective system alternatives to develop a more efficient production system design is more intense than ever. Manufacturing systems simulation as a key technology of Industry 4.0 is a powerful tool towards optimizing manufacturing processes. Computational Fluid Dynamics (CFD) simulation can help smart factories gain a competitive advantage due to low cost, low risk and the provided meaningful insights. However, while CFD tools can provide users with precise simulations, there is strong need for interactive design and analysis tools. Additionally, fluid flow behavior is a challenging problem for cognitive interpretation and visualization. Therefore, engineers/technicians need to visualize the behavior of the desired flow field to improve their cognitive ability in problem-solving. As a result, Augmented Reality (AR) technology is proposed as the solution for an enhanced virtual learning and training environment in smart factories. To address these challenges, this research work presents an Industrial Internet of Things (IIoT) framework for simulation and visualization of selected monitored values based on AR and CFD in a cloud-computing environment for the intuitive visualization of the data on smart devices. The applicability of the proposed framework has been validated in a real-life case study derived from an Original Equipment Manufacturer (OEM) supplier of Industrial refrigerator systems.}
}
@article{RUBO2024107915,
title = {Social stress in an interaction with artificial agents in virtual reality: Effects of ostracism and underlying psychopathology},
journal = {Computers in Human Behavior},
volume = {153},
pages = {107915},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107915},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002662},
author = {Marius Rubo and Simone Munsch},
keywords = {Social stress, Virtual reality, Ostracism, Social exclusion, General psychopathology, Anxiety, Social anxiety, Cyberball},
abstract = {Social stress can emerge from situational as well as dispositional factors. Here we tested in direct juxtaposition how ostracism in a Cyberball game and underlying psychopathology levels both influence subjective and objective stress markers in an interaction with artificial agents in virtual reality (VR) in 80 participants from a student population. Ostracism led to moderately enhanced subjective stress and negative mood but not to alterations on objective markers of stress. By contrast, underlying psychopathology levels were associated with substantially stronger alterations on subjective stress markers and were additionally associated with reduced eye gaze at virtual agents' heads, larger pupil size, larger high-frequency pupil-size variability, higher heart rate and reduced high-frequency heart-rate variability. Effects for social anxiety, general anxiety and depression levels were overall similar with largest effects on objective stress markers linked to general anxiety. These findings contest the suitability of the Cyberball game as a model for real-life ostracism but demonstrate the utility of gaze and physiological data in predicting psychopathology levels during interactions in VR. These findings furthermore highlight the need for data security solutions when using social VR applications where rich data streams are exposed publicly.}
}
@article{OUALI2022158,
title = {Augmented Reality for Scene Text Recognition, Visualization and Reading to Assist Visually Impaired People},
journal = {Procedia Computer Science},
volume = {207},
pages = {158-167},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009218},
author = {Imene OUALI and Mohamed BEN HALIMA and Ali WALI},
keywords = {Text Visualization, Text detection, Text recognition, Natural Scene, Augmented Reality, VGG19},
abstract = {Reading traffic signs while driving a car for visually impaired people and people with visual problems is a very difficult task for them. This task is encountered every day, sometimes incorrect reading of traffic signs can lead to very serious results. In particular, the Arabic language is very difficult, making recognizing and viewing Arabic text a difficult task. In this context, we are looking for an effective solution to remove errors and results that can sometimes end someone's life. This article aims to correctly read traffic signs with Arabic text using augmented reality technology. Our system is composed of three modules. The first is text detection and recognition. The second is Text visualization. The third is Text to speech methods conversion. With this system, the user can have two different results. The first result is visual with much-improved text and enhancement. The second result is sound, he can hear the text aloud. This system is very applicable and effective for daily life. To assess the effectiveness of our work, we offer a survey to a group of visually impaired people to give their opinion on the use of our application. The results have been good for most people.}
}
@article{HAO2024100669,
title = {Action capture and VR interactive system for online experimental teaching},
journal = {Entertainment Computing},
volume = {50},
pages = {100669},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000375},
author = {Gang Hao and Ling Cao},
keywords = {Online teaching, Action capture, Virtual reality, Feature localization, Joint judgment, Face detection},
abstract = {The development of technology has intensified people’s interest in virtual reality technology. Virtual reality technology is also widely used in fields such as teaching and healthcare. To improve the real-time and operability of virtual reality interaction systems, a motion capture and virtual reality interaction system design for online experimental teaching is proposed. The active apparent model is used for facial feature point localization, and the joint capture method is improved by combining threshold segmentation and forward kinematics algorithms. Experimental data confirms that compared to Kinect method, the improved hand joint detection method has higher joint positioning accuracy, with a loss rate of less than 40 % for hand joints. The average judgment accuracy of the six facial expression tests is 82 %, 81 %, 79 %, 78 %, 80 %, and 81 %, respectively. The comprehensive recognition accuracy of facial expression recognition is 80.17 %. The read, write, and update times for each frame of the virtual reality interaction system are 102.6 ms and 427 ms, respectively. The system memory usage is 920.5 M, and the CPU and GPU usage rates are 25.2 % and 4.4 %, respectively. The system can run smoothly. The virtual reality system can run continuously for 24 h, with strong operability and low difficulty in expanding system functions, achieving design goals.}
}
@article{MATTHEWS2020330,
title = {Interaction design for paediatric emergency VR training},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {330-344},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300590},
author = {Tj Matthews and Feng Tian and Tom Dolby},
keywords = {Virtual reality, Medical training, Human-Centred design, Interaction design},
abstract = {Background
Virtual reality (VR) in healthcare training has increased adoption and support, but efforts are still required to mitigate usability concerns.
Methods
This study conducted a usability study of an in-use emergency medicine VR training application, available on commercially available VR hardware and with a standard interaction design. Nine users without prior VR experience but with relevant medical expertise completed two simulation scenarios for a total of 18 recorded sessions. They completed NASA Task Load Index and System Usability Scale questionnaires after each session, and their performance was recorded for the tracking of user errors.
Results and Conclusion
s Our results showed a medium (and potentially optimal) Workload and an above average System Usability Score. There was significant improvement in several factors between users' first and second sessions, notably increased Performance evaluation. User errors with the strongest correlation to usability were not directly tied to interaction design, however, but to a limited 'possibility space'. Suggestions for closing this 'gulf of execution' were presented, including 'voice control' and 'hand-tracking', which are only feasible for this commercial product now with the availability of the Oculus Quest headset. Moreover, wider implications for VR medical training were outlined, and potential next steps towards a standardized design identified.}
}
@article{ALAM2024100059,
title = {ASL champ!: a virtual reality game with deep-learning driven sign recognition},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100059},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000096},
author = {Md Shahinur Alam and Jason Lamberton and Jianye Wang and Carly Leannah and Sarah Miller and Joseph Palagano and Myles de Bastion and Heather L. Smith and Melissa Malzkuhn and Lorna C. Quandt},
keywords = {Virtual reality, Deep learning, American sign language, Interactive learning, VR learning, Avatar interaction},
abstract = {We developed an American Sign Language (ASL) learning platform in a Virtual Reality (VR) environment to facilitate immersive interaction and real-time feedback for ASL learners. We describe the first game to use an interactive teaching style in which users learn from a fluent signing avatar and the first implementation of ASL sign recognition using deep learning within the VR environment. Advanced motion-capture technology powers an expressive ASL teaching avatar within an immersive three-dimensional environment. The teacher demonstrates an ASL sign for an object, prompting the user to copy the sign. Upon the user’s signing, a third-party plugin executes the sign recognition process alongside a deep learning model. Depending on the accuracy of a user’s sign production, the avatar repeats the sign or introduces a new one. We gathered a 3D VR ASL dataset from fifteen diverse participants to power the sign recognition model. The proposed deep learning model’s training, validation, and test accuracy are 90.12%, 89.37%, and 86.66%, respectively. The functional prototype can teach sign language vocabulary and be successfully adapted as an interactive ASL learning platform in VR.}
}
@article{MURA2021225,
title = {A proposal of an assembly workstation for car panel fitting aided by an augmented reality device},
journal = {Procedia CIRP},
volume = {103},
pages = {225-230},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008775},
author = {Michela Dalle Mura and Gino Dini},
keywords = {Assembly, Augmented Reality, Car Body Fitting, Collaborative robots, Automotive sector},
abstract = {In the automotive sector, panel fitting operations represent a delicate phase of the assembly process, during which car bodywork components are manually aligned attempting to comply with spacing tolerances. Digital technologies currently promoted by the Industry 4.0 approach, such as collaborative robots and augmented reality (AR), could assist workers for reducing execution times of this “trial and error” procedure. On the one hand, human-robot collaboration (HRC) can enhance assembly process efficiency, by combining human dexterity and cognitive capabilities with robot repetitiveness and support in heavy load handling; on the other, AR can provide interactive instructions to guide workers during the process. In this regard, the present paper is focused on the development of a novel assembly workstation using HRC and AR to support workers during car panel fitting operations. The system is based on the implementation of an algorithm able to convert the values on panel spacing, which are measured by laser gauge positioned on the collaborative robot, in the instructions for the worker to correctly carry out the panel fitting operation. The paper finally presents the application of the system to a real assembly example.}
}
@article{ULMER2021140,
title = {Adapting Augmented Reality Systems to the users’ needs using Gamification and error solving methods},
journal = {Procedia CIRP},
volume = {104},
pages = {140-145},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009227},
author = {Jessica Ulmer and Sebastian Braun and Chi-Tsun Cheng and Steve Dowey and Jörg Wollert},
keywords = {Augmented Reality, Adaptive Systems, Gamification, Error Recovery},
abstract = {Animations of virtual items in AR support systems are typically predefined and lack interactions with dynamic physical environments. AR applications rarely consider users’ preferences and do not provide customized spontaneous support under unknown situations. This research focuses on developing adaptive, error-tolerant AR systems based on directed acyclic graphs and error resolving strategies. Using this approach, users will have more freedom of choice during AR supported work, which leads to more efficient workflows. Error correction methods based on CAD models and predefined process data create individual support possibilities. The framework is implemented in the Industry 4.0 model factory at FH Aachen.}
}
@article{TSUJIMOTO2024106054,
title = {Server-enabled mixed reality for flood risk communication: On-site visualization with digital twins and multi-client support},
journal = {Environmental Modelling & Software},
volume = {177},
pages = {106054},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106054},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224001154},
author = {Ryoma Tsujimoto and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Distributed computer systems, Mobile augmented reality, Real-time visualization, Remote rendering, Risk communication, Web based},
abstract = {This study introduces a novel mobile mixed reality (MR) system to improve communication among stakeholders regarding climate-change-driven flood risk. This MR system enables stakeholders to visualize flood risks on common mobile devices, such as smartphones and tablets, offering perspectives from their respective environments. The system overcomes the computational limitations of previous mobile MR systems by rendering the MR images on a server. Furthermore, distributing users across multiple servers enables many users to simultaneously perform server-enabled MR, facilitating widespread engagement in flood risk communication. The system uses a viewport transformation to accurately display virtual floods and correct for real-time posture changes. A prototype tested in a high-risk flood area achieved real-time MR visualization at more than 16 frames per second, even on multiple devices. Future research will aim to promote stakeholder participation through more accurate visualization of virtual floods and improved interactive information sharing among users.}
}
@article{DOMINGUEZ2008407,
title = {Modeling and application of MR dampers in semi-adaptive structures},
journal = {Computers & Structures},
volume = {86},
number = {3},
pages = {407-415},
year = {2008},
note = {Smart Structures},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2007.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S004579490700082X},
author = {A. Dominguez and R. Sedaghati and I. Stiharu},
keywords = {Controllable fluids, Magnetorheological fluid, MR damper, Semi-active control, Adaptive structure, Modeling and simulation},
abstract = {The developing of technology has discovered new materials which have been applied to improve the performance of structures. The researchers have recently increased the attention in controllable fluids and its applications. Magnetorhelological (MR) dampers are devices that employ rheological fluids to modify their mechanical properties. Their mechanical simplicity, high dynamic range, lower power requirements, large force capacity, robustness and safe manner operation in case of fail have made them attractive devices to semi-active control in civil, aerospace and automotive applications. The characteristics of the MR damper change when the rheological fluid is exposed to a magnetic field changing its stiffness and damping coefficients. A non-linear new model based on the Bouc–Wen model, is employed to simulate the hysteresis behavior of the damper. The model considers the frequency, amplitude and current excitation as dependent variables. The finite element model (FEM) of the MR damper element has also been developed based on the proposed model. Subsequently finite element of the adaptive structure embedded with MR dampers has been established and the non-linear response of the whole structure is obtained. Experimental work was carried out to validate the simulations. For this study, a cantilever 3-D space truss structure with 4 bays exposed to different excitations has been considered in which one of the members has been substituted by the MR damper. A good agreement has been observed between the simulations and experimental data.}
}
@article{HUUSKONEN2019367,
title = {Augmented Reality for Supervising Multirobot System in Agricultural Field Operation},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {30},
pages = {367-372},
year = {2019},
note = {6th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.568},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319324899},
author = {Janna Huuskonen and Timo Oksanen},
keywords = {autonomous vehicles, situational awareness, tractors, wearable headset, mixed reality, MR},
abstract = {Agriculture is shifting from farmers manually operating machines to monitoring autonomous machines. Thus, the task of a farmer is fleet management and taking care of safe operation. Situational awareness during the operation is important. Augmented reality (AR) is a powerful tool for visualizing information in real-time. To demonstrate the use of AR in agricultural fleet management, in this paper we present a novel AR system to help the farmer supervise the operation of two autonomous agricultural machines. The paper discusses the requirements for AR application, and we present the architecture of the system and the results of a demonstration carried out in a test field.}
}
@article{STAVROPOULOS2022976,
title = {Augmented Reality tool for the facilitation of the UAV flight inspection process during aircraft de-icing as a maintenance procedure},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {976-982},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.480},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017748},
author = {Panagiotis Stavropoulos and Lydia Athanasopoulou and Alexios Papacharalampopoulos and Ioannis Kanellopoulos and Dimitris Mourtzis},
keywords = {Augmented Reality, Unmanned Aerial Vehicles, Industry 4.0, mesh reconstruction, flight inspection, de-icing},
abstract = {During the past few years, industry has begun to recognize the operational and economic value of Unmanned Aerial Vehicles (UAVs) operations. Internet of Things (IoT), Augmented Reality (AR), UAVs etc. are being continuously applied within Industry 4.0 applications due to their major capabilities on the optimization of modern manufacturing concepts. The scope of this paper is the development of an AR application to support the inspection process performed by UAVs during the de-icing procedure on aircrafts before take-off. A mixed reality interface is being designed for providing virtual feedback to the operator regarding the position and extension of ice on the aircraft. The main objective is to significantly reduce the inspection time and increase the accuracy of detecting ice through an immersive environment, exceeding the standard time consuming pre-flight methods relying on visual inspection. To broaden applicability, two different development strategies are being implemented: for Android devices and for Microsoft HoloLens Head-Mounted Display. Both scenarios have been tested in a mixed reality to evaluate the performance of the Augmented Reality application.}
}
@article{DALLEMURA2021251,
title = {An augmented reality approach for supporting panel alignment in car body assembly},
journal = {Journal of Manufacturing Systems},
volume = {59},
pages = {251-260},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000595},
author = {Michela {Dalle Mura} and Gino Dini},
keywords = {Assembly, Augmented reality, Car body fitting},
abstract = {One of the major problems in automotive assembly consists in achieving alignment within the specified tolerances between car panels that make up the exterior bodywork. To reduce errors and time requested to perform the assigned assembly tasks, workers should be guided during these panel fitting operations. Augmented Reality (AR) could be particularly suitable in this regard, as it represents one of the most promising tools to support personnel, with constantly growing applications in production processes. Following this trend, the present work aims to present an AR prototype system for supporting the operator during panel fitting operations of car body assembly, by providing instructions to correct alignment errors in terms of gap and flushness. A real case study concerning the fine alignment of car body panels with respect to the front light projector is also presented.}
}
@article{FILLATREAU20131253,
title = {Using virtual reality and 3D industrial numerical models for immersive interactive checklists},
journal = {Computers in Industry},
volume = {64},
number = {9},
pages = {1253-1262},
year = {2013},
note = {Special Issue: 3D Imaging in Industry},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513000717},
author = {P. Fillatreau and J.-Y. Fourquet and R. {Le Bolloc’h} and S. Cailhol and A. Datas and B. Puel},
keywords = {Virtual reality, 3D industrial inspection, Quality and control processes, 3D visualization, Sensorimotor interfaces},
abstract = {At the different stages of the PLM, companies develop numerous checklist-based procedures involving prototype inspection and testing. Besides, techniques from CAD, 3D imaging, animation and virtual reality now form a mature set of tools for industrial applications. The work presented in this article develops a unique framework for immersive checklist-based project reviews that applies to all steps of the PLM. It combines immersive navigation in the checklist, virtual experiments when needed and multimedia update of the checklist. It provides a generic tool, independent of the considered checklist, relies on the integration of various VR tools and concepts, in a modular way, and uses an original gesture recognition. Feasibility experiments are presented, validating the benefits of the approach.}
}
@article{KOSTOV2022896,
title = {Designing a Framework for Collaborative Mixed Reality Training},
journal = {Procedia Computer Science},
volume = {200},
pages = {896-903},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002964},
author = {Georgi Kostov and Josef Wolfartsberger},
keywords = {multi-device, collaboration, tool, virtual reality, augmented reality, network, human-computer interaction},
abstract = {Training simulations in Virtual Reality (VR) benefit from the technology’s interactive and intuitive nature. Typically, the VR user is isolated in the virtual environment and an external instructor or spectators cannot support the learning progress. Previous research work presents solutions for isolated use cases, but lacks specific guidelines for the implementation of collaborative multi-device systems. We propose a method for expanding collaborative extended reality (XR) applications to multiple platforms to support collaborative learning, but also other types of applications such as remote collaboration and maintenance. In order to test our approach, we expanded an existing application to four different platforms - Steam VR, Microsoft Mixed Reality, smartphone/tablet and desktop computer. Based on lessons learned from our prototype, we propose a solution for multi-device networking and interaction on each platform. The main strengths of our approach are in the decoupling of local and networked objects and the common interface for interaction, which accommodates multiple platforms. We believe our approach can provide useful insights into collaborative training and serve as a good starting point for future projects.}
}
@article{AKPAN2017197,
title = {The confirmed realities and myths about the benefits and costs of 3D visualization and virtual reality in discrete event modeling and simulation: A descriptive meta-analysis of evidence from research and practice},
journal = {Computers & Industrial Engineering},
volume = {112},
pages = {197-211},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217303790},
author = {Ikpe Justice Akpan and Murali Shanker},
keywords = {Discrete event modeling, Simulation methodology, Three-dimensional display, Virtual reality user interfaces, Visualization techniques},
abstract = {The past seventeen years has witnessed a significant transformation in discrete event modeling and simulation, and the introduction of new modeling methodology based on three-dimensional (3D) visualization and virtual reality (VR) technologies. While several studies demonstrate the benefits and costs of 3D and VR modeling in discrete event simulation (DES) based on empirical evidence, others rely on anecdotal evidence and experience from practice. Further, there are still some unsubstantiated claims about the gains and challenges arising from the application of 3D and VR technologies in DES. This paper synthesizes the realized benefits and costs associated with modeling and simulation in 3D and VR while differentiating the realities from claims (myths). The results show that 3D and VR techniques offer significant benefits in the major modeling and simulation tasks than speculated among practitioners in the simulation community.}
}
@article{TIAN202184,
title = {Interactive prostate MR image segmentation based on ConvLSTMs and GGNN},
journal = {Neurocomputing},
volume = {438},
pages = {84-93},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.121},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221001053},
author = {Zhiqiang Tian and Xiaojian Li and Zhang Chen and Yaoyue Zheng and Hongcheng Fan and Zhongyu Li and Ce Li and Shaoyi Du},
keywords = {Medical image segmentation, Gated graph neural network, Long short term memory, User interaction},
abstract = {Accurate segmentation of the prostate on magnetic resonance (MR) images plays an important role for prostate cancer diagnosis and treatment. Although many automated prostate segmentation methods have been proposed, the performance still faces several challenges, which includes large variability in prostate shape, unclear boundary, and complex intensity distribution. Therefore, the results obtained from the automated methods should be further refined by users to get a more accurate and reliable segmentation. In this paper, we propose an end-to-end interactive segmentation method to refine the automated results. A convolutional long short term memory (convLSTM) module and a gated graph neural network (GGNN) are presented in the proposed method for prostate segmentation in both automated and interactive manners. A boundary loss is proposed to train our model. We evaluated the proposed method on two public available datasets and one in–house dataset. Experimental results show that the proposed convLSTM module could obtain a DSC of 91.78% on the test dataset, which outperforms eight state-of-the-art methods. A further 1.5% improvements can be obtained by user interactions based on the GGNN. The segmentation time including user interactions and inference time was 2.3 min on average for segmenting one volume.}
}
@article{ZHAO2024102415,
title = {In-situ observation and calibration for structure safety diagnosis through finite element analysis and mixed reality},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102415},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102415},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000636},
author = {Xuefeng Zhao and Wangbing Li and Zhe Sun and Meng Zhang and Lingli Huang},
keywords = {Structure safety diagnosis, Mixed reality, Finite element analysis, In-situ},
abstract = {In-situ observation and calibration through non-intrusive approaches are necessary for diagnosing structural safety risks and predicting structure deterioration patterns during field inspections of critical civil infrastructures. Conventional visual inspections always subject to engineering knowledge and field experiences, which could hardly examine structure risks under as-is conditions. Finite Element Analysis (FEA) tools and model updating algorithms could help examine changes of stress and displacements under extreme environmental conditions according to the discovered structure defects and spatiotemporal changes. However, such tools usually require extensive computing resources for running simulations at a location that is usually far from the jobsite. Mixed Reality (MR) techniques could help in visualizing the virtual geometric composition of physical structural components. Unfortunately, existing MR device could hardly visualize tedious FEA and model updating processes of critical structural components. Integrated used of FEA tools and MR techniques could help support real-time safety diagnosis of critical structural components at remote locations. In this study, the authors established a framework that integrate sensing techniques, FEA tools, model updating algorithms, and MR for supporting structure safety diagnosis of civil infrastructures. The proposed method aims to 1) capture and analyze structural defects, 2) calibrate the virtual model with the real structure for visualizing FEA results of critical structural components through MR. The authors validate the proposed method through a case study. Results show that the proposed method provides fundamental support for effective on-site structure safety diagnosis.}
}
@article{KIM2023100343,
title = {Mixed reality-integrated soft wearable biosensing glove for manipulating objects},
journal = {Biosensors and Bioelectronics: X},
volume = {14},
pages = {100343},
year = {2023},
issn = {2590-1370},
doi = {https://doi.org/10.1016/j.biosx.2023.100343},
url = {https://www.sciencedirect.com/science/article/pii/S2590137023000407},
author = {Jihoon Kim and Allison Bayro and Jaeho Lee and Ira Soltis and Myunghee Kim and Heejin Jeong and Woon-Hong Yeo},
keywords = {Mixed reality, Biosensing glove, Soft wearables, Object manipulation},
abstract = {Recent advances in flexible sensors and wireless electronics have driven the development of lightweight and ergonomic wearable sensing gloves. Such gloves can be employed in mixed reality (MR) environments to give haptic capabilities during interactions with various objects. However, no prior study shows a quantitative measurement of physical user interactions of object manipulation in MR. Here, we report an MR-integrated soft bioelectronic system on a glove for quantifying the changes in the user's pinching tasks. We use nanomanufacturing techniques to fabricate flexible sensors, wireless circuits, and stretchable interconnectors seamlessly integrated with a wearable glove. The wearable biosensing glove with an integrated capacitive pressure sensor evaluates how users interact directly and indirectly interact with objects. The direct mode describes a user's direct touching and manipulating objects in MR. In contrast, in the indirect mode, objects are located far away and touched via a narrow light beam. The virtual object measurement parameters include mass, movement latency, dynamic friction coefficient, angular drag coefficient, and linear drag coefficient. The experimental results with human subjects show positive, linear relationships between pinching force and dynamic friction coefficient and mass parameters during the direct manipulation mode. Collectively, the MR-enabled wearable biosensing glove system offers unique advantages in detecting physical interactions and sensory feedback for various rehabilitation applications and MR human-machine interfaces.}
}
@article{YANG201860,
title = {Neural multi-atlas label fusion: Application to cardiac MR images},
journal = {Medical Image Analysis},
volume = {49},
pages = {60-75},
year = {2018},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2018.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S136184151830553X},
author = {Heran Yang and Jian Sun and Huibin Li and Lisheng Wang and Zongben Xu},
keywords = {Multi-atlas label fusion, Left ventricle segmentation, Deep fusion net, Atlas selection},
abstract = {Multi-atlas segmentation approach is one of the most widely-used image segmentation techniques in biomedical applications. There are two major challenges in this category of methods, i.e., atlas selection and label fusion. In this paper, we propose a novel multi-atlas segmentation method that formulates multi-atlas segmentation in a deep learning framework for better solving these challenges. The proposed method, dubbed deep fusion net (DFN), is a deep architecture that integrates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. The network parameters are learned by end-to-end training for automatically learning deep features that enable optimal performance in a NL-PLF framework. The learned deep features are further utilized in defining a similarity measure for atlas selection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09 for left ventricle segmentation, our approach achieved 0.833 in averaged Dice metric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on LV-09 dataset, comparing favorably with the other automatic left ventricle segmentation methods. We also tested our approach on Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method achieved 0.815 in ADM, ranking highest at the time of writing.}
}
@article{WANG2023106661,
title = {Multimodal registration of ultrasound and MR images using weighted self-similarity structure vector},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106661},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106661},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523001269},
author = {Yifan Wang and Tianyu Fu and Chan Wu and Jian Xiao and Jingfan Fan and Hong Song and Ping Liang and Jian Yang},
keywords = {Self-similarity structure, Texture weight, Multimodal image registration, Ultrasound, Liver},
abstract = {Propose
Multimodal registration of 2D Ultrasound (US) and 3D Magnetic Resonance (MR) for fusion navigation can improve the intraoperative detection accuracy of lesion. However, multimodal registration remains a challenge because of the poor US image quality. In the study, a weighted self-similarity structure vector (WSSV) is proposed to registrate multimodal images.
Method
The self-similarity structure vector utilizes the normalized distance of symmetrically located patches in the neighborhood to describe the local structure information. The texture weights are extracted using the local standard deviation to reduce the speckle interference in the US images. The multimodal similarity metric is constructed by combining a self-similarity structure vector with a texture weight map.
Results
Experiments were performed on US and MR images of the liver from 88 groups of data including 8 patients and 80 simulated samples. The average target registration error was reduced from 14.91 ± 3.86 mm to 4.95 ± 2.23 mm using the WSSV-based method.
Conclusions
The experimental results show that the WSSV-based registration method could robustly align the US and MR images of the liver. With further acceleration, the registration framework can be potentially applied in time-sensitive clinical settings, such as US-MR image registration in image-guided surgery.}
}
@article{ABBAS2024100065,
title = {Task difficulty impact on multitasking in mixed reality environments},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100065},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000151},
author = {Safanah Abbas and Heejin Jeong},
keywords = {Task difficulty, Digital-physical hybrid, Hand use, Priority, Multitasking, Learning environment},
abstract = {Mixed reality applications allow users to experience both physical and digital worlds simultaneously. However, limited research has compared multitasking performance in these two worlds with different difficulty levels. This study investigated the effect of task difficulty on mixed-reality multitasking performance. A block-matching task was used for the physical task experiment, and the N-back test was used for the virtual task experiment. Thirty-six participants completed eight experimental conditions and were tested on four measures: NASA-TLX, accuracy, priority, and hand use. Physical demand and effort subscales were significant in all the experimental manipulations: real-world tasks, virtual-world tasks, and multitasking. The priority variable was not found to be substantial. Most participants prioritized working on either real-world tasks or multitasking, leaving the virtual-world tasks to the least preferred. Such a study can empower instructional designers to tailor tasks that align with learners’ cognitive abilities for an optimal learning experience.}
}
@article{HU2023102574,
title = {AR-based deep learning for real-time inspection of cable brackets in aircraft},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102574},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102574},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000509},
author = {Jingyu Hu and Gang Zhao and Wenlei Xiao and Rupeng Li},
keywords = {Augmented reality, Deep learning, Aeronautics intelligent manufacturing, Assembly inspection, CAD model checking},
abstract = {In the process of aircraft assembly, there exist numerous and ubiquitous cable brackets that shall be installed on frames and subsequently need to be manually verified with CAD models. Such a task is usually performed by special operators, hence is time-consuming, labor-intensive, and error-prone. In order to save the inspection time and increase the reliability of results, many researchers attempt to develop intelligent inspection systems using robotic, AR, or AI technologies. However, there is no comprehensive method to achieve enough portability, intelligence, efficiency, and accuracy while providing intuitive task assistance for inspectors in real time. In this paper, a combined AR+AI system is introduced to assist brackets inspection in a more intelligent yet efficient manner. Especially, AR-based Mask R-CNN is proposed by skillfully integrating markerless AR into deep learning-based instance segmentation to generate more accurate and fewer region proposals, and thus alleviates the computation load of the deep learning program. Based on this, brackets segmentation can be performed robustly and efficiently on mobile devices such as smartphones or tablets. By using the proposed system, CAD model checking can be automatically performed between the segmented physical brackets and the corresponding virtual brackets rendered by AR in real time. Furthermore, the inspection results can be directly projected on the corresponding physical brackets for the convenience of maintenance. To verify the feasibility of the proposed method, experiments are carried out on a full-scale mock-up of C919 aircraft main landing gear cabin. The experimental results indicate that the inspection accuracy is up to 97.1%. Finally, the system has been deployed in the real C919 aircraft final-assembly workshop. The preliminary evaluation reveals that the proposed real-time AR-assisted intelligent inspection approach is effective and promising for large-scale industrial applications.}
}
@article{KWOK2019711,
title = {Crisis management training using discrete-event simulation and virtual reality techniques},
journal = {Computers & Industrial Engineering},
volume = {135},
pages = {711-722},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219303651},
author = {Pak Ki Kwok and Mian Yan and Bill K.P. Chan and Henry Y.K. Lau},
keywords = {Collaborative simulation-based training, Crisis management, Discrete-event simulation, Simulation application in industry, Virtual reality, Internet of things},
abstract = {Conducting emergency drills in an actual environment is workforce and resources intensive. Hence, organisations often hesitate to conduct emergency exercises frequently. Because of the limited number of opportunities to conduct drills in a year, the content of the emergency drills can only focus on common cases and exclude rare cases. This constraint also restricts the members of crisis response teams from exploring and verifying new methods for tackling a crisis. Therefore, this research uses information and communication technology (ICT), virtual reality (VR) and discrete-event simulation (DES) technologies to develop a hazard simulation system with the capability to recreate large scale and multi-agency emergency incidents that would be otherwise too costly, complex and dangerous to reproduce in the actual system. With this system, organisations can conduct emergency drills inside a virtual world having a close correspondence with their real physical apparition. This training method is called virtual collaborative simulation-based training (VCST). Two laboratory-based studies were conducted to examine user perceptions about the VCST method. A total of 60 university students majored in managerial-related subjects were enrolled, and the results showed that the proposed method appears to be a feasible approach for practising crisis management training. Further research is needed to verify the findings in larger samples and different populations.}
}
@article{LI2024102264,
title = {Efficient evaluation of misalignment between real and virtual objects for HMD-Based AR assembly assistance system},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102264},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102264},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003920},
author = {Ting-Hao Li and Hiromasa Suzuki and Yutaka Ohtake and Tatsuya Yatagawa and Shinji Matsuda},
keywords = {Augmented reality, Assembly assistance, Head-mounted display},
abstract = {Augmented reality (AR) technology is a promising tool for field service applications such as assembly. AR can show computer-aided design (CAD) models of assembly parts at installation locations. This provides a means for delivering assembly instructions to an operator. Furthermore, evaluating the assembly process can be beneficial for preventing assembly errors. Field service engineers who perform assembly operations may choose to use a head-mounted display (HMD) for its good portability and hands-free operation. In this study, we propose a method for efficiently evaluating the misalignment between real and virtual objects, which is a mismatch between the two objects’ poses, in an HMD-based AR assembly assistance system. By using a depth camera in the HMD, we evaluate the misalignment by comparing the depth maps of the real and virtual worlds. With our preliminary prototype, the system can evaluate misalignment with an accuracy of ±1 cm at a rate of 30 fps in real time. Moreover, we also propose a coordinate calibration method and establish a basic HMD-based AR assembly assistance system to demonstrate the methods. The efficient evaluation of misalignment could help to monitor operations to prevent assembly errors and decrease operation time.}
}
@article{OSTI20171481,
title = {Semi-automatic Design for Disassembly Strategy Planning: An Augmented Reality Approach},
journal = {Procedia Manufacturing},
volume = {11},
pages = {1481-1488},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.279},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917304870},
author = {Francesco Osti and Alessandro Ceruti and Alfredo Liverani and Gianni Caligiana},
keywords = {Design for Disassembly, Augmented Reality, CAD, Disassembly Sequence Optimization},
abstract = {The mounting attention to environmental issues requires adopting better disassembly procedures at the product's End of Life. Planning and reckoning different disassembly strategies in the early stage of the design process can improve the development of sustainable products with an easy dismissing and recycling oriented approach. Nowadays many Computer Aided Process Planning software packages provide optimized assembly or disassembly sequences, but they are mainly based on a time and cost compression approach, neglecting the human factor. The environment we developed is based upon the integration of a CAD, an Augmented Reality tool, a Leap Motion Controller device, see-through glasses and an algorithm for disassembly strategies evaluation: this approach guarantees a more effective interaction with the 3D real and virtual assembly than an approach relying only on a CAD based disassembly sequence planning. In such a way, the operator may not test in a more natural and intuitive way automatic disassembly sequences, but he/she can also propose different strategies to improve the ergonomics. The methodology has been tested in a real case study to evaluate the strength points and criticalities of this approach.}
}
@article{YANG2023317,
title = {Design of a new kind of chemical experiment container with virtual reality fusion},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {4},
pages = {317-337},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000948},
author = {Lurong Yang and Zhiquan Feng and Junhong Meng},
keywords = {Multichannel intent fusion understanding, Virtual-real fusion, Natural interaction, Remove image highlights, Nonlinear curve fitting},
abstract = {Background
At present, the teaching of experiments in primary and secondary schools is affected by cost and security factors. Existing research on virtual experiment platforms has alleviated these problems. However, the lack of real experimental equipment and use of a single channel to understand user intentions weaken these platforms operationally and degrade the naturalness of interactions.
Methods
To solve these problems, we propose an intelligent experimental container structure and a situational awareness algorithm, both of which are verified and applied to a chemical experiment involving virtual-real fusion. First, the acquired images are denoised in the visual channel using the maximum diffuse reflection chroma to remove overexposure. Second, container situational awareness is realized by segmenting the image liquid level and establishing a relation-fitting model. Then, strategies for constructing complete behaviors and making priority comparisons among behaviors are adopted for information complementarity and independence, respectively. A multichannel intentional understanding model and an interactive paradigm that integrates vision, hearing, and touch are proposed.
Results
The experimental results show that the accuracy of the intelligent container situation awareness proposed in this paper reaches 99%, and the accuracy of the proposed intention understanding algorithm reaches 94.7%. The test shows that the intelligent experimental system based on the new interaction paradigm also has better performance and a more realistic sense of operation experience in terms of experimental efficiency.
Conclusion
The results indicate that the proposed experimental container and algorithm can achieve a natural level of human-computer interaction in a virtual chemical experiment platform, enhance the user′s sense of operation, and achieve high levels of user satisfaction.}
}
@article{NEGRAO2024103936,
title = {Characterizing head-gaze and hand affordances using AR for laparoscopy},
journal = {Computers & Graphics},
pages = {103936},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103936},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000712},
author = {Matheus D. Negrão and Anderson Maciel},
keywords = {Augmented reality, Head-gaze interaction, Laparoscopy interface},
abstract = {Laparoscopic surgery techniques are complex and impose postural and communication constraints on the surgeon that may affect surgery outcomes. This paper explores the possibilities of designing intraoperative AR interfaces for laparoscopy surgery. We suggest that the laparoscopic video be displayed on an AR headset and that surgeons consult preoperative image data on that display. Interaction with these elements is necessary. Thus, we propose a head-gaze and hand clicker approach that is effective and minimalist, as well as the implementation of a prototype. We conduct a user study to evaluate the prototype and to comprehend the impact and improvements that headgaze average filtering and the scale method can bring to perform annotations in the laparoscopy video feed on a virtual monitor positioned straight in front of the user. The user experiment was performed in a between-subject protocol with 32 volunteers from the Institute of Informatics, and the proposed task involves communication in the interface through drawing annotations with proposed interaction approaches: A hand device for input confirmations and the head-gaze stabilization methods to pointing and selection. The study found that the users were confident about their performance and demonstrated low physical and temporal demand. The proposed head-gaze methods showed that independent of the stabilization applied, the difference between the error sensibility of the axis of the head-gaze in annotation positioning is significant. The vertical axis presented a higher error rate than the horizontal axis. When we compared other variables, we found some differences in specific circumstances, but overall, the interaction with HL1 is very distributed.}
}
@article{MUNOZ202024,
title = {Camera 3D positioning mixed reality-based interface to improve worker safety, ergonomics and productivity},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {28},
pages = {24-37},
year = {2020},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1755581720300055},
author = {Adolfo Muñoz and Ana Martí and Xavier Mahiques and Luis Gracia and J. Ernesto Solanes and Josep Tornero},
keywords = {Camera 3D positioning, Camera calibration, Mixed reality interface, Assisted maintenance, Quality control},
abstract = {This research develops a new mixed reality-based worker interface for industrial camera 3D positioning, which is intuitive and easy to manage, in order to enhance the worker safety, ergonomics and productivity. An experimental prototype to be used in the car body quality control is developed in the paper. The benefits and drawbacks of the proposed interface are discussed along the paper and sustained through several usability tests conducted with users familiar and not-familiar with mixed reality devices. Furthermore, the feasibility of the proposed approach is demonstrated by tests made in an industrial environment with skilled workers from Alfatec Sistemas company.}
}
@article{FIGEYS2023105235,
title = {Challenges and promises of mixed-reality interventions in acquired brain injury rehabilitation: A scoping review},
journal = {International Journal of Medical Informatics},
volume = {179},
pages = {105235},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105235},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623002538},
author = {Mathieu Figeys and Farnaz Koubasi and Doyeon Hwang and Allison Hunder and Antonio Miguel-Cruz and Adriana {Ríos Rincón}},
keywords = {Augmented reality, Mixed reality, Extended reality, Acquired brain injury, Stroke, Rehabilitation},
abstract = {Background
Acquired brain injury (ABI) can lead to significant impairments and difficulties in everyday life, necessitating the need for rehabilitation. Mixed-reality (MR) technologies have revolutionized the delivery of neurorehabilitation therapies. However, inconsistencies in research methodology, diverse study populations and designs, and exaggerated claims in the research, media, and private consumer sectors have impacted the knowledge base of the field, including within the context of ABI rehabilitation.
Objective
This scoping review aims to explore MR-systems in ABI rehabilitation, while assessing the evidence base and technology readiness levels of these systems.
Methods
Seven databases were searched for studies, which were screened and analyzed by two independent raters. The types of MR systems, levels of evidence, and technology readiness levels were extracted and analyzed using descriptive analyses.
Results
Twenty-six studies were included in the review, all of which focused on ABI etiologies stemming from strokes. Across studies, upper-limb motor rehabilitation was the most common rehabilitation target of MR interventions, followed by gait, cognition, and lower-extremity functioning. At present, overall results indicate low evidence for MR-applications in ABI rehabilitation, with a median technology readiness level of 6, corresponding to system prototypes being tested in relevant environments.
Conclusion
Although challenges regarding system usability and design were reported, results appear promising with ongoing research. With variability across studies, technologies, and populations, determining the effectiveness of MR interventions in ABI remains a challenge, necessitating the need for ongoing innovation, research, and development of these systems.}
}
@article{DINIS2020101287,
title = {Improving project communication in the architecture, engineering and construction industry: Coupling virtual reality and laser scanning},
journal = {Journal of Building Engineering},
volume = {30},
pages = {101287},
year = {2020},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219315347},
author = {Fábio Matoseiro Dinis and Luís Sanhudo and João Poças Martins and Nuno M.M. Ramos},
keywords = {Virtual reality, Laser scanning surveying, Building information modelling, Building retrofitting, Project communication},
abstract = {In recent years, the demand for accurate, clear and easy-understandable information has been steadily rising within the Architecture, Engineering and Construction (AEC) industry's stakeholders. Despite this, a sizeable portion of this industry still considers the traditional approach to Construction Engineering, disregarding major innovations and technologic advances. This not only poses a great obstacle to proper communication between project-related entities but also presents a significant challenge for retrofitting projects. This article proposes a workflow for the improvement of communication in construction projects, in particular between professionals who lack specific BIM skills. Thus, by coupling laser scanning and Virtual Reality (VR), within a Building Information Modelling (BIM) work environment, this workflow comprises the entire process from on-site geometric data acquisition, through data treatment and analysis, culminating with the point-cloud importation into a game engine and the development of navigation and interaction tools within the VR environment. The framework is validated through its application to a proof of concept, from which conclusions regarding the workflow success, limitations, optimization, among other topics are discussed.}
}
@article{YU2019206,
title = {An evaluation for VR glasses system user experience: The influence factors of interactive operation and motion sickness},
journal = {Applied Ergonomics},
volume = {74},
pages = {206-213},
year = {2019},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2018.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0003687018302850},
author = {Mengli Yu and Ronggang Zhou and Huiwen Wang and Weihua Zhao},
keywords = {VR glasses system, User experience evaluation, Interactive operation, Motion sickness},
abstract = {As a new type of Virtual Reality (VR) headset, VR glasses rise rapidly in a number of areas. It's essential to understand the importance of user experience (UX) on VR glasses design. This study aimed to develop questionnaires for evaluating VR glasses' UX, as well as to investigate the relationship between various UX variables. With using lab-based usability tests, this study analyzed participants' self-reports and performance based on testing eight VR glasses and seven mobile applications. A nine-item questionnaire and a ten-item questionnaire were successfully developed to measure VR glasses systems' UX quality in terms of hardware and application, respectively. Within a proposed UX evaluation framework, the perceived UX quality relative to VR glasses hardware emerged as a core predictor in predicting interactive operation performance, whereas the application UX perception was a significant predictor of motion sickness.}
}
@article{KIM2024104319,
title = {Development of a virtual reality system usability questionnaire (VRSUQ)},
journal = {Applied Ergonomics},
volume = {119},
pages = {104319},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2024.104319},
url = {https://www.sciencedirect.com/science/article/pii/S0003687024000966},
author = {Yong Min Kim and Ilsun Rhiu},
keywords = {Virtual reality, Usability evaluation, Perceived usability, Human-computer interaction},
abstract = {Virtual reality (VR) has gained significant attention as a technology that provides immersive experiences similar to the real world. In order for a VR system to be accepted, usability needs to be guaranteed. Accordingly, VR-related researchers are continuing their efforts to improve VR systems by conducting usability evaluations. However, existing studies have limitations in that they cannot comprehensively evaluate the detailed properties of VR systems by using questionnaires developed for general product usability evaluation or focusing only on some usability aspects of VR systems. This suggests it may be difficult to fully capture usability issues in a VR system, and that it is necessary to develop a usability evaluation tool that reflects the specific characteristics of the VR system. Therefore, this study develops and proposes the Virtual Reality System Usability Questionnaire (VRSUQ). In the development of the questionnaire, items were structured based on a literature review and discussions with experts. To account for the diverse characteristics of VR systems, the validity of the questionnaire was verified through both exploratory and confirmatory factor analysis, utilizing data obtained from three distinct experimental studies that employed different VR systems. In addition, by comparing the results of VRSUQ with the results from the System Usability Scale, which is widely used for perceived usability evaluation, alternative possibilities for using VRSUQ are presented. Further testing on various VR platforms is needed to ensure the reliability and validity of VRSUQ, and as results from using VRSUQ are accumulated, it is expected to be widely used as a more powerful and robust VR-specific perceived usability evaluation tool.}
}
@article{BOONBRAHM2020337,
title = {The Use of Marker-Based Augmented Reality in Space Measurement},
journal = {Procedia Manufacturing},
volume = {42},
pages = {337-343},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.081},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306466},
author = {Salin Boonbrahm and Poonpong Boonbrahm and Charlee Kaewrat},
keywords = {augmented reality, marker-based AR, space measurement},
abstract = {There are many applications on using AR in supporting manufacturing and related areas ranging from designing, simulation, testing, repairing, and maintenance. The objective of this research is to find the algorithm for measuring the size and find the shape of the room for facility management using marker-based AR technology. The applications program was designed and developed using the Unity 3D game engine and OpenCV library. It was designed to be used by many devices running on various operating systems such as Windows, OSX, iOS, and Android. Test of the concept was done in three situations, i.e., three rooms with different sizes. The first room is the smallest one with the size = 25.245 m3. The second room is the biggest one with the size of 117.35 m3 and the third one with the size = 56.1 m3. The results calculating from the distances of the markers show that the sizes of the space obtaining from the application are different from the manual measure less than 3%, which means that the system can provide high accuracy in measurement.}
}
@article{HERATH2021113202,
title = {An Augmented Reality tourniquet tightening trainer for peripheral venepuncture},
journal = {Sensors and Actuators A: Physical},
volume = {332},
pages = {113202},
year = {2021},
issn = {0924-4247},
doi = {https://doi.org/10.1016/j.sna.2021.113202},
url = {https://www.sciencedirect.com/science/article/pii/S0924424721006658},
author = {Sadeepa Herath and Chin Doong Yau and Peck Chin Hoh and Oi Wah Liew and Tuck Wah Ng},
keywords = {Augmented Reality, tourniquet, venepuncture, trainer, haptic, kinesthetic},
abstract = {Appropriate tightening of a tourniquet at 5–10 cm above a venepuncture site assists in inducing stasis and identifying veins for blood draw in clinical settings. Here, a setup that physically simulates a section of the human arm has a force sensor built in to send its signals wirelessly to an Augmented Reality (AR) tool for data interpretation. The tool presents a computer-generated rendition of a patient to the user via a tablet and facilitates user guidance to achieve the desired tourniquet force applied. System tests indicated low levels of signal noise and drift. Reference force values and range were obtained by testing the performance of 3 healthcare participants with deep experience in venepuncture procedures. These values were incorporated into the AR app in the training mode to provide visual guidance of target performance. Tests with 5 non-healthcare participants revealed an ability to raise average force readings by 2 times to be within 6% of the values attained by the healthcare participants after training with the AR tool. The AR tool offers opportunities for participants to train and assess themselves more frequently. When incorporated into phlebotomy training programs, this AR tool holds promise in facilitating rapid proficiency acquisition and performance consistency when applying tourniquet for venepuncture procedures.}
}
@article{INCETAN2021101990,
title = {VR-Caps: A Virtual Environment for Capsule Endoscopy},
journal = {Medical Image Analysis},
volume = {70},
pages = {101990},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.101990},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000360},
author = {Kağan İncetan and Ibrahim Omer Celik and Abdulhamid Obeid and Guliz Irem Gokceler and Kutsev Bengisu Ozyoruk and Yasin Almalioglu and Richard J. Chen and Faisal Mahmood and Hunter Gilbert and Nicholas J. Durr and Mehmet Turan},
keywords = {Capsule endoscopy, Deep reinforcement learning, Area coverage, Disease classification, Synthetic data generation},
abstract = {Current capsule endoscopes and next-generation robotic capsules for diagnosis and treatment of gastrointestinal diseases are complex cyber-physical platforms that must orchestrate complex software and hardware functions. The desired tasks for these systems include visual localization, depth estimation, 3D mapping, disease detection and segmentation, automated navigation, active control, path realization and optional therapeutic modules such as targeted drug delivery and biopsy sampling. Data-driven algorithms promise to enable many advanced functionalities for capsule endoscopes, but real-world data is challenging to obtain. Physically-realistic simulations providing synthetic data have emerged as a solution to the development of data-driven algorithms. In this work, we present a comprehensive simulation platform for capsule endoscopy operations and introduce VR-Caps, a virtual active capsule environment that simulates a range of normal and abnormal tissue conditions (e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope designs (e.g., mono, stereo, dual and 360∘ camera), and the type, number, strength, and placement of internal and external magnetic sources that enable active locomotion. VR-Caps makes it possible to both independently or jointly develop, optimize, and test medical imaging and analysis software for the current and next-generation endoscopic capsule systems. To validate this approach, we train state-of-the-art deep neural networks to accomplish various medical image analysis tasks using simulated data from VR-Caps and evaluate the performance of these models on real medical data. Results demonstrate the usefulness and effectiveness of the proposed virtual platform in developing algorithms that quantify fractional coverage, camera trajectory, 3D map reconstruction, and disease classification. All of the code, pre-trained weights and created 3D organ models of the virtual environment with detailed instructions how to setup and use the environment are made publicly available at https://github.com/CapsuleEndoscope/VirtualCapsuleEndoscopy and a video demonstration can be seen in the supplementary videos (Video-I).}
}
@article{ZHANG2021101432,
title = {Enhancing human indoor cognitive map development and wayfinding performance with immersive augmented reality-based navigation systems},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101432},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101432},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001841},
author = {Jinyue Zhang and Xiaolu Xia and Ruiqi Liu and Nan Li},
keywords = {Immersive augmented reality (IAR), Navigation, Wayfinding, Cognitive map, Workload},
abstract = {Augmented reality (AR) is an interactive experience where computer-generated perceptual information is superimposed into the real-world environment. Most existing research in AR-based wayfinding has focused on the technological aspects of developing AR-based software or devices to realize navigation. No previous investigations have focused on understanding the impact of immersive augmented reality (IAR)–based systems on human wayfinding performance from the cognitive perspective. Aimed at investigating the influence of IAR-based systems on people’s cognitive map development and their subsequent wayfinding performance as well as the effect of using three-dimensional (3D) layout models in IAR environments in addition to superimposed guideposts, an experiment was carried out in a building with a complex floor plan. A total of 54 university students were evenly divided into three groups: a control group with no IAR assistance, a second group using an IAR-based navigation system that includes only superimposed guideposts, and a third group using an IAR-based navigation system that includes both guideposts and a 3D layout model. Each participant was asked to conduct a spatial exploration task in the environment, sketch a floor map based on their spatial cognition, and perform a wayfinding task to find eight specific locations in the building. An analysis of the participants’ performance and responses to a number of self-evaluation questionnaires collected in the experiment indicates that IAR technology can help people develop their cognitive maps more effectively and can substantially improve their wayfinding performance with a much lower workload. A second finding is that adding a 3D layout model can enhance the effect of an IAR-based navigation system in terms of cognitive map development. The findings from this research extend the existing knowledge about IAR-based navigation and further verify that AR technology has the potential to reduce human workload for cognitive tasks. The results also could support its more effective application in various scenarios that require assisted wayfinding and cognitive map training, such as emergency evacuation drills.}
}
@article{SIDANI2021102500,
title = {Recent tools and techniques of BIM-Based Augmented Reality: A systematic review},
journal = {Journal of Building Engineering},
volume = {42},
pages = {102500},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.102500},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221003570},
author = {Adeeb Sidani and Fábio {Matoseiro Dinis} and Joana Duarte and Luís Sanhudo and Diego Calvetti and João {Santos Baptista} and João {Poças Martins} and Alfredo Soeiro},
keywords = {Construction, Augmented reality (AR), Building information modelling (BIM), Usability assessment},
abstract = {The Architecture, Engineering, Construction, and Operations (AECO) sector has always been recognised as a competitive and complex sector. In recent years, increased demands on construction projects in different domains such as safety, energy, time and cost management have pushed the industry towards new tools and methods, including more efficient use of digital technologies. Among the multiple available digital solutions, Building Information Modelling (BIM) is quickly positioning itself as a fundamental methodology, with its practices and tools being increasingly deployed. However, the remaining challenges to BIM adoption provide an opportunity for the development of supporting tools. The present article analyses Augmented Reality (AR) as such a tool. A systematic review was conducted to examine previous studies in the field of BIM-based AR, shedding light on the integration of this technology in the AECO sector. A list of questions was presented and answered to understand this technology's role in multiple aspects, from its main stage and field of application to the targeted stakeholders. The applied review methodology is based on PRISMA-P. The databases exploited to search for eligible articles were Scopus, Academic Search Ultimate, Current Content, Web of Science, and ScienceDirect. From an initial cohort of 671 articles, 24 were selected for analysis. Additionally, a comparison between AR and Virtual Reality (VR) applications is presented. This review shows that AR implementation is far from its desired state, showing several limitations such as connection and localisation problems, lack of non-geometric information, and other challenges in using AR techniques in the construction site, (e.g., marker-based AR).}
}
@article{ELAMMARI2019102940,
title = {Remote interactive collaboration in facilities management using BIM-based mixed reality},
journal = {Automation in Construction},
volume = {107},
pages = {102940},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102940},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300500},
author = {Khaled {El Ammari} and Amin Hammad},
keywords = {Facilities management, Mixed reality, Feature-based tracking, Building information modeling, Remote interactive visual collaboration},
abstract = {Facilities Management (FM) day-to-day tasks require suitable methods to facilitate work orders and improve performance by better collaboration between the office and the field. Building Information Modeling (BIM) provides opportunities to support collaboration and to improve the efficiency of Computerized Maintenance Management Systems (CMMSs) by sharing building information between different applications/users throughout the lifecycle of the facility. However, manual retrieval of building element information can be challenging and time consuming for field workers during FM operations. Mixed Reality (MR) is a visualization technique that can be used to improve visual perception of the facility by superimposing 3D virtual objects and textual information on top of the view of real-world building objects. This paper discusses the development of a collaborative BIM-based MR approach to support facilities field tasks. The proposed framework integrates multisource facilities information, BIM models, and feature-based tracking in an MR-based setting to retrieve information based on time (e.g. inspection schedule) and the location of the field worker, visualize inspection and maintenance operations, and support remote collaboration and visual communication between the field worker and the manager at the office. The field worker uses an Augmented Reality (AR) application installed on his/her tablet. The manager at the office uses an Immersive Augmented Virtuality (IAV) application installed on a desktop computer. Based on the field worker location, as well as the inspection or maintenance schedule, the field worker is given work orders and instructions from the office. Other sensory data (e.g., infrared thermography) can provide additional layers of information by augmenting the actual view of the field worker and supporting him/her in making effective decisions about existing and potential problems while communicating with the office in an Interactive Virtual Collaboration (IVC) mode. Finally, to investigate the applicability of the proposed method, a prototype system is developed and tested in a case study.}
}
@article{RODRIGUEZ2015327,
title = {Developing a Mixed Reality Assistance System Based on Projection Mapping Technology for Manual Operations at Assembly Workstations},
journal = {Procedia Computer Science},
volume = {75},
pages = {327-333},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.254},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037151},
author = {Leonardo Rodriguez and Fabian Quint and Dominic Gorecky and David Romero and Héctor R. Siller},
keywords = {Virtual Reality, Augument Reality, Mixed Reality, Projection Mapping, Manual Assembly, Assistance.},
abstract = {Manual tasks play an important role in social sustainable manufacturing enterprises. Commonly, manual operations are used for low volume productions, but are not limited to. Operational models in manufacturing systems based on “x-to-order” paradigms (e.g. assembly-to-order) may require manual operations to speed-up the ramp-up time of new product configuration assemblies. The implications of manual operations in any production line may imply that any manufacturing or assembly process become more susceptible to human errors and therefore translate into delays, defects and/or poor product quality. In this scenario, virtual and augmented realities can offer significant advantages to support the human operator in manual operations. This research work presents the development of a mixed (virtual and augmented) reality assistance system that permits real-time support in manual operations. A review of mixed reality techniques and technologies was conducted, where it was determined to use a projection mapping solution for the proposed assistance system. According to the specific requirements of the demonstration environment, hardware and software components were chosen. The developed mixed reality assistance system was able to guide any user without any prior knowledge through the successful completion of the specific assembly task.}
}
@article{ENKLER202483,
title = {Towards engineering a portable platform for laparoscopic pre-training in virtual reality with haptic feedback},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {83-99},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S209657962300075X},
author = {Hans-Georg Enkler and Wolfgang Kunert and Stefan Pfeffer and Kai-Jonas Bock and Steffen Axt and Jonas Johannink and Christoph Reich},
keywords = {Laparoscopic surgery, Training, Virtual reality, Controller, Haptic feedback, Kinesthetic skills},
abstract = {Background
Laparoscopic surgery is a surgical technique in which special instruments are inserted through small incision holes inside the body. For some time, efforts have been made to improve surgical pre-training through practical exercises on abstracted and reduced models.
Methods
The authors strive for a portable, easy to use and cost-effective Virtual Reality-based (VR) laparoscopic pre-training platform and therefore address the question of how such a system has to be designed to achieve the quality of today's gold standard using real tissue specimens. Current VR controllers are limited regarding haptic feedback. Since haptic feedback is necessary or at least beneficial for laparoscopic surgery training, the platform to be developed consists of a newly designed prototype laparoscopic VR controller with haptic feedback, a commercially available head-mounted display, a VR environment for simulating a laparoscopic surgery, and a training concept.
Results
To take full advantage of benefits such as repeatability and cost-effectiveness of VR-based training, the system shall not require a tissue sample for haptic feedback. It is currently calculated and visually displayed to the user in the VR environment. On the prototype controller, a first axis was provided with perceptible feedback for test purposes. Two of the prototype VR controllers can be combined to simulate a typical both-handed use case, e.g., laparoscopic suturing. A Unity-based VR prototype allows the execution of simple standard pre-trainings.
Conclusions
The first prototype enables full operation of a virtual laparoscopic instrument in VR. In addition, the simulation can compute simple interaction forces. Major challenges lie in a realistic real-time tissue simulation and calculation of forces for the haptic feedback. Mechanical weaknesses were identified in the first hardware prototype, which will be improved in subsequent versions. All degrees of freedom of the controller are to be provided with haptic feedback. To make forces tangible in the simulation, characteristic values need to be determined using real tissue samples. The system has yet to be validated by cross-comparing real and VR haptics with surgeons.}
}
@article{KARDOULAKI201548,
title = {Thin-film micro-coil detectors: Application in MR-thermometry},
journal = {Sensors and Actuators A: Physical},
volume = {226},
pages = {48-58},
year = {2015},
issn = {0924-4247},
doi = {https://doi.org/10.1016/j.sna.2015.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0924424715000874},
author = {Evdokia M. Kardoulaki and Richard R.A. Syms and Ian R. Young and Marc Rea and Wladyslaw M.W. Gedroyc},
keywords = {Micro-coils, MR-thermometry, Laser ablation, LITT},
abstract = {Thin-film micro-coils are integrated with commercially available ablation catheters, for MR-thermometry during laser interstitial thermal therapies (LITTs). The coils are formed on a flexible polyimide substrate and consist of a two-turn electroplated copper inductor and integrated parallel plate capacitors for tuning and matching. Their performance was assessed during Nd:YAG laser ablations in a static phantom study carried out in a 3T clinical scanner. Further moving phantom studies were performed to calculate errors due to motion. The temperature accuracy is improved by 1.5–10 times in a radius matching the dimensions of the lesions typically treated. Resolution of 1mm can be maintained during motion by using short acquisition time sequences while the SNR remains sufficient for accurate MR-thermometry. The temperature error on a moving un-heated phantom under respiratory gating does not exceed 1°C. This demonstration suggests a possible improvement of the overall control of MR-guided LITTs by local temperature monitoring.}
}
@article{KLEPACZKO2016293,
title = {Simulation of MR angiography imaging for validation of cerebral arteries segmentation algorithms},
journal = {Computer Methods and Programs in Biomedicine},
volume = {137},
pages = {293-309},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304016},
author = {Artur Klepaczko and Piotr Szczypiński and Andreas Deistung and Jürgen R. Reichenbach and Andrzej Materka},
keywords = {MR angiography, Vessel segmentation, Cerebral vasculature modeling, MRI simulation, Quantitative validation},
abstract = {Background and objective
Accurate vessel segmentation of magnetic resonance angiography (MRA) images is essential for computer-aided diagnosis of cerebrovascular diseases such as stenosis or aneurysm. The ability of a segmentation algorithm to correctly reproduce the geometry of the arterial system should be expressed quantitatively and observer-independently to ensure objectivism of the evaluation.
Methods
This paper introduces a methodology for validating vessel segmentation algorithms using a custom-designed MRA simulation framework. For this purpose, a realistic reference model of an intracranial arterial tree was developed based on a real Time-of-Flight (TOF) MRA data set. With this specific geometry blood flow was simulated and a series of TOF images was synthesized using various acquisition protocol parameters and signal-to-noise ratios. The synthesized arterial tree was then reconstructed using a level-set segmentation algorithm available in the Vascular Modeling Toolkit (VMTK). Moreover, to present versatile application of the proposed methodology, validation was also performed for two alternative techniques: a multi-scale vessel enhancement filter and the Chan–Vese variant of the level-set-based approach, as implemented in the Insight Segmentation and Registration Toolkit (ITK). The segmentation results were compared against the reference model.
Results
The accuracy in determining the vessels centerline courses was very high for each tested segmentation algorithm (mean error rate = 5.6% if using VMTK). However, the estimated radii exhibited deviations from ground truth values with mean error rates ranging from 7% up to 79%, depending on the vessel size, image acquisition and segmentation method.
Conclusions
We demonstrated the practical application of the designed MRA simulator as a reliable tool for quantitative validation of MRA image processing algorithms that provides objective, reproducible results and is observer independent.}
}
@article{PAES2024105371,
title = {Optical see-through augmented reality fire safety training for building occupants},
journal = {Automation in Construction},
volume = {162},
pages = {105371},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105371},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001079},
author = {Daniel Paes and Zhenan Feng and Maddy King and Hesam {Khorrami Shad} and Prasanth Sasikumar and Diego Pujoni and Ruggiero Lovreglio},
keywords = {Augmented reality, Fire safety, Emergency evacuation, Training, Human-computer interaction, Human factors},
abstract = {Fire safety training is crucial to increase building occupants' chances of surviving a fire emergency. Traditional training methods, such as lectures using video presentations, have limitations that can hinder learning performance. This article describes the development and testing of an alternative training solution using Augmented Reality (AR) technology. Through a controlled between-subject experiment, the AR-based fire safety training method was compared against a conventional video-based one based on participants' knowledge acquisition and retention, intrinsic motivation, and self-efficacy. Results suggest that the AR system was well-designed and as effective as the traditional method in terms of knowledge acquisition and retention and overall learning experience. However, it was found to be superior in terms of intrinsic motivation gain and self-efficacy retention. These findings demonstrate the potential of AR-based training methods to enhance building occupants' safety and provide directions for future developments and research in the field.}
}
@article{ALAM2017109,
title = {Augmented and virtual reality based monitoring and safety system: A prototype IoT platform},
journal = {Journal of Network and Computer Applications},
volume = {89},
pages = {109-119},
year = {2017},
note = {Emerging Services for Internet of Things (IoT)},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517301315},
author = {Md Fasiul Alam and Serafeim Katsikas and Olga Beltramello and Stathes Hadjiefthymiades},
keywords = {Safety system/application, Mobile, Modular, AR/VR design, Maintenance, IoT, Prototype},
abstract = {This paper presents an Augmented and Virtual Reality (AR/VR) based IoT prototype system. Performing maintenance tasks in a complex environment is quite challenging and difficult due to complex, and possibly, underground facilities, uneasy access, human factors, heavy machineries, etc. Current technology is not acceptable because of significant delays in communication and data transmission, missing multi-input interfaces, and simultaneous supervision of multiple workers who are working in the extreme environment. The aim is to technically advance and combine several technologies and integrate them as integral part of a personnel safety system to improve safety, maintain availability, reduce errors and decrease the time needed for scheduled or ad hoc interventions. We emphasize on the aspects that were made “feasible” on the worker's side due to the equipment used (mobile computing equipment). We present that the demanding tasks that previously were simply undertaken on the fixed infrastructure are now possible on the mobile end. The research challenges lie in the development of real-time data-transmission, instantaneous analysis of data coming from different inputs, local intelligence in low power embedded systems, interaction with multiple on-site users, complex user interfaces, portability and wearability. This work is part EDUSAFE, a Marie Curie ITN (Initial Training Network) project focusing on research into the use of Augmented and Virtual Reality (AR/VR) during planned and ad hoc maintenance in extreme work environments.}
}
@article{KELLER2015186,
title = {Use of virtual reality for optimizing the life cycle of a fusion component},
journal = {Fusion Engineering and Design},
volume = {101},
pages = {186-191},
year = {2015},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0920379615302441},
author = {D. Keller and L. Doceul and F. Ferlay and C. Louison and A. Pilia and K. Pavy and L. Chodorge and C. Andriot},
keywords = {ITER, Virtual reality, Integration, Assembly, Maintenance, Safety},
abstract = {Efficient development of a complex system such as a fusion component needs a stringent integration of standard and new constraints. For example, compared to the previous fusion experimental devices, remote handling (RH) and safety requirements are in ITER key parameters which must be integrated since the earliest design. For optimizing such integration studies, CEA, IRFM decided in 2010 to implement the use of virtual reality (VR) tools during the life cycle (from design to operation) of a fusion component. This paper describes a first feedback of such use for fusion engineering purposes. After a short overview of the CEA, IRFM VR platform capabilities, three main uses will be described: design review, simulation of remote handling and hands-on operations, with man in the loop. The Design review mode was intensively used within the framework of a fruitful collaboration with ITER design Integration Team. This mode, fully compatible with CAD software, enables scale one data visualization with stereoscopic rendering. It improves the efficiency in detecting inconsistencies inside models and machine sub-system design optimization needs. Several accessibility cases of major Safety Important Components (SIC-1) were studied giving important requirements to the design at an early stage. CEA, IRFM, in close collaboration with expertise of CEA, LIST for VR simulation software, applies VR technologies for designing RH maintenance scenario for ITER Test Blanket System (TBS) and Ion cyclotron Resonance Heating (ICRH) Port Plugs. RH compatibility studies using VR pointed out major design drivers while helping to propose credible solution. VR platform is intensively used in the design of WEST (Tungsten (W) Environment Steady-state Tokamak) components and assembly studies, providing important information about the feasibility of assembly processes, optimization of physical mock-ups and ergonomic posture and gestures of operator. Finally, new perspectives, as the integration of safety constraints (dose calculation) will be described, demonstrating the powerful of VR tools at different stages of the component lifecycle.}
}
@article{NEKTARIOSBOLIERAKIS2023201,
title = {Training on LSA lifeboat operation using Mixed Reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {3},
pages = {201-212},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000128},
author = {Spyridon {Nektarios Bolierakis} and Margarita Kostovasili and Lazaros Karagiannidis and Angelos Amditis},
keywords = {Augmented Reality, Mixed Reality, Maritime training, Cruise industry, Lifeboat operation, Lifeboat maintenance, H2020 research project},
abstract = {Background
This work aims to provide an overview of the Mixed Reality (MR) technology's use in maritime industry for training purposes. Current training procedures cover a broad range of procedural operations for Life-Saving Appliances (LSA) lifeboats; however, several gaps and limitations have been identified related to the practical training that can be addressed through the use of MR. Augmented, Virtual and Mixed Reality applications are already used in various fields in maritime industry, but their full potential have not been yet exploited. SafePASS project aims to exploit MR advantages in the maritime training by introducing a relevant application focusing on use and maintenance of LSA lifeboats.
Methods
An MR Training application is proposed supporting the training of crew members in equipment usage and operation, as well as in maintenance activities and procedures. The application consists of the training tool that trains crew members on handling lifeboats, the training evaluation tool that allows trainers to assess the performance of trainees, and the maintenance tool that supports crew members to perform maintenance activities and procedures on lifeboats. For each tool, an indicative session and scenario workflow are implemented, along with the main supported interactions of the trainee with the equipment.
Results
The application has been tested and validated both in lab environment and using a real LSA lifeboat, resulting to improved experience for the users that provided feedback and recommendations for further development. The application has also been demonstrated onboard a cruise ship, showcasing the supported functionalities to relevant stakeholders that recognized the added value of the application and suggested potential future exploitation areas.
Conclusions
The MR Training application has been evaluated as very promising in providing a user-friendly training environment that can support crew members in LSA lifeboat operation and maintenance, while it is still subject to improvement and further expansion.}
}
@article{FERRAGUTI2019158,
title = {Augmented reality based approach for on-line quality assessment of polished surfaces},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {59},
pages = {158-167},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518305131},
author = {Federica Ferraguti and Fabio Pini and Thomas Gale and Franck Messmer and Chiara Storchi and Francesco Leali and Cesare Fantuzzi},
keywords = {Robotic polishing, Augmented reality, Industrial robotic solutions},
abstract = {Augmented reality is considered one of the enabling technologies of the fourth industrial revolution, within the Industry 4.0 program and beyond. Indeed, augmented reality solutions can increase the working quality and the productivity and allow a better use of the human resources. This technology can help the operator in the industrial applications during the crucial phases of the processes. Since the quality assessment of the surfaces is recognized to be a key phase in the polishing process, in this paper we propose a novel method that exploits augmented reality to support the operators during this phase. The metrology data measured by a surface measurement system are directly projected on the polished component through an augmented reality headset worn by the operators and used to assess the quality of the worked surfaces. Rather than imagine how a certain parameter change can affect the result achieved, the information is directly there on the component’s surface. Users can see from the data where refinements are required and make better and faster decisions, which is compelling for its potential beyond industrial polishing. The proposed method is implemented and validated on an industrial cell, where the robot automatically perform the polishing task and move the head of the surface measurement system along the surface to measure the metrology parameters. Thanks to the proposed approach, the end-user and the operator can directly see on the component if the quality reached satisfies the specifications or if some parts of the surface require further refinements through additional polishing steps.}
}
@article{AURICH2012337,
title = {Noise investigation in manufacturing systems: An acoustic simulation and virtual reality enhanced method},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {5},
number = {4},
pages = {337-347},
year = {2012},
note = {Special issue from 44th CIRP Conference on Manufacturing Systems},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2012.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1755581712000636},
author = {Jan C. Aurich and Xiang Yang and Simon Schröder and Martin Hering-Bertram and Tim Biedert and Hans Hagen and Bernd Hamann},
keywords = {Noise investigation, Manufacturing systems, Sound simulation, Visualization, Virtual reality, CAVE},
abstract = {Even though the noise issue in manufacturing is widely discussed from legal and health aspects, there is still no comprehensive method to simulate and analyze it. In this paper, a novel concept to investigate the noise level is proposed. Therefore a simulation method and the virtual reality (VR) implementation are involved. Acoustic measurements in real factory provide validation data for a realistic simulation. Furthermore, a representation of simulation results in the virtual environment is visualized in a Cave Automatic Virtual Environment (CAVE). The analysis and evaluation of potential noise reduction are realized by using described methods.}
}
@article{JOST2019109379,
title = {A quantitative method for evaluation of 6 degree of freedom virtual reality systems},
journal = {Journal of Biomechanics},
volume = {97},
pages = {109379},
year = {2019},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2019.109379},
url = {https://www.sciencedirect.com/science/article/pii/S0021929019306050},
author = {Tyler A. Jost and Grant Drewelow and Scott Koziol and Jonathan Rylander},
keywords = {Virtual reality, VR, Clinical rehabilitation, Controllers, Exergaming, HTC Vive},
abstract = {Modern virtual reality systems such as the HTC Vive enable users to be immersed in a virtual world. Validation of the HTC Vive and other contemporaneous systems for use in clinic, research, and industry applications will assure users and developers that games and applications made for these systems are accurate representations of the real world. The purpose of this study was to develop a standardized method for testing the translational and rotational capabilities of VR systems such as the HTC Vive. The translational and rotational capabilities of the HTC Vive were investigated using an industry grade robot arm and a gold standard motion capture system. It was found that the average difference between reported translational distances traveled was 0.74 ± 0.42 mm for all room-scale calibration trials and 0.63 ± 0.27 mm for all standing calibration trials. The mean difference in angle rotated was 0.46 ± 0.46° for all room-scale calibration trials and 0.66 ± 0.40° for all standing calibration trials. When tested using human movement, the average difference in distance traveled was 3.97 ± 3.37 mm. Overall, the HTC Vive shows promise as a tool for clinic, research, and industry and its controllers can be accurately tracked in a variety of situations. The methodology used for this study can easily be replicated for other VR systems so that direct comparisons can be made as new systems become available.}
}
@article{NEUGEBAUER2012103,
title = {Realistic Machine Simulation with Virtual Reality},
journal = {Procedia CIRP},
volume = {3},
pages = {103-108},
year = {2012},
note = {45th CIRP Conference on Manufacturing Systems 2012},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2012.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S2212827112001916},
author = {R. Neugebauer and P. Klimant and M. Witt},
keywords = {Machine Simulation, Multi-Body Simulation, Virtual Reality},
abstract = {Today highly complex components are manufactured on NC-controlled machine tools. The NC programs, controlling these machines, are usually automatically generated by CAM software. This automatic processing is often erroneous. The VR-based realistic machine simulation, presented in this paper, extends the usual content of a machine simulation, like material removal and collision detection, by various new aspects. The coupling of a real NC unit allows the recognition and elimination of all process- as well as controller-caused errors. The integration of the multi-body simulation enables the consideration of inertia, machine rigidity and milling cutter deflection.}
}
@article{OCONNOR2023107963,
title = {Exploring the impact of augmented reality on student academic self-efficacy in higher education},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107963},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107963},
url = {https://www.sciencedirect.com/science/article/pii/S074756322300314X},
author = {Yvonne O'Connor and Carolanne Mahony},
keywords = {Academic self-efficacy, Augmented reality, Conceptual model, Cognitive strategies, Learning space},
abstract = {This study develops and empirically tests a conceptual model comprising eight hypotheses that focus on the impact of Augmented Reality (AR) on student academic self-efficacy. A controlled experiment was conducted, followed by an online questionnaire with 65 students. Partial Least Square (PLS) Structural Equation Modelling (SEM) is used to assess and test the model. The results show that student cognitive strategies impact student perception and engagement with technology and learning tasks, with AR positively impacting student academic self-efficacy in higher education. Interestingly, neither the learning space nor the students' perception of task value was shown to impact their intention to use AR. Instead, it was the characteristics of the technology itself which encouraged adoption. The study's findings reinforce the need to (1) teach and facilitate effective cognitive strategies among third level students and (2) identify and overcome negative cognitive strategies that harm student self-efficacy and engagement.}
}
@article{LIU2023170643,
title = {Depth-map-based augmented reality holographic stereogram: Method and verification},
journal = {Optik},
volume = {276},
pages = {170643},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2023.170643},
url = {https://www.sciencedirect.com/science/article/pii/S0030402623001390},
author = {Yunpeng Liu and Tao Jing and Ming Lei and Hairong Hu and Min Lin and Pei Li and Xiaoyu Jiang and Xingpeng Yan},
keywords = {3D display, Holographic stereogram, Virtual-real fusion, Augmented reality},
abstract = {In this paper, a perspective image fusion algorithm of real-world and artificial scenes based on depth maps is proposed and applied to holographic printing to realize augmented reality display of holographic stereograms. The basic principle of the fusion algorithm is introduced, and the perspective image fusion with correct occlusion relation is implemented theoretically. Under the condition of sampling only once, an image generation algorithm with adjustable depth of artificial scenes is proposed and implemented. We use effective perspective image segmentation and mosaicking method for optical experiments, and verify the effectiveness of the proposed method.}
}
@article{GILL2024100070,
title = {Implementing Universal Design through augmented-reality game-based learning},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100070},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000205},
author = {Amarpreet Gill and Derek Irwin and Dave Towey and Yanhui Zhang and Pinzhuang Long and Linjing Sun and Wanling Yu and Yaxin Zheng},
keywords = {Augmented reality, Digital game-based learning, Microlearning, Collaborative learning, Engineering education, Educational technology},
abstract = {Technology integration in higher education (HE) provides educators with the opportunity to design stimulating learning environments, especially in design and engineering education (DEE) where it plays a unique role in nurturing creativity, problem-solving and innovation. This study investigates the integration of an augmented reality (AR) educational game in DEE, focusing on the teaching of Design for Manufacturing and Assembly (DfMA) principles. The primary aim of this research is to enhance traditional DfMA teaching and learning (T&L) practices by applying innovative T&L strategies. The resulting AR DfMA game, developed using digital game-based learning, microlearning and collaborative learning techniques, aligns with the Universal Design for Learning framework to create an inclusive learning environment that encourages participation from all students and supports several learning styles. The study tests the individual features designed for personal study opportunities to discover which elements are optimal in that environment, and then on the proposed in-class group components of the AR game, involving 68 participants from the appropriate program, though it is not currently implemented as part of a module. We utilize a mixed methods approach to examine the students’ experience and identify key design features that contribute to an inclusive educational experience. The findings highlight positive student experiences, and preferences for hands-on engagement, multimodal content, and collaborative activities. The AR DfMA game also has the potential to enhance intrinsic motivation and create an active and inclusive learning environment. Challenges and areas for improvement are also discussed.}
}
@article{MENDOZA20155,
title = {Augmented Reality as a Tool of Training for Data Collection on Torque Auditing},
journal = {Procedia Computer Science},
volume = {75},
pages = {5-11},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036479},
author = {Manuel Mendoza and Miguel Mendoza and Eloy Mendoza and Eduardo González},
keywords = {Augmented Reality, Residual Torque, Auditing torque, Variability, Validating Torque},
abstract = {The objective of this research is to implement an augmented reality system focused on the training of audit torque, where a person without any knowledge of the theories and application of torque, will be able to understand, implement and audit the pair residual strength in the necessary processes in order to improve the quality of a product that has bolted joints. Today manufacturing companies and especially those that focus on the automotive, aerospace and consumer need to apply precise torque and validated to the large number of bolted connections they have in their products. For the application of torque there are several very complete theories because it is a very critical process for the final quality product. Theories of torque can become complex because they need to have a preliminary knowledge of engineering, as well as a company training to implement this. Each torque required to enter certain applied force specifications so to see if the applied torque is within these, you need to audit the residual torque. The residual torque refers to know the force which is at the junction after being screwed tight. It was observed as a result of this experiment that the inherent difference between the performance of the torque wrench and screw connection performance in delivering the force required for efficient clamping force decreases their variability with the help of augmented reality.}
}
@article{BERENDSEN20131119,
title = {Free-form image registration regularized by a statistical shape model: application to organ segmentation in cervical MR},
journal = {Computer Vision and Image Understanding},
volume = {117},
number = {9},
pages = {1119-1127},
year = {2013},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2012.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1077314213000660},
author = {Floris F. Berendsen and Uulke A. {van der Heide} and Thomas R. Langerak and Alexis N.T.J. Kotte and Josien P.W. Pluim},
keywords = {Inter-subject, Regularization, Abdomen, Shape model, Registration},
abstract = {Deformable registration is prone to errors when it involves large and complex deformations, since the procedure can easily end up in a local minimum. To reduce the number of local minima, and thus the risk of misalignment, regularization terms based on prior knowledge can be incorporated in registration. We propose a regularization term that is based on statistical knowledge of the deformations that are to be expected. A statistical model, trained on the shapes of a set of segmentations, is integrated as a penalty term in a free-form registration framework. For the evaluation of our approach, we perform inter-patient registration of MR images, which were acquired for planning of radiation therapy of cervical cancer. The manual delineations of structures such as the bladder and the clinical target volume are available. For both structures, leave-one-patient-out registration experiments were performed. The propagated atlas segmentations were compared to the manual target segmentations by Dice similarity and Hausdorff distance. Compared with registration without the use of statistical knowledge, the segmentations were significantly improved, by 0.1 in Dice similarity and by 8mm Hausdorff distance on average for both structures.}
}
@article{HARUN2020660,
title = {Experience Fleming’s rule in Electromagnetism Using Augmented Reality: Analyzing Impact on Students Learning},
journal = {Procedia Computer Science},
volume = {172},
pages = {660-668},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314150},
author = { Harun and Neha Tuli and Archana Mantri},
keywords = {Augmented Reality, Electromagnetism, Experimentation, Fleming’s rule, Virtual Reality},
abstract = {Education trends are changing from traditional teaching to Electronic-learning and Mobile-learning. Through the development of information and communication technology, many e-learning systems were developed but, they lack in terms of user interaction. This affects student’s learning, motivation, performance, thinking level, technical skills, and expertise. The accessibility of practical labs in science and engineering education with expensive equipment and apparatus is restricted for a limited amount of time for large number of students. According to the previous research, it was found that there are fewer studies in the field of electromagnetism by using Augmented Reality (AR) application. Moreover, students are facing difficulties to understand Fleming’s rule and for this, no practical experiment is available in the physics lab. The solution to overcome these problems is through the use of Augmented Reality (AR) and Virtual Reality (VR) labs. In this research the Concept of Fleming’s rule in Electromagnetism is explained by using AR technique and the visualization of magnetic field lines, electric current and force exerted on current-carrying conductor has been done. The efficiency of the proposed system was calculated and team-based activity among secondary and applied engineering students was analyzed. The knowledge gain by the students was measured using the pre-test and post-test method. The outcomes of this research revealed that AR technology was more efficient in enhancing students’ knowledge in the field of Electromagnetism. The investigation also suggests that AR application helps the students to gain higher flow understanding levels than those attained by a web-based application.}
}
@article{STECHERT2023913,
title = {Integrated Approach of Model-Based Systems Engineering and Augmented Reality for the Development of Rail Vehicles with Alternative Drives},
journal = {Procedia CIRP},
volume = {119},
pages = {913-918},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.170},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123005401},
author = {Carsten Stechert},
keywords = {Modelbased Systems Engineering (MBSE), Augmented Reality (AR), Railway Vehicle, Alternative Drives},
abstract = {In order to contribute to the reduction of CO2 emissions, rail vehicles are increasingly being equipped with alternative drives. Multi-system vehicles in particular are becoming very complex as a result. This results in elaborate testing as well as complex training for production, operation and maintenance. This publication is intended to contribute to the systematic and targeted development of new rail vehicles for passenger transport. Using model-based systems engineering and the modelling language SysML, harmonised requirements are documented, assigned to the new systems and targeted test cases are developed. Augmented reality experiences are then created to support the execution of the test cases during validation runs, commissioning and maintenance activities. The augmented reality experiences allow virtual information and the real environment to be linked and displayed on a mobile device. The aim of this publication is to present an end-to-end process that already takes the requirements of training into account during product development, so that it is easy to implement training using augmented reality technologies.}
}
@article{TSEPAPADAKIS2023101797,
title = {Are you talking to me? An Audio Augmented Reality conversational guide for cultural heritage},
journal = {Pervasive and Mobile Computing},
volume = {92},
pages = {101797},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101797},
url = {https://www.sciencedirect.com/science/article/pii/S157411922300055X},
author = {Michalis Tsepapadakis and Damianos Gavalas},
keywords = {Audio Augmented Reality, Conversational audio guide, AI chatbot, Cultural heritage, Cultural user experience, IoT, Context-awareness, Spatial sound, User evaluation, IBM watson, Exhibot},
abstract = {Augmented Reality (AR) technologies are increasingly utilized as a means of stimulating immersive experiences to cultural site visitors, mainly through visual superimposition of interactive digital elements onto the physical world. Recent research has investigated the use of Audio AR (AAR) in heritage sites, wherein visitors listen to spatially registered sound which could be attributed to ‘talking’ physical artefacts. A parallel trend in the audience engagement programs of cultural institutions involves the employment of AI chatbots which are engaged in dialogues with followers or visitors to provide meaningful responses to a number of user questions. Herein, we present Exhibot, an intelligent audio guide system aiming at enhancing the user experience of cultural site visitors. Exhibot involves the combination of AAR and chatbot technologies to enable natural visitor-exhibit interaction, while also leveraging IoT devices to contextualize the delivered information. The key contribution of the proposed system lies in the interplay of AAR, chatbot and IoT technologies to create immersive learning experiences in the context of an integrated cultural guide system. Exhibot has undergone field trials to validate its usability and utility in realistic operational conditions. As a case study, we have chosen the statue of a prominent politician situated at a central square in Heraklion, Greece. The evaluation results indicated a very positive attitude of users, which is attributed both to the sense of immersion evoked by the AAR-powered storytelling and the natural human-like conversation enabled by the chatbot.}
}
@article{HOWARD2021106808,
title = {A meta-analysis of virtual reality training programs},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106808},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106808},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100131X},
author = {Matt C. Howard and Melissa B. Gutworth and Rick R. Jacobs},
keywords = {Virtual reality, Head mounted display, Computer-based training, Training and development, Learning, Meta-analysis},
abstract = {Virtual reality (VR) is the three-dimensional digital representation of a real or imagined space with interactive capabilities. The application of VR for organizational training purposes has been surrounded by much fanfare; however, mixed results have been provided for the effectiveness of VR training programs, and the attributes of effective VR training programs are still unknown. To address these issues, we perform a meta-analysis of controlled experimental studies that tests the effectiveness of VR training programs. We obtain an estimate of the overall effectiveness of VR training programs, and we identify features of VR training programs that systematically produce improved results. Our meta-analytic findings support that VR training programs produce better outcomes than tested alternatives. The results also show that few moderating effects were significant. The applied display hardware, input hardware, and inclusion of game attributes had non-significant moderating effects; however, task-technology fit and aspects of the research design did influence results. We suggest that task-technology fit theory is an essential paradigm for understanding VR training programs, and no set of VR technologies is “best” across all contexts. Future research should continue studying all types of VR training programs, and authors should more strongly integrate research and theory on employee training and development.}
}
@article{ESWARAN2023109663,
title = {Augmented reality aided object mapping for worker assistance/training in an industrial assembly context: Exploration of affordance with existing guidance techniques},
journal = {Computers & Industrial Engineering},
volume = {185},
pages = {109663},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109663},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223006873},
author = {M. Eswaran and M.V.A. {Raju Bahubalendruni}},
keywords = {Augmented reality, Virtual reality, Assembly, Disassembly, Object mapping},
abstract = {In recent years, the affordances of immersive technologies in the industrial context have enabled the onsite worker to have increased potential towards handling the complex procedural and cognitive tasks involved in the strategy of manufacturing an appropriate product. This article describes the potential of Augmented Reality (AR) in terms of upgraded workplace understanding, training, and assisting semi-skilled/new workers in an industrial production environment. The different modes of instruction visualization techniques are investigated for the execution of assembly tasks and explored their impact in accordance to the efficiency of task execution, completion time, error rate perception, cognitive load, and independence. Progress outcomes of 24 participants are explored for real-time execution of a Reverse osmosis (RO) booster pump assembly in terms of different modes of visualization, such as the conventional document method, virtual /desktop mode, and object mapping-based AR. Additionally, the user studies on different visualization devices involving hand-held visualization devices (HHVD) and Head-mounted visualization devices (HMVD) are investigated and validated in terms of adaptability, acceptability, and usability. Furthermore, the effectiveness of the AR-aided object mapping technology versus other methods is explored through the feedback received from both experts and academia participants, which increases the insight of the technology and provides an intelligent mechanism for semi-skilled/new workers to handle complex or challenging operations in the production environment. The studies indicate that object mapping-based AR visualization facilitates increased assembly task efficiency and the minimum number of errors compared to the other guidance modes. Notably, the HMVD affords an increased realism experience, minimized errors, and less completion time compared to the HHVD.}
}
@article{MOGHADDAM2021101410,
title = {Exploring augmented reality for worker assistance versus training},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101410},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101410},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001622},
author = {Mohsen Moghaddam and Nicholas C. Wilson and Alicia Sasser Modestino and Kemi Jona and Stacy C. Marsella},
keywords = {Augmented reality, Electro-mechanical assembly, Workforce training, Workplace-based learning},
abstract = {This paper aims at advancing the fundamental understanding of the affordances of Augmented Reality (AR) as a workplace-based learning and training technology in supporting manual or semi-automated manufacturing tasks that involve both complex manipulation and reasoning. Between-subject laboratory experiments involving 20 participants are conducted on a real-life electro-mechanical assembly task to investigate the impacts of various modes of information delivery through AR compared to traditional training methods on task efficiency, number of errors, learning, independence, and cognitive load. The AR application is developed in Unity and deployed on HoloLens 2 headsets. Interviews with experts from industry and academia are also conducted to create new insights into the affordances of AR as a training versus assistive tool for manufacturing workers, as well as the need for intelligent mechanisms that enable adaptive and personalized interactions between workers and AR. The findings indicate that despite comparable performance between the AR and control groups in terms of task completion time, learning curve, and independence from instructions, AR dramatically decreases the number of errors compared to traditional instruction, which is sustained after the AR support is removed. Several insights drawn from the experiments and expert interviews are discussed to inform the design of future AR technologies for both training and assisting incumbent and future manufacturing workers on complex manipulation and reasoning tasks.}
}
@article{ARPAIA2021109280,
title = {Design, implementation, and metrological characterization of a wearable, integrated AR-BCI hands-free system for health 4.0 monitoring},
journal = {Measurement},
volume = {177},
pages = {109280},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.109280},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121002839},
author = {Pasquale Arpaia and Egidio {De Benedetto} and Luigi Duraccio},
keywords = {Augmented Reality, Brain–Computer Interface, SSVEP, Digital transformation, Medical instruments, Health 4.0, Healthcare, Monitoring systems, Patient’s vitals, Wearable},
abstract = {An integrated real-time monitoring system based on Augmented Reality (AR) and Brain–Computer Interface (BCI) for hands-free acquisition and visualization of remote data is proposed. As a case study, the monitoring of patients’ vitals in the operating room (OR) is considered; in particular, through the suitable combination of BCI and AR, the anesthetist can monitor in real-time (through a set of AR glasses), the patient’s vitals acquired from the electromedical equipment. Healthcare-related applications are particularly demanding in terms of real-time requirements; hence, the considered scenario represents an interesting and challenging testbed for the proposed system. Experimental tests were carried out at the University Hospital Federico II (Naples, Italy), employing pieces of equipment that are generally available in the OR. After the preliminary functional validation, accuracy and delay were measured, demonstrating the effectiveness and reliability of the proposed AR-BCI-based monitoring system.}
}
@article{OSTERLUND2012139,
title = {Virtual reality: Avatars in human spaceflight training},
journal = {Acta Astronautica},
volume = {71},
pages = {139-150},
year = {2012},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2011.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0094576511002724},
author = {Jeffrey Osterlund and Brad Lawrence},
keywords = {Avatar, Human spaceflight, Virtual reality, Modeling, Simulation, Human engineering},
abstract = {With the advancements in high spatial and temporal resolution graphics, along with advancements in 3D display capabilities to model, simulate, and analyze human-to-machine interfaces and interactions, the world of virtual environments is being used to develop everything from gaming, movie special affects and animations to the design of automobiles. The use of multiple object motion capture technology and digital human tools in aerospace has demonstrated to be a more cost effective alternative to the cost of physical prototypes, provides a more efficient, flexible and responsive environment to changes in the design and training, and provides early human factors considerations concerning the operation of a complex launch vehicle or spacecraft. United Space Alliance (USA) has deployed this technique and tool under Research and Development (R&D) activities on both spacecraft assembly and ground processing operations design and training on the Orion Crew Module. USA utilizes specialized products that were chosen based on functionality, including software and fixed based hardware (e.g., infrared and visible red cameras), along with cyber gloves to ensure fine motor dexterity of the hands. The key findings of the R&D were: mock-ups should be built to not obstruct cameras from markers being tracked; a mock-up toolkit be assembled to facilitate dynamic design changes; markers should be placed in accurate positions on humans and flight hardware to help with tracking; 3D models used in the virtual environment be striped of non-essential data; high computational capable workstations are required to handle the large model data sets; and Technology Interchange Meetings with vendors and other industries also utilizing virtual reality applications need to occur on a continual basis enabling USA to maintain its leading edge within this technology. Parameters of interest and benefit in human spaceflight simulation training that utilizes virtual reality technologies are to familiarize and assess operational processes, allow the ability to train virtually, experiment with “what if” scenarios, and expedite immediate changes to validate the design implementation are all parameters of interest in human spaceflight. Training benefits encompass providing 3D animation for post-training assessment, placement of avatars within 3D replicated work environments in assembling or processing hardware, offering various viewpoints of processes viewed and assessed giving the evaluators the ability to assess task feasibility and identify potential support equipment needs; and provide human factors determinations, such as reach, visibility, and accessibility. Multiple object motion capture technology provides an effective tool to train and assess ergonomic risks, simulations for determination of negative interactions between technicians and their proposed workspaces, and evaluation of spaceflight systems prior to, and as part of, the design process to contain costs and reduce schedule delays.}
}
@article{KANANGKAEW2023105883,
title = {A real-time fire evacuation system based on the integration of Building Information Modeling and Augmented Reality},
journal = {Journal of Building Engineering},
volume = {67},
pages = {105883},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.105883},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223000621},
author = {Somjintana Kanangkaew and Noppadon Jokkaw and Tanit Tongthong},
keywords = {Fire evacuation system, Building information modeling, Augmented reality},
abstract = {In the past decade, the drawings in construction projects' standard fire evacuation routes are usually guided in two-dimensional (2-D) views to explain hazardous areas and evacuation routes. Common users cannot interpret the drawings quickly, understand their exact position within the building, and select appropriate evacuation routes. Thus, when fires occur, the construction project's fire can cause damages, and the number of dead or injured workers involved can be high. Nowadays, Building Information Modeling (BIM) and Augmented Reality (AR) are used in construction projects to create construction in a three-dimensional (3-D) visualization. This study aims to create a prototype application that combines Building Information Modeling (BIM) and Augmented Reality (AR) to improve the fire evacuation system and provide real-time access to information for evacuating from hazardous locations. A marker-based location system was implemented, using a marker as a spatial index to link the physical location and virtual information. The system was tested for accuracy using the proposed system and the MATALL Laser Distance Meter on the 4th floor of the Department of Civil Engineering Building at Chulalongkorn University. The results showed that the average percentage difference from the current location to the exit measured by the proposed system was lower than that measured by the MATALL Laser Distance meter, with an average percentage difference of less than 2.2%. Therefore, the proposed system effectively provides real-time access to information such as the current location, exit, distance of the shortest route from the current location to the destination, and virtual green line, voice, and arrow direction for evacuation guidance. It is convenient for decision-making and helps users find destinations quickly and efficiently.}
}
@article{MOURTZIS2018368,
title = {Augmented Reality based Visualization of CAM Instructions towards Industry 4.0 paradigm: a CNC Bending Machine case study},
journal = {Procedia CIRP},
volume = {70},
pages = {368-373},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.045},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304529},
author = {Dimitris Mourtzis and Vasilios Zogopoulos and Ioannis Katagis and Panagiotis Lagios},
keywords = {Augmented Reality, Computer-Aided Manufacturing, Production Monitoring},
abstract = {Skilled shop-floor technicians are playing an important role in modern Industry 4.0 manufacturing environments, especially in small and customized manufacturing systems. Moving towards Industry 4.0 business models, it is essential to develop advanced solutions that will reinforce the position of technicians in manufacturing, by implementing key enabling technologies. Augmented Reality (AR) has arisen as a key enabling technology, which will enable the transfer of information from digitalized design systems and databases towards human operators, especially including production and maintenance information. Reinforced by the development of advanced communication technologies that support high speed data transfer, such as 5G, and networks that combine low latency with increased availability, such as tactile internet, AR proposes a way for agile information distribution and increased shop-floor status awareness. This study presents a mobile application for visualizing Computer Aided Manufacturing (CAM) instructions for bending processes using Augmented Reality. The application integrates step-by-step process instructions, highly usable menus and advanced visualization that includes safety zones, indications, and text instructions for the operators. The developed framework is applied and validated in a CNC bending machine in a real world scenario provided by a CNC machines manufacturer.}
}
@article{WEISS2024104185,
title = {Performance on a target acquisition task differs between augmented reality and touch screen displays},
journal = {Applied Ergonomics},
volume = {116},
pages = {104185},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104185},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023002235},
author = {Hannah Weiss and Jianyang Tang and Connor Williams and Leia Stirling},
keywords = {Augmented reality, Hand-eye coordination, Fitts' law},
abstract = {Target acquisition tasks quantify human motor and perceptual abilities while performing discrete tasks to support interface design and sensorimotor assessments. This study investigated the effects of display, Touchscreen and Augmented Reality (AR), on a standardized 2D multidirectional target acquisition task. Thirty-two participants performed the target acquisition task with both modality types and at two indexes of difficulty. The touchscreen modality yielded improved performance over AR as measured by accuracy, precision, error rates, throughput, and movement time. Throughput using the nominal index of difficulty was 10.12 bits/s for touchscreen and 3.11 bits/s for AR. AR designers can use the results to improve performance when designing AR interfaces by selecting larger buttons when accuracy and efficiency are required and by embedding perception cues to button target surfaces such as depth and proximity cues.}
}
@article{WANG2023102078,
title = {User-centric immersive virtual reality development framework for data visualization and decision-making in infrastructure remote inspections},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102078},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102078},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002069},
author = {Zhong Wang and Yulun Wu and Vicente A. González and Yang Zou and Enrique {del Rey Castillo} and Mehrdad Arashpour and Guillermo Cabrera-Guerrero},
keywords = {Immersive virtual reality, Remote infrastructure inspection, Usability, Agile user experience design, Human-computer interaction},
abstract = {Data visualization and decision-making are critical aspects of infrastructure inspections. Numerous research efforts have been made to develop practical remote inspection systems that overcome the limitations of conventional inspection methods. Although the integration of Immersive Virtual Reality (IVR) into remote inspection is currently being addressed, most IVR frameworks have focused on data accuracy and there has been a lack of research on developing human-centered IVR frameworks through usability evaluation. This paper addresses this limitation by proposing a new IVR development framework utilizing agile User Experience (UX) design techniques, which can be implemented in remote infrastructure inspections for data visualization and decision-making. Usability tests were conducted through a case study with 22 participants, using an IVR prototype developed in accordance with this framework. Following its implementation, the framework demonstrated its ability to identify damage via remote bridge inspection and achieved acceptable usability. Future research can build on the framework for semantic enrichment tasks.}
}
@article{CHU2021685,
title = {An experimental study on augmented reality assisted manual assembly with occluded components},
journal = {Journal of Manufacturing Systems},
volume = {61},
pages = {685-695},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000832},
author = {Chih-Hsing Chu and Ching-Hung Ko},
keywords = {Augmented reality, Manual assembly, Occluded components, Hand movement, Ergonomic assessment},
abstract = {Augmented Reality (AR) technology has increasingly been applied to facilitate manufacturing tasks such as training, maintenance, and safety management, in which manual assembly frequently occurs. Previously, studies confirmed the values of AR-based assembly systems, but few explored support of spatially restricted assembly where components are visually occluded from operator. In this regard, this research aims to develop new AR functions that assist manual assembly in such situations. The focus is on validating the effectiveness of various assistive information in AR, including assembly interface, operator’s hand movement, and operator held components. Subjective and objective measures are used to evaluate assembly experiments of different degrees of difficulty. Analyses of the experimental data reveal whether or not and how effectively each information offers guidance in occluded condition. In particular, a time reduction in more difficult assembly is realized by showing operator hand movement in AR. The hand model initially offers an operator a visual clue for quickly and roughly locating the assembly interface in a large unseen area, prior to precisely localizing in a smaller region guided by tactile sensing. However, the effectiveness of incorporating the held component is not evident, as positional deviation between real and virtual objects may reduce human’s hand-eye coordination. These findings not only provide preliminary design guidelines of AR assembly functions for occluded components but also demonstrates a novel yet practical application of AR technology in smart manufacturing.}
}
@article{URREA2020103447,
title = {Development of a virtual reality simulator for a strategy for coordinating cooperative manipulator robots using cloud computing},
journal = {Robotics and Autonomous Systems},
volume = {126},
pages = {103447},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103447},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019307651},
author = {Claudio Urrea and Rodrigo Matteoda},
keywords = {Cloud, Services, Collaborative, Cooperative, Coordination, Virtual reality},
abstract = {This article deals with the development of a simulator that recreates, in virtual reality, a team of Selectively Compliance Assembly Robot Arms (SCARA). This team works cooperatively to fulfill the task of stacking rectangular objects coordinated through a strategy that includes a cloud server responsible for communication between the robots. The execution of the task is based on a leader/follower configuration. In this configuration, the leader performs a computed trajectory constantly reporting its position to the remote server. The remote server, in turn, sends this information back to the follower so this can follow the leader. The application combines MatLab® and Java. The latter is specifically used for communication routines, since its versatility makes it easy to be incorporated into any type of machine. This paper seeks to demonstrate the advantages of incorporating cloud resources into a multi-robot system and how its performance can be tested by means of the application developed.}
}
@article{ZHANG2012185,
title = {Design and Development of an in situ Machining Simulation System Using Augmented Reality Technology},
journal = {Procedia CIRP},
volume = {3},
pages = {185-190},
year = {2012},
note = {45th CIRP Conference on Manufacturing Systems 2012},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2012.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S2212827112002053},
author = {J. Zhang and S.K. Ong and A.Y.C. Nee},
keywords = {CNC machining simulation, Augmented reality, Enhanced dexel model},
abstract = {CNC machining simulation has been developed to validate NC codes and optimize the machining process. Conventional simulation systems are usually performed in virtual environments, and this has the limitation that the operator needs to transfer the knowledge from the software environment to the real machining environment. The ARCNC system is proposed in this paper where the operator can observe in situ simulation between the real cutter and a virtual workpiece. The system design and the research problems addressed are discussed in this paper, including the tracking and registration methods and the physical simulation approach based on an enhanced dexel model.}
}
@article{MOURTZIS2019363,
title = {An Adaptive Framework for Augmented Reality Instructions Considering Workforce Skill},
journal = {Procedia CIRP},
volume = {81},
pages = {363-368},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119303683},
author = {Dimitris Mourtzis and Fotini Xanthi and Vasilios Zogopoulos},
keywords = {Human Skills, Augmented Reality, Workforce Development},
abstract = {Human resources have always been a key component in manufacturing systems. The digital transformation of modern shop floor undoubtedly induced the evolution of new requirements in workforce skills. In recent years, Augmented Reality (AR) has arisen as a promising technology for knowledge transfer across the factory. AR applications need to be tailored into the workforce skills, considering their limited experience. This paper presents an approach for AR instruction generation, adaptive to the shop floor operators’ level of experience, to assist them in production machinery operation. The proposed framework is validated in a press brake manufacturer use case.}
}
@article{SUN2024101200,
title = {3D Model Topology Algorithm Based on Virtual Reality Visual Features},
journal = {Measurement: Sensors},
pages = {101200},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101200},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424001764},
author = {Junhai Sun and Yi Wang and Dan Jiang},
keywords = {3D images, Virtual reconstruction, Visual feature extraction, Adaptive pixel reconstruction, Simulation analysis},
abstract = {This study proposes a virtual reconstruction method for fuzzy 3D images based on visual communication effects to improve recognition ability. This method includes constructing a grid distribution model for 3D images, recombining spatial information features using visual feature extraction, and detecting edges using contour feature extraction. A three-dimensional image visual communication model was established, and adaptive pixel reconstruction was achieved through fuzzy structure reconstruction method. Reconstruct the texture structure and grayscale histogram of 3D images to achieve high-quality virtual reconstruction. The simulation results verified the effectiveness of the method, and the designed simulation platform verified the accuracy and completeness of the method, demonstrating the improvement of existing systems and the potential for advancing the field of visual communication.}
}
@article{TURKAN201790,
title = {Mobile augmented reality for teaching structural analysis},
journal = {Advanced Engineering Informatics},
volume = {34},
pages = {90-100},
year = {2017},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616302944},
author = {Yelda Turkan and Rafael Radkowski and Aliye Karabulut-Ilgu and Amir H. Behzadan and An Chen},
abstract = {Structural analysis is an introductory core course that is taught in every civil engineering program as well as in most architectural and construction engineering programs. Previous research unveils students' deficits in understanding the behavior of structural elements in a three-dimensional (3D) context due to the shortcomings of traditional lecturing approaches, which put too much emphasis on the analysis of individual structural members, thereby falling short in providing a solid, easy-to-follow, and holistic approach to analyzing complex structures with a large number of interconnected elements. In this paper, the authors introduce a new pedagogy for teaching structural analysis that incorporates mobile augmented reality (AR) and interactive 3D visualization technology. The goal of this study is to enhance the contents used in structural analysis textbooks and on worksheets by visualizing discrete structural members employing AR along with interactive 3D models in order to illustrate how the structures behave under different loading conditions. Students can interactively change the load and observe the reaction resulting from this change with the instant feedback provided by the AR interface. The feasibility of AR concepts and interaction metaphors, as well as the potential of using AR for teaching structural analysis are investigated, specifically by focusing on challenges regarding content integration and interaction. An AR application is designed and developed, and a pilot study is conducted in a junior level structural analysis class to assess the pedagogical impact and the design concepts employed by the AR tool. Control and test groups are deployed, and students’ performance is measured using pre- and post-tests. The results of the pilot study indicate that the utilized AR design concepts have potential to contribute to students’ learning by providing interactive and 3D visualization features, which support constructive engagement and retention of information in students.}
}
@article{MA2024105508,
title = {A novel Residual and Gated Network for prostate segmentation on MR images},
journal = {Biomedical Signal Processing and Control},
volume = {87},
pages = {105508},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105508},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423009412},
author = {Ling Ma and Qiliang Fan and Zhiqiang Tian and Lizhi Liu and Baowei Fei},
keywords = {Prostate cancer, Prostate segmentation, Gated convolutional neural network, Residual convolutional neural network, Magnetic resonance imaging},
abstract = {Reliable and automated segmentation of the prostate in magnetic resonance (MR) images is critical for the diagnosis and treatment of prostate cancer. The unclear boundary of the prostate increases the difficulty of correctly differentiating the prostate region from the surrounding regions. This paper proposes a novel approach with Residual and Gated Network (ResGNet) for prostate segmentation on MR images. The ResGNet block fuses the residual and gated connections in a multi-directional and multi-path learning strategy for enhancing information propagation and preserving important boundary information. The proposed method is conducted on several public databases, and an average DSC of 94.4%, 95-HD of 3.280 mm, and ABD of 0.919 mm are obtained in the 5-fold cross-validation experiment on the PROMISE12 dataset. Experimental results demonstrate that the ResGNet outperforms the state-of-the-art methods and is effective and robust regardless of the variability of MR scanners and acquisition protocols or the size and shape of the prostate gland for the segmentation of healthy or diseased prostates.}
}
@article{BACCA201549,
title = {Mobile Augmented Reality in Vocational Education and Training},
journal = {Procedia Computer Science},
volume = {75},
pages = {49-58},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.203},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036649},
author = {Jorge Bacca and Silvia Baldiris and Ramon Fabregat and  Kinshuk and Sabine Graf},
keywords = {Augmented Reality, Vocational Education and Training, Motivation, Universal Design for Learning, Special Educational Needs},
abstract = {In Vocational Education and Training (VET) institutions, teachers face important difficulties in the teaching process due to a wide variety of student's special educational needs as well as student's lack of: the adequate level of basic competence, motivation, concentration, attention, confidence and background knowledge, among other aspects. Regarding the attention to these aspects, many studies have reported positive impact of Augmented Reality (AR) applications in primary, secondary and higher education in terms of student's motivation, learning gains, collaboration, interaction, learning attitudes and enjoyment, among others. However, very little has been done in terms of AR applications in VET as well as their impact on wide variety of student's special educational needs such as learning difficulties. This paper introduces a marker-based mobile AR application named Paint-cAR for supporting the learning process of repairing paint on a car in the context of a vocational education programme of car's maintenance. The application was developed using a methodology for developing mobile AR applications for educational purposes from a collaborative creation process (Co-Creation) and based on the Universal Design for Learning (UDL). A cross-sectional evaluation study was conducted to validate the Paint-cAR application in a real scenario.}
}
@article{JODA201993,
title = {Augmented and virtual reality in dental medicine: A systematic review},
journal = {Computers in Biology and Medicine},
volume = {108},
pages = {93-100},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S001048251930085X},
author = {T. Joda and G.O. Gallucci and D. Wismeijer and N.U. Zitzmann},
keywords = {Systematic review, Augmented reality (AR), Virtual reality (VR), Computer simulation, Dentistry, Oral medicine},
abstract = {Background
The aim of this systematic review was to provide an update on the contemporary knowledge and scientific development of augmented reality (AR) and virtual reality (VR) in dental medicine, and to identify future research needs to accomplish its clinical translation.
Method
A modified PICO-strategy was performed using an electronic (MEDLINE, EMBASE, CENTRAL) plus manual search up to 12/2018 exploring AR/VR in dentistry in the last 5 years. Inclusion criteria were limited to human studies focusing on the clinical application of AR/VR and associated field of interest in dental medicine.
Results
The systematic search identified 315 titles, whereas 87 abstracts and successively 32 full-texts were selected for review, resulting in 16 studies for final inclusion. AR/VR-technologies were predominantly used for educational motor skill training (n = 9 studies), clinical testing of maxillofacial surgical protocols (n = 5), investigation of human anatomy (n = 1), and the treatment of patients with dental phobia (n = 1). Due to the heterogeneity of the included studies, meta-analyses could not be performed.
Conclusions
The overall number of includable studies was low; and scientifically proven recommendations for clinical protocols could not be given at this time. However, AR/VR-applications are of increasing interest and importance in dental under- and postgraduate education offering interactive learning concepts with 24/7-access and objective evaluation. In maxillofacial surgery, AR/VR-technology is a promising tool for complex procedures and can help to deliver predictable and safe therapy outcomes. Future research should focus on establishing technological standards with high data quality and developing approved applications for dental AR/VR-devices for clinical routine.}
}
@article{HAN2024100703,
title = {Virtual reality with e-learning model in business management and knowledge complex environment for gaming analysis},
journal = {Entertainment Computing},
volume = {50},
pages = {100703},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100703},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000715},
author = {Mengmeng Han and Imelda {Lorenzo Najord}},
keywords = {Business intelligence modelling, Virtual reality, E-learning, Gamification, ML},
abstract = {Nearly real-world experiences in a variety of industries may be had with virtual reality (VR) technology. Virtual reality (VR) settings are widely used, and as a result, a large quantity of data is being collected from them. The data-driven development paradigm must be advanced and analytical procedures must be created as the amount of data increases. There are several trends and possibilities that have emerged from integration of artificial intelligence (AI) as well as machine learning (ML) in business intelligence. Modern technology has completely changed how companies gather data, draw conclusions, decide. In order to analyse complex environments, this study uses gamification and virtual reality in business intelligence modelling for e-learning. Here, virtual reality-based e-learning is modelled for business intelligence, and gamification using reinforcement alexnet Q-kernel component neural networks is used to analyse the network’s complexity. Prediction accuracy, AUC, average precision, sensitivity, quadratic normalised square error are metrics used in experimental study. The evaluation’s findings demonstrate the usefulness of our solution for classifying and forecasting pay levels in the business sector, which in turn helps business analytics in intricate AI settings.}
}
@article{LIU2023109257,
title = {Mixed Reality collaboration environment improves the efficiency of human-centered industrial system: A case study in the mining industry},
journal = {Computers & Industrial Engineering},
volume = {180},
pages = {109257},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109257},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223002814},
author = {Shuguang Liu and Jiacheng Xie and Xuewen Wang and Hao Meng},
keywords = {Mixed Reality, Collaboration environment, Human-centered, Intelligent industrial system, Mining industry},
abstract = {The emerging information and communication technologies (ICT) promote the intelligence of industrial systems. However, humans still play an important role in industrial systems, and they are required to collaborate in many scenarios to perform complex tasks. Mixed Reality (MR) technology allows the virtual-real fusion and real-time interaction, and has unparalleled advantages in building a collaborative environment. In this study, Theoretical and technical studies have been conducted to provide a universal solution for the industrial applications of MR collaboration. In terms of theory, a framework of the intelligent industrial system is first proposed by combining the human-centered idea and cloud-edge-terminal collaborative architecture. Based on this framework, the MR collaboration environment is then designed. Finally, the MR task scheduling model is developed to illustrate how the MR collaboration environment uses the cloud-edge-terminal collaborative architecture to improve the performance. The technical study is conducted based on the theoretical foundation. The key technologies involved in both co-located and remote collaboration modes, that can be achieved by the MR collaboration environment, are studied. In addition, the human–computer interaction (HCI) technology in MR collaboration is explored. Based on this study, the proposed MR collaboration environment is developed, tested, and evaluated by considering the mining industry as an example. Moreover, user surveys from a human-centered perspective are conducted. The results show that the MR collaboration environment has high applicability in multiple links in industrial scenarios represented by longwall mining. It can improve the quality, increase the efficiency, and reduce the costs. It can also provide a human-centered collaboration experience.}
}
@article{DELAMO2018362,
title = {An innovative user-centred support tool for Augmented Reality maintenance systems design: a preliminary study},
journal = {Procedia CIRP},
volume = {70},
pages = {362-367},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300854},
author = {Iñigo Fernández {del Amo} and Elisa Galeotti and Riccardo Palmarini and Gino Dini and John Erkoyuncu and Rajkumar Roy},
keywords = {Augmented Reality, Maintenance, Systems Design, Analytical Hierarchy Process},
abstract = {Augmented Reality (AR) technologies in maintenance are demonstrated to positively impact on technicians’ work and performance. Current research in this area is mainly focused on solving technical challenges related with AR in industrial environments. Limited attention has been put into the user perception, ergonomics and usability aspects of AR systems design. This paper proposes an innovative user-centred design support tool for AR systems in maintenance contexts. The tool is based on the Analytic Hierarchy Process (AHP), which is a well-established multi-criteria decision-making approach. In this research, AHP is utilised for guiding designers in the evaluation of application-contexts and AR-technologies for selecting the most suitable and effective AR interaction solution. The tool’s validation has been conducted with twelve maintenance-experts in a design workshop using two case studies. The quantitative results obtained in both case studies reveal the applicability of the AHP model, as well as the effectiveness of the design support tool for complex decisions in AR for maintenance. The use of AHP methods for AR design enable experts to deal with complex and contrasting concepts and express a preference among them with a subjective judgement based on their personal understanding of the problem. Therefore, simplifying the design of AR systems for complex maintenance contexts.}
}
@article{BRESSAN2024108575,
title = {Case studies of eXtended reality combined with Building Information Modeling: A literature review},
journal = {Journal of Building Engineering},
volume = {84},
pages = {108575},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.108575},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224001438},
author = {Nicolò Maria Bressan and Massimiliano Scarpa and Fabio Peron},
keywords = {XR technology, AEC industry, BIM, Case study, Purpose-base classification},
abstract = {This review article deals with the integration of eXtended Reality (XR) into Building Information Modeling (BIM), based on sixty recent case studies. This pair raises interest because, even if XR is still weakly used in AEC firms, it may increase the exploitation degree of BIM tools, which are instead becoming comprehensive information repositories and design and management tools along the building's life cycle. The considered case studies have been grouped into the following 4 categories: I) training, II) design, III) construction, and IV) operation. The works considered in each group were studied by identifying the main purpose, novelty, used tools, country, etc. Group I includes ten works, prominently based on VR, which mainly aim at facilitating visualization and increasing the users' overall engagement into the training activity. Group II includes nineteen works (prominently VR-based), which mainly aim at increasing the stakeholders' collaboration and design understanding, for more informed decision-making. The eighteen works in Group III (mainly AR- and MR-based) provide deeper insights about information accessibility, inspection, and defect management, by means of site walkthroughs and enhanced position registration solutions. Finally, the thirteen works in Group IV mainly use VR and MR, for higher understanding of the relevant sites. BIM is mostly paired with VR (52 % of the case studies). Moreover, Unity3D may be identified as the prominent development platform. Even if many purposes, tools, and approaches are covered, documented case studies seem still few if compared with the overall economic and human resources involved in the building industry.}
}
@article{LIU2023347,
title = {Cloud computing-enabled IIOT system for neurosurgical simulation using augmented reality data access},
journal = {Digital Communications and Networks},
volume = {9},
number = {2},
pages = {347-357},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000682},
author = {Jun Liu and Kai Qian and Zhibao Qin and Mohammad Dahman Alshehri and Qiong Li and Yonghang Tai},
keywords = {Neurosurgery, IIOT, Cloud computing, Intelligent medical, 5G, AR},
abstract = {In recent years, statistics have indicated that the number of patients with malignant brain tumors has increased sharply. However, most surgeons still perform surgical training using the traditional autopsy and prosthesis model, which encounters many problems, such as insufficient corpse resources, low efficiency, and high cost. With the advent of the 5G era, a wide range of Industrial Internet of Things (IIOT) applications have been developed. Virtual Reality (VR) and Augmented Reality (AR) technologies that emerged with 5G are developing rapidly for intelligent medical training. To address the challenges encountered during neurosurgery training, and combining with cloud computing, in this paper, a highly immersive AR-based brain tumor neurosurgery remote collaborative virtual surgery training system is developed, in which a VR simulator is embedded. The system enables real-time remote surgery training interaction through 5G transmission. Six experts and 18 novices were invited to participate in the experiment to verify the system. Subsequently, the two simulators were evaluated using face and construction validation methods. The results obtained by training the novices 50 times were further analyzed using the Learning Curve-Cumulative Sum (LC-CUSUM) evaluation method to validate the effectiveness of the two simulators. The results of the face and content validation demonstrated that the AR simulator in the system was superior to the VR simulator in terms of vision and scene authenticity, and had a better effect on the improvement of surgical skills. Moreover, the surgical training scheme proposed in this paper is effective, and the remote collaborative training effect of the system is ideal.}
}
@article{HAVARD20191295,
title = {Collaborative Virtual Reality Decision Tool for Planning Industrial Shop Floor Layouts},
journal = {Procedia CIRP},
volume = {81},
pages = {1295-1300},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306304},
author = {Vincent Havard and Anirudh Trigunayat and Killian Richard and David Baudry},
keywords = {collaborative virtual reality, decision tool, planning industrial shop floor layouts},
abstract = {This paper presents a methodology and the associated tool for enhancing decision making in shop floor layout using collaborative virtual reality, associated to real-time performance indicators in Industry 4.0. It first describes the collaborative architecture of the application. Subsequently, application features like: visualization at scale one (machines, industrial environment’s top view, product flow path), 3D model interaction (setting layout), physical process flow evaluation (spaghetti diagrams, Key Performance Indicators-KPI), testing multiple configurations (via save and load). Finally, paper illustrates possible usages and benefits of how VR tool helps improve ergonomics and safety; helps reduce deliberation time and bottlenecks through machine spatial footprint, visualization, scale one experience and referring KPIs.}
}
@article{PANYA2023106030,
title = {An interactive design change methodology using a BIM-based Virtual Reality and Augmented Reality},
journal = {Journal of Building Engineering},
volume = {68},
pages = {106030},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106030},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223002097},
author = {David Stephen Panya and Taehoon Kim and Seungyeon Choo},
keywords = {Building information modeling, Virtual reality, Augmented reality, Change management},
abstract = {Changes in design and construction constitute variations from the original design prepared by a design team and are inevitable. Despite the fact that rework has been identified as having adverse effects on performance, there have been limited studies on how to mitigate rework in design changes. Despite improving change management, Building Information Modeling (BIM) is insufficient in mitigating rework. However, the convergence of the capabilities of BIM, Virtual Reality (VR), and Augmented Reality (AR) are integrated solutions to reduce the impact of rework in design change. In this research, we improve the requirements for reworking design changes, which include redesign, information flow, and delivery for users with no experience with BIM. For interactive design change with BIM in VR and AR, the proposed methodology integrates BIM, as-built data from web sources, and VR and AR functions using game engines and programming. In VR and AR, BIM objects are used to create multiple variants that are matched with BIM databases that provide options for design changes. The interactive design change platform is validated in a VR and AR environment and presented multiple design change options for wall elements based on a set required change factor of a building project. This paper highlights the need for a change platform that reduces the rework of design changes in BIM through VR and AR in three main areas redesign, maintaining the flow of information and delivery of solutions for design changes. The VR and AR proposed solution to design change improves rework collaboration and communication among stakeholders of a building project.}
}
@article{ZHANG2023105133,
title = {Risk analysis of people evacuation and its path optimization during tunnel fires using virtual reality experiments},
journal = {Tunnelling and Underground Space Technology},
volume = {137},
pages = {105133},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105133},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823001530},
author = {Xiaochun Zhang and Linjie Chen and Junhao Jiang and Yixin Ji and Shuyang Han and Ting Zhu and Wenbin Xu and Fei Tang},
keywords = {Tunnel fire, Virtual reality (VR）, Evacuation behavior, Risk analysis, Tunnel lifecycle safety management},
abstract = {The number of urban tunnels has been increasing rapidly, accompanied by frequent tunnel fire accidents owing to the complex tunnel structure and large traffic flow. In this study, a full-size tunnel virtual reality (VR) scenario and computational fluid dynamics (CFD) construction model were established to investigate the evacuation behavior and corresponding risk of people in the early stage of vehicle fires considering four scenarios: normal circumstance, without VR agents, without emergency evacuation signs, and without fire extinguishers. Firstly, the cumulative values of CO, CO2, and temperature along the evacuation path were monitored using CFD. Secondly, the smoke toxicity was calculated using the N-GAS model, and the total risk value was computed based on the analytic hierarchy process which was defined as “smoke hazard: temperature hazard = 7:3.” Thirdly, a multiple regression model was created based on accident data. Finally, to minimize accidents, the design of the evacuation path was optimized using the established mathematical model and A* algorithm to verify the effectiveness of the risk assessment model. The results show that the effects of VR agents, emergency evacuation signs, and fire extinguishers on the evacuation behavior of people are mutual influence. This study combined time and route in the VR evacuation experiments to overcome the limitations of the existing control and VR experiments in quantifying the evacuation results. This research can be utilized to improve emergency evacuation plans and emergency response decision making. Furthermore, it can broaden the application of VR in the field of tunnel lifecycle safety management.}
}
@article{MARTINETTI201714,
title = {Shaping the Future Maintenance Operations: Reflections on the Adoptions of Augmented Reality Through Problems and Opportunities},
journal = {Procedia CIRP},
volume = {59},
pages = {14-17},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.10.130},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116312732},
author = {Alberto Martinetti and Mohammad Rajabalinejad and Leo {van Dongen}},
keywords = {Augmented Reality, maintenance operations, safety, information and communication selection},
abstract = {Maintenance technicians play a fundamental role in ensuring the success of the system reliability, but several fatal accidents occur during the maintenance operations in Europe due to the nature of the job and to human errors. Augmented Reality (AR) technology could ease the execution of complex maintenance operations under specified conditions reducing the probability of failures, guiding through the task the technicians, updating in real-time the operators about the environmental and boundary conditions, reducing costs and time for service and maintenance, reducing documentation of work processes, increasing the level of safety and decreasing the human error probability. This paper investigates possible applications of AR technologies for assisting the workers during the maintenance operations, identifies challenges for the use of this technology in maintenance applications, and discusses the actual level of feasibility of the technology and the possible improvements in terms of information and communication selection and reduction of human errors.}
}
@article{DESHPANDE2018760,
title = {The effects of augmented reality on improving spatial problem solving for object assembly},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {760-775},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618300971},
author = {Abhiraj Deshpande and Inki Kim},
keywords = {Augmented reality, Spatial problem solving, Mental representation, Ready-to-assemble furniture},
abstract = {The capability of Augmented Reality (AR) technology to track and visualize relations of objects in space has led to diverse industry applications to support complex engineering tasks. Object assembly is one of them. For an AR aid to support Ready-to-Assemble (RTA) furniture particularly, the challenge is to effectively design the visual features and mode of interaction, so that the first-time users can quickly conceive spatial relations of its parts. However, AR developers and engineers do not have sufficient guidelines to achieve such performance-driven goals. The scientific evidence and account of how one could cognitively benefit in object assembly can be useful to guide them. This experimental research developed an Augmented Reality (AR) application on the Microsoft HoloLens™ headset, and tested it on the first-time users of RTA furniture. The controlled experiments and behavioral analyses of fourteen participants in working out the two RTA furniture with different assembly complexity showed that, the application was effective to improve spatial problem-solving abilities. Especially, the positive effects of the AR-based supports stood out against the assembly of higher complexity. The findings have implications for the information design of advanced AR applications to support assembly tasks with high demands of spatial knowledge. In addition, the extensive analyses of the participants’ performance, behaviors, cognitive workload, and subjective responses helped identify usability issues with the Microsoft HoloLens™.}
}
@article{BOFFI2023100014,
title = {An educational experience in ancient Rome to evaluate the impact of virtual reality on human learning processes},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100014},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000089},
author = {Paolo Boffi and Monica Clerici and Alberto Gallace and Pier Luca Lanzi},
keywords = {Virtual reality, Media in education, Adult learning, Human-computer interaction},
abstract = {Immersive Virtual Reality technology has recently gained significant attention and is expanding its applications to various fields. It also has many advantages in education, as it allows to both simplify the explanation of complex topics through their visualization, and explore lost or unreachable environments. To evaluate the impact of immersive experiences on learning outcomes we developed an educational experience that lets users visit an ancient Roman Domus and provides information about daily life in Roman times. We designed a between-subjects data collection to investigate learning ratio, user experience, and cybersickness of participants through anonymous questionnaires. We collected 76 responses of participants (18–35 y.o.) divided into three conditions: a Immersive Virtual Reality experience, a slide-based lecture and a 2D desktop-based experience. Our results show that the virtual reality experience is considered more engaging and as effective as more traditional 2D and slide-based experiences in terms of learning.}
}
@article{KWON201474,
title = {A defect management system for reinforced concrete work utilizing BIM, image-matching and augmented reality},
journal = {Automation in Construction},
volume = {46},
pages = {74-81},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514001162},
author = {Oh-Seong Kwon and Chan-Sik Park and Chung-Rok Lim},
keywords = {Construction defect management, Image-matching, Augmented reality, Mobile application, BIM},
abstract = {The repeated and inevitable occurrence of defects remains one of the primary causes of project schedule and cost overruns in construction. Currently, quality management on site is done by site managers through their visual senses. However, it is often challenging for a handful of site managers to perform quality management efficiently for an entire site based on inspected progress of construction works. As a consequence, inspections are often omitted, resulting in faulty construction and quality degradation which may necessitate rework. This may adversely affect project costs and schedule. This paper addresses these issues by utilizing BIM, image-matching, and augmented reality (AR) to develop two types of defect management systems including: 1) an image-matching system to enable quality inspection without visiting the real work site; and 2) a mobile DM-AR app which enables workers and managers to automatically detect dimension errors and omissions on the jobsite. The two systems are tested and evaluated by lab and site experiments for reinforcement concrete (RC) work to assess the benefits and limitations. Experimental results demonstrate the defect management system's effectiveness in automatic omission and error detection at real job sites. The study emphasizes the potential applicability of BIM, image-matching, and AR technologies that could be utilized to improve construction defect management.}
}
@article{QIU2019597,
title = {Digital assembly technology based on augmented reality and digital twins: a review},
journal = {Virtual Reality & Intelligent Hardware},
volume = {1},
number = {6},
pages = {597-610},
year = {2019},
note = {VR/AR in industry},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579619300841},
author = {Chan Qiu and Shien Zhou and Zhenyu Liu and Qi Gao and Jianrong Tan},
keywords = {Augmented reality, Digital twin, Digital assembly, Virtual simulation, Assembly analysis},
abstract = {Product assembly simulation is considered as one of the key technologies in the process of complex product design and manufacturing. Virtual assembly realizes the assembly process design, verification, and optimization of complex products in the virtual environment, which plays an active and effective role in improving the assembly quality and efficiency of complex products. In recent years, augmented reality (AR) and digital twin (DT) technology have brought new opportunities and challenges to the digital assembly of complex products owing to their characteristics of virtual reality fusion and interactive control. This paper expounds the concept and connotation of AR, enumerates a typical AR assembly system structure, analyzes the key technologies and applications of AR in digital assembly, and notes that DT technology is the future development trend of intelligent assembly research.}
}
@article{BOLOGNA2020197,
title = {An Augmented Reality Platform for training in the industrial context},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {197-202},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301786},
author = {Jennifer K. Bologna and Carlos A. Garcia and Alexandra Ortiz and Paulina X. Ayala and Marcelo V. Garcia},
keywords = {Augmented Reality, Unity 3D, HART Devices, Industrial Training},
abstract = {Augmented Reality (AR) is considered within the Internet of Things (IIoT) goals as a training tool because it provides virtual experiences that are impossible in a real environment, giving to the employee safety on equipment operation and low-cost of training. For this reason, this research presents an AR system that allows users to train in the calibration of HART instrumentation, using the Unity 3D platform and Meta 2 glasses. This application offers a platform that has advantages for users since it can train in the operation of expensive industrial equipment in an error-prone system. The validation of the system was carried out through an evaluation module, which allows the user to determine the level of training and autonomously build their own learning, obtaining positive responses that show that around 82% of the population evaluated.}
}