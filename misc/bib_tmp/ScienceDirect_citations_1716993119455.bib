@article{CHEN2011368,
title = {Defect structure for the ultra-nanocrystalline diamond films synthesized in H2-containing Ar/CH4 plasma},
journal = {Diamond and Related Materials},
volume = {20},
number = {3},
pages = {368-373},
year = {2011},
issn = {0925-9635},
doi = {https://doi.org/10.1016/j.diamond.2011.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925963511000276},
author = {Huang-Chin Chen and Chuan-Sheng Wang and I-Nan Lin and Hsiu-Fung Cheng},
keywords = {UNCD, TEM microstructure, Diamond flakes},
abstract = {The modification on the microstructure of diamond films due to the addition of H2 species into the Ar/CH4 plasma was investigated. While the Ar/CH4 plasma produced UNCD films with equiaxed grains (about 5nm in size), the (Ar-H2)/CH4 plasma produced acicular-shaped grains (about 5×20nm in size). Transmission electron microscopy studies indicate that these acicular-shaped grains actually are agglomerates of diamond flakes, which contain stacking faults lying on the (111) lattice plane. Presumably, the incorporation of H2 species in the plasma leads to partial etching of hydrocarbons adhered onto the diamond clusters, such that the C2- (or active carbon) species contained in the plasma can attach to the diamond surface anisotropically, leading to diamond flakes. The incorporation of H2 in Ar plasma can also suppress the formation of i-carbons, an allotropic phase of diamonds. The critical proportion of H2 in Ar plasma for inducing the changes in the granular structure is around 0.03%. The proportion of grain boundaries was thus reduced and the electron field emission properties of the materials were thus degraded. However, the suppression of the film electrical conductivity without sacrificing the smooth surface characteristic has the applications as high-thermal-conductivity heat spreaders and substrates for surface-acoustic-wave devices.}
}
@article{JUNG2015902,
title = {Exploration and evaluation of AR, MPCA and KL anomaly detection techniques to embankment dam piezometer data},
journal = {Advanced Engineering Informatics},
volume = {29},
number = {4},
pages = {902-917},
year = {2015},
note = {Collective Intelligence Modeling, Analysis, and Synthesis for Innovative Engineering Decision Making Special Issue of the 1st International Conference on Civil and Building Engineering Informatics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1474034615001056},
author = {In-Soo Jung and Mario Berges and James H. Garrett and Barnabas Poczos},
keywords = {Structural health monitoring, Dam safety, Anomaly detection, Statistical techniques},
abstract = {In the U.S., the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers, leaving much room for improvement through the application of advanced data analysis techniques. In this research, different types of anomaly detection techniques are examined in an effort to propose which data analytics are appropriate for various anomaly scenarios as well as piezometer locations. Moreover, both the parametric (Auto Regressive [AR] and Moving Principal Component Analysis [MPCA]) and nonparametric (Kullback–Leibler Divergence [KL]) techniques are applied in order to test if the widely-held assumptions about piezometer data, i.e., linearity between piezometer data and pool levels, as well as normally distributed piezometer data, are necessary in the anomaly detection task. In general, KL performs better than MPCA and AR, and delivers more consistent results throughout the different piezometers and anomaly scenarios. Given that KL is a nonparametric technique, the authors conclude that the prior assumptions about piezometer data do not always provide the best performance for anomaly prediction.}
}
@article{LIANG2021115,
title = {Analyzing bicycle level of service using virtual reality and deep learning technologies},
journal = {Transportation Research Part A: Policy and Practice},
volume = {153},
pages = {115-129},
year = {2021},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0965856421002275},
author = {Xiao Liang and Tianyu Zhang and Meiquan Xie and Xudong Jia},
keywords = {Bicycle traffic, Bicycle Level of Service (BLOS), Virtual Reality (VR) method, Symbolic regression, LOS criteria},
abstract = {Bicycle Level of Service (BLOS) provides an essential tool for evaluating the operations of low-carbon bicycle facilities and prioritizing investment in new bicycle facilities under various constraints. This study aimed at developing a LOS method for assessing bicycle facilities in the metropolitan areas of China. Using this method, we addressed major challenges in obtaining user ratings of bicycle facilities and captured senses of satisfaction of bike users riding on bicycle facilities. Virtual Reality (VR) technique was introduced to obtain data by creating 120 immersive settings or scenarios for participants. A hundred of bicyclists or participants with a wide range of characters were recruited. These participants were asked to express their senses of satisfaction under predefined physical conditions of bike facilities and traffic conditions. Their Satisfaction Rating Scores (SRS) were documented. The statistical relationships between rider’s feelings and bike facilities/traffic conditions were modeled and verified through a symbolic regression (or an effective deep learning) approach. The model is demonstrated to be reliable in predicting SRS of bicyclists with a high correlation coefficient. This study also developed a set of LOS criteria based on the cumulative distribution of satisfaction scores. These LOS criteria are simple to use and effective in assessing operational performance of existing bicycle facilities and providing decision makers with insightful guidance for planning, designing, and operating new active transportation facilities.}
}
@article{LU2013234,
title = {Hierarchical segmentation-assisted multimodal registration for MR brain images},
journal = {Computerized Medical Imaging and Graphics},
volume = {37},
number = {3},
pages = {234-244},
year = {2013},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2013.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0895611113000347},
author = {Huanxiang Lu and Roland Beisteiner and Lutz-Peter Nolte and Mauricio Reyes},
keywords = {Multimodal non-rigid registration, Tissue classification, EPI distortion correction},
abstract = {Information theory-based metric such as mutual information (MI) is widely used as similarity measurement for multimodal registration. Nevertheless, this metric may lead to matching ambiguity for non-rigid registration. Moreover, maximization of MI alone does not necessarily produce an optimal solution. In this paper, we propose a segmentation-assisted similarity metric based on point-wise mutual information (PMI). This similarity metric, termed SPMI, enhances the registration accuracy by considering tissue classification probabilities as prior information, which is generated from an expectation maximization (EM) algorithm. Diffeomorphic demons is then adopted as the registration model and is optimized in a hierarchical framework (H-SPMI) based on different levels of anatomical structure as prior knowledge. The proposed method is evaluated using Brainweb synthetic data and clinical fMRI images. Both qualitative and quantitative assessment were performed as well as a sensitivity analysis to the segmentation error. Compared to the pure intensity-based approaches which only maximize mutual information, we show that the proposed algorithm provides significantly better accuracy on both synthetic and clinical data.}
}
@article{LIANG201663,
title = {Improving the discrimination of hand motor imagery via virtual reality based visual guidance},
journal = {Computer Methods and Programs in Biomedicine},
volume = {132},
pages = {63-74},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301073},
author = {Shuang Liang and Kup-Sze Choi and Jing Qin and Wai-Man Pang and Qiong Wang and Pheng-Ann Heng},
keywords = {Brain–computer interface, Hand motor imagery, Visual guidance, Subject-specific frequency and time bands, Event-related desynchronization, Virtual reality},
abstract = {While research on the brain–computer interface (BCI) has been active in recent years, how to get high-quality electrical brain signals to accurately recognize human intentions for reliable communication and interaction is still a challenging task. The evidence has shown that visually guided motor imagery (MI) can modulate sensorimotor electroencephalographic (EEG) rhythms in humans, but how to design and implement efficient visual guidance during MI in order to produce better event-related desynchronization (ERD) patterns is still unclear. The aim of this paper is to investigate the effect of using object-oriented movements in a virtual environment as visual guidance on the modulation of sensorimotor EEG rhythms generated by hand MI. To improve the classification accuracy on MI, we further propose an algorithm to automatically extract subject-specific optimal frequency and time bands for the discrimination of ERD patterns produced by left and right hand MI. The experimental results show that the average classification accuracy of object-directed scenarios is much better than that of non-object-directed scenarios (76.87% vs. 69.66%). The result of the t-test measuring the difference between them is statistically significant (p = 0.0207). When compared to algorithms based on fixed frequency and time bands, contralateral dominant ERD patterns can be enhanced by using the subject-specific optimal frequency and the time bands obtained by our proposed algorithm. These findings have the potential to improve the efficacy and robustness of MI-based BCI applications.}
}
@article{PAES2017292,
title = {Immersive environment for improving the understanding of architectural 3D models: Comparing user spatial perception between immersive and traditional virtual reality systems},
journal = {Automation in Construction},
volume = {84},
pages = {292-303},
year = {2017},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517308361},
author = {Daniel Paes and Eduardo Arantes and Javier Irizarry},
keywords = {Virtual reality, Immersive environment, Design technology, Spatial perception, Human factors},
abstract = {The use of immersive virtual reality has been considered a beneficial strategy for architectural design. Many studies on this topic, however, avoid discussing the extent to which this technology offers better support to design practices compared to non-immersive virtual reality platforms. Nonetheless, this knowledge is precisely what would justify a wider adoption of immersive systems by the AEC industry. In this context, this study sought to quantitatively verify the ability of a particular immersive system in providing users with a better spatial understanding of the virtual mock-up. The method compares users' spatial perception using a conventional workstation versus an immersive platform. Findings indicate an overall better spatial perception of the virtual model when using the immersive environment. The study concludes that the immersive environment may benefit current design practices by improving professionals' understanding of the spatial arrangement of the virtual model.}
}
@article{ZAINAN2009325,
title = {Virtual Reality-based Teleoperation with Robustness Against Modeling Errors},
journal = {Chinese Journal of Aeronautics},
volume = {22},
number = {3},
pages = {325-333},
year = {2009},
issn = {1000-9361},
doi = {https://doi.org/10.1016/S1000-9361(08)60106-5},
url = {https://www.sciencedirect.com/science/article/pii/S1000936108601065},
author = {Jiang Zainan and Liu Hong and Wang Jie and Huang Jianbin},
keywords = {space robot, teleoperation, virtual reality, model error, visual recognition, compliance control},
abstract = {This article investigates virtual reality (VR)-based teleoperation with robustness against modeling errors. VR technology is an effective way to overcome the large time delay during space robot teleoperation. However, it depends highly on the accuracy of model. Model errors between the virtual and real environment exist inevitably. The existing way to deal with the problem is by means of either model matching or robot compliance control. As distinct from the existing methods, this article tries to combine model matching and robot compliance control. On one hand, the status of the virtual robot is corrected by using the position sensor data from robot joints before and during teleoperation, and the pose of the virtually manipulated object is obtained with visual recognition technology. On the other hand, compliance control algorithms of impedance control based on joint torque sensors and hybrid position/force control based on a wrist sensor have been executed in order to eliminate the small sustaining model errors. A VR-based teleoperation system of satellite on-orbit self-serving is built up. In order to verify the proposed method, an experiment deploying the solar panel troubled by malfunction is carried out through teleoperation. It shows that the large model errors are removed with the model matching method and the adopted compliance control is robust against the remaining small model errors.}
}
@article{DESENNEVILLE2021101834,
title = {Deep correction of breathing-related artifacts in real-time MR-thermometry},
journal = {Computerized Medical Imaging and Graphics},
volume = {87},
pages = {101834},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2020.101834},
url = {https://www.sciencedirect.com/science/article/pii/S0895611120301294},
author = {B. Denis {de Senneville} and P. Coupé and M. Ries and L. Facq and C.T.W. Moonen},
keywords = {Interventional procedures, MR-thermometry, Motion artifacts, Deep neural network, Real-time systems},
abstract = {Real-time MR-imaging has been clinically adapted for monitoring thermal therapies since it can provide on-the-fly temperature maps simultaneously with anatomical information. However, proton resonance frequency based thermometry of moving targets remains challenging since temperature artifacts are induced by the respiratory as well as physiological motion. If left uncorrected, these artifacts lead to severe errors in temperature estimates and impair therapy guidance. In this study, we evaluated deep learning for on-line correction of motion related errors in abdominal MR-thermometry. For this, a convolutional neural network (CNN) was designed to learn the apparent temperature perturbation from images acquired during a preparative learning stage prior to hyperthermia. The input of the designed CNN is the most recent magnitude image and no surrogate of motion is needed. During the subsequent hyperthermia procedure, the recent magnitude image is used as an input for the CNN-model in order to generate an on-line correction for the current temperature map. The method's artifact suppression performance was evaluated on 12 free breathing volunteers and was found robust and artifact-free in all examined cases. Furthermore, thermometric precision and accuracy was assessed for in vivo ablation using high intensity focused ultrasound. All calculations involved at the different stages of the proposed workflow were designed to be compatible with the clinical time constraints of a therapeutic procedure.}
}
@article{GALVANDEBARBA2018142,
title = {Self-attribution of distorted reaching movements in immersive virtual reality},
journal = {Computers & Graphics},
volume = {76},
pages = {142-152},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301353},
author = {Henrique {Galvan Debarba} and Ronan Boulic and Roy Salomon and Olaf Blanke and Bruno Herbelin},
keywords = {Virtual reality, Self-attribution, Evaluation},
abstract = {This study explores the extent to which individuals embodied in Virtual Reality tend to self-attribute the movements of their avatar. More specifically, we tested subjects performing goal-directed movements and distorted the mapping between user and avatar movements by decreasing or increasing the amplitude of the avatar hand movement required to reach for a target, while maintaining the apparent amplitude – visual distance – fixed. In two experiments, we asked subjects to report whether the movement that they have seen matched the movement that they have performed, or asked them to classify whether a distortion was making the task easier or harder to complete. Our results show that subjects perform poorly in detecting discrepancies when the nature of the distortion is not made explicit and that subjects are biased to self-attributing distorted movements that make the task easier. These findings, in line with previous accounts on the sense of agency, demonstrate the flexibility of avatar embodiment and open new perspectives for the design of guided interactions in Virtual Reality.}
}
@article{DAS2020103017,
title = {Assessing mental workload in virtual reality based EOT crane operations: A multi-measure approach},
journal = {International Journal of Industrial Ergonomics},
volume = {80},
pages = {103017},
year = {2020},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2020.103017},
url = {https://www.sciencedirect.com/science/article/pii/S0169814120302560},
author = {Souvik Das and J. Maiti and O.B. Krishna},
keywords = {EOT crane, Eye tracking, Virtual reality, NASA-TLX, Repeated measure ANOVA},
abstract = {Background
Eye-movement metrics and subjective workload measures are extensively used to determine mental workload of participants. The aim of this study was to assess Electric overhead travelling (EOT) crane operators’ mental workload variability based on eye movement metrics such as fixation frequency, fixation duration, saccade duration, saccade amplitude, and fixation/saccade ratio during EOT crane operations in virtual reality (VR) based EOT crane simulator.
Methods
A 2k (k = 3) factorial experiment with factors namely, hazardous scenario, activity level, and trial was designed and conducted to demonstrate the proposed assessment approach. Throughout the experiment, we recorded the eye movements of 12 EOT crane operators of a steel industry of authors’ country. Post experiment, the National Aeronautics and Space Administration task load index (NASA-TLX) was adopted as a subjective workload measure and run time of task completion was recorded. Eye-movement metrics, subjective workload measure, run time were tested with multivariate analysis of variance (MANOVA), and three way repeated measure analysis of variance (ANOVA).
Results
At the level of α = 0.05, the experimental factors significantly influence the means of eye movement metrics, subjective ratings and run time. There was also significant influence among their interactions. A positive correlation was also found for eye movements metrics with NASA-TLX and run time.
Conclusions
Eye movement metrics help in understanding the mental workload of participants unobtrusively and continuously. Analysis of subjective workload measure and run time along with eye-gaze analysis provide a deeper understanding on the pattern of mental workload.}
}
@article{HUTTINGA2023102843,
title = {Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy},
journal = {Medical Image Analysis},
volume = {88},
pages = {102843},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102843},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523001032},
author = {Niek R.F. Huttinga and Tom Bruijnen and Cornelis A.T. {van den Berg} and Alessandro Sbrizzi},
keywords = {Respiratory motion, MR-guided radiotherapy, MR-linac, Real-time, Motion estimation, Uncertainty estimation, Gaussian Processes},
abstract = {Respiratory motion during radiotherapy causes uncertainty in the tumor’s location, which is typically addressed by an increased radiation area and a decreased dose. As a result, the treatments’ efficacy is reduced. The recently proposed hybrid MR-linac scanner holds the promise to efficiently deal with such respiratory motion through real-time adaptive MR-guided radiotherapy (MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the radiotherapy plan should be adapted in real-time according to the estimated motion-fields. All of this should be performed with a total latency of maximally 200 ms, including data acquisition and reconstruction. A measure of confidence in such estimated motion-fields is highly desirable, for instance to ensure the patient’s safety in case of unexpected and undesirable motion. In this work, we propose a framework based on Gaussian Processes to infer 3D motion-fields and uncertainty maps in real-time from only three readouts of MR-data. We demonstrated an inference frame rate up to 69 Hz including data acquisition and reconstruction, thereby exploiting the limited amount of required MR-data. Additionally, we designed a rejection criterion based on the motion-field uncertainty maps to demonstrate the framework’s potential for quality assurance. The framework was validated in silico and in vivo on healthy volunteer data (n=5) acquired using an MR-linac, thereby taking into account different breathing patterns and controlled bulk motion. Results indicate end-point-errors with a 75th percentile below 1 mm in silico, and a correct detection of erroneous motion estimates with the rejection criterion. Altogether, the results show the potential of the framework for application in real-time MR-guided radiotherapy with an MR-linac.}
}
@article{CHENG2023100767,
title = {Does the AR-HUD system affect driving behaviour? An eye-tracking experiment study},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {18},
pages = {100767},
year = {2023},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2023.100767},
url = {https://www.sciencedirect.com/science/article/pii/S2590198223000143},
author = {Yu-nuo Cheng and Xia Zhong and Li-wei Tian},
keywords = {AR-HUD, Driving behaviour, Risk driving, Eye movement, Area of interest},
abstract = {In recent years, the continuous development of in-vehicle information systems (IVIS) has greatly enriched the driving experience, while also occupying the driver’s cognitive resources to varying degrees, causing driving distraction. Therefore, studying the influence of the AR-HUD (Augmented Reality Head-up Display, AR-HUD) system on driving behaviour is of great significance for the in-vehicle information system to manage the complexity of information and enhance driving safety. This experiment applied five typical risky driving scenarios under two lighting conditions (daytime and night), as stimulus materials. The effects of eye movement behaviour and risk reaction time indicators during driving were comprehensively analysed. The experiment results showed that the AR-HUD system can significantly improve the subjects' attention to risky AOIs (Area of Interest, AOI) in night driving situations, as well as reduce the difficulty of processing information in risky driving scenarios, thus reducing cognitive load; In terms of reaction time, the AR-HUD system can significantly reduce the driver's perception time for risky driving scenarios, thus responding to risky situations more quickly. The experiment conclusions verified the role of AR-HUD technology in improving driving safety and driving behaviour, and provide a new direction for the future development of in-vehicle information systems.}
}
@article{FU2020294,
title = {High-order Taylor expansion based image space transform method for real-time augmented reality},
journal = {Computer Communications},
volume = {153},
pages = {294-301},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419318559},
author = {Feiran Fu and Ming Fang and Huamin Yang and Zhe Li},
keywords = {Real-time augment reality, Space transformation, Taylor expansion, Tracking},
abstract = {This paper proposes a real-time augmented reality method based on Taylor expansion formula. This method has the advantage that the pixel relationship in the discrete image space is converted to a continuous high-order Taylor space and maintains pixel invariance. After conversion to Taylor space, the ability to resist dramatic changes in illumination between frames is enhanced and robust to intra-frame local illumination mutations. In the spatial transformation process, differential low-order features are used to represent higher-order features, multiplied by appropriate feature coefficients. These coefficients are first determined theoretically and then experimentally verified, allowing us to obtain high-order feature information and approximate the original pixel values based on the features. We then applied this technique to color-based mean shift tracking problems to achieve promising results.}
}
@article{LOEVE201638,
title = {Workflow and intervention times of MR-guided focused ultrasound – Predicting the impact of new techniques},
journal = {Journal of Biomedical Informatics},
volume = {60},
pages = {38-48},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416000022},
author = {Arjo J. Loeve and Jumana Al-Issawi and Fabiola Fernandez-Gutiérrez and Thomas Langø and Jan Strehlow and Sabrina Haase and Matthias Matzko and Alessandro Napoli and Andreas Melzer and Jenny Dankelman},
keywords = {Workflow analysis, Focused ultrasound surgery, Workflow simulation, MRgFUS, High intensity focused ultrasound},
abstract = {Magnetic resonance guided focused ultrasound surgery (MRgFUS) has become an attractive, non-invasive treatment for benign and malignant tumours, and offers specific benefits for poorly accessible locations in the liver. However, the presence of the ribcage and the occurrence of liver motion due to respiration limit the applicability MRgFUS. Several techniques are being developed to address these issues or to decrease treatment times in other ways. However, the potential benefit of such improvements has not been quantified. In this research, the detailed workflow of current MRgFUS procedures was determined qualitatively and quantitatively by using observation studies on uterine MRgFUS interventions, and the bottlenecks in MRgFUS were identified. A validated simulation model based on discrete events simulation was developed to quantitatively predict the effect of new technological developments on the intervention duration of MRgFUS on the liver. During the observation studies, the duration and occurrence frequencies of all actions and decisions in the MRgFUS workflow were registered, as were the occurrence frequencies of motion detections and intervention halts. The observation results show that current MRgFUS uterine interventions take on average 213min. Organ motion was detected on average 2.9 times per intervention, of which on average 1.0 actually caused a need for rework. Nevertheless, these motion occurrences and the actions required to continue after their detection consumed on average 11% and up to 29% of the total intervention duration. The simulation results suggest that, depending on the motion occurrence frequency, the addition of new technology to automate currently manual MRgFUS tasks and motion compensation could potentially reduce the intervention durations by 98.4% (from 256h 5min to 4h 4min) in the case of 90% motion occurrence, and with 24% (from 5h 19min to 4h 2min) in the case of no motion. In conclusion, new tools were developed to predict how intervention durations will be affected by future workflow changes and by the introduction of new technology.}
}
@article{TSAI2001333,
title = {Virtual reality orthopedic surgery simulator},
journal = {Computers in Biology and Medicine},
volume = {31},
number = {5},
pages = {333-351},
year = {2001},
issn = {0010-4825},
doi = {https://doi.org/10.1016/S0010-4825(01)00014-2},
url = {https://www.sciencedirect.com/science/article/pii/S0010482501000142},
author = {Ming-Dar Tsai and Ming-Shium Hsieh and Shyan-Bin Jou},
keywords = {Volume visualization, Volume manipulation, Orthopedic surgical simulation, Preoperative rehearsal and training, Arthroplasty},
abstract = {This paper describes a highly interactive virtual reality orthopedic surgery simulator. The simulator allows surgeons to use various surgical instruments to operate on virtual rigid anatomic structures, such bones, prostheses and bone grafts, to simulate every procedure on the rigid structures for complex orthopedic surgeries, including arthroplasty, corrective or open osteotomy, open reduction of fractures and amputation. A comparative study of the simulator with paper simulation was performed and showed that interns and residents found the simulator to be a useful learning tool, and that visiting doctors could use it effectively for planning verification and rehearsal of operations.}
}
@article{CHU2017241,
title = {Registration and fusion quantification of augmented reality based nasal endoscopic surgery},
journal = {Medical Image Analysis},
volume = {42},
pages = {241-256},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517301263},
author = {Yakui Chu and Jian Yang and Shaodong Ma and Danni Ai and Wenjie Li and Hong Song and Liang Li and Duanduan Chen and Lei Chen and Yongtian Wang},
keywords = {Endoscope, Image-guided surgery, Augmented reality, Image registration, Fusion, Optical tracking},
abstract = {This paper quantifies the registration and fusion display errors of augmented reality-based nasal endoscopic surgery (ARNES). We comparatively investigated the spatial calibration process for front-end endoscopy and redefined the accuracy level of a calibrated endoscope by using a calibration tool with improved structural reliability. We also studied how registration accuracy was combined with the number and distribution of the deployed fiducial points (FPs) for positioning and the measured registration time. A physically integrated ARNES prototype was customarily configured for performance evaluation in skull base tumor resection surgery with an innovative approach of dynamic endoscopic vision expansion. As advised by surgical experts in otolaryngology, we proposed a hierarchical rendering scheme to properly adapt the fused images with the required visual sensation. By constraining the rendered sight in a known depth and radius, the visual focus of the surgeon can be induced only on the anticipated critical anatomies and vessel structures to avoid misguidance. Furthermore, error analysis was conducted to examine the feasibility of hybrid optical tracking based on point cloud, which was proposed in our previous work as an in-surgery registration solution. Measured results indicated that the error of target registration for ARNES can be reduced to 0.77 ± 0.07 mm. For initial registration, our results suggest that a trade-off for a new minimal time of registration can be reached when the distribution of five FPs is considered. For in-surgery registration, our findings reveal that the intrinsic registration error is a major cause of performance loss. Rigid model and cadaver experiments confirmed that the scenic integration and display fluency of ARNES are smooth, as demonstrated by three clinical trials that surpassed practicality.}
}
@article{QIN2021101423,
title = {Impact of information display on worker performance for wood frame wall assembly using AR HMD under different task conditions},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101423},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101423},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001750},
author = {Y. Qin and E. Bloomquist and T. Bulbul and J. Gabbard and K. Tanous},
keywords = {Augmented Reality (AR), Head-Mounted Display (HMD), Information display, Wood frame assembly},
abstract = {Augmented Reality (AR) head-mounted displays (HMDs) provide users with an immersive virtual experience in the real world. The portability of this technology affords various information display options for construction workers that are not possible otherwise. However, the impact of these different information presentation options on human performance should be carefully evaluated before such technology is deployed in the jobsite. In this paper, we describe a research effort examining how different information displays presented via AR HMD influence task performance when assembling three sized wooden wall frame assembly tasks. We asked 18 construction engineering students with framing experience to finish three wood frame assembly tasks (large, medium, and small) using one of the three information displays (AR 3D conformal, AR 2D tag-along, and paper blueprints). The task performance was measured by time of completion and framing errors, which were analyzed and compared among each factor.}
}
@article{RASOOL2014327,
title = {Surface myoelectric signal classification using the AR-GARCH model},
journal = {Biomedical Signal Processing and Control},
volume = {13},
pages = {327-336},
year = {2014},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1746809414000883},
author = {Ghulam Rasool and Nidhal Bouaynaya and Kamran Iqbal and Gannon White},
keywords = {Myoelectric signal, Autoregressive (AR) model, Heteroscedasticity, Autoregressive-autoregressive generalized conditional heteroscedastic (AR-GARCH) model, Myoelectric control},
abstract = {In myoelectric prostheses design, it is normally assumed that the necessary control information can be extracted from the surface myoelectric signals. In the pattern classification paradigm for controlling myoelectric prosthesis, the autoregressive (AR) model coefficients are generally considered an efficient and robust feature set. However, no formal statistical methodologies or tests are reported in the literature to analyze and model the myoelectric signal as an AR process. We analyzed the myoelectric signal as a stochastic time-series and found that the signal is heteroscedastic, i.e., the AR modeling residuals exhibit a time-varying variance. Heteroscedasticity is a major concern in statistical modeling because it can invalidate statistical tests of significance which may assume that the modeling errors are uncorrelated and that the error variances do not vary with the effects being modeled. We subsequently proposed to model the myoelectric signal as an autoregressive-generalized autoregressive conditional heteroscedastic (AR-GARCH) process and used the model parameters as a feature set for signal classification. Multiple statistical tests including the Ljung–Box Q-test, Engle's test for heteroscedasticity, the Kolmogorov–Smirnov test and the goodness of fit test were performed to show the validity of the proposed model. Our experimental results show that the proposed AR-GARCH model coefficients, when used as a feature set in two different classification schemes, significantly outperformed (p<.01) the conventional AR model coefficients.}
}
@article{CHOI2023105115,
title = {Forecasting personal learning performance in virtual reality-based construction safety training using biometric responses},
journal = {Automation in Construction},
volume = {156},
pages = {105115},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105115},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003758},
author = {Dajeong Choi and Seungwon Seo and Hyunsoo Park and Taehoon Hong and Choongwan Koo},
keywords = {VR-based construction safety training, Learning performance, Forecast model, Biometric response, Machine learning algorithm, Construction worker},
abstract = {During virtual reality-based safety training, it is necessary to immediately and objectively evaluate personal learning performance. In light of this, this study proposed an interpretable machine learning approach for forecasting personal learning performance in VR-based construction safety training using real-time biometric responses. During VR-based safety training (‘fall accidents on scaffolding’), eye-tracking and EEG data were collected in real time from 30 participants (i.e., construction workers). The main findings can be summarized as follows. Compared to the full forecast model (FM), the support vector regression algorithm of the simplified forecast model (SM), which considers only principal features as independent variables, demonstrated better prediction performance (i.e., accuracy improvement: 0.087 of mean absolute error, overfitting: one-third level of the FM). This study creates new ground in the field of personalized safety training by enabling real-time monitoring and diagnosis for the cognitive states (i.e., learning performance) of construction workers during VR-based construction safety training.}
}
@article{SENKAL2011951,
title = {Haptic joystick with hybrid actuator using air muscles and spherical MR-brake},
journal = {Mechatronics},
volume = {21},
number = {6},
pages = {951-960},
year = {2011},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957415811000468},
author = {Doruk Senkal and Hakan Gurocak},
keywords = {Air muscle, Magnetorheological fluid, MRF brake, Hybrid actuation, Multi-DOF actuator, Haptics, Force feedback, Inertial measurement unit, Joystick},
abstract = {In this research, a new 2-DOF hybrid actuator concept is explored as a powerful and compact alternative to conventional haptic actuators. The actuator combines a spherical MR-brake and three air muscles and is integrated into a joystick that can apply forces in two degrees-of-freedom. The air muscles are used to create high active forces in a compact volume and the brake compensates for the “spongy” feeling associated with air muscles. To decrease the overall size of the system an inertial measurement unit has been implemented as a position measurement solution. As high as 16N of total force output could be achieved at the tip of the joystick. Also, up to 16 times improvement in the stable virtual wall stiffness was obtained when the MR-brake was used to compensate for force errors. Experiments with an impedance-based haptic controller with force-feedback gave satisfactory wall following performance. This device can be employed in applications including computer games, military or medical training applications, rehabilitation and in teleoperation of equipment where high force feedback in 2-DOF in a compact work volume may be desirable while interacting with rigid or elastic virtual objects.}
}
@article{SCHOUTERDEN2019167,
title = {Development of a membrane-shaped MR-based composite draping tool},
journal = {Procedia CIRP},
volume = {86},
pages = {167-172},
year = {2019},
note = {7th CIRP Global Web Conference – Towards shifted production value stream patterns through inference of data, models, and technology (CIRPe 2019)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120300627},
author = {Gert Schouterden and Jeroen Cramer and Eric Demeester and Karel Kellens},
keywords = {Dry fibre sheets, Composite, Draping},
abstract = {Nowadays, the manufacturing process of composite parts is still dominated by a high level of cost-intensive manual tasks which impedes the use of these materials in composite processing SMEs as well as in high-end automotive and aerospace industries. The draping of fibre sheets is often still carried out manually because of the difficult handling properties, the high variety and complexity of the materials and the product contours. Even the current manipulation tools, for composite draping or preforming in moulds, often lack controllability and flexibility to cope with a high mix of features in the composite product. In automation, these problems are often answered by cost-increasing multi-robotic systems, using multiple degrees of freedom in combination with a large range of feature-specific tools. The need for rather simple, cost-effective manipulation tools triggered the development of a membrane-shaped magnetorheological (MR) based composite draping tool. First, a fluid-filled bag will cover the mould entirely and will perform the initial forming of the composite material whilst both preventing the creation of wrinkles and securing the readily draped areas during further processing. Subsequently, by applying local pressure on the fibre sheet through magnetic activation, draping in narrow or corner-like features will be enhanced. Furthermore, slippage of fibrous material between the membrane and the mould surface can be mastered during forming, facilitating the control of fibre orientation and shear angles. First experimental tests indicate that this technique shows large potential to enable full automation of the entire draping process by the flexible use of a single robot arm and multitool whilst being product and feature independent.}
}
@article{ZHANG2021103854,
title = {Virtual reality supported interactive tower crane layout planning for high-rise modular integrated construction},
journal = {Automation in Construction},
volume = {130},
pages = {103854},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103854},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003058},
author = {Zhiqian Zhang and Wei Pan},
keywords = {Tower crane layout planning (TCLP), Virtual reality (VR), Human-in-the-loop simulation, High-rise building, Modular integrated construction (MiC)},
abstract = {Tower crane layout planning (TCLP) is critical to achieving safe and efficient module installation for high-rise modular integrated construction (MiC). Nevertheless, the existing TCLP methods are inefficient in addressing the multiple heavy lifts in high-rise MiC. Therefore, this paper aims to develop an innovative virtual reality (VR) tool to support main contractors in selecting the optimal tower crane layout plan for high-rise MiC. The paper adopts the design science methodology for tool development and evaluation. The tool is designed with three functional components, i.e., crane layout generation with real-time feasibility checking, layout selection using multi-criteria performance evaluation, and interactive crane-lift simulation. Using a real-life high-rise MiC project, the tool is proved to be innovative in interactive human-in-the-loop planning, effective in determining the safe and efficient crane layout, and efficient in use. The work described integrates human-in-the-loop simulation and multi-criteria decision-making via VR technologies.}
}
@article{BRUNNSTROM2020116005,
title = {Latency impact on Quality of Experience in a virtual reality simulator for remote control of machines},
journal = {Signal Processing: Image Communication},
volume = {89},
pages = {116005},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.116005},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520301648},
author = {Kjell Brunnström and Elijs Dima and Tahir Qureshi and Mathias Johanson and Mattias Andersson and Mårten Sjöström},
keywords = {Quality of Experience (QoE), Virtual reality, Latency, Head-Mounted Displays (HMD), Forestry crane},
abstract = {In this article, we have investigated a VR simulator of a forestry crane used for loading logs onto a truck. We have mainly studied the Quality of Experience (QoE) aspects that may be relevant for task completion, and whether there are any discomfort related symptoms experienced during the task execution. QoE experiments were designed to capture the general subjective experience of using the simulator, and to study task performance. The focus was to study the effects of latency on the subjective experience, with regards to delays in the crane control interface. Subjective studies were performed with controlled delays added to the display update and hand controller (joystick) signals. The added delays ranged from 0 to 30 ms for the display update, and from 0 to 800 ms for the hand controller. We found a strong effect on latency in the display update and a significant negative effect for 800 ms added delay on latency in the hand controller (in total approx. 880 ms latency including the system delay). The Simulator Sickness Questionnaire (SSQ) gave significantly higher scores after the experiment compared to before the experiment, but a majority of the participants reported experiencing only minor symptoms. Some test subjects ceased the test before finishing due to their symptoms, particularly due to the added latency in the display update.}
}
@article{GINTERS201380,
title = {Markerless Outdoor AR-RFID Solution for Logistics},
journal = {Procedia Computer Science},
volume = {25},
pages = {80-89},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012155},
author = {Egils Ginters and Arnis Cirulis and Gatis Blums},
keywords = {Augmented reality (AR), Radio-frequency identification (RFID), logistics, markerless tracking},
abstract = {The main objective of this paper is to describe the logistics processes improvement by use of AR (augmented reality) and hybrid RFID (Radio-frequency identification) technologies in outdoor environment. Augmented reality and RFID nowadays are well known and widespread technologies. It is possible to achieve functional and perspective system by merging these technologies together. The paper provides theoretical characteristics of logistics, RFID, AR technologies and offers theoretical model for idea implementation and approbation. Model depicts markerless AR-RFID solution for outdoor object tracking and 3D model visualization. Also physical structure and basic calculations are offered to provide necessary fundamentals for development of test platform and pilot product. Provision of outdoor object tracking and 3D model visualization may significantly impact and improve logistics processes in varies of fields.}
}
@article{ELWIN2022103712,
title = {Ar-HGSO: Autoregressive-Henry Gas Sailfish Optimization enabled deep learning model for diabetic retinopathy detection and severity level classification},
journal = {Biomedical Signal Processing and Control},
volume = {77},
pages = {103712},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103712},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422002348},
author = {J. Granty Regina Elwin and Jyothi Mandala and Balajee Maram and R. Ramesh Kumar},
keywords = {Diabetic retinopathy, Entropy Weighted and kernalized Power k-Means Clustering, Deep Convolutional Neural Network, Shepard Convolutional Neural network, Henry Gas Solubility Optimization model},
abstract = {Diabetic Retinopathy (DR) is one the most important problems of diabetics and it directs to the main cause of blindness. When proper treatment is afforded for DR patients, almost 90% of patients are protected from visual damage. DR does not produce any symptoms at the initial phase of the disease, thus various physical assessments, namely pupil dilation, visual acuity test, and so on are needed for DR disease detection. It is more complex to detect the DR during manual testing, because of the variations and complications of DR. The early detection and appropriate treatment assist to prevent vision loss for DR patients. Thus, it is very indispensable to categorize the levels and severity of DR for recommendation of essential treatment. In this paper, Autoregressive-Henry Gas Sailfish Optimization (Ar-HGSO)-based deep learning technique is proposed for DR detection and severity level classification of DR and Macular Edema (ME) based on color fundus images. The segmentation process is more essential for proper detection and classification process, which segments the image into various subgroups. The Deep Learning approach is utilized for effective identification of DR and severity classification of DR and ME. Moreover, the deep learning technique is trained by the designed Ar-HGSO scheme for obtaining better performance. The performance of the devised technique is evaluated using the IDRID dataset and DDR dataset. The introduced Ar-HGSO-based deep learning approach obtained better performance than other existing DR detection and classification techniques with regards to testing accuracy, sensitivity, and specificity of 0.9142, 0.9254, and 0.9142 using the IDRID dataset.}
}
@article{KUNNEN202015,
title = {System-based concept for a mixed reality supported maintenance phase of an industrial plant},
journal = {Procedia CIRP},
volume = {91},
pages = {15-20},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.096},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307782},
author = {Steffen Kunnen and Dmytro Adamenko and Robin Pluhnau and André Loibl and Arun Nagarajah},
keywords = {System Design, Extended Reality, Maintenance, Digital Twin},
abstract = {Inspection and maintenance processes have been researched for many years. However, the possible applications of extended reality in these processes is a new field of research that has to be developed. Within current paper an approach to optimize the existing, analogously documented processes by using extended reality is presented. The maintenance of an industrial plant is mainly based on analogue and manual methods and processes. Usually all maintenance information is documented analogously and transferred to the corresponding management systems with a certain delay after the inspection. The time delay can lead to data loss and errors and can also occur during transmission to an appropriate system. In the future, the maintenance data and changes to the real object should be determined digital and transferred to the data management system in real time. A digital twin should enable the unambiguous assignment of the data to the corresponding elements and thus represent a virtual representation of the AS-IS state of the industrial plant. The required process data will be collected in Mixed Reality and then transferred to a PDM-System. To transfer the data records without loss, a system model can be used as an interface between the Microsoft HoloLens mixed reality device and the PDM system. The HoloLens enables the determination of a coordinate point from the combination of gaze vector and the position of the observer. The coupled system model allows this coordinate point to be compared with a coordinate field within the reference model to determine the component. This paper presents a concept to create a system-based digital twin for the maintenance phase of an industrial plant to enable the data transmission from mixed reality to a PDM-System.}
}
@article{SAMPAIO2016894,
title = {Pedagogical Strategies for the Integration of Augmented Reality in ICT Teaching and Learning Processes},
journal = {Procedia Computer Science},
volume = {100},
pages = {894-899},
year = {2016},
note = {International Conference on ENTERprise Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.240},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324097},
author = {Daniel Sampaio and Pedro Almeida},
keywords = {Information and Communication Technologies, Educational Technologies, Augmented Reality, Digital skills, Traching/Learning strategies, Google Cardbords},
abstract = {The technologies that are being gradually introduced in educational contexts enable students to diversify the ways for knowledge building. However, the exploitation of new technologies in the classroom is always a challenge for all interveners in the educational process. The arrival of a new technology, as is the case of augmented reality devices, captures the teachers’ attention. It creates the expectation that its uses may provide students with new ways to interact, new possibilities of collaboration between students and between students and teachers and potentially an increase in the motivation for learning. In this context we may find some examples of this technological introduction in learning scenarios. But the referred expectations and the most suitable strategies for its use need to be validated and new ones to be explored. This is the purpose of this research. This project is being carried with sixty-two students from the 8th grade in the Information and Communication Technologies subject. It follows an action-research methodology and for the implementation of the several research cycles that follows this methodology different prototypes for students to use in classroom were created and are being evaluated. The prototypes range from simple technological integrations to more complex ones with the introduction of an augmented reality system. The study aims to identify and explore the pedagogical strategies by evaluating them in real teaching and learning scenarios, particularly in terms of competences developed and student motivation levels, as well as on the ways of integrating different devices for augmented reality. The preliminary results show that students understand the support and contents provided by the prototypes and feel higher motivation when using it in the assignments’.}
}
@article{LAMATA2007273,
title = {SINERGIA laparoscopic virtual reality simulator: Didactic design and technical development},
journal = {Computer Methods and Programs in Biomedicine},
volume = {85},
number = {3},
pages = {273-283},
year = {2007},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2006.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260706002975},
author = {Pablo Lamata and Enrique J. Gómez and Francisco M. Sánchez-Margallo and Óscar López and Carlos Monserrat and Verónica García and Carlos Alberola and Miguel Ángel {Rodríguez Florido} and Juan Ruiz and Jesús Usón},
keywords = {Laparoscopy, Surgical training, Virtual reality, Simulation design, Biomechanical model, Collision detection and handling},
abstract = {VR laparoscopic simulators have demonstrated its validity in recent studies, and research should be directed towards a high training effectiveness and efficacy. In this direction, an insight into simulators’ didactic design and technical development is provided, by describing the methodology followed in the building of the SINERGIA simulator. It departs from a clear analysis of training needs driven by a surgical training curriculum. Existing solutions and validation studies are an important reference for the definition of specifications, which are described with a suitable use of simulation technologies. Five new didactic exercises are proposed to train some of the basic laparoscopic skills. Simulator construction has required existing algorithms and the development of a particle-based biomechanical model, called PARSYS, and a collision handling solution based in a multi-point strategy. The resulting VR laparoscopic simulator includes new exercises and enhanced simulation technologies, and is finding a very good acceptance among surgeons.}
}
@article{XIE2023116955,
title = {Generalized Kelvin-Voigt viscoelastic modeling and numerical study of Free-Damped vibrations in MR elastomer reinforced with graphene platelets},
journal = {Engineering Structures},
volume = {296},
pages = {116955},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116955},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623013706},
author = {Maoqing Xie and Yunhe Zou and Artin Hozuri},
keywords = {GPL reinforced MR elastomer, Nonlinear regression analysis, Nonlinear least squares fitting, Generalized Kelvin-Voigt viscoelastic model, Third-order shear deformation theory (TSDT)},
abstract = {This study focuses on the modeling and numerical analysis of a novel composite plate, which consists of a laminated magnetorheological elastomer (MRE) reinforced with graphene platelets (GPLs). The investigation begins with the determination of the mechanical properties of the MRE, utilizing a modified generalized Kelvin-Voigt viscoelastic model. Through nonlinear regression analysis and the nonlinear least squares technique, the dependencies of the storage and loss modulus of the magnetorheological (MR) matrix are evaluated, considering factors such as the magnetic field, iron particles, and excitation frequency. The proposed model is validated by comparing the obtained results with existing experimental data from the literature, employing root mean square error and correlation coefficients as metrics of consistency. Next, the homogenization process is applied to the composite media, which involves integrating the MR elastomer matrix and GPL reinforcements using the Halpin-Tsai micromechanical approach. This procedure enables the extraction of effective material properties governing the behavior of the composite structure. The theoretical framework, encompassing third-order plate theory, linear elasticity, and viscoelasticity, is then employed to derive the dynamic equations of the composite plate, employing Hamilton's principle as a guiding principle. To solve the dynamic problem and obtain the complex frequencies characterizing the system, the generalized differential quadrature (GDQ) method is implemented. This numerical technique offers a robust and accurate solution approach, providing comprehensive insights into the vibrational behavior of the composite plate. The study conducts a thorough investigation, exploring the performance achieved by incorporating GPL reinforcements within the MRE matrix. Specifically, the effects of different volume fractions of iron particles and varying magnetic fields are comprehensively examined and analyzed. Through this comprehensive exploration, a profound understanding of the behavior of the composite plate and its response to external stimuli is attained. The findings highlight the potential advantages of integrating GPL reinforcements and offer valuable insights for optimizing the design and performance of such composite structures in practical applications. The research contributes to the advancement of knowledge in the field of composite materials and opens up new possibilities for innovative and high-performance structural designs.}
}
@article{PIECHOWSKI2020731,
title = {Virtual reality as training aid for manual spacecraft docking},
journal = {Acta Astronautica},
volume = {177},
pages = {731-736},
year = {2020},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2020.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0094576520305129},
author = {Sarah Piechowski and Willi Pustowalow and Michael Arz and Jörn Rittweger and Edwin Mulder and Oliver Tobias Wolf and Bernd Johannes and Jens Jordan},
keywords = {Manual docking, Skill acquisition, Virtual reality, Stereoscopy},
abstract = {The ability to manually dock a spacecraft to a space station can be crucial for astronauts during space missions. The computer-based self-learning program 6df is an abstract docking simulation for acquisition and maintenance of the underlying skill to control six degrees of freedom. One of the difficulties of this complex task is to construct a mental representation of the own position and orientation in space, based only on two-dimensional information. To facilitate this and possibly further improve the learning process, a new three-dimensional (3D) stereoscopic presentation of the program is tested. This study investigates whether there is faster learning progress with 3D presentation compared to standard 2D presentation. 24 participants of the Artificial Gravity Bed Rest Study with ESA (AGBRESA) participated in the 6df docking experiment. Each of them completed 20 training sessions which lasted approximately 45 min and were conducted twice a week. The learning program is self-sufficient and adapts itself to individual learning speed. Half of the participants were presented with an UNITY-based stereoscopic visualization of docking, whereas the other half used the standard 2D version of the learning program 6df. Learning progress was measured as the number of tasks needed to reach a target task. Results overall indicate a slightly faster learning progress when using 3D technology, but no long-term performance advantages. The small benefit might not justify the usage of costlier and operationally limiting 3D systems.}
}
@article{ZHANG2018549,
title = {Atlas-based reconstruction of high performance brain MR data},
journal = {Pattern Recognition},
volume = {76},
pages = {549-559},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S003132031730479X},
author = {Mingli Zhang and Christian Desrosiers and Caiming Zhang},
keywords = {Multi-subject MRI, Weighted TV, Sparse representation, ADMM},
abstract = {Image priors based on total variation (TV) and nonlocal patch similarity have shown to be powerful techniques for the reconstruction of magnetic resonance (MR) images from undersampled k-space measurements. However, due to the uniform regularization of gradients, standard TV approaches often over-smooth edges in the image, resulting in the loss of important details. This paper proposes a novel compressed sensing method which combines both external and internal information for the high-performance reconstruction of MRI data. A probabilistic atlas is used to model the spatial distribution of gradients that correspond to various anatomical structures in the image. This atlas is then employed to control the level of gradient regularization at each image location, within a weighted TV regularization prior. The proposed method also leverages the redundancy of nonlocal similar patches through a sparse representation model. Experiments on T1-weighted images from the ABIDE dataset show the proposed method to outperform state-of-the-art approaches, for different sampling rates and noise levels.}
}
@article{RIVERAFLOR2019641,
title = {Evaluation of Task Workload and Intrinsic Motivation in a Virtual Reality Simulator of Electric-Powered Wheelchairs},
journal = {Procedia Computer Science},
volume = {160},
pages = {641-646},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931734X},
author = {Hamilton Rivera-Flor and Kevin A. Hernandez-Ossa and Berthil Longo and Teodiano Bastos},
keywords = {Electric-Powered Wheelchair, Driving Training, Virtual Reality Simulator},
abstract = {For some people with severe physical disabilities, the immediate driving of an Electric-Powered Wheelchair (EPW) appears as a safety problem, which can be solved by the use of a virtual reality (VR) simulator for safe-driving learning purposes. There are several VR environment approaches in the literature applied to EPW driving training, including several tests that were performed to validate these simulators. This work evaluates the influence of different display devices on task workload and intrinsic motivation of participants, using the Simcadrom EPW Simulator developed at UFES/Brazil. Results from two qualitative tests: Intrinsic Motivation Inventory (IMI) and NASA Task Load Index (NASA-TLX) were compared for three displays (Head Mounted Display – HMD, desktop screen, and video projector). The results show that the HMD provided the highest usefulness score. On the other hand, the desktop screen reported the lowest task workload.}
}
@article{CORRADINI2006407,
title = {A VIRTUAL REALITY BASED TELELABORATORY FOR THE REMOTE LEARNING OF ROBOTICS},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {15},
pages = {407-412},
year = {2006},
note = {8th IFAC Symposium on Robot Control},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060906-3-IT-2910.00069},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016385482},
author = {M.L. CORRADINI and G. SAMMARCO},
keywords = {Virtual reality, Remote laboratory, Active Learning, Robotics},
abstract = {In the present work, a virtual reality system, used in the framework of a web-based educational tool, is presented. Such tool, called VRL, can be used as a support for active learning of robotics in a web-learning environment. VRL guarantees forms of flexible learning, and couples virtual reality experiments, obtained through the integration of Matlab and Shockwave technology, and remote lab experiments, in order to allow a preliminary verification of theoretical concepts without the risks of damaging the remote experimental device.}
}
@article{LEE2016488,
title = {Augmented reality technology combined with three-dimensional holography to train the mental rotation ability of older adults},
journal = {Computers in Human Behavior},
volume = {65},
pages = {488-500},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216306410},
author = {I-Jui Lee and Chien-Hsu Chen and Kuo-Ping Chang},
keywords = {Mental rotation, Augmented reality, 3D holography, Older adults},
abstract = {A decline in the cognitive ability of mental rotation causes a poor sense of spatial direction and environmental cognitive capacity. Currently, training tasks for the elderly thus affected are still presented in 2D form. However, clinical research indicates that this strategy generates a cognitive load that reduces the interest of the trainees and diminishes the effects of training. In contrast, augmented reality (AR) is a rising solution that effectively reduces cognitive load, improves the sense of spatial direction of the elderly, and helps increase interest in training. We recruited 28 elderly (age ≥ 65 years) for this study. Fourteen were randomly assigned to an active Intervention group and were given AR-based 3D hologram (AR-3DH) mental rotation training, and 14 were assigned to a group that used the traditional 2D model. Both groups took ABA-designed pre-and post-tests that required inferring the rotating shapes' states from standard mental rotation tasks. Change scores for the two groups were compared using error rates and reaction times as covariates. After six-week of training, the mental rotation ability of the Intervention group improved through the use of AR-3DH training system during the intervention phase. The practical and developmental implications of the findings are discussed.}
}
@article{MICHEL201896,
title = {Attitude estimation for indoor navigation and augmented reality with smartphones},
journal = {Pervasive and Mobile Computing},
volume = {46},
pages = {96-121},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217303371},
author = {Thibaud Michel and Pierre Genevès and Hassen Fourati and Nabil Layaïda},
keywords = {Attitude estimation, Smartphone, Inertial sensors, Augmented reality motions, Magnetic field, Perturbations, Benchmark},
abstract = {We investigate the precision of attitude estimation algorithms in the particular context of pedestrian navigation with commodity smartphones and their inertial/magnetic sensors. We report on an extensive comparison and experimental analysis of existing algorithms. We focus on typical motions of smartphones when carried by pedestrians. We use a precise ground truth obtained from a motion capture system. We test state-of-the-art and built-in attitude estimation techniques with several smartphones, in the presence of magnetic perturbations typically found in buildings. We discuss the obtained results, analyze advantages and limits of current technologies for attitude estimation in this context. Furthermore, we propose a new technique for limiting the impact of magnetic perturbations with any attitude estimation algorithm used in this context. We show how our technique compares and improves over previous works. A particular attention was paid to the study of attitude estimation in the context of augmented reality motions when using smartphones.}
}
@article{PENG2021106667,
title = {Study on the effect of Ar-containing work gas on the microstructure and tribological behavior of nanocrystalline diamond coatings},
journal = {Tribology International},
volume = {153},
pages = {106667},
year = {2021},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2020.106667},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X20304928},
author = {Jihua Peng and Chao Xiong and Jiacheng Liao and Jingwen Liao and Liangchuan Yuan and Liejun Li},
keywords = {Nanocrystalline diamond films, HFCVD, Ar addition, Microstructure, Tribological properties},
abstract = {Using a combined pretreatment of sand-blasting and two-step etching, nanocrystalline diamond coatings with primarily diamond crystallites and good adhesion were deposited onto carbide cement by tuning the Ar/H2 ratio in the hot filament chemical vapor deposition (HFCVD) work gas. The microstructure, bonding structure, surface morphology and roughness, and wear scar were characterized using a scanning electron microscope, an atomic force microscope, a white light interferometer, and a Raman spectrometer. The dry sliding behavior of the coating against Si3N4 was tested using a pin-on-disc instrument. It was found that the addition of Ar to the HFCVD work gas can enhance grain refinement and tune the morphology of the nanocrystalline coatings. The roughness was reduced from 87.7 nm for the coating prepared without Ar to 6.21 nm for the coating prepared with 90% Ar. With the addition of Ar, the friction coefficient and wear rate decrease monotonically. The wear rate of the coating prepared with 90% Ar was less than 5% that of the coating prepared without Ar, and the wear mechanism transforms from spallation to be limited by the stress-induced sp2 tribo-layer.}
}
@article{CAPUTO2018225,
title = {The Smart Pin: An effective tool for object manipulation in immersive virtual reality environments},
journal = {Computers & Graphics},
volume = {74},
pages = {225-233},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318300840},
author = {Fabio M. Caputo and Marco Emporio and Andrea Giachetti},
keywords = {Mid-air manipulation technique, Virtual reality, Hand tracking, User tests},
abstract = {In this paper, we present a novel method for object manipulation in immersive virtual environments. The proposed technique exploits a novel widget, called “Smart Pin”, to enable the user to select, translate, rotate and scale objects relying entirely on the positional tracking of a single hand. It is, therefore, suitable for several real-world applications where handheld devices are not available, the surrounding environmental conditions make orientation tracking hard, and the non-dominant hand may be involved in different tasks. We evaluated the method with users on two classical tasks such as detail search and docking, comparing it with a successful two-handed manipulation technique (Handlebar). The measured execution efficiency obtained with the two methods was similar, but most users preferred the Smart Pin for its gestural comfort and ease of use.}
}
@article{LLOPIS2020106553,
title = {Development of an auditory virtual reality system based on pre-computed B-format impulse responses for building design evaluation},
journal = {Building and Environment},
volume = {169},
pages = {106553},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.106553},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319307656},
author = {Hermes Sampedro Llopis and Finnur Pind and Cheol-Ho Jeong},
keywords = {Auditory virtual reality, Room acoustic simulations, Localization, Pre-computed impulse responses, Building design evaluation},
abstract = {This study presents an auditory virtual reality system which relies on pre-computed B-format impulse responses in a grid across the domain. Spatial information is encoded in the impulse responses by means of higher order Ambisonics and decoded to a virtual loudspeaker array, which follows the listener during run-time. The impulse responses are convolved with source signals, played back through the virtual loudspeaker array and synthesized for binaural headphone reproduction through head related transfer functions. This approach allows for a completely free movement and orientation of the listener in the virtual scene. Furthermore, it allows for the usage of highly accurate offline simulations of room impulse responses. The system is validated with two listening tests. First, the sound source localization performance in virtual reverberant rooms is tested while varying the order of Ambisonics, visuals and head movement. Second, the effects of the grid resolution on the perceived realism, sound source size and sound continuity are investigated. The results reveal that when using second order Ambisonics, together with visuals and allowing head movement, the localization error is very low, being less than one just-noticeable difference. Furthermore, when using a coarse grid, the perceptual sound source size is increased and spread out. Perceived realism and sound continuity are not significantly affected by the grid resolution. Two video files that show the proposed system in use are provided.}
}
@article{LI2021104864,
title = {Homography-based robust pose compensation and fusion imaging for augmented reality based endoscopic navigation system},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104864},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104864},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006582},
author = {Wenjie Li and Jingfan Fan and Shaowen Li and Zhaorui Tian and Danni Ai and Hong Song and Jian Yang},
keywords = {Augmented reality, Online calibration, Endoscope calibration, Camera pose compensation, Image-to-patient registration},
abstract = {Background
Augmented reality (AR) based fusion imaging in endoscopic surgeries rely on the quality of image-to-patient registration and camera calibration, and these two offline steps are usually performed independently to get the target transformation separately. The optimal solution can be obtained under independent conditions but may not be globally optimal. All residual errors will be accumulated and eventually lead to inaccurate AR fusion.
Methods
After a careful analysis of the principle of AR imaging, a robust online calibration framework was proposed for an endoscopic camera to enable accurate AR fusion. A 2D checkerboard-based homography estimation algorithm was proposed to estimate the local pose of the endoscopic camera, and the least square method was used to calculate the compensation matrix in combination with the optical tracking system.
Results
In comparison with conventional methods, the proposed compensation method improved the performance of AR fusion, which reduced physical error by up to 82%, reduced pixel error by up to 83%, and improved target coverage by up to 6%. Experimental results of simulating mechanical noise revealed that the proposed compensation method effectively corrected the fusion errors caused by the rotation of the endoscopic tube without recalibrating the camera. Furthermore, the simulation results revealed the robustness of the proposed compensation method to noises.
Conclusions
Overall, the experiment results proved the effectiveness of the proposed compensation method and online calibration framework, and revealed a considerable potential in clinical practice.}
}
@article{VANBENTHEM2021103169,
title = {A virtual reality cognitive health screening tool for aviation: Managing accident risk for older pilots},
journal = {International Journal of Industrial Ergonomics},
volume = {85},
pages = {103169},
year = {2021},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2021.103169},
url = {https://www.sciencedirect.com/science/article/pii/S0169814121000871},
author = {Kathleen {Van Benthem} and Chris M. Herdman},
keywords = {Virtual reality, Cognitive health, Older adults, General aviation, Pilots, Risk identification},
abstract = {To address elevated risk for older pilots, we examined the efficacy of a virtual reality (VR) cognitive health screening tool (integrated into simulated flight scenarios) in identifying general aviation pilots who experienced a critical incident during flight in a full-scale Cessna 172 simulator. Performance data were obtained from 51 certified pilots (17–71 years). Machine learning classification algorithms, based on key data from the VR flight, were used to validate the utility of the screening tool for identifying pilot risk. The results showed that aviation-relevant cognitive factors obtained in the VR screening tool, including situation awareness and prospective memory, predicted risk of a critical incident with good sensitivity (0.83) and specificity (0.85), AUC = 0.82. These results support VR-based cognitive screening to identify at-risk older pilots. The present findings inform procedures for optimizing safety and reducing critical incidents at any point in the pilot lifespan and are timely in view of the impending pilot workforce shortage.}
}
@article{CANESSA2014227,
title = {Calibrated depth and color cameras for accurate 3D interaction in a stereoscopic augmented reality environment},
journal = {Journal of Visual Communication and Image Representation},
volume = {25},
number = {1},
pages = {227-237},
year = {2014},
note = {Visual Understanding and Applications with RGB-D Cameras},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S104732031300031X},
author = {Andrea Canessa and Manuela Chessa and Agostino Gibaldi and Silvio P. Sabatini and Fabio Solari},
keywords = {RGB-D cameras, Human–computer interactions, Calibration and data pre-processing, Kinect device, Virtual reality, Spatially variant depth correction, Eyes′ tracking, Mixed reality},
abstract = {A Human–machine interaction system requires precise information about the user’s body position, in order to allow a natural 3D interaction in stereoscopic augmented reality environments, where real and virtual objects should coherently coexist. The diffusion of RGB-D sensors seems to provide an effective solution to such a problem. Nevertheless, the interaction with stereoscopic 3D environments, in particular in peripersonal space, requires a higher degree of precision. To this end, a reliable calibration of such sensors and an accurate estimation of the relative pose of different RGB-D and visualization devices are crucial. Here, robust and straightforward procedures to calibrate a RGB-D camera, to improve the accuracy of its 3D measurements, and to co-register different calibrated devices are proposed. Quantitative measures validate the proposed approach. Moreover, calibrated devices have been used in an augmented reality system, based on a dynamic stereoscopic rendering technique that needs accurate information about the observer’s eyes position.}
}
@article{OZKAYA2021104680,
title = {Brain-mimicking phantom for biomechanical validation of motion sensitive MR imaging techniques},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {122},
pages = {104680},
year = {2021},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2021.104680},
url = {https://www.sciencedirect.com/science/article/pii/S1751616121003556},
author = {E. Ozkaya and E.R. Triolo and F. Rezayaraghi and J. Abderezaei and W. Meinhold and K. Hong and A. Alipour and P. Kennedy and L. Fleysher and J. Ueda and P. Balchandani and M. Eriten and C.L. Johnson and Y. Yang and M. Kurt},
keywords = {Brain biomechanics, Brain-mimicking phantom, CINE imaging, MR elastography},
abstract = {Motion sensitive MR imaging techniques allow for the non-invasive evaluation of biological tissues by using different excitation schemes, including physiological/intrinsic motions caused by cardiac pulsation or respiration, and vibrations caused by an external actuator. The mechanical biomarkers extracted through these imaging techniques have been shown to hold diagnostic value for various neurological disorders and conditions. Amplified MRI (aMRI), a cardiac gated imaging technique, can help track and quantify low frequency intrinsic motion of the brain. As for high frequency actuation, the mechanical response of brain tissue can be measured by applying external high frequency actuation in combination with a motion sensitive MR imaging sequence called Magnetic Resonance Elastography (MRE). Due to the frequency-dependent behavior of brain mechanics, there is a need to develop brain phantom models that can mimic the broadband mechanical response of the brain in order to validate motion-sensitive MR imaging techniques. Here, we have designed a novel phantom test setup that enables both the low and high frequency responses of a brain-mimicking phantom to be captured, allowing for both aMRI and MRE imaging techniques to be applied on the same phantom model. This setup combines two different vibration sources: a pneumatic actuator, for low frequency/intrinsic motion (1 Hz) for use in aMRI, and a piezoelectric actuator for high frequency actuation (30–60 Hz) for use in MRE. Our results show that in MRE experiments performed from 30 Hz through 60 Hz, propagating shear waves attenuate faster at higher driving frequencies, consistent with results in the literature. Furthermore, actuator coupling has a substantial effect on wave amplitude, with weaker coupling causing lower amplitude wave field images, specifically shown in the top-surface shear loading configuration. For intrinsic actuation, our results indicate that aMRI linearly amplifies motion up to at least an amplification factor of 9 for instances of both visible and sub-voxel motion, validated by varying power levels of pneumatic actuation (40%–80% power) under MR, and through video analysis outside the MRI scanner room. While this investigation used a homogeneous brain-mimicking phantom, our setup can be used to study the mechanics of non-homogeneous phantom configurations with bio-interfaces in the future.}
}
@article{HULSMANN201847,
title = {Classification of motor errors to provide real-time feedback for sports coaching in virtual reality — A case study in squats and Tai Chi pushes},
journal = {Computers & Graphics},
volume = {76},
pages = {47-59},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301304},
author = {Felix Hülsmann and Jan Philip Göpfert and Barbara Hammer and Stefan Kopp and Mario Botsch},
keywords = {Sports coaching in virtual reality, Motor learning environments, Motor performance quality, Human motion analysis, Auto-generated augmented feedback},
abstract = {For successful fitness coaching in virtual reality, movements of a trainee must be analyzed in order to provide feedback. To date, most coaching systems only provide coarse information on movement quality. We propose a novel pipeline to detect a trainee’s errors during exercise that is designed to automatically generate feedback for the trainee. Our pipeline consists of an online temporal warp of a trainee’s motion, followed by Random-Forest-based feature selection. The selected features are used for the classification performed by Support Vector Machines. Our feedback to the trainee can consist of predefined verbal information as well as automatically generated visual augmentations. For the latter, we exploit information on feature importance to generate real-time feedback in terms of augmented color highlights on the trainee’s avatar. We show our pipeline’s superiority over two popular approaches from human activity recognition applied to our problem, k-Nearest Neighbor, combined with Dynamic Time Warping (KNN-DTW), as well as a recent combination of Convolutional Neural Networks with a Long Short-term Memory Network. We compare classification quality, time needed for classification, as well as the classifiers’ ability to automatically generate augmented feedback. In an exemplary application, we demonstrate that our pipeline is suitable to deliver verbal as well as automatically generated augmented feedback inside a CAVE-based sports training environment in virtual reality.}
}
@article{MEUSEL2019179,
title = {The importance of operator knowledge in evaluating virtual reality cue fidelity},
journal = {Computers and Electronics in Agriculture},
volume = {160},
pages = {179-187},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918309657},
author = {Chase Meusel and Chase Grimm and Jordan Starkey and Stephen B. Gilbert and Brian Gilmore and Greg Luecke and Don Kieu and Timothy Hunt},
keywords = {Virtual reality, Simulator, Fidelity, Operator knowledge, Automation},
abstract = {Research, development, testing, and operator training of large agricultural harvesting equipment has become increasingly expensive and complex. Operational simulators can offset these costs, but it is critical to evaluate the fidelity of the simulator experience to ensure that it will lead to operator behaviors that match those in the real world. This research describes a validation process for new visual cues within simulators and focuses on the presentation and fidelity of cues in a combine harvest simulator. The fully functional immersive combine simulator in this work provides a mixed-reality interface that combines a physically accurate operator interface with displays of virtual reality (VR) combine header and grain. Grain combine operators participated to assess how well they perceived the VR cues as representing actual field and crop activity. Results showed that operators successfully identified 85% of the visual cues in the combine simulator and recognized when these cues indicated machine settings adjustment needs but that operators' ability to choose the correct action to take correlated with their knowledge of the combine. Results also showed wide variation in the number of the grain combine’s head reel adjustments made by operators, likely reflecting strong individual preferences or habits around reel adjustment. Operators' number of adjustment interactions with the reel was also significantly correlated with the number of correct actions chosen, suggesting that careful attention to the reel is related to good farming performance. This research demonstrates the importance of taking operator knowledge and preferences into account when designing new agricultural products and offers a systematic method of validating cues within an agricultural simulator.}
}
@article{ESPINDOLA2013376,
title = {A model-based approach for data integration to improve maintenance management by mixed reality},
journal = {Computers in Industry},
volume = {64},
number = {4},
pages = {376-391},
year = {2013},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2013.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361513000043},
author = {Danúbia Bueno Espíndola and Luca Fumagalli and Marco Garetti and Carlos E. Pereira and Silvia S.C. Botelho and Renato {Ventura Henriques}},
keywords = {Industrial maintenance, Mixed reality, Product data management, Data modeling/visualization},
abstract = {Facilitating interaction with maintenance systems through intuitive interfaces is a competitive advantage in terms of time and costs for industry. This work presents the CARMMI approach, which aims to integrate information coming from CAx tools, mixed/augmented reality tools and embedded intelligent maintenance systems. CARMMI aims to provide support to operators/technicians during maintenance tasks through mixed reality, providing an easier access, understanding and comprehension of information from different systems. Information about where, when and which data will be presented in interface are defined by CARMMI. The paper presents three test cases that were performed using the proposed concepts and infrastructure. The main benefit of the approach is to provide an extensive and generic model for the integration and management of maintenance data through the use of CARMMI.}
}
@article{PARK2018177,
title = {A Dual-cable Hand Exoskeleton System for Virtual Reality},
journal = {Mechatronics},
volume = {49},
pages = {177-186},
year = {2018},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2017.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S095741581730185X},
author = {Yeongyu Park and Inseong Jo and Jeongsoo Lee and Joonbum Bae},
keywords = {Wearable system, Hand exoskeleton, Force feedback, Haptic interface, Virtual reality},
abstract = {In this paper, a hand exoskeleton system for virtual reality is proposed. As a virtual reality interface for the hand, a wearable system should be able to measure the finger joint angles and apply force feedback to the fingers at the same time with a simple and light structure. In the proposed system, two different cable mechanisms are applied to achieve such requirements; three finger joint angles in the direction of the flexion/extension (F/E) motion are measured by a tendon-inspired cable mechanism and another cable is used for force feedback to the finger for one degree of freedom (DOF) actuation per finger. As two different types of cables are used, the system is termed a dual-cable hand exoskeleton system. Using the measured finger joint angles and motor current, the cable-driven actuation system applies the desired force to the fingers. That is, when the desired force is zero, the motor position is controlled to follow the finger posture while maintaining the appropriate cable slack; when the desired force needs to be applied, the motor current is controlled to generate the desired force. To achieve a smooth transition between the two control strategies, the control inputs were linearly integrated; and the desired motor position was generated to prevent a sudden motor rotation. A prototype of the proposed system was manufactured with a weight of 320g, a volume of 13 × 23 × 8cm3, maximum force up to 5 N. The proposed control algorithms were verified by experiments with virtual reality applications.}
}
@article{KIA2021103502,
title = {The effects of target size and error rate on the cognitive demand and stress during augmented reality interactions},
journal = {Applied Ergonomics},
volume = {97},
pages = {103502},
year = {2021},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2021.103502},
url = {https://www.sciencedirect.com/science/article/pii/S0003687021001496},
author = {Kiana Kia and Jaejin Hwang and In-Sop Kim and Hakim Ishak and Jeong Ho Kim},
keywords = {Functional near infrared spectroscopy, NASA task Load index, Computer human interaction, Usability, Cerebral oxygenation},
abstract = {This study investigated the effects of target size and error rate on cognitive demand during augmented reality (AR) interactions. In a repeated-measures laboratory study, twenty participants performed two AR tasks (omni-directional pointing and cube placing) with different target sizes and error rates. During the AR tasks, we measured cerebral oxygenation using functional near-infrared spectroscopy (fNIRS), perceived workload using the NASA-TLX questionnaire, stress using the Short Stress State Questionnaire, and task performance (task completion time). The results showed that the AR tasks with more interaction errors increased cerebral oxygenation, perceived workload, and task completion time while the target size significantly affected physical demand and task completion time. These results suggest that appropriate target sizes and low system errors may reduce potential cognitive demand in AR interactions.}
}
@article{SIMONELLI20176073,
title = {An MR-Compatible Stage for Respiratory Motion Emulation},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {6073-6078},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1381},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317319237},
author = {James Simonelli and Yu-Hsiu Lee and Samantha Mikaiel and Cheng-Wei Chen and Xinzhou Li and Kyung Sung and David Lu and Holden Wu and Tsu-Chin Tsao},
keywords = {Mechatronic systems, medical applications, robotic manipulators, learning control, hydraulic actuators},
abstract = {We present the development of a system that physically emulates respiratory motion for phantoms under Magnetic Resonance Imaging (MRI). The system is designed to be inherently MR-compatible—with no ferrous materials or electromagnetic actuators present in the exam room—and uses hydrostatic actuators to control motion of the phantom. MR-compatibility of the system is verified through both signal-to-noise ratio (SNR) and imaging distortion tests, and several control strategies are then implemented to achieve tracking of prerecorded respiratory motion profiles obtained from a human subject.}
}
@article{DONG201324,
title = {Sensitivity analysis of augmented reality-assisted building damage reconnaissance using virtual prototyping},
journal = {Automation in Construction},
volume = {33},
pages = {24-36},
year = {2013},
note = {Augmented Reality in Architecture, Engineering, and Construction},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2012.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580512001549},
author = {Suyang Dong and Chen Feng and Vineet R. Kamat},
keywords = {Building damage, Earthquake, Reconnaissance, Augmented reality, Line segment detection, Nondestructive evaluation},
abstract = {The timely and accurate assessment of the damage sustained by a building during catastrophic events, such as earthquakes or blasts, is critical in determining the building's structural safety and suitability for future occupancy. Among many indicators proposed for measuring structural integrity, especially inelastic deformations, Interstory Drift Ratio (IDR) remains the most trustworthy and robust metric at the story level. In order to calculate IDR, researchers have proposed several nondestructive measurement methods. Most of these methods rely on pre-installed target panels with known geometric shapes or with an emitting light source. Such target panels are difficult to install and maintain over the lifetime of a building. Thus, while such methods are nondestructive, they are not entirely non-contact. This paper proposes an Augmented Reality (AR)-assisted non-contact method for estimating IDR that does not require any pre-installed physical infrastructure on a building. The method identifies corner locations in a damaged building by detecting the intersections between horizontal building baselines and vertical building edges. The horizontal baselines are superimposed on the real structure using an AR algorithm, and the building edges are detected via a Line Segment Detection (LSD) approach. The proposed method is evaluated using a Virtual Prototyping (VP) environment that allows testing of the proposed method in a reconfigurable setting. A sensitivity analysis is also conducted to evaluate the effect of instrumentation errors on the method's practical use. The experimental results demonstrate the potential of the new method to facilitate rapid building damage reconnaissance, and highlight the instrument precision requirements necessary for practical field implementation.}
}
@article{SELVARAJ2021114560,
title = {Experimental and numerical studies on dynamic performance of the rotating composite sandwich panel with CNT reinforced MR elastomer core},
journal = {Composite Structures},
volume = {277},
pages = {114560},
year = {2021},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2021.114560},
url = {https://www.sciencedirect.com/science/article/pii/S0263822321010229},
author = {Rajeshkumar Selvaraj and Manoharan Ramamoorthy and Ananda Babu Arumugam},
keywords = {Laminated composites, Sandwich structure, CNT-MR elastomer, Damping performance, Finite element method, Rotating condition},
abstract = {The work presented in this paper is focused on the investigation of vibration and damping characteristics of the rotating laminated composite hybrid MR elastomer sandwich panel. The laminated composite hybrid MR elastomer sandwich panel contains the constraining and base layers made of composite laminates and the core layer made of carbon nanotubes reinforced magnetorheological elastomer (hybrid MR elastomer). Higher-order shear deformation theory (HSDT) and finite element (FE) formulations are employed to derive the governing equations of the laminated composite hybrid MR elastomer sandwich panel. The performance of the derived numerical model is validated by comparing the results evaluated using experimental investigations on the prototype of laminated composite MR elastomer and hybrid MR elastomer sandwich panels under various rotating speeds. The influence of rotating speed, magnetic field, core-face sheet thickness ratio, ply orientations and support conditions on the dynamic properties of hybrid MR elastomer sandwich panel are explored. Also, the forced vibrations of the laminated composite hybrid MR elastomer sandwich panel are examined at different magnetic fields. Further, it can be observed that the reinforcement of CNTs in MR elastomer significantly influences the natural frequencies, loss factors, mode shapes and transverse displacements of the composite sandwich panels. This research work provides the guidelines for the researchers/designers on improving the dynamic properties and reduces the transverse vibrations of composite sandwich structures using the reinforcement of CNTs in smart materials.}
}
@article{PALONEN20175410,
title = {Augmented Reality in Forest Machine Cabin},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {5410-5417},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1075},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317315616},
author = {Tuomo Palonen and Heikki Hyyti and Arto Visala},
keywords = {Attitude algorithms, Augmented reality, Calibration, Forestry, Position estimation, Real time, Sensor fusion},
abstract = {Augmented reality human machine interface is demonstrated in the cabin of a forest machine outdoors for the first time in real time. In this work, we propose a system setup and a real-time capable algorithm to augment the operator’s visual field with measurements from the forest machine and its environment. In the demonstration, an instrumented forestry crane and a lidar are used to model the pose of the crane and its surroundings. In our approach, a camera and an inertial measurement unit are used to estimate the pose of the operator’s head in difficult lighting conditions with the help of planar markers placed on the cabin structures. Using the estimate, a point cloud and a crane model are superimposed on the video feed to form an augmented reality view. Our system is tested to work outdoors using a forest machine research platform in real time with encouraging initial results.}
}
@article{ZHOU2023102260,
title = {Unsupervised registration for liver CT-MR images based on the multiscale integrated spatial-weight module and dual similarity guidance},
journal = {Computerized Medical Imaging and Graphics},
volume = {108},
pages = {102260},
year = {2023},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2023.102260},
url = {https://www.sciencedirect.com/science/article/pii/S0895611123000782},
author = {Zhiyong Zhou and Shuaikun Wang and Jisu Hu and Anqi Liu and Xusheng Qian and Chen Geng and Jian Zheng and Guangqiang Chen and Jiansong Ji and Yakang Dai},
keywords = {Medical image registration, Deep learning, Multimodal registration, Spatial-weight module, Multi-resolution},
abstract = {Purpose
Multimodal registration is a key task in medical image analysis. Due to the large differences of multimodal images in intensity scale and texture pattern, it is a great challenge to design distinctive similarity metrics to guide deep learning-based multimodal image registration. Besides, since the limitation of the small receptive field, existing deep learning-based methods are mainly suitable for small deformation, but helpless for large deformation. To address the above issues, we present an unsupervised multimodal image registration method based on the multiscale integrated spatial-weight module and dual similarity guidance.
Methods
In this method, a U-shape network with our multiscale integrated spatial-weight module is embedded into a multi-resolution image registration architecture to achieve end-to-end large deformation registration, where the spatial-weight module can effectively highlight the regions with large deformation and aggregate discriminative features, and the multi-resolution architecture further helps to solve the optimization problem of the network in a coarse-to-fine pattern. Furthermore, we introduce a special loss function based on dual similarity, which represents both global gray-scale similarity and local feature similarity, to optimize the unsupervised multimodal registration network.
Results
We verified the effectiveness of the proposed method on liver CT-MR images. Experimental results indicate that the proposed method achieves the optimal DSC value and TRE value of 92.70 ± 1.75(%) and 6.52 ± 2.94(mm), compared with other state-of-the-art registration algorithms.
Conclusion
The proposed method can accurately estimate the large deformation field by aggregating multiscale features, and achieve higher registration accuracy and fast registration speed. Comparative experiments also demonstrate the effectiveness and generalization ability of the algorithm.}
}
@article{WOODWORTH2024194,
title = {Visual cues in VR for guiding attention vs. restoring attention after a short distraction},
journal = {Computers & Graphics},
volume = {118},
pages = {194-209},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323003023},
author = {Jason W. Woodworth and Christoph W. Borst},
keywords = {Virtual reality, Attention guidance, Attention restoration, Visual cues},
abstract = {Distraction in VR training environments may be mitigated with a visual cue intended to guide user attention to a target. A survey of related literature suggests a past focus on “search and selection” tasks to evaluate a cue’s capability for guidance. We investigate the capability of 9 eye-tracked cues with a new type of task that focuses on how to restore attention when a short distraction (e.g., a notification) shifts focus away from a target. Our study includes a guidance task in which subjects gaze at objects in a randomized order and a restoration task in which gaze sequences are interrupted by distraction events after which gaze must be returned to an object. We consider a wider variety of factors and metrics than previous studies, varying object spacing, gaze dwell time, and distraction distance and duration, and breaking down guidance time into subcomponents. Results show a general positive trend for cues that directly connect the user’s gaze to the target rather than indirectly suggesting direction. Results further reveal different patterns of cue effectiveness for the restoration task than for conventional guidance. This may be attributed to knowledge that subjects have about the location of the object from which they were distracted. An implication for more complex distraction tasks is that we expect them to be between the short distraction and regular guidance in terms of memory of object position. So, we speculate cue performance for other tasks would vary between the short distraction and guidance results. For restoration, some cues add complexity that reduces, rather than improves, performance.}
}
@article{LE2022107071,
title = {A 3D Multi-task Regression and Ordinal Regression Deep Neural Network for Collateral Imaging from Dynamic Susceptibility Contrast-Enhanced MR perfusion in Acute Ischemic Stroke},
journal = {Computer Methods and Programs in Biomedicine},
volume = {225},
pages = {107071},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107071},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004527},
author = {Hoang Long Le and Hong Gee Roh and Hyun Jeong Kim and Jin Tae Kwak},
keywords = {collateral imaging, stroke, multi-task learning, ordinal regression},
abstract = {Background and Objective
Cerebral collaterals have been identified as one of the primary determinants for treatment options in acute ischemic stroke. Several works have been proposed, but these have not been adopted for a routine clinical usage due to their manual and heuristic nature as well as inconsistency and instability of the assessment. Herein, we present an advanced deep learning-based method that can automatically generate a multiphase collateral imaging (collateral map) derived from dynamic susceptibility contrast-enhanced MR perfusion (DSC-MRP) in an accurate and robust manner.
Methods
We develop a 3D multi-task regression and ordinal regression deep neural network for generating collateral maps from DSC-MRP, which formulates the prediction of collateral maps as both a regression task and an ordinal regression task. For an ordinal regression task, we introduce a spacing-decreasing discretization (SDD) strategy to represent the intensity of the collateral status on a discrete, ordinal scale. We also devise loss functions to achieve effective and efficient multi-task learning.
Results
We systematically evaluated the performance of the proposed network using DSC-MRP from 802 patients. On average, the proposed network achieved ≥0.900 squared correlation coefficient (R-Squared), ≥0.916 Tanimoto measure (TM), ≥0.0913 structural similarity index measure (SSIM), and ≤0.564 × 10−1 mean absolute error (MAE), outperforming eight competing models that have been recently developed in medical imaging and computer vision. We also found that the proposed network could provide an improved contrast between the low and high intensity regions in the collateral maps, which is a key to an accurate evaluation of the collateral status.
Conclusions
The experimental results demonstrate that the proposed network is able to generate collateral maps with high accuracy, facilitating a timely and prompt assessment of the collateral status in clinlcs. The future study will entail the optimization of the proposed network and its clinical evalution in a prospective manner.}
}
@article{HASSENFELDT2020301011,
title = {Exploring the Learning Efficacy of Digital Forensics Concepts and Bagging & Tagging of Digital Devices in Immersive Virtual Reality},
journal = {Forensic Science International: Digital Investigation},
volume = {33},
pages = {301011},
year = {2020},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2020.301011},
url = {https://www.sciencedirect.com/science/article/pii/S2666281720302602},
author = {Courtney Hassenfeldt and Jillian Jacques and Ibrahim Baggili},
keywords = {Virtual reality, Cyber forensics, Digital forensics, Education, Gamification},
abstract = {This work presents the first account of evaluating learning inside a VR experience created to teach Digital Forensics (DF) concepts, and a hands-on laboratory exercise in Bagging & Tagging a crime scene with digital devices. First, we designed and developed an immersive VR experience which included a lecture and a lab. Next, we tested it with (n = 57) participants in a controlled experiment where they were randomly assigned to a VR group or a physical group. Both groups were subjected to the same lecture and lab, but one was in VR and the other was in the real world. We collected pre- and post-test results to assess the participants’ knowledge in DF concepts learned. Our experimental results indicated no significant differences in scores between the immersive VR group and the physical group. However, our results showed faster completion times in VR by the participants, which hints at VR being more time efficient, as virtual environments can be spun programmatically with little downtime.}
}
@article{DAESCU2016176,
title = {Innovation-Weight Parametrization in Data Assimilation: Formulation & Analysis with NAVDAS-AR/NAVGEM**This work was supported by the Naval Research Laboratory Atmospheric Effects, Analysis, and Prediction BAA #75-11-01 under award N00173-13-1-G903. Support for the second author from the sponsor ONR-PR-0602435N is gratefully acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {18},
pages = {176-181},
year = {2016},
note = {10th IFAC Symposium on Nonlinear Control Systems NOLCOS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.159},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316317384},
author = {Dacian N. Daescu and Rolf H. Langland},
keywords = {Parameter Estimation, State Estimation, Applications, Performance Issues},
abstract = {Abstract:
An innovation-weight parametrization is introduced as a practical approach to account for deficiencies in the representation of both background error and observation error covariance in a variational data assimilation system. The adjoint-based evaluation of the forecast error sensitivity provides a computationally efficient diagnosis to observation-space distributed parameters and guidance for tuning the analysis Kalman gain operator. Theoretical aspects are discussed and preliminary results are presented with the adjoint versions of the Naval Research Laboratory Atmospheric Variational Data Assimilation System-Accelerated Representer (NAVDAS-AR) and the Navy’s Global Environmental Model (NAVGEM).}
}
@article{LACRUZ202014600,
title = {A New Virtual Reality Interface for Underwater Intervention Missions},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {14600-14607},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1468},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320318802},
author = {Marcos de {la Cruz} and Gustavo A. Casañ and Pedro J. Sanz and Raúl Marín},
keywords = {HRI, Virtual Reality, Marine Robotics, Underwater Intervention, Problem Based Learning},
abstract = {Nowadays, most underwater intervention missions are developed through the well-known work-class ROVs (Remote Operated Vehicles), equipped with teleoperated arms under human supervision. Thus, despite the appearance on the market of the first prototypes of the so-called I-AUV (Autonomous Underwater Vehicles for Intervention), the most mature technology associated with ROVs continues to be trusted. In order to fill the gap between ROVs and incipient I-AUVs technology, new research is under progress in our laboratory. In particular, new HRI (Human Robot Interaction) capabilities are being tested inside a three-year Spanish coordinated project focused on cooperative underwater intervention missions. In this work new results are presented concerning a new user interface which includes immersion capabilities through Virtual Reality (VR) technology. It is worth noting that a new HRI module has been demonstrated, through a pilot study, in which the users had to solve some specific tasks, with minimum guidance and instructions, following simple Problem Based Learning (PBL) scheme. Finally, it is noticeable that, although this is only a work in progress, the obtained results are promising concerning friendly and intuitive characteristics of the developed HRI module. Thus, some critical aspects, like complexity fall, training time and cognitive fatigue of the ROV pilot, seem more affordable now.}
}
@article{SENER201631,
title = {Bayesian segmentation of human facial tissue using 3D MR-CT information fusion, resolution enhancement and partial volume modelling},
journal = {Computer Methods and Programs in Biomedicine},
volume = {124},
pages = {31-44},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2015.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715002692},
author = {Emre Şener and Erkan U. Mumcuoglu and Salih Hamcan},
keywords = {Image segmentation, Information fusion, Partial volume, Resolution enhancement, Superresolution, Human facial tissue},
abstract = {Background
Accurate segmentation of human head on medical images is an important process in a wide array of applications such as diagnosis, facial surgery planning, prosthesis design, and forensic identification.
Objectives
In this study, a Bayesian method for segmentation of facial tissues is presented. Segmentation classes include muscle, bone, fat, air and skin.
Methods
The method presented incorporates information fusion from multiple modalities, modelling of image resolution (measurement blurring), image noise, two priors helping to reduce noise and partial volume. Image resolution modelling employed facilitates resolution enhancement and superresolution capabilities during image segmentation. Regularization based on isotropic and directional Markov Random Field priors is integrated. The Bayesian model is solved iteratively yielding tissue class labels at every voxel of the image. Sub-methods as variations of the main method are generated by using a combination of the models.
Results
Testing of the sub-methods is performed on two patients using single modality three-dimensional (3D) image (magnetic resonance, MR or computerized tomography, CT) as well as registered MR-CT images with information fusion. Numerical, visual and statistical analyses of the methods are conducted. High segmentation accuracy values are obtained by the use of image resolution and partial volume models as well as information fusion from MR and CT images. The methods are also compared with our Bayesian segmentation method proposed in a previous study. The performance is found to be similar to our previous Bayesian approach, but the presented methods here eliminates ad hoc parameter tuning needed by the previous approach which is system and data acquisition setting dependent.
Conclusions
The Bayesian approach presented provides resolution enhanced segmentation of very thin structures of the human head. Meanwhile, free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study.}
}
@article{LEE2020103229,
title = {Experiencing immersive virtual reality in museums},
journal = {Information & Management},
volume = {57},
number = {5},
pages = {103229},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103229},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618310280},
author = {Hyunae Lee and Timothy Hyungsoo Jung and M.Claudia {tom Dieck} and Namho Chung},
keywords = {Experience economy, Absorption, Immersion, Virtual reality, Museum, Tourism},
abstract = {Virtual Reality (VR) has been regarded as a highly effective technology that enables people to gain enjoyable and immersive information about museum collections. Drawing from the four realms of the experience economy, we assume absorptive experiences influence immersive experiences, overall museum VR tour experience, and intention to visit a museum. The results show that all the hypotheses are supported. Furthermore, we compared and tested the proposed model and its rival model (postulating the direct influence of the four realms of the experience economy on museum VR experience) and found that the proposed model is better than the rival model.}
}
@article{YARDIMCI2011124,
title = {Improving the bond characteristics of AR-glass strands by microstructure modification technique},
journal = {Cement and Concrete Composites},
volume = {33},
number = {1},
pages = {124-130},
year = {2011},
issn = {0958-9465},
doi = {https://doi.org/10.1016/j.cemconcomp.2010.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0958946510001356},
author = {Mert Yücel Yardımcı and Ronit Tirosh and Pavel Larianovsky and Moshe Puterman and Arnon Bentur},
keywords = {AR-glass fiber, Fiber modification, Pull-out test, FILT test, Image analysis},
abstract = {This paper introduces a fiber modification technique for enhancing the bond characteristics of AR-glass strands embedded in cement paste. AR-glass strands were modified by absorption of four different types of slurries containing sub-micron organic and inorganic particles. Two different sample preparation processes were applied prior to incorporation the modified strands in the cement paste. Bond characteristics of unmodified and modified AR-glass strands embedded in cement paste were determined by pull-out test and the failure mechanism of pulling out strands was determined by Failure Investigation by Light Transmission (FILT) test. Test results showed that the bond properties and failure type of AR-glass strands can be enhanced by the microstructure modification technique. The efficiency of the modification was highly dependent on filler type, structure and also the production method.}
}
@article{DANIELSSON20201298,
title = {Augmented reality smart glasses for operators in production: Survey of relevant categories for supporting operators},
journal = {Procedia CIRP},
volume = {93},
pages = {1298-1303},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.099},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307332},
author = {Oscar Danielsson and Magnus Holm and Anna Syberfeldt},
keywords = {augmented reality, assembly operator, literature survey, augmented reality smart glasses},
abstract = {The aim of this paper is to give an overview of the current knowledge and future challenges of augmented reality smart glasses (ARSG) for use by industrial operators. This is accomplished through a survey of the operator perspective of ARSG for industrial application, aiming for faster implementation of ARSG for operators in manufacturing. The survey considers the categories assembly instructions, human factors, design, support, and training from the operator perspective to provide insights for efficient use of ARSG in production. The main findings include a lack of standards in the design of assembly instructions, the field of view of ARSG are limited, and the guidelines for designing instructions focus on presenting context-relevant information and limiting the disturbance of reality. Furthermore, operator task routine is becoming more difficult to achieve and testing has mainly been with non-operator testers and overly simplified tasks. Future challenges identified from the review include: longitudinal user-tests of ARSG, a deeper evaluation of how to distribute the weight of ARSG, further improvement of the sensors and visual recognition to facilitate better interaction, and task complexity is likely to increase.}
}
@article{PAWAR2021101968,
title = {Domain knowledge augmentation of parallel MR image reconstruction using deep learning},
journal = {Computerized Medical Imaging and Graphics},
volume = {92},
pages = {101968},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101968},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121001178},
author = {Kamlesh Pawar and Gary F. Egan and Zhaolin Chen},
keywords = {Image processing, MR image reconstruction, Deep learning image reconstruction, Parallel MRI, Convolutional neural network},
abstract = {A deep learning (DL) method for accelerated magnetic resonance (MR) imaging is presented that incorporates domain knowledge of parallel MR imaging to augment the DL networks for accurate and stable image reconstruction. The proposed DL method employs a novel loss function consisting of a combination of mean absolute error, structural similarity, and sobel edge loss. The DL model takes both original measurements and images reconstructed by the parallel imaging method as inputs to the network. The accuracy of the proposed method was evaluated using two anatomical regions and six MRI contrasts and was compared with state-of-the-art parallel imaging and deep learning methods. The proposed method significantly outperformed the other methods for all the six different contrasts in terms of structural similarity, peak signal to noise ratio, and normalized mean squared error. The out-of-sample performance of the proposed method was assessed for a truly “unseen” case in a volunteer scan. The method produced images without any artificial features, often occurring in the DL image reconstruction methods. A stability analysis was performed by adding perturbations to the input, which demonstrated that the proposed method is robust and stable with respect to small structural changes, and different undersampling ratios. Comprehensive validation on large datasets demonstrated that incorporation of domain knowledge sufficiently regularizes the DL based image reconstruction and produces accurate and stable image enhancement.}
}
@article{NIKOLIC2006902,
title = {A MONITOR-BASED AR SYSTEM AS A SUPPORT TOOL FOR INDUSTRIAL MAINTENANCE},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {16},
pages = {902-907},
year = {2006},
note = {4th IFAC Symposium on Mechatronic Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060912-3-DE-2911.00155},
url = {https://www.sciencedirect.com/science/article/pii/S147466701534283X},
author = {Vesna Nikolic and Peter F. Elzer and Christian Vetter},
keywords = {Augmented Reality, Monitor-based, Maintenance, Maintenance and Operating Instructions, Usability, Cognitive Time},
abstract = {A prototype of a monitor-based augmented reality (AR) system is presented in this paper. The system is completely mobile, consisting of a motorized controllable camera and a laptop with a head set. The human-system interaction is realized by user-oriented interfaces and is speech-based. Advantages of the system are its simplicity and low cost combined with proven usability. Compared to other present tools for presentation of instructions for maintenance and operation, the use of this system results in time savings and lower error rates, as shown by experiments undertaken at the IPP.}
}
@article{MANNA20223208,
title = {Mesoporous silica-based abrasion resistant antireflective (AR)-cum-hydrophobic coatings on textured solar cover glasses by a spray coating technique††Electronic supplementary information (ESI) available: Water contact angle hysteresis of coated solar cover glass. See DOI: 10.1039/d1ma01141c},
journal = {Materials Advances},
volume = {3},
number = {7},
pages = {3208-3217},
year = {2022},
issn = {2633-5409},
doi = {https://doi.org/10.1039/d1ma01141c},
url = {https://www.sciencedirect.com/science/article/pii/S2633540923017085},
author = {Srikrishna Manna and Milan Kanti Naskar and Samar Kumar Medda},
abstract = {ABSTRACT
The present work describes the fabrication of functionalized mesoporous silica based antireflective-cum-hydrophobic coatings on textured solar cover glasses via a spray-coating technique; the fabricated coatings exhibit good abrasion resistance. The thicknesses of the AR and hydrophobic layers have been varied in the ranges of 350–500 nm and 30–45 nm, respectively. The application of a bilayer coating on textured solar cover glass resulted in a significant enhancement of ≥4% in the average solar transmission in the wavelength range of 400–1100 nm. A hydrophobic coating with self-cleaning properties was designed on the mesoporous silica coating, and it exhibited an enhancement of the photo-current density (mA cm−2) of 4.65% under simulated solar light of 1 sun with a maximum output power enhancement of ≥4% based on I–V (ampere-volt) characterization. After 50 cycles of sandpaper (80 mesh) abrasion testing of the coated solar cover glass, its water contact angle value was reduced from 130° to 105°, and 50 h UV radiation exposure testing confirmed that there were no significant changes in the coating quality. These results indicate the good durability, abrasion resistance, and AR/hydrophobic properties of the coatings.}
}
@article{SHI2020101153,
title = {A neurophysiological approach to assess training outcome under stress: A virtual reality experiment of industrial shutdown maintenance using Functional Near-Infrared Spectroscopy (fNIRS)},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101153},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101153},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301245},
author = {Yangming Shi and Yibo Zhu and Ranjana K. Mehta and Jing Du},
keywords = {Shutdown maintenance training, Virtual reality, Eye-tracking, fNIRS},
abstract = {Shutdown maintenance, i.e., turning off a facility for a short period for renewal or replacement operations is a highly stressful task. With the limited time and complex operation procedures, human stress is a leading risk. Especially shutdown maintenance workers often need to go through excessive and stressful on-site trainings to digest complex operation information in limited time. The challenge is that workers’ stress status and task performance are hard to predict, as most trainings are only assessed after the shutdown maintenance operation is finished. A proactive assessment or intervention is needed to evaluate workers’ stress status and task performance during the training to enable early warning and interventions. This study proposes a neurophysiological approach to assess workers’ stress status and task performance under different virtual training scenarios. A Virtual Reality (VR) system integrated with the eye-tracking function was developed to simulate the power plant shutdown maintenance operations of replacing a heat exchanger in both normal and stressful scenarios. Meanwhile, a portable neuroimaging device – Functional Near-Infrared Spectroscopy (fNIRS) was also utilized to collect user’s brain activities by measuring hemodynamic responses associated with neuron behavior. A human–subject experiment (n = 16) was conducted to evaluate participants’ neural activity patterns and physiological metrics (gaze movement) related to their stress status and final task performance. Each participant was required to review the operational instructions for a pipe maintenance task for a short period and then perform the task based on their memory in both normal and stressful scenarios. Our experiment results indicated that stressful training had a strong impact on participants’ neural connectivity patterns and final performance, suggesting the use of stressors during training to be an important and useful control factors. We further found significant correlations between gaze movement patterns in review phase and final task performance, and between the neural features and final task performance. In summary, we proposed a variety of supervised machine learning classification models that use the fNIRS data in the review session to estimate individual’s task performance. The classification models were validated with the k-fold (k = 10) cross-validation method. The Random Forest classification model achieved the best average classification accuracy (80.38%) in classifying participants’ task performance compared to other classification models. The contribution of our study is to help establish the knowledge and methodological basis for an early warning and estimating system of the final task performance based on the neurophysiological measures during the training for industrial operations. These findings are expected to provide more evidence about an early performance warning and prediction system based on a hybrid neurophysiological measure method, inspiring the design of a cognition-driven personalized training system for industrial workers.}
}
@article{POLONI2021126,
title = {Brain MR image classification for Alzheimer’s disease diagnosis using structural hippocampal asymmetrical attributes from directional 3-D log-Gabor filter responses},
journal = {Neurocomputing},
volume = {419},
pages = {126-135},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.102},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220312972},
author = {Katia M. Poloni and Italo A. {Duarte de Oliveira} and Roger Tam and Ricardo J. Ferrari},
keywords = {Structural hippocampal asymmetries, 3D log-Gabor filters, Alzheimer’s disease, Magnetic resonance imaging, MR image classification},
abstract = {Alzheimer’s disease (AD) is a progressive and irreversible neurodegenerative condition whose development is characterized by lateralized brain atrophies. In AD, the hippocampus is the first brain structure to present atrophy, which, although to a lesser extent, is also a precursor to the broader asymmetrical development of the human brain. Structural magnetic resonance (MR) imaging is capable of detecting the disease-induced anatomical changes in the brain, thus aiding the diagnosis of AD. MR image attributes extracted from the hippocampal regions are commonly used for the AD classification task. However, most of the published methods do not explore hippocampal asymmetries for image classification. In this study, we propose a new technique for performing the classification of MR images for AD using only hippocampal asymmetrical attributes. By using the new proposed asymmetry index (AI), we assessed the attributes and the ones that passed the analysis of variance test, i.e., showing statistically mean differences among the classes (CN, MCI, and AD), were selected for classification. As a result of our study, the statistical analysis of our AI has shown a significant increase in hippocampal asymmetry as disease progress (CN < MCI < AD). Moreover, for the classification using clinical MR images, we obtained accuracy values of 69.44% and 82.59%; and AUC values of 0.76 and 0.9 for CN × MCI and CN × AD, respectively. Last, we found the results of our asymmetry analysis consistent with other statistical assessments and our classification results, using only asymmetry attributes comparable to (or even higher than) existing hippocampus studies.}
}
@article{KNYAZKOV2022112106,
title = {Cationic structure of premixed near-stoichiometric CH4/O2/Ar flames at atmospheric pressure: New insights from mass spectrometry, quantum chemistry, and kinetic modeling},
journal = {Combustion and Flame},
volume = {241},
pages = {112106},
year = {2022},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2022.112106},
url = {https://www.sciencedirect.com/science/article/pii/S0010218022001250},
author = {Denis A. Knyazkov and Ilya E. Gerasimov and Tatyana A. Bolshova and Vitaly G. Kiselev and Andrey G. Shmakov and Alexander A. Paletsky},
keywords = {Ions in flames, Flame ionization, Flame sampling molecular beam mass spectrometry, Electric-field assisted combustion, Quantum chemistry},
abstract = {The spatial distributions of naturally occurring cations in near-stoichiometric (equivalence ratios ϕ = 0.8 and 1.2) burner-stabilized methane/oxygen/argon flames at atmospheric pressure are studied by molecular beam mass spectrometry (MBMS) and kinetic modeling. The disturbing effect of a nickel sampling cone used in the measurements on the flame was comprehensively modeled using a two-dimensional direct numerical simulation of reacting flow near the axially symmetric probe along with a reduced chemical kinetic mechanism. The use of one-dimensional numerical simulation for predicting the flame structure perturbed by the metallic probe was firmly justified. We also proposed and applied a procedure to correct the measured spatial profiles of cations on the contribution of signals from the hydrates formed slightly upstream the reaction zone due to the sampling probe effects. With all these methodological advantages, we validated the ion chemistry mechanisms proposed earlier in the literature against the novel experimental data on the spatial distributions of the following cations: HCO+, CH3+, H3O+, C2H3O+, CH5O+, C3H3+. The kinetic mechanism published recently by Chen et al. [Combust. Flame 202 (2019) 208] for the charged species formed in methane flames was revised in order to improve its predictive ability. To this end, the highly accurate W2-F12 quantum chemical calculations were used to obtain the reliable formation enthalpies of all cations considered in the mechanism. In the case of C2H3O+ and C3H3+ cations, the calculated values turned out to be profoundly lower than those reported before. Apart from this, the theory also predicted another exit channel for H3O+ + acetylene reaction yielding HCO+ + CH4 instead of C2H3O+ + H2 proposed earlier. The corrections significantly improved the predictive ability of the ion chemistry mechanism. We also considered the most important directions for the further refinement of the mechanism.}
}
@article{ZENG20191066,
title = {Miniaturization design of head-mounted display optical system based on double threshold method for virtual reality},
journal = {Optik},
volume = {183},
pages = {1066-1074},
year = {2019},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2019.01.083},
url = {https://www.sciencedirect.com/science/article/pii/S003040261930083X},
author = {Zongshun Zeng and Xiaozhe Ma and Fang Zhang and Jing Zhu and Avakaw Syarhei and Huijie Huang},
keywords = {Virtual reality, Head-mounted display, Optical design, Delano theory, Double threshold method, Fresnel lens},
abstract = {To increase the sense of three-dimensional immersion and comfort, a head-mounted display (HMD) system used for virtual reality (VR) should achieve an excellent optical performance and a compact structure. In general, a Fresnel lens is applied to decrease the dimensions and weight of the monocular optics. However, the amount of decrease is limited when utilizing a Fresnel lens alone. To further improve the compactness of an HMD optical system, a double threshold method based on the Delano theory and a genetic algorithm is proposed. The first-order structure gained using this method is suitable for the design of a miniaturized HMD optical system. To verify its feasibility, a three-piece optical system utilizing PMMA and PC was designed. The results indicate that the total length of the optical system is approximately 55 mm and the weight of the monocular optics is approximately 5.6 g without a decrease in the optical performance. The reductions in the dimensions and weight are obvious.}
}
@article{KIM2019243,
title = {Development of a VR simulator for educating CFD-computed internal environment of piglet house},
journal = {Biosystems Engineering},
volume = {188},
pages = {243-264},
year = {2019},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2019.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S1537511019308712},
author = {Rack-woo Kim and Jun-gyu Kim and In-bok Lee and Uk-hyeon Yeo and Sang-yeon Lee},
keywords = {Aerodynamic environment, Computational fluid dynamics, Environmental control, Piglet house, Simulator, Virtual reality},
abstract = {In terms of maintaining an optimum micro-climates in livestock facilities, many problems exist. In particular, many consultants, as well as farmers, have misunderstand the process and often made wrong judgements on ventilation. Airflow is the main mechanism of internal environmental distribution. However, airflow is invisible and difficult to predict and measure. Computational fluid dynamics (CFD) simulations have been used to analyse the aerodynamics of livestock building micro-climates. CFD-computed results can be used to educate farmers and consultants. However, they can be of limited use when providing education via a two-dimensional screen. This could be improved by visualising the computed results in a three-dimensional space rather than on a two-dimensional surface. This could be accomplished using virtual reality (VR). In this study, CFD-computed results were combined with VR technology to develop an educational simulator. Firstly, an extensive review was carried out of research papers, reports, journals, and publications on the livestock industry, to find seasonally representative problems that occurred at piglet rearing houses in Korea. Then, a CFD simulation model was designed for computing the micro-climate of a piglet house according to its external climate and ventilation type. These CFD models were designed based on a 2009 Korean standard for piglet houses using validation results of a previous study (Kim et al., 2017). The CFD-computed results, such as internal airflow, air temperature, humidity, and gas, were then applied to a VR simulator for educating farmers and consultants. Finally, a user interface was developed to maximise accessibility and usability for VR users.}
}
@article{VAUGHAN201659,
title = {A review of virtual reality based training simulators for orthopaedic surgery},
journal = {Medical Engineering & Physics},
volume = {38},
number = {2},
pages = {59-71},
year = {2016},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1350453315002799},
author = {Neil Vaughan and Venketesh N. Dubey and Thomas W. Wainwright and Robert G. Middleton},
keywords = {Orthopaedic, Hip replacement, Resurfacing, Simulator, Haptics, Modelling},
abstract = {This review presents current virtual reality based training simulators for hip, knee and other orthopaedic surgery, including elective and trauma surgical procedures. There have not been any reviews focussing on hip and knee orthopaedic simulators. A comparison of existing simulator features is provided to identify what is missing and what is required to improve upon current simulators. In total 11 hip replacements pre-operative planning tools were analysed, plus 9 hip trauma fracture training simulators. Additionally 9 knee arthroscopy simulators and 8 other orthopaedic simulators were included for comparison. The findings are that for orthopaedic surgery simulators in general, there is increasing use of patient-specific virtual models which reduce the learning curve. Modelling is also being used for patient-specific implant design and manufacture. Simulators are being increasingly validated for assessment as well as training. There are very few training simulators available for hip replacement, yet more advanced virtual reality is being used for other procedures such as hip trauma and drilling. Training simulators for hip replacement and orthopaedic surgery in general lag behind other surgical procedures for which virtual reality has become more common. Further developments are required to bring hip replacement training simulation up to date with other procedures. This suggests there is a gap in the market for a new high fidelity hip replacement and resurfacing training simulator.}
}
@article{PARK2021102638,
title = {WeHAPTIC-light: A cable slack-based compact hand force feedback system for virtual reality},
journal = {Mechatronics},
volume = {79},
pages = {102638},
year = {2021},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2021.102638},
url = {https://www.sciencedirect.com/science/article/pii/S0957415821001136},
author = {Yeongyu Park and Sangyeop Lee and Joonbum Bae},
keywords = {Wearable system, Hand exoskeleton, Finger motion measurement, Force feedback, Haptic interface, Virtual reality},
abstract = {Various force feedback devices for the hand have been developed for the virtual reality field, but many systems are heavy, and various functions have not been fully developed with respect to finger motion measurement and stiffness generation. We propose a light and wearable force feedback device that can measure finger motion without calibration and can implement different stiffnesses using a cable slack-based mechanism. The proposed system has a small weight of 298 g, using a 10 g motor. To measure finger motion without calibration, an equation for each finger joint angle is derived from the three-dimensional position and orientation information of the fingertip. To realize different stiffnesses, the proposed system connects a motor and a finger structure with a cable; zero impedance is realized by maintaining a slack. The adjustable cable slack mechanism ensures to restrict the finger motion when the fingertip touches the surface of a virtual object even in fast motion; different stiffnesses are realized by proportional control input to the motor according to the deformation of the virtual object. We manufactured a prototype for three fingers and experimentally verified the performance of the proposed system.}
}
@article{KAMATHE201841,
title = {A novel method based on independent component analysis for brain MR image tissue classification into CSF, WM and GM for atrophy detection in Alzheimer’s disease},
journal = {Biomedical Signal Processing and Control},
volume = {40},
pages = {41-48},
year = {2018},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1746809417302069},
author = {Rupali S. Kamathe and Kalyani R. Joshi},
keywords = {Independent component analysis, Band expansion process, Brain MRI, Support vector machines, Alzheimer’s disease},
abstract = {Brain Magnetic Resonance Image (MRI) plays a vital role in diagnosis of diseases like Brain Tumor, Alzheimer, Multiple Sclerosis, Schizophrenia and other White Matter Lesions. In most of the cases accurate segmentation of Brain MRI into tissue types like Cerebro-Spinal Fluid (CSF), White Matter (WM) and Grey Matter (GM) is of interest. The diagnostic accuracy of expert and non-expert Radiologists can be improved with accurate and automated tissue segmentation and classification system. Such system can also be used for trainees to understand the individual tissue distribution in MRI scans. In this paper, we propose a novel automated tissue segmentation and classification method based on Independent Component Analysis (ICA) with Band Expansion Process (BEP) and Support Vector Machine (SVM) classifier which with input as T1, T2 and Proton Density (PD) scans of patient, provides output as CSF, WM and GM indicating the possible atrophy in brain which can help in diagnosis of Alzheimer’s disease (AD). The objective of this work is to test the effectiveness of ICA with different input images generated using BEP for accurate brain tissue segmentation by validating results with different quality metrics. The novel method for generating input images for ICA has been implemented and segmented tissues are used for atrophy detection. The BEP+ICA+Thresholding+‘SVM trained with Grey Level Co-occurrence Matrix (GLCM) based texture features’ is giving 100% tissue classification accuracy for test samples under consideration.}
}
@article{YANG201669,
title = {Local structure orientation descriptor based on intra-image similarity for multimodal registration of liver ultrasound and MR images},
journal = {Computers in Biology and Medicine},
volume = {76},
pages = {69-79},
year = {2016},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2016.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S001048251630169X},
author = {Minglei Yang and Hui Ding and Jingang Kang and Longfei Cong and Lei Zhu and Guangzhi Wang},
keywords = {Local structure orientation, Multimodal registration, US and MR, Fusion imaging, Liver},
abstract = {Purpose
Ultrasound (US)-magnetic resonance (MR) fusion imaging is a profitable tool for image-guided abdominal diagnosis and biopsy. However, the automatic registration of liver US and MR images remains a challenging task. An effective local structure orientation descriptor (LSOD) for use in registering multimodal images is proposed in this study.
Methods
LSOD utilizes a normalized similarity distance vector of intra-image patch pairs to extract intensity change orientations from intensity value changes in a local area. The multimodal similarity measure is then derived using the LSOD vector difference. Experiments were performed on simulated US and liver 2D US–3D MR images from a phantom, two healthy volunteers, and seven patients.
Results
Using the LSOD-based method, the root-mean-square target registration errors (RMS-TREs) were 1.76±1.90mm/2.03±0.84mm in phantom/clinical experiments. All of the results outperformed those obtained using modality independent neighborhood descriptor (MIND)- and linear correlation of linear combination (LC2)-based methods (phantom/clinical: 5.23±3.35mm/4.32±3.63mm and 9.79±5.03mm/6.29±3.85mm, respectively). The registration cover range for all subjects of the LSOD-based method was 9.16mm, which was larger than those of the MIND- and LC2-based methods (5.06 and 5.12mm, respectively).
Conclusions
The results demonstrated that the LSOD-based registration method could robustly register 2D US and 3D MR images of different liver sections with acceptable accuracy for clinical requirements. This approach is useful for the practical clinical application of the US–MR fusion imaging technique.}
}
@article{HOAR2021106375,
title = {Combined Transfer Learning and Test-Time Augmentation Improves Convolutional Neural Network-Based Semantic Segmentation of Prostate Cancer from Multi-Parametric MR Images},
journal = {Computer Methods and Programs in Biomedicine},
volume = {210},
pages = {106375},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106375},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721004491},
author = {David Hoar and Peter Q. Lee and Alessandro Guida and Steven Patterson and Chris V. Bowen and Jennifer Merrimen and Cheng Wang and Ricardo Rendon and Steven D. Beyea and Sharon E. Clarke},
keywords = {Convolutional neural network, Prostate cancer, Segmentation, MRI, Computer aided diagnosis, Machine learning},
abstract = {Purpose
Multiparametric MRI (mp-MRI) is a widely used tool for diagnosing and staging prostate cancer. The purpose of this study was to evaluate whether transfer learning, unsupervised pre-training and test-time augmentation significantly improved the performance of a convolutional neural network (CNN) for pixel-by-pixel prediction of cancer vs. non-cancer using mp-MRI datasets.
Methods
154 subjects undergoing mp-MRI were prospectively recruited, 16 of whom subsequently underwent radical prostatectomy. Logistic regression, random forest and CNN models were trained on mp-MRI data using histopathology as the gold standard. Transfer learning, unsupervised pre-training and test-time augmentation were used to boost CNN performance. Models were evaluated using Dice score and area under the receiver operating curve (AUROC) with leave-one-subject-out cross validation. Permutation feature importance testing was performed to evaluate the relative value of each MR contrast to CNN model performance. Statistical significance (p<0.05) was determined using the paired Wilcoxon signed rank test with Benjamini-Hochberg correction for multiple comparisons.
Results
Baseline CNN outperformed logistic regression and random forest models. Transfer learning and unsupervised pre-training did not significantly improve CNN performance over baseline; however, test-time augmentation resulted in significantly higher Dice scores over both baseline CNN and CNN plus either of transfer learning or unsupervised pre-training. The best performing model was CNN with transfer learning and test-time augmentation (Dice score of 0.59 and AUROC of 0.93). The most important contrast was apparent diffusion coefficient (ADC), followed by Ktrans and T2, although each contributed significantly to classifier performance.
Conclusions
The addition of transfer learning and test-time augmentation resulted in significant improvement in CNN segmentation performance in a small set of prostate cancer mp-MRI data. Results suggest that these techniques may be more broadly useful for the optimization of deep learning algorithms applied to the problem of semantic segmentation in biomedical image datasets. However, further work is needed to improve the generalizability of the specific model presented herein.}
}
@article{FATHI20242670,
title = {Unveiling the Potential of Mixed Reality: Enhancing Time Measurement and Operator Support in Manual Assembly Processes},
journal = {Procedia Computer Science},
volume = {232},
pages = {2670-2679},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002618},
author = {Masood Fathi and Ingemar Karlsson and Göran Grahn and Andreas Björnsson},
keywords = {Mixed Reality, Operator Support, Time study, Manual Assembly},
abstract = {This study investigates the potential of Mixed Reality (MR) in the manual assembly processes and conducts a case study at a pump manufacturing plant in Sweden. An MR solution is developed to assist operators through visual instructions and guiding aides. The solution also captures the operator's motions using advanced hand and eye tracking features for real-time guidance and accurate time measurement. The proposed MR solution uses the build feature of HoLolens and a workstation editor, which facilitates the use of the solution in diverse assembly environments. The results of the experiments show that the developed MR solution can improve operator support, reduce errors, and enhance the overall efficiency of manual assembly processes. Moreover, it is shown to be an efficient tool for time measurement of the manual assembly process that has promising potential to replace sophisticated and time-consuming traditional time study methods.}
}
@article{SAHAYAM2022103939,
title = {Brain tumor segmentation using a hybrid multi resolution U-Net with residual dual attention and deep supervision on MR images},
journal = {Biomedical Signal Processing and Control},
volume = {78},
pages = {103939},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103939},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422004396},
author = {Subin Sahayam and Rahul Nenavath and Umarani Jayaraman and Surya Prakash},
keywords = {Brain tumor segmentation, BraTS 2020 dataset, Multi-resolution, Residual block, Dual attention, Deep supervision},
abstract = {Manual identification of brain tumors in Magnetic Resonance (MR) images is laborious, time-consuming, and human error-prone. Automatic segmentation of brain tumors from MR images aims to bridge the gap. U-Net, a deep learning model, has delivered promising results in generating brain tumor segments. However, the model tends to over-segment the tumor volume than required. It will have a significant impact on deploying the model for practical use. In this work, the baseline U-Net model has been studied with the addition of residual, multi-resolution, dual attention, and deep supervision blocks. The goal of residual blocks is to efficiently extract features to reduce the semantic gap between low-level features from the decoder and high-level features from skip connections. The multiple resolution blocks have been added to extract features and analyze tumors of varying scales. The dual attention mechanism has been incorporated to highlight tumor representations and reduce over-segmentation. Finally, the deep supervision blocks have been added to utilize features from various decoder layers to obtain the target segmentation. The design of the proposed model has been justified with several experiments and ablation studies. The proposed model has been trained and evaluated on the BraTS2020 training and validation datasets. On the validation data, the proposed model has achieved a dice score of 0.60, 0.75, 0.62 for enhancing tumor (ET), whole tumor (WT), and tumor core (TC), respectively, and a Hausdorff 95 score of 46.84, 11.05, and 22.5, respectively. Compared to the baseline U-Net, the proposed model has outperformed WT and TC volumes in the Hausdorff 95 distance metric except for the ET volume.}
}
@article{BHAIYA2019129,
title = {Modified semiactive control with MR dampers for partially observed systems},
journal = {Engineering Structures},
volume = {191},
pages = {129-147},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S0141029618334047},
author = {Vishisht Bhaiya and M.K. Shrimali and S.D. Bharti and T.K. Datta},
keywords = {Linear Quadratic Gaussian (LQG), MR damper, Kalman filter, Clipped optimal control, Semiactive control, Sliding mode control},
abstract = {A modified semiactive control scheme with the MR damper for partially observed system is presented. The proposed control scheme augments the state variables by two filter variables and passes a white noise through the filters to obtain the desired seismic excitation to the structure. The two filters are incorporated at the base of the structure, making the input excitation to the structure-filter system a white noise. Thus, a more theoretical rigor is incorporated in the use of the Kalman filter for the state estimation as both the excitation and measurement noise should be ideally white, if the full state of the system is to be derived from the measured states using the Kalman filter. Further, using the results of a sensitivity analysis, the proposed algorithm fixes the covariances of the excitation and noise, that are provided as inputs to the Kalman filter. This is done in order to avoid any numerical instability of the control algorithm. Semi active control of a ten story building frame fitted with three MR dampers under earthquake ground motion is taken as an illustrative example. Theoretically obtained results of the proposed algorithm are compared with those of the conventional algorithm in which the ground motion is directly provided as input to the structure without the use of filters. Further, the use of the developed control algorithm in real time application which requires a trained ANN to generate compatible white noise signal from the online measurement of the ground motion, is described. The generated white noise signal (in real time) is provided as input to the proposed algorithm. The online application of the control algorithm is validated by a numerical experimentation in which three different types of specified time histories of ground acceleration are assumed as the expected future ground accelerations, which are measured online. The results of the numerical experiment are compared with those obtained from the theoretical analysis. It is shown that the scheme of the online application of the proposed algorithm performs satisfactorily. Further, it is shown that the proposed control algorithm may become unstable if the covariance parameters of the excitation and noise are not properly adjusted with respect to the expected mean square value of the ground acceleration.}
}
@article{LU2024103148,
title = {A model-based MR parameter mapping network robust to substantial variations in acquisition settings},
journal = {Medical Image Analysis},
volume = {94},
pages = {103148},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103148},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524000732},
author = {Qiqi Lu and Jialong Li and Zifeng Lian and Xinyuan Zhang and Qianjin Feng and Wufan Chen and Jianhua Ma and Yanqiu Feng},
keywords = {Magnetic resonance imaging, Parameter mapping, Deep learning, Regularization},
abstract = {Deep learning methods show great potential for the efficient and precise estimation of quantitative parameter maps from multiple magnetic resonance (MR) images. Current deep learning-based MR parameter mapping (MPM) methods are mostly trained and tested using data with specific acquisition settings. However, scan protocols usually vary with centers, scanners, and studies in practice. Thus, deep learning methods applicable to MPM with varying acquisition settings are highly required but still rarely investigated. In this work, we develop a model-based deep network termed MMPM-Net for robust MPM with varying acquisition settings. A deep learning-based denoiser is introduced to construct the regularization term in the nonlinear inversion problem of MPM. The alternating direction method of multipliers is used to solve the optimization problem and then unrolled to construct MMPM-Net. The variation in acquisition parameters can be addressed by the data fidelity component in MMPM-Net. Extensive experiments are performed on R2 mapping and R1 mapping datasets with substantial variations in acquisition settings, and the results demonstrate that the proposed MMPM-Net method outperforms other state-of-the-art MR parameter mapping methods both qualitatively and quantitatively.}
}
@article{SERRANO20161,
title = {Virtual reality and stimulation of touch and smell for inducing relaxation: A randomized controlled trial},
journal = {Computers in Human Behavior},
volume = {55},
pages = {1-8},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215300856},
author = {Berenice Serrano and Rosa M. Baños and Cristina Botella},
keywords = {Virtual reality, Mood-induction, Relaxation, Touch, Smell, Augmented virtuality},
abstract = {The aim of this study was to test the efficacy of a mood-induction procedure in a Virtual Reality (VR-MIP) environment for inducing relaxation and generating sense of presence, and to test whether the stimulation of the senses of touch and smell improves the efficacy of this VR-MIP. A controlled study was carried out with four experimental conditions. All of them included the VR-MIP to induce relaxation, but varying the senses stimulated. The sample consisted of 136 participants randomly assigned to one of the four experimental conditions. Emotions and sense of presence were evaluated. The results showed statistical differences before and after mood-induction and a high sense of presence in all groups. However, no statistical differences were found among the four groups on emotions and sense of presence. The results showed that the VR-MIP was effective; however, the stimulation of the senses of touch and smell did not show significate improve of the mood-induction or the sense of presence. It was identified a trend in favor of the groups where the sense of touch was stimulated, they seemed more relaxed and the sense of presence was higher. We hypothesized that the stimulation of sense of touch, could improve the efficacy when using VR-MIP because it provides more sensory information.}
}
@article{MENDES2020101731,
title = {PIÑATA: Pinpoint insertion of intravenous needles via augmented reality training assistance},
journal = {Computerized Medical Imaging and Graphics},
volume = {82},
pages = {101731},
year = {2020},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2020.101731},
url = {https://www.sciencedirect.com/science/article/pii/S0895611120300343},
author = {Helena Catarina Margarido Mendes and Cátia Isabel Andrade Botelho Costa and Nuno André {da Silva} and Francisca Pais Leite and Augusto Esteves and Daniel Simões Lopes},
keywords = {Medical training, Optical see-through augmented reality, Central venous catheterization, Needle placement, Professional assessment},
abstract = {Conventional needle insertion training relies on medical dummies that simulate surface anatomy and internal structures such as veins or arteries. These dummies offer an interesting space to augment with useful information to assist training practices, namely, internal anatomical structures (subclavian artery and vein, internal jugular vein and carotid artery) along with target point, desired inclination, position and orientation of the needle. However, limited research has been conducted on Optical See-Through Augmented Reality (OST-AR) interfaces for training needle insertion, especially for central venous catheterization (CVC). In this work we introduce PIÑATA, an interactive tool to explore the benefits of OST-AR in CVC training using a dummy of the upper torso and neck; andexplore if PIÑATA complements conventional training practices.. Our design contribution also describes the observation and co-design sessions used to collect user requirements, usability aspects and user preferences. This was followed by a comparative study with 18 participants - attending specialists and medical residents - that performed needle insertion tasks for CVC with PIÑATAand the conventional training system. The performance was objectively measured by task completion time and number of needle insertion errors. A correlation was found between the task completion time in the two training methods, suggesting the concurrent validity of our OST-AR tool. An inherent difference in the task completion time (p =0.040) and in the number of errors (p = 0.036) between novices and experts proved the construct validity of the new tool. The qualitative answers of the participants also suggest its face and content validity, a high acceptability rate and a medium perceived workload. Finally, the result of semi-structured interviews with these 18 participants revealed that 14 of them considered that PIÑATA can complement the conventional training system, especially due to the visibility of the vessels inside the simulator. 13 agreed that OST-AR adoption in these scenarios is likely, particularly during early stages of training. Integration with ultrasound information was highlighted as necessary future work. In sum, the overall results show that the OST-AR tool proposed can complement the conventional training of CVC.}
}
@article{MATSAS2018168,
title = {Prototyping proactive and adaptive techniques for human-robot collaboration in manufacturing using virtual reality},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {50},
pages = {168-180},
year = {2018},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736584516303611},
author = {Elias Matsas and George-Christopher Vosniakos and Dimitris Batras},
keywords = {Human-robot collaboration, Virtual environments, Human-robot interaction, Safety, Proactive techniques, Adaptive trajectory, Collision Avoidance},
abstract = {Human-Robot Interaction (HRI) has emerged in recent years as a need for common collaborative execution of manufacturing tasks. This work examines two types of techniques of safe collaboration that do not interrupt the flow of collaboration as far as possible, namely proactive and adaptive. The former are materialised using audio and visual cognitive aids, which the user receives as dynamic stimuli in real time during collaboration, and are aimed at information enrichment of the latter. Adaptive techniques investigated refer to the robot; according to the first one of them the robot decelerates when a forthcoming contact with the user is traced, whilst according to the second one the robot retracts and moves to the final destination via a modified, safe trajectory, so as to avoid the human. The effectiveness as well as the activation criteria of the above techniques are investigated in order to avoid possible pointless or premature activation. Such investigation was implemented in a prototype highly interactive and immersive Virtual Environment (VE), in the framework of H-R collaborative hand lay-up process of carbon fabric in an industrial workcell. User tests were conducted, in which both subjective metrics of user satisfaction and performance metrics of the collaboration (task completion duration, robot mean velocity, number of detected human-robot collisions etc.) After statistical processing, results do verify the effectiveness of safe collaboration techniques as well as their acceptability by the user, showing that collaboration performance is affected to a different extent.}
}
@article{LUQUETTIDOSSANTOS2009159,
title = {The use of questionnaire and virtual reality in the verification of the human factors issues in the design of nuclear control desk},
journal = {International Journal of Industrial Ergonomics},
volume = {39},
number = {1},
pages = {159-166},
year = {2009},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2008.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169814108001376},
author = {Isaac José Antonio {Luquetti dos Santos} and Cláudio Henrique {dos Santos Grecco} and Antonio Carlos {Abreu Mol} and Paulo Victor {Rodrigues Carvalho}},
keywords = {Human factors, Control desk, Nuclear reactor, Virtual reality, Questionnaire},
abstract = {A nuclear control room is a complex system that controls a thermodynamic process, where the operators interact with systems and human-system interfaces that have significant implications for the nuclear plant safety. This paper presents a case study in which an evaluation using a questionnaire, based on human factors guidelines, has been applied in a real and a virtual control desk of a research nuclear reactor. The aims of this study are, firstly, to provide designers a comprehensible human factors questionnaire for the evaluation of nuclear control desks and, secondly, to show that the verification of the human factors issues in the design of nuclear control desks can be carried out by experts using virtual models. The evaluation has been carried out by two human factors experts and two licensed operators. The level of agreement among the evaluators for all portions of the human factors questionnaire was greater than 0.70. The results indicated the validity of the human factors questionnaire and that the virtual reality technology is an useful tool, assisting the designers and evaluators during the design review of nuclear control desks, making possible the identification of problems in the early phase of the design.
Relevance to industry
Human factors review process is an integral component of the final safety analysis report of a nuclear power plant. The human factors questionnaire developed in this study will provide valuable support for the enhancement of the efficiency of the licensing process and adequacy of the design of nuclear control desks to the legal requirements of the international regulatory agency, making possible the identification of design problems that can influence the evaluation of the nuclear safety risk.}
}
@article{ZHANG201859,
title = {KaraKter: An autonomously interacting Karate Kumite character for VR-based training and research},
journal = {Computers & Graphics},
volume = {72},
pages = {59-69},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318300086},
author = {Liang Zhang and Guido Brunnett and Katharina Petri and Marco Danneberg and Steffen Masik and Nicole Bandow and Kerstin Witte},
keywords = {Virtual reality, Virtual character, Motion generation, Sports simulation},
abstract = {We report on the creation of an autonomous Karate Kumite character (KaraKter) that can be used for VR based training and research in Karate Kumite. For the real time interaction with KaraKter, a human athlete is tracked in a virtual environment. KaraKter moves in Karate specific ways, approaches the athlete and realizes adequate attacks depending on the behavior of the human. KaraKter passed tests on functionality and performance and has been evaluated by high ranking Karate experts. The evaluation showed that the athletes accept KaraKter as an actual opponent. All experts rated the system to be useful in the training of Karate Kumite.}
}
@article{PARVEAU2018263,
title = {3iVClass: a new classification method for Virtual, Augmented and Mixed Realities},
journal = {Procedia Computer Science},
volume = {141},
pages = {263-270},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.180},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318301},
author = {Marc Parveau and Mehdi Adda},
keywords = {MR, Mixed Reality, Augmented Reality, AR, VR, Definition, SLAM, 3iVClass},
abstract = {For several years, augmented and virtual reality technologies have attracted increasing interest in all areas. In the midst of this universe, the concept, already well known, of mixed reality has established itself as a distinct paradigm. However, and contrary to augmented and virtual realities, there is not a clear and one definition of what it is exactly and why it is different from the other concepts. In this article, we attempt to provide a new classification method to standardize the definition of virtual, augmented and mixed realities. First, a quick overview of existing taxonomies is made, then we present our classification which is based on three criteria we called 3iVClass (Immersion, Interaction, Information). Finally, in order to verify its reliability, we used this classification to propose a definition of mixed reality.}
}
@article{CHAN2024105675,
title = {Impacts of connections to the outside on underground space occupants’ psychophysiological health: A virtual reality-based experimental approach},
journal = {Tunnelling and Underground Space Technology},
volume = {147},
pages = {105675},
year = {2024},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2024.105675},
url = {https://www.sciencedirect.com/science/article/pii/S0886779824000932},
author = {Isabelle Y.S. Chan and Zhao Dong and Hao Chen},
keywords = {Underground space utilization, Connection to the outside environments, Psychophysiological health, Electroencephalogram (EEG), Virtual reality, Experimental research},
abstract = {Underground space occupants tend to face more psychophysiological health risks than their aboveground counterparts. Enabling a certain level of connection (e.g., physical, visual, or a mixture thereof) to the outside environments has been recognized as a way to mitigate the risks. However, their impacts on occupants’ psychophysiological health have not been known with any precision. This research, therefore, aims to improve our understanding of the impacts of connection to the outside on occupants’ psychophysiological health. In reality, it would be too expensive to build the physical connections and test their impacts. Virtual reality (VR) can circumvent this problem by bringing a person to immersive environments to carry out experiments. This study firstly developed four VR scenes with different connection designs. Then, it recruited forty-seven participants randomly assigned to the four scenes for the experiments. The participants’ psychophysiological responses, including mental workload, alertness, and mental fatigue, were measured continuously using electroencephalogram (EEG) signals. A series of tests, including Wilcoxon signed-rank and Kruskal-Wallis tests, were performed. The results indicate that i) physical connection to the outside environments has no significant impact on underground space occupants’ psychophysiological states under stress, and ii) while visual connection has no significant impact on occupants’ mental workload and fatigue, it buffers the increase in their alertness levels under stressful situations. Low alertness levels are associated with creativity and novelty. Thus, visual connection to the outside environments is recommended for underground spaces for creative purposes (e.g., art studios and creative media production). This research provides a significant reference for better harnessing underground space by considering their connection to the outside environments.}
}
@article{LI2021101905,
title = {Towards quantitative and intuitive percutaneous tumor puncture via augmented virtual reality},
journal = {Computerized Medical Imaging and Graphics},
volume = {90},
pages = {101905},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.101905},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121000549},
author = {Ruotong Li and Yuqi Tong and Tianpei Yang and Jianxi Guo and Weixin Si and Yanfang Zhang and Reinhard Klein and Pheng-Ann Heng},
keywords = {Augmented virtual reality, Optimal path planning, Collaborative holographic navigation, Respiratory tumor puncture},
abstract = {In recent years, the radiofrequency ablation (RFA) therapy has become a widely accepted minimal invasive treatment for liver tumor patients. However, it is challenging for doctors to precisely and efficiently perform the percutaneous tumor punctures under free-breathing conditions. This is because the traditional RFA is based on the 2D CT Image information, the missing spatial and dynamic information is dependent on surgeons’ experience. This paper presents a novel quantitative and intuitive surgical navigation modality for percutaneous respiratory tumor puncture via augmented virtual reality, which is to achieve the augmented visualization of the pre-operative virtual planning information precisely being overlaid on intra-operative surgical scenario. In the pre-operation stage, we first combine the signed distance field of feasible structures (like liver and tumor) where the puncture path can go through and unfeasible structures (like large vessels and ribs) where the needle is not allowed to go through to quantitatively generate the 3D feasible region for percutaneous puncture. Then we design three constraints according to the RFA specialists consensus to automatically determine the optimal puncture trajectory. In the intra-operative stage, we first propose a virtual-real alignment method to precisely superimpose the virtual information on surgical scenario. Then, a user-friendly collaborative holographic interface is designed for real-time 3D respiratory tumor puncture navigation, which can effectively assist surgeons fast and accurately locating the target step-by step. The validation of our system is performed on static abdominal phantom and in vivo beagle dogs with artificial lesion. Experimental results demonstrate that the accuracy of the proposed planning strategy is better than the manual planning sketched by experienced doctors. Besides, the proposed holographic navigation modality can effectively reduce the needle adjustment for precise puncture as well. Our system shows its clinical feasibility to provide the quantitative planning of optimal needle path and intuitive in situ holographic navigation for percutaneous tumor ablation without surgeons’ experience-dependence and reduce the times of needle adjustment. The proposed augmented virtual reality navigation system can effectively improve the precision and reliability in percutaneous tumor ablation and has the potential to be used for other surgical navigation tasks.}
}
@article{MOON2021108026,
title = {AR and ARMA model order selection for time-series modeling with ImageNet classification},
journal = {Signal Processing},
volume = {183},
pages = {108026},
year = {2021},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2021.108026},
url = {https://www.sciencedirect.com/science/article/pii/S0165168421000657},
author = {Jihye Moon and Md Billal Hossain and Ki H. Chon},
keywords = {ImageNet classification, Convolutional neural network, Time series modeling, AR model identification, ARMA model identification, Deep neural network, Physical system modeling},
abstract = {We propose model order selection methods for autoregressive (AR) and autoregressive moving average (ARMA) time-series modeling based on ImageNet classifications with a 2-dimensional convolutional neural network (2-D CNN). We designed two models for two realistic scenarios: (1) a general model which emulates the scenario that validation and test datasets do not necessarily have the same dynamics as the training data, (2) a specific model which emulates the opposite scenario—the validation and test datasets share the dynamics of the training data. The results were compared to those of both Akaike Information criterion (AIC) and Bayesian Information criterion (BIC). Using simulation examples, we trained 2-D CNN-based Inception-v3 and ResNet50-v2 models for either AR or ARMA order selection for each of the two scenarios. The proposed ResNet50-v2 to use both time-frequency and the original time series data outperformed AIC and BIC for all scenarios. For the general model, the average of relative error reduction (ARER) when compared to the BIC method in the clean and three noisy environments was 19.07% (±14.22%) for the AR order for an AR process, and 5.67% (±2.83%) for the ARMA order for an ARMA process. The ARERs significantly improved to 73.92% (±30.95%) and 65.58% (±38.61%) for the AR and ARMA models, respectively, for the specific model scenario.}
}
@article{CHOOI2008473,
title = {Design, modelling and testing of magnetorheological (MR) dampers using analytical flow solutions},
journal = {Computers & Structures},
volume = {86},
number = {3},
pages = {473-482},
year = {2008},
note = {Smart Structures},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2007.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0045794907000739},
author = {Weng W. Chooi and S. Olutunde Oyadiji},
keywords = {Magnetorheological (MR) fluids, Magnetorheological dampers, Semi-active dampers, Annular flow, Herschel–Bulkley fluid model, Bingham plastic model},
abstract = {The most widely used configuration of MR dampers incorporates an annular gap through which the magnetically active MR fluid is forced to flow. The damping force, which is also known as the restoring force and opposes the applied external force, is produced by the movement of the MR fluid through this annular gap. The design of this annular gap usually utilises an approximation that represents this assembly as a flow of fluids between two infinitely wide parallel plates. This assumption has been found to be fairly accurate and sufficient for engineering design purposes, especially when the size of the annular gap is small relative to the mean radius of the annulus, which is usually the case for MR dampers. However, it is still important to have expressions that represent the physical processes exactly. In this paper, the general solutions are derived and the corresponding methodology for representing exactly the flow of fluids with a yield stress through annuli is given. Computational fluid dynamics simulations were also carried out to validate the general expressions presented. The general expression is applicable to any models of fluids with a yield stress. An example of the application of the general analytical expressions using the Herschel–Buckley model is given, and the limitations of the parallel-plate approximation is illustrated for configurations whereby the size of the annular gap relative to the mean radius is large. A mathematical model for a double-tube MR damper fabricated at the University of Manchester, UK is developed based on the annular solution and on the compressibility of MR fluid inside the chambers. It is shown that the modelling procedure represents the MR damper satisfactorily.}
}
@article{FERNANDEZDELAMO2020101096,
title = {Structured authoring for AR-based communication to enhance efficiency in remote diagnosis for complex equipment},
journal = {Advanced Engineering Informatics},
volume = {45},
pages = {101096},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101096},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300653},
author = {Iñigo {Fernández del Amo} and John Erkoyuncu and Rok Vrabič and Romain Frayssinet and Cristina {Vazquez Reynel} and Rajkumar Roy},
keywords = {Augmented reality, Maintenance, Diagnosis, Remote collaboration},
abstract = {Remote diagnosis procedures are prone to communication errors due to varying levels of experience and knowledge between expert maintainers and technicians. These result in inefficiencies that delay the diagnosis process. The aim of the paper is to develop a Structured-Message Authoring framework for Augmented Reality (AR) Remote Communication (SMAARRC) and to evaluate its ability to enhance the efficiency of remote diagnosis services. The framework proposes a message structure and automatic AR content creation rules for it that enable data capture and sharing within a remote context. Laboratory experiments present an average time reduction of 56% for remote calls while maintaining same quality compared to traditional remote communication methods (phone calls and emails). Remote experts feedback evidence the usability and feasibility of this framework to work in real-life conditions.}
}
@article{LUSTERMANS2022107116,
title = {Optimized automated cardiac MR scar quantification with GAN‐based data augmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107116},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107116},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004977},
author = {Didier R.P.R.M. Lustermans and Sina Amirrajab and Mitko Veta and Marcel Breeuwer and Cian M. Scannell},
keywords = {Deep learning, Cardiac MRI, Myocardial scar quantification, Synthetic data, Generative adversarial networks},
abstract = {Background
The clinical utility of late gadolinium enhancement (LGE) cardiac MRI is limited by the lack of standardization, and time-consuming postprocessing. In this work, we tested the hypothesis that a cascaded deep learning pipeline trained with augmentation by synthetically generated data would improve model accuracy and robustness for automated scar quantification.
Methods
A cascaded pipeline consisting of three consecutive neural networks is proposed, starting with a bounding box regression network to identify a region of interest around the left ventricular (LV) myocardium. Two further nnU-Net models are then used to segment the myocardium and, if present, scar. The models were trained on the data from the EMIDEC challenge, supplemented with an extensive synthetic dataset generated with a conditional GAN.
Results
The cascaded pipeline significantly outperformed a single nnU-Net directly segmenting both the myocardium (mean Dice similarity coefficient (DSC) (standard deviation (SD)): 0.84 (0.09) vs 0.63 (0.20), p < 0.01) and scar (DSC: 0.72 (0.34) vs 0.46 (0.39), p < 0.01) on a per-slice level. The inclusion of the synthetic data as data augmentation during training improved the scar segmentation DSC by 0.06 (p < 0.01). The mean DSC per-subject on the challenge test set, for the cascaded pipeline augmented by synthetic generated data, was 0.86 (0.03) and 0.67 (0.29) for myocardium and scar, respectively.
Conclusion
A cascaded deep learning-based pipeline trained with augmentation by synthetically generated data leads to myocardium and scar segmentations that are similar to the manual operator, and outperforms direct segmentation without the synthetic images.}
}
@article{MINISSI2024108194,
title = {Biosignal comparison for autism assessment using machine learning models and virtual reality},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108194},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108194},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002786},
author = {Maria Eleonora Minissi and Alberto Altozano and Javier Marín-Morales and Irene Alice {Chicchi Giglioli} and Fabrizia Mantovani and Mariano Alcañiz},
keywords = {Virtual reality, Statistical machine learning, Biosignal, Autism spectrum disorder, Eye movements, Motor skills},
abstract = {Clinical assessment procedures encounter challenges in terms of objectivity because they rely on subjective data. Computational psychiatry proposes overcoming this limitation by introducing biosignal-based assessments able to detect clinical biomarkers, while virtual reality (VR) can offer ecological settings for measurement. Autism spectrum disorder (ASD) is a neurodevelopmental disorder where many biosignals have been tested to improve assessment procedures. However, in ASD research there is a lack of studies systematically comparing biosignals for the automatic classification of ASD when recorded simultaneously in ecological settings, and comparisons among previous studies are challenging due to methodological inconsistencies. In this study, we examined a VR screening tool consisting of four virtual scenes, and we compared machine learning models based on implicit (motor skills and eye movements) and explicit (behavioral responses) biosignals. Machine learning models were developed for each biosignal within the virtual scenes and then combined into a final model per biosignal. A linear support vector classifier with recursive feature elimination was used and tested using nested cross-validation. The final model based on motor skills exhibited the highest robustness in identifying ASD, achieving an AUC of 0.89 (SD = 0.08). The best behavioral model showed an AUC of 0.80, while further research is needed for the eye-movement models due to limitations with the eye-tracking glasses. These findings highlight the potential of motor skills in enhancing objectivity and reliability in the early assessment of ASD compared to other biosignals.}
}
@article{ZHOU2018904,
title = {Prediction of a New Kind of MR Data},
journal = {Procedia Computer Science},
volume = {131},
pages = {904-910},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918306793},
author = {Hong Zhou and Shenghui Zhao},
keywords = {MR, LSTM, Data Prediction},
abstract = {In the field of TD-LTE network problem analysis, compared with traditional methods such as DT(Driver Test) and CQT(Call Quality Test), MR (Measurement Report) has the advantages of comprehensive information and high efficiency, begins to get more and more attention and application. In order to solve the problem of limited open time of measurement data acquisition system, a data prediction method based on LSTM (Long Short-Term Memory) model is proposed. Selecting part of the MR parameters as the experimental object, training LSTM Model with measurement data in a district of Beijing. Experimental results show that the proposed method can predict MR data accurately. Compared with the traditional prediction model ARIMA (Autoregressive Integrated Moving Average Model), this method has lower prediction error and more stable generalization ability.}
}
@article{LI2020502,
title = {Who will use augmented reality? An integrated approach based on text analytics and field survey},
journal = {European Journal of Operational Research},
volume = {281},
number = {3},
pages = {502-516},
year = {2020},
note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718308725},
author = {Han Li and Ashish Gupta and Jie Zhang and Nick Flor},
keywords = {Text analytics, Multi-method approach, Technical features, Augmented reality, Post-adoption use intention},
abstract = {Next-generation technologies such as Augmented Reality and Virtual Reality are fast permeating many industry and society sectors. Their market is projected to reach $95 billion by 2025, representing a large portion of the economy within the next decade. With these technologies gaining wide popularity, it is critical to understand their usage in the context of the various benefits and perils that they offer. Even though top-rated mobile applications face an increasing challenge to retain users, few studies have attempted to decipher the dilemma in their continuance momentum. In this study, we focus on Pokémon GO, a top-rated Augmented Reality app, using it as a special case to investigate factors influencing user continuance and more use intention. We extend expectation confirmation theory by incorporating the effects of subjective norm, perceived risk, technical features and sense of direction. To increase the relevance and richness of our understanding of risks and benefits, we integrate the text analytics and survey-based theory-validating research methodology to build and test our research model. Our findings suggest that rational risk/benefit calculus and satisfaction are two primary inputs for continuance intention. Besides physical health benefits, users also value the benefits in mental health and relationship building. The risks in performance, time and safety are salient risk dimensions that negatively impact satisfaction. Furthermore, we find technical features play a strong role in influencing perceived benefits and user satisfaction. The findings also provide important practical implications for the designers of next-generation mobile apps enabled by Augmented Reality.}
}
@article{TUCKER20181,
title = {The effects of information and hazard on evacuee behavior in virtual reality},
journal = {Fire Safety Journal},
volume = {99},
pages = {1-11},
year = {2018},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0379711217300152},
author = {A. Tucker and K.L. Marsh and T. Gifford and X. Lu and P.B. Luh and R.S. Astur},
keywords = {Virtual reality, Evacuation, Anxiety, Information, Hazard, Navigation},
abstract = {Many contextual factors can influence evacuees' choice of egress route during an emergency. Anxiety caused by the emergency situation may lead to suboptimal choices, resulting in slower evacuation and greater risk of injury or death. The present pilot study tests the influence of hazard level (presence of visible fire and smoke) and information about an obstacle (delivered verbally or through signage) on evacuees' anxiety levels and choice of egress route in a virtual reality (VR) simulation of a fire evacuation with multiple possible exits. Physiological measures were recorded and used to validate the efficacy of VR in inducing anxiety germane to the situation of interest. Consistent with our expectations, providing information about the obstacle was shown to decrease total evacuation time. Contrary to our predictions, it did not significantly impact evacuees' choice of exit. Information also had a marginally significant effect on participants' self-reported anxiety. Providing more targeted information may further reduce anxiety and evacuation time. More generally, VR appears well-suited to assessing individual and psychological factors in evacuations.}
}
@article{ALEOTTI2006388,
title = {A VIRTUAL REALITY BASED SYSTEM FOR PROGRAMMING MOBILE MANIPULATION TASKS},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {15},
pages = {388-393},
year = {2006},
note = {8th IFAC Symposium on Robot Control},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060906-3-IT-2910.00066},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016385457},
author = {Jacopo Aleotti and Alessandro Melzi and Stefano Caselli},
keywords = {Robot Programming by Demonstration, Telemanipulation, Virtual Reality},
abstract = {This paper presents a virtual reality based system for programming mobile manipulation tasks. The system supports two modes of operation. The first mode implements a virtual reality interface for teleoperation, while the second mode is based on the programming by demonstration paradigm. The system exploits an advanced user interface allowing users with little or no technical expertise to integrate new tasks in a mobile robotic platform. The user can specify trajectories to be performed by the mobile base by means of a natural interface. Moreover the operator can supervise and validate the demonstrated task through a simulation module, thereby reducing errors in the generation process. Some experiments involving the whole set of system components demonstrate the viability and effectiveness of the approach.}
}
@article{GUO2021101835,
title = {Fully automated 3D segmentation of MR-imaged calf muscle compartments: Neighborhood relationship enhanced fully convolutional network},
journal = {Computerized Medical Imaging and Graphics},
volume = {87},
pages = {101835},
year = {2021},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2020.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0895611120301300},
author = {Zhihui Guo and Honghai Zhang and Zhi Chen and Ellen {van der Plas} and Laurie Gutmann and Daniel Thedens and Peggy Nopoulos and Milan Sonka},
keywords = {Calf muscle compartment segmentation, Fully convolutional network, Edge constraint, Magnetic resonance image, 3D},
abstract = {Automated segmentation of individual calf muscle compartments from 3D magnetic resonance (MR) images is essential for developing quantitative biomarkers for muscular disease progression and its prediction. Achieving clinically acceptable results is a challenging task due to large variations in muscle shape and MR appearance. In this paper, we present a novel fully convolutional network (FCN) that utilizes contextual information in a large neighborhood and embeds edge-aware constraints for individual calf muscle compartment segmentations. An encoder–decoder architecture is used to systematically enlarge convolution receptive field and preserve information at all resolutions. Edge positions derived from the FCN output muscle probability maps are explicitly regularized using kernel-based edge detection in an end-to-end optimization framework. Our method was evaluated on 40 T1-weighted MR images of 10 healthy and 30 diseased subjects by fourfold cross-validation. Mean DICE coefficients of 88.00–91.29% and mean absolute surface positioning errors of 1.04–1.66 mm were achieved for the five 3D muscle compartments.}
}
@article{SARMADI2021106296,
title = {Joint scene and object tracking for cost-Effective augmented reality guided patient positioning in radiation therapy},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106296},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106296},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003709},
author = {Hamid Sarmadi and Rafael Muñoz-Salinas and M. {Álvaro Berbís} and Antonio Luna and R. Medina-Carnicer},
keywords = {Patient positioning, Augmented reality, Aruco markers, Generalized ICP, Marker mapper, Surface guided radiation therapy (SGRT)},
abstract = {Background and Objective
The research is done in the field of Augmented Reality (AR) for patient positioning in radiation therapy is scarce. We propose an efficient and cost-effective algorithm for tracking the scene and the patient to interactively assist the patient’s positioning process by providing visual feedback to the operator. Up to our knowledge, this is the first framework that can be employed for mobile interactive AR to guide patient positioning.
Methods
We propose a pointcloud processing method that, combined with a fiducial marker-mapper algorithm and the generalized ICP algorithm, tracks the patient and the camera precisely and efficiently only using the CPU unit. The 3D reference model and body marker map alignment is calculated employing an efficient body reconstruction algorithm.
Results
Our quantitative evaluation shows that the proposed method achieves a translational and rotational error of 4.17 mm/0.82∘ at 9 fps. Furthermore, the qualitative results demonstrate the usefulness of our algorithm in patient positioning on different human subjects.
Conclusion
Since our algorithm achieves a relatively high frame rate and accuracy employing a regular laptop (without a dedicated GPU), it is a very cost-effective AR-based patient positioning method. It also opens the way for other researchers by introducing a framework that could be improved upon for better mobile interactive AR patient positioning solutions in the future.}
}
@article{CACKOWSKI2023102799,
title = {ImUnity: A generalizable VAE-GAN solution for multicenter MR image harmonization},
journal = {Medical Image Analysis},
volume = {88},
pages = {102799},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102799},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523000609},
author = {Stenzel Cackowski and Emmanuel L. Barbier and Michel Dojat and Thomas Christen},
keywords = {Brain, Deep Adversarial Network, Data harmonization, Self-supervised learning, Radiomic features},
abstract = {ImUnity is an original 2.5D deep-learning model designed for efficient and flexible MR image harmonization. A VAE-GAN network, coupled with a confusion module and an optional biological preservation module, uses multiple 2D slices taken from different anatomical locations in each subject of the training database, as well as image contrast transformations for its training. It eventually generates ‘corrected’ MR images that can be used for various multi-center population studies. Using 3 open source databases (ABIDE, OASIS and SRPBS), which contain MR images from multiple acquisition scanner types or vendors and a large range of subjects ages, we show that ImUnity: (1) outperforms state-of-the-art methods in terms of quality of images generated using traveling subjects; (2) removes sites or scanner biases while improving patients classification; (3) harmonizes data coming from new sites or scanners without the need for an additional fine-tuning and (4) allows the selection of multiple MR reconstructed images according to the desired applications. Tested here on T1-weighted images, ImUnity could be used to harmonize other types of medical images.}
}
@article{BAYAHYA2019275,
title = {Virtual Reality in Dementia Diseases},
journal = {Procedia Computer Science},
volume = {163},
pages = {275-282},
year = {2019},
note = {16th Learning and Technology Conference 2019Artificial Intelligence and Machine Learning: Embedding the Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.109},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321489},
author = {Areej Y. Bayahya and Wadee AlHalabi and Sultan H. Al-Amri},
keywords = {Dementia, Geriatric Medicine, Memory, Virtual Reality},
abstract = {Most medical research that related to detection and training patients with dementia diseases are based on the traditional clinic tests with high cost devices in hospitals such as Magnetic Resonance Imaging (MRI). Accordingly, there is an expanding requirement for utilizing automated techniques in dementia's to completely use the advantages circumstances by advanced innovations. Subsequently, it was fitting to investigate the more current, more successful applications that receiving the psychological techniques with automated strategies. One of the techniques depends on virtual conditions which are given extra aces to psychological test and permit patients to immersive in a controlled domain. This paper proposed a serious game, intellectual electronic test for assessment of dementia's patients dependent on 3D virtual environments platform.}
}
@article{KIRAN2024100674,
title = {Accelerating autonomous vehicle safety through Real-Time Immersive virtual reality gaming simulations},
journal = {Entertainment Computing},
volume = {50},
pages = {100674},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100674},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000429},
author = {Ajmeera Kiran and Satish S. Salunkhe and B. Srinivas and M. Vanitha and Mohammed {Altaf Ahmed} and Janjhyam {Venkata Naga Ramesh}},
keywords = {Autonomous Vehicles, Virtual Reality, Driving Simulation, Real-Time Rendering, Vehicle Safety, Immersive Environments, Gaming Engines},
abstract = {Although autonomous vehicles (AVs) have great potential for improving traffic safety, several questions about their practicality persist. Although it is crucial, doing tests on the road may be risky and costly. An easier, quicker, and less expensive option is to use virtual reality (VR) driving simulators. In this research, we investigate how virtual reality (VR) game simulations may speed up the process of validating the safety of AVs. We suggest building complex dynamic environments using consumer VR game engines to put AVs through their paces. Thanks to the physics modelling and real-time rendering features, you can test your car in realistic conditions. We review the advantages of conventional testing methods, including quickly iterating environments, assessing potentially dangerous edge situations, scaling effectively, and saving money. Additionally, our method enables programmers to collect information and opinions from big groups of real drivers via online games. We draw attention to unanswered questions concerning data-driven simulation adaptation, variability modelling, and fidelity. Achieving thorough, quick, and ethical safety validation has never been more feasible than using game-inspired VR simulations, significantly because the development of AVs is exceeding physical testing. This technique can potentially speed up the safe deployment of autonomous functions by making high-quality simulation more accessible to a broader audience.}
}