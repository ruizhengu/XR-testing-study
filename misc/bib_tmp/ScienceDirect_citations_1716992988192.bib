@article{KLASER2021102079,
title = {Imitation learning for improved 3D PET/MR attenuation correction},
journal = {Medical Image Analysis},
volume = {71},
pages = {102079},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521001250},
author = {Kerstin Kläser and Thomas Varsavsky and Pawel Markiewicz and Tom Vercauteren and Alexander Hammers and David Atkinson and Kris Thielemans and Brian Hutton and M.J. Cardoso and Sébastien Ourselin},
keywords = {MR to CT synthesis, Deep learning, Imitation learning, Convolutional neural network},
abstract = {The assessment of the quality of synthesised/pseudo Computed Tomography (pCT) images is commonly measured by an intensity-wise similarity between the ground truth CT and the pCT. However, when using the pCT as an attenuation map (μ-map) for PET reconstruction in Positron Emission Tomography Magnetic Resonance Imaging (PET/MRI) minimising the error between pCT and CT neglects the main objective of predicting a pCT that when used as μ-map reconstructs a pseudo PET (pPET) which is as similar as possible to the gold standard CT-derived PET reconstruction. This observation motivated us to propose a novel multi-hypothesis deep learning framework explicitly aimed at PET reconstruction application. A convolutional neural network (CNN) synthesises pCTs by minimising a combination of the pixel-wise error between pCT and CT and a novel metric-loss that itself is defined by a CNN and aims to minimise consequent PET residuals. Training is performed on a database of twenty 3D MR/CT/PET brain image pairs. Quantitative results on a fully independent dataset of twenty-three 3D MR/CT/PET image pairs show that the network is able to synthesise more accurate pCTs. The Mean Absolute Error on the pCT (110.98 HU ± 19.22 HU) compared to a baseline CNN (172.12 HU ± 19.61 HU) and a multi-atlas propagation approach (153.40 HU ± 18.68 HU), and subsequently lead to a significant improvement in the PET reconstruction error (4.74% ± 1.52% compared to baseline 13.72% ± 2.48% and multi-atlas propagation 6.68% ± 2.06%).}
}
@article{ZHANG2024594,
title = {Integrate augmented reality and force sensing devices to assist blind area assembly},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {594-605},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000840},
author = {Xiaotian Zhang and Weiping He and Jilong Bai and Mark Billinghurst and Daisong Liu and Jiepeng Dong and Yunfei Qin and Tianyu Liu and Zenglei Wang},
keywords = {Augmented reality, Manual assembly, Blind area, Occluded components},
abstract = {Blind area assembly is one of the challenges of manual assembly. The obscured pose of occluded components can lead to errors and reduce assembly efficiency. It has been observed that the sense of touch is commonly used to localize the assembly interface to complete the assembly in the blind area. Based on this, a blind area assembly assistance system was developed, which integrates augmented reality and force sensing devices. The system estimates the component poses during the tactile localizing stage based on the measured forces and moments applied to the product by the components, and then overlays virtual models of the components in situ to support assembly in the blind area. A user study was conducted to evaluate the performance and user experience of the system. The results showed that the system could successfully track and visualize target components during the tactile localizing stage, demonstrating great performance and user experience. The target component visualization can expedite assembly in the blind area, improve the system ease-of-use, and reduce the perceived workload. Finally, the limitations of the research are discussed and some directions for future work are provided.}
}
@article{CHEN2024105158,
title = {Augmented reality, deep learning and vision-language query system for construction worker safety},
journal = {Automation in Construction},
volume = {157},
pages = {105158},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105158},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004181},
author = {Haosen Chen and Lei Hou and Shaoze Wu and Guomin Zhang and Yang Zou and Sungkon Moon and Muhammed Bhuiyan},
keywords = {Construction safety, Deep learning, Vision-language models, Augmented reality},
abstract = {Low situational awareness contributes to safety incidents in construction. Existing Deep Learning (DL)-based applications lack the capability to provide context-specific and interactive feedback that is essential for workers to fully understand their surrounding environments. This paper proposes the Visual Construction Safety Query (VCSQ) system. The system encompasses real-time Image Captioning (IC), safety-centric Visual Question Answering (VQA), and keyword-based Image-Text Retrieval (ITR), integrated with head-mounted Augmented Reality (AR) devices. System validation includes benchmarks and real-world images. The ITR module posted high recall rates of 0.801 and 0.835 for Recall@5 and @10. The VQA module achieved an 89.7% accuracy rate, and the IC module had a SPICE score of 0.449. Feasibility tests and surveys confirmed the system's practical advantages in different construction scenarios. This study establishes an integration roadmap adaptable to future advancements in interactive DL and immersive AR.}
}
@article{GUALTIERI2023765,
title = {A human-centered conceptual model for integrating Augmented Reality and Dynamic Digital Models to reduce occupational risks in industrial contexts},
journal = {Procedia Computer Science},
volume = {217},
pages = {765-773},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.273},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023511},
author = {Luca Gualtieri and Andrea Revolti and Patrick Dallasega},
keywords = {Augmented reality, AR, safety, training, risk management},
abstract = {The article proposes a human-centered conceptual model to integrate Augmented Reality (AR) and Dynamic Digital Models (DDM) to improve training and reduce occupational risks in industrial contexts. A general model integrating DDM and AR to support immersive training and customized and real-time risk management is missing in the literature. The proposed conceptual model was preliminarily validated with company experts in a laboratory environment. According to the expert's feedback, the system can improve the efficacy of training by means of an immersive environment where users can better perceive hazards and safety-critical situations. Considering the capability of the conceptual model to support real-time and customized risk management, the participating experts argue that the technology seems not to be ready yet, even if it is very interesting and it would be very useful in practice to mitigate occupational risks. Future research activities will consist of the development of a prototypical system based on the presented conceptual model by considering specific user requirements and experts’ feedback.}
}
@article{SAMEI2020101588,
title = {A partial augmented reality system with live ultrasound and registered preoperative MRI for guiding robot-assisted radical prostatectomy},
journal = {Medical Image Analysis},
volume = {60},
pages = {101588},
year = {2020},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101588},
url = {https://www.sciencedirect.com/science/article/pii/S1361841519301288},
author = {Golnoosh Samei and Keith Tsang and Claudia Kesch and Julio Lobo and Soheil Hor and Omid Mohareri and Silvia Chang and S. Larry Goldenberg and Peter C. Black and Septimiu Salcudean},
keywords = {Image-guidance, Surgery, Ultrasound, Magnetic resonance imaging, Image fusion, Registration},
abstract = {We propose an image guidance system for robot assisted laparoscopic radical prostatectomy (RALRP). A virtual 3D reconstruction of the surgery scene is displayed underneath the endoscope’s feed on the surgeon’s console. This scene consists of an annotated preoperative Magnetic Resonance Image (MRI) registered to intraoperative 3D Trans-rectal Ultrasound (TRUS) as well as real-time sagittal 2D TRUS images of the prostate, 3D models of the prostate, the surgical instrument and the TRUS transducer. We display these components with accurate real-time coordinates with respect to the robot system. Since the scene is rendered from the viewpoint of the endoscope, given correct parameters of the camera, an augmented scene can be overlaid on the video output. The surgeon can rotate the ultrasound transducer and determine the position of the projected axial plane in the MRI using one of the registered da Vinci instruments. This system was tested in the laboratory on custom-made agar prostate phantoms. We achieved an average total registration accuracy of 3.2  ±  1.3 mm. We also report on the successful application of this system in the operating room in 12 patients. The average registration error between the TRUS and the da Vinci system for the last 8 patients was 1.4  ±  0.3 mm and average target registration error of 2.1  ±  0.8 mm, resulting in an in vivo overall robot system to MRI mean registration error of 3.5 mm or less, which is consistent with our laboratory studies.}
}
@article{FERNANDEZDELAMO2018148,
title = {Augmented Reality in Maintenance: An information-centred design framework},
journal = {Procedia Manufacturing},
volume = {19},
pages = {148-155},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300210},
author = {Iñigo {Fernández del Amo} and John Ahmet Erkoyuncu and Rajkumar Roy and Stephen Wilding},
keywords = {Augmented Reality, Maintenance, Information},
abstract = {Augmented Reality (AR) visualization capabilities can impact on maintenance. From enhancing performance to retrieving feedback, AR can close the information loop between maintenance information systems and the operations supported. Though, the design of AR applications is not aligned with current information systems, which prevents maintenance information to be used and improved properly. In this paper, industrial collaboration contributed to determine a framework for AR integration in maintenance systems. The framework describes information types, formats and interactions modes for AR to enhance efficiency improvements in maintenance of complex equipment. Semi-structured interviews and surveys with maintainers were conducted to determine the maintenance challenges and also to validate the framework proposed. Therefore, exposing future research in topics such as multimodal interaction, information contextualization and performance analysis to achieve the complete integration of AR in maintenance.}
}
@article{DEKLERK2019104,
title = {Usability studies on building early stage architectural models in virtual reality},
journal = {Automation in Construction},
volume = {103},
pages = {104-116},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517311378},
author = {Rui {de Klerk} and André Mendes Duarte and Daniel Pires Medeiros and José Pinto Duarte and Joaquim Jorge and Daniel Simões Lopes},
keywords = {Architectural design, Voxel-based modeling, Box shaped objects, Case studies, Early stage maquette, User participation, Professional evaluation, Virtual reality},
abstract = {Despite its marked success in recent years, it is still not clear how Virtual Reality (VR) can assist architects at the early stages of ideation and design. In this paper, we approach VR to build and explore maquettes at different scales in early design stages. To this end we developed a VR environment where user interactions are supported by untethered, easy to operate, peripherals, using a mobile virtual reality headset to provide virtual immersion and simplified geometric information to create voxel-based maquettes. Usability studies with laypeople suggest that the proposed system is both easier to use and more effective [better suited] than current CAD software to rapidly create simplified models. Additionally, tests with architects have shown the system's potential to improve their toolset. This is partly due to VR combining real-time performance with immersive exploration of the content, where body-scale relationships become visible to support the creative process, allowing architects to become both builders and explores of spatial constructs.}
}
@article{RASTOGI201947,
title = {Control system design for tokamak remote maintenance operations using assisted virtual reality and haptic feedback},
journal = {Fusion Engineering and Design},
volume = {139},
pages = {47-54},
year = {2019},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.12.094},
url = {https://www.sciencedirect.com/science/article/pii/S0920379618308603},
author = {Naveen Rastogi and Amit {Kumar Srivastava}},
keywords = {Remote handling, Haptic feedback, Virtual reality, Robotics},
abstract = {To meet the future electricity demands, tokamak based nuclear fusion machine is an attractive option for the researchers. The availability and reliability of these machines over a reasonable time span will be largely dependent on the planning of the remote maintenance scheme. The efficiency of the remote maintenance operations can be effectively increased by designing specialized remote handling control systems with high precision and accuracy. Huge efforts are needed to equip the RH operators with the advanced technologies that provide the real knowledge and interactions with the operating environment without being physically present. This can be achieved by using virtual reality techniques for tele-presence that may be extremely beneficial for the offline training as well as in assisting during the online RH operations. This paper outlines a concept with prototype experiments for an interactive control system using virtual reality integration and haptic feedback. Virtual 3D models of the complete work-cell have been developed and are updated synchronously for the monitoring and controlling of the robotic equipment. Force reflection servo type control strategy has been employed for the master-slave control. The proposed control system architecture and results from various test cases have been summarized here.}
}
@article{REDDY2001443,
title = {EMG Interfaces for VR and Telematic Control Applications},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {9},
pages = {443-446},
year = {2001},
note = {IFAC Conference on Telematics Applications in Automation and Robotics, Weingarten, Germany, 24-26 July 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41746-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017417460},
author = {Narender P. Reddy and Rajeev Unnikrishnan},
keywords = {biocontrol, telemanipulation, cybernetics, force control, position control},
abstract = {The actions of the human operator are measured and used to control VR environments and telemanipulators. Current interfacing devices have to be externally worn and are rather restrictive. The authors have developed interfaces using surface electromyograhy (EMG) signals. Surface EMG signals from digitorum superficialis were measured, processed and used to control simple geometric computer models of the index finger in free form flexion and squeezing of model objects held by the model finger. The results revealed that RMS errors were significantly lower when the subject was holding a real object in his/her hand when compared to the control of the model finger without holding any real object in the operator's hand. The present study established the proof of concept for object manipulation in VR environments using surface EMG signals from the operator 's hand.}
}
@incollection{SHERMAN2018724,
title = {Chapter 9 - Experience Conception and Design: Applying VR to a Problem},
editor = {William R. Sherman and Alan B. Craig},
booktitle = {Understanding Virtual Reality (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {724-779},
year = {2018},
series = {The Morgan Kaufmann Series in Computer Graphics},
isbn = {978-0-12-800965-9},
doi = {https://doi.org/10.1016/B978-0-12-800965-9.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800965900009X},
author = {William R. Sherman and Alan B. Craig},
keywords = {Art, Deploy, Design, Education, Engagement, Entertainment, Games, Marketing, Prototype, Training, User Testing, Virtual Reality (VR), Visualization, VR Experience, VR Experience Evaluation},
abstract = {Chapter 9 is where it all comes together. The focus is on how to apply virtual reality (VR) to solve a problem. The problem might range from an engineering problem, or an education problem, how to tell a story, or any number of other types of problems. The first issue at hand is whether or not VR can help you meet your goals. In other words, is VR an appropriate medium? This chapter takes a look at how one conceives of a new VR application, whether it is drawn from a different medium, or from another VR application, or is generated from scratch. As an aid in doing this, we enumerate a number of application areas and provide a number of exemplary VR experiences to inspire, and to show ideas and techniques. Additionally, we address a process for each of these methods. The chapter provides a method and workflow that integrates the following guidelines, each of which is elucidated and expanded:Design deliberately;Prototype;Design with the system in mind;Design with the venue in mind;Design with the audience in mind;Design to engage the audience;Consider social interactions;Consider design tradeoffs;Design the user objective;Design the end of the experience;Employ User testing;Document, deploy, and evaluate the experience. This chapter also provides a perspective on how the VR application development process has changed over time, and how we expect it to change in the future.}
}
@article{OCK2024108002,
title = {Evaluation of the accuracy of an augmented reality-based tumor-targeting guide for breast-conserving surgery},
journal = {Computer Methods and Programs in Biomedicine},
volume = {245},
pages = {108002},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.108002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723006685},
author = {Junhyeok Ock and Sojin Moon and MinKyeong Kim and Beom Seok Ko and Namkug Kim},
keywords = {3D printing guide, Augmented reality guide, Breast cancer, Breast conservation surgery, Phantom study, Surgical planning},
abstract = {Background and Objectives
Although magnetic resonance imaging (MRI) is commonly used for breast tumor detection, significant challenges remain in determining and presenting the three-dimensional (3D) morphology of tumors to guide breast-conserving surgery. To address this challenge, we have developed the augmented reality-breast surgery guide (AR-BSG) and compared its performance with that of a traditional 3D-printed breast surgical guide (3DP-BSG).
Methods
Based on the MRI results of a breast cancer patient, a breast phantom made of skin, body, and tumor was fabricated through 3D printing and silicone-casting. AR-BSG and 3DP-BSG were executed using surgical plans based on the breast phantom's computed tomography scan images. Three operators independently inserted a catheter into the phantom using each guide. Their targeting accuracy was then evaluated using Bland–Altman analysis with limits of agreement (LoA). Differences between the users of each guide were evaluated using the intraclass correlation coefficient (ICC).
Results
The entry and end point errors associated with AR-BSG were −0.34±0.68 mm (LoA: −1.71–1.01 mm) and 0.81±1.88 mm (LoA: −4.60–3.00 mm), respectively, whereas 3DP-BSG was associated with entry and end point errors of −0.28±0.70 mm (LoA: −1.69–1.11 mm) and −0.62±1.24 mm (LoA: −3.00–1.80 mm), respectively. The AR-BSG's entry and end point ICC values were 0.99 and 0.97, respectively, whereas 3DP-BSG was associated with entry and end point ICC values of 0.99 and 0.99, respectively.
Conclusions
AR-BSG can consistently and accurately localize tumor margins for surgeons without inferior guiding accuracy AR-BSG can consistently and accurately localize tumor margins for surgeons without inferior guiding accuracy compared to 3DP-BSG. Additionally, when compared with 3DP-BSG, AR-BSG can offer better spatial perception and visualization, lower costs, and a shorter setup time.}
}
@article{SAUER201083,
title = {An Augmented Reality Supported Control System for Remote Operation and Monitoring of an Industrial Work Cell},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {23},
pages = {83-88},
year = {2010},
note = {2nd IFAC Symposium on Telematics Applications},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101005-4-RO-2018.00030},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015343287},
author = {Markus Sauer and Florian Leutert and Klaus Schilling},
keywords = {Telematics, Teleoperation, Robotic Manipulators, Augmented Reality, Remote control},
abstract = {Telematic control systems have a lot of potential applications in teleservicing of remote sites: they range from surveillance of regular operations over optimization of existing processes to remote error resolving. In this paper, we describe the general architecture of a telematic control system employed to survey an industrial work cell, and show how Augmented Reality (AR) can support an operator in grasping events and conditions at the remote site more clearly, thus getting a better situational awareness. After describing several specific examples from the AR tool-range, potential uses of the system are presented.}
}
@article{ZHANG2023107629,
title = {Depth error correction for plenoptic cameras based on an innovative AR system combining geometrical waveguide and micro-lens array},
journal = {Optics and Lasers in Engineering},
volume = {167},
pages = {107629},
year = {2023},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2023.107629},
url = {https://www.sciencedirect.com/science/article/pii/S0143816623001586},
author = {Jufan Zhang and Yao Zhou and Fengzhou Fang},
keywords = {Plenoptic camera, Depth error, Light field, Geometrical waveguide, Micro-lens array, Augmented reality, Depth of field, 3D image},
abstract = {Plenoptic cameras have been increasingly applied in three-dimensional (3D) optical imaging and measurements. Micro-lens array is the key component to capture the images at different depths simultaneously to realize the 3D reconstruction of the scenery. However, only a part of the captured information will be used eventually when focusing on specific objects, which implies a waste of time and resources that are incompetent for fast-response tasks like gesture recognition. Furthermore, it is still quite challenging to accurately calibrate the depths under complex circumstances, even sometimes losing a part of the target information due to the low signal-to-noise ratio. In this paper, a new hybrid optical system is developed combining a geometrical waveguide and micro-lens array to project a virtual scale network for quantitative depth calibration and fast-tracking of targets. Such a methodology based on the augmented reality (AR) mechanism helps to rapidly scope the targeted objects/features without reconstructing the full range model, significantly saving the processing time for a low-latency response. By establishing a geometrical model to quantitatively correlate the images in the auxiliary coordinate system and the virtual scale coordinate system, the depth error caused by the plenoptic camera can be calibrated and corrected. The coefficient of determination R2is used to evaluate the depth accuracy of 3D images and acts as the threshold to control the depth correction iterations. The closer the value of R2 is to 1, the more accurate the depth information is. Experiments proved that even under complex backgrounds or insufficient light, in virtue of the virtual coordinate networks, backgrounds could be rapidly filtered, and targets were effectively identified with accurate depth information. The algorithm flow chart for the correction of depth error is given. In such a way, the system can achieve faster and more accurate capturing of 3D objects in a real-time manner than conventional plenoptic cameras.}
}
@article{BUAYAI2023108194,
title = {Supporting table grape berry thinning with deep neural network and augmented reality technologies},
journal = {Computers and Electronics in Agriculture},
volume = {213},
pages = {108194},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108194},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923005823},
author = {Prawit Buayai and Kabin Yok-In and Daisuke Inoue and Hiromitsu Nishizaki and Koji Makino and Xiaoyang Mao},
keywords = {Smart agriculture, Grape detection, Berry thinning, Berry removing, Instance segmentation, Deep neural network},
abstract = {Berry thinning is a crucial process in table grape cultivation. Such visual features as bunch compactness, bunch form, and berry size are important factors affecting market value. Moreover, sufficient space for each berry to grow also largely influences the final product’s quality, such as the sugar concentration. Berry thinning requires professional skills, and it is usually accomplished only by experienced farmers. Furthermore, the appropriate period for berry thinning is limited to two weeks; hence, berry-thinning tasks have led to a bottleneck in terms of increasing the yield of table grape products. This paper addresses the aforementioned issue by proposing a system for empowering unskilled farmers to begin berry thinning without in-person coaching from expert farmers. The proposed system employs a deep neural network model to learn the knowledge required for identifying berries to be removed, and it uses augmented reality technology to display instructions based on this knowledge to naïve farmers through smart glasses. The proposed system was validated throughout the entire growing season in a real table grape field in Yamanashi Prefecture, Japan. It was confirmed that unskilled farmers can execute berry-thinning tasks immediately without training. Furthermore, they can become familiar with the proposed system quickly. The grape products from unskilled farmers who used the proposed system also had an 8.18 % higher average quality score than those from the skilled farmers.}
}
@article{CHEN2024105287,
title = {Mixed reality-based active Hazard prevention system for heavy machinery operators},
journal = {Automation in Construction},
volume = {159},
pages = {105287},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105287},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000232},
author = {Tingsong Chen and Nobuyoshi Yabuki and Tomohiro Fukuda},
keywords = {Virtual reality, Mixed reality, Construction sites, Heavy machinery, Safety management},
abstract = {The use of heavy machinery in construction has significantly improved efficiency but poses a risk of serious accidents when not operated safely. While warning systems can mitigate accidents from conscious operator errors, addressing risky behaviors stemming from unconscious operators remains a challenge, with mixed reality (MR) research in the construction industry often lacking focus on operator-related aspects. This paper describes a monitoring method for heavy machinery operators is proposed. It identifies whether an operator is inadvertently engaging in behavior that could lead to danger when they are not in good shape. This method uses MR devices worn by the operator to analyze the state of their eyes, head, and hands to determine whether the operator is currently capable of safely operating the machine. Real-time judgment is achieved by classifying specific movements and abnormal behaviors in advance, further reducing the occurrence of accidents.}
}
@article{CAO2021106634,
title = {Multi-sensor spatial augmented reality for visualizing the invisible thermal information of 3D objects},
journal = {Optics and Lasers in Engineering},
volume = {145},
pages = {106634},
year = {2021},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2021.106634},
url = {https://www.sciencedirect.com/science/article/pii/S0143816621001044},
author = {Yanpeng Cao and Yafei Dong and Fan Wang and Jiangxin Yang and Yanlong Cao and Xin Li},
keywords = {Thermal imaging, Spatial augmented reality, 3D reconstruction, Multi-sensor system, Non-destructive testing},
abstract = {In recent years, infrared imaging technology has been widely used to capture valuable thermal information of target objects for various applications such as construction engineering, advanced manufacturing, and medical diagnosis. However, it still remains a challenging task to visualize the invisible thermal information of three-dimensional objects for further evaluation and decision making. In this paper, we build a multi-sensor system consisting of a long-wave infrared camera, a RGBD camera, and a digital projector for multimodal data acquisition, real-time 3D thermographic reconstruction, and projector-based spatial augmented reality. We propose a hybrid method that combines the benefits of model-based ray casting and frame-based image warping to generate high-quality projected images with complete view coverage and abundant thermal details for visualizing the invisible thermal information on 3D objects. The effectiveness of the proposed method has been validated for two typical thermal imaging applications including human body temperature monitoring and non-destructive evaluation of composite materials.}
}
@article{MARTINEZ20151552,
title = {Structural damages prevention of the ITER vacuum vessel and ports by elasto-plastic analysis with regards to RCC-MR},
journal = {Fusion Engineering and Design},
volume = {98-99},
pages = {1552-1555},
year = {2015},
note = {Proceedings of the 28th Symposium On Fusion Technology (SOFT-28)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2015.06.159},
url = {https://www.sciencedirect.com/science/article/pii/S0920379615302131},
author = {Jean-Marc Martinez and Chang Hoon Jun and Christophe Portafaix and Alexander Alekseev and Carlo Sborchia and Chang-Ho Choi and Vincent Albin and Stephane Borrelly and Magali Cambazar and Thomas Gaucher and Samir Sfarni and Olivier Tailhardat},
keywords = {ITER, Vacuum vessel, Structural integrity, Analysis, Elasto-plasticity, RCC-MR},
abstract = {Several types of damages have to be prevented in order to guarantee the structural integrity of a structure with regards to RCC-MR; the P-type damages which can result from the application to a structure of a steadily and regularly increasing loading or a constant loading and the S-type damages during operational loading conditions which can only result from repeated application of loadings associated to the progressive deformations and fatigue. Following RCC-MR, the S-type damages prevention has to be started only when the structural integrity is guaranteed against P-type damages. The verification of the last one on the ITER vacuum vessel and ports has been performed by limit analysis with elasto-(perfectly)plastic material behavior. It is usual to employ non-linear analysis when the “classical” elastic analysis reaches its limit of linear application. Some elasto-plastic analyses have been performed considering several cyclic loadings to evaluate also more realistic structural margins of the against S-type damages.}
}
@article{SUN201315,
title = {Lung segmentation refinement based on optimal surface finding utilizing a hybrid desktop/virtual reality user interface},
journal = {Computerized Medical Imaging and Graphics},
volume = {37},
number = {1},
pages = {15-27},
year = {2013},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0895611113000050},
author = {Shanhui Sun and Milan Sonka and Reinhard R. Beichel},
keywords = {Lung segmentation, Virtual reality, Segmentation refinement, Optimal surface finding, Computed tomography},
abstract = {Recently, the optimal surface finding (OSF) and layered optimal graph image segmentation of multiple objects and surfaces (LOGISMOS) approaches have been reported with applications to medical image segmentation tasks. While providing high levels of performance, these approaches may locally fail in the presence of pathology or other local challenges. Due to the image data variability, finding a suitable cost function that would be applicable to all image locations may not be feasible. This paper presents a new interactive refinement approach for correcting local segmentation errors in the automated OSF-based segmentation. A hybrid desktop/virtual reality user interface was developed for efficient interaction with the segmentations utilizing state-of-the-art stereoscopic visualization technology and advanced interaction techniques. The user interface allows a natural and interactive manipulation of 3-D surfaces. The approach was evaluated on 30 test cases from 18 CT lung datasets, which showed local segmentation errors after employing an automated OSF-based lung segmentation. The performed experiments exhibited significant increase in performance in terms of mean absolute surface distance errors (2.54±0.75mm prior to refinement vs. 1.11±0.43mm post-refinement, p≪0.001). Speed of the interactions is one of the most important aspects leading to the acceptance or rejection of the approach by users expecting real-time interaction experience. The average algorithm computing time per refinement iteration was 150ms, and the average total user interaction time required for reaching complete operator satisfaction was about 2min per case. This time was mostly spent on human-controlled manipulation of the object to identify whether additional refinement was necessary and to approve the final segmentation result. The reported principle is generally applicable to segmentation problems beyond lung segmentation in CT scans as long as the underlying segmentation utilizes the OSF framework. The two reported segmentation refinement tools were optimized for lung segmentation and might need some adaptation for other application domains.}
}
@article{WANG2023451,
title = {Eye-shaped keyboard for dual-hand text entry in virtual reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {5},
pages = {451-469},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000438},
author = {Kangyu Wang and Yangqiu Yan and Hao Zhang and Xiaolong Liu and Lili Wang},
keywords = {Virtual reality, Human computer interaction, Text entry, Shaped keyboard, QWERTY-like key sequence, Dual-hand cooperation},
abstract = {We propose an eye-shaped keyboard for high-speed text entry in virtual reality (VR), having the shape of dual eyes with characters arranged along the curved eyelids, which ensures low density and short spacing of the keys. The eye-shaped keyboard references the QWERTY key sequence, allowing the users to benefit from their experience using the QWERTY keyboard. The user interacts with an eye-shaped keyboard using rays controlled with both the hands. A character can be entered in one step by moving the rays from the inner eye regions to regions of the characters. A high-speed auto-complete system was designed for the eye-shaped keyboard. We conducted a pilot study to determine the optimal parameters, and a user study to compare our eye-shaped keyboard with the QWERTY and circular keyboards. For beginners, the eye-shaped keyboard performed significantly more efficiently and accurately with less task load and hand movement than the circular keyboard. Compared with the QWERTY keyboard, the eye-shaped keyboard is more accurate and significantly reduces hand translation while maintaining similar efficiency. Finally, to evaluate the potential of eye-shaped keyboards, we conducted another user study. In this study, the participants were asked to type continuously for three days using the proposed eye-shaped keyboard, with two sessions per day. In each session, participants were asked to type for 20min, and then their typing performance was tested. The eye-shaped keyboard was proven to be efficient and promising, with an average speed of 19.89 words per minute (WPM) and mean uncorrected error rate of 1.939%. The maximum speed reached 24.97 WPM after six sessions and continued to increase.}
}
@article{GU2023100126,
title = {Transparent and energy-efficient electrochromic AR display with minimum crosstalk using the pixel confinement effect},
journal = {Device},
volume = {1},
number = {6},
pages = {100126},
year = {2023},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2023.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2666998623001850},
author = {Chang Gu and Yan Yan and Jiale He and Donglin Pu and Linsen Chen and Yu-Mo Zhang and Sean Xiao-An Zhang},
keywords = {transparent electrochromic display, energy-efficient bistability, the confinement effect between pixels,  photolithographic electrochromic materials, augmented reality interaction, signal crosstalk},
abstract = {Summary
Transparent non-emissive displays are ideal for augmented reality (AR) applications due to their ability to modulate ambient light as colored and transparent pixels. Electrochromic (EC) displays with transparent “ON” and “OFF” states can help enable such a display. However, unwanted image diffusion and signal crosstalk due to the imbalance between performance and the preparation process limit their practical application. In this work, we present a transparent and energy-efficient EC display prototype (4.2 inch, 16 × 16 pixels). We minimize the signal crosstalk problem by exploiting the confinement effect between EC pixels using the in situ photolithography of EC materials with the application of a photolithographic pixel definition layer and digital dispensing printing. The prototype has an optical modulation of ∼60%, a full-contrast lifetime tested beyond 20,000 cycles, an ultra-low power consumption (9.5 μW/cm2), and the ability to provide spatiotemporally tunable information for potential AR applications.}
}
@article{CHEN2015124,
title = {Development of a surgical navigation system based on augmented reality using an optical see-through head-mounted display},
journal = {Journal of Biomedical Informatics},
volume = {55},
pages = {124-131},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046415000702},
author = {Xiaojun Chen and Lu Xu and Yiping Wang and Huixiang Wang and Fang Wang and Xiangsen Zeng and Qiugen Wang and Jan Egger},
keywords = {Surgical navigation, Augmented reality, Optical see-through HMD, Intra-operative motion tracking},
abstract = {The surgical navigation system has experienced tremendous development over the past decades for minimizing the risks and improving the precision of the surgery. Nowadays, Augmented Reality (AR)-based surgical navigation is a promising technology for clinical applications. In the AR system, virtual and actual reality are mixed, offering real-time, high-quality visualization of an extensive variety of information to the users (Moussa et al., 2012) [1]. For example, virtual anatomical structures such as soft tissues, blood vessels and nerves can be integrated with the real-world scenario in real time. In this study, an AR-based surgical navigation system (AR-SNS) is developed using an optical see-through HMD (head-mounted display), aiming at improving the safety and reliability of the surgery. With the use of this system, including the calibration of instruments, registration, and the calibration of HMD, the 3D virtual critical anatomical structures in the head-mounted display are aligned with the actual structures of patient in real-world scenario during the intra-operative motion tracking process. The accuracy verification experiment demonstrated that the mean distance and angular errors were respectively 0.809±0.05mm and 1.038°±0.05°, which was sufficient to meet the clinical requirements.}
}
@article{TADEJA2024103919,
title = {Immersive presentations of real-world medical equipment through interactive VR environment populated with the high-fidelity 3D model of mobile MRI unit},
journal = {Computers & Graphics},
volume = {120},
pages = {103919},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103919},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000542},
author = {Sławomir Konrad Tadeja and Thomas Bohné and Kacper Godula and Artur Cybulski and Magdalena Maria Woźniak},
keywords = {Virtual reality, VR, Immersive interface, Magnetic resonance imaging, MRI, Virtual presentation},
abstract = {The primary goal behind the system presented in this paper is to investigate the efficacy of using virtual reality (VR) for showcasing sizable medical equipment. Specifically, we focused on a mobile magnetic resonance imaging (MRI) scanner mounted on a truck trailer. The latter is integral to the mobile MRI setup and must be presented as part of the immersive experience. Therefore, we not only have to depict the medical apparatus but also provide the means of understanding its surroundings. This is especially important to radiologists and other medical personnel to ascertain if a given mobile medical facility fulfills their needs and wants. Furthermore, despite such MRI devices being designed for mobility, their long-distance transportation can be time-consuming, troublesome and expensive. Therefore, we can observe the need for showcasing such mobile MRI units without additional cost and burden related to transportation. To achieve this, we designed an immersive environment in which the users can interact with the real-life scale 3D model of a mobile MRI. In addition, we also verified the usability and expressiveness of our system using established heuristical approaches.}
}
@article{STOIAN2003173,
title = {Flexible Manufacturing System Simulation Based on Virtual Reality},
journal = {IFAC Proceedings Volumes},
volume = {36},
number = {23},
pages = {173-178},
year = {2003},
note = {IFAC Workshop on Intelligent Assembly and Disassembly (IAD'03), Bucharest, Romania, 9-11 October 2003},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)37681-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017376814},
author = {Viorel Stoian and Dorin Popescu and Dorin Sendrescu},
keywords = {flexible manufacturing system, virtual reality, assembly/disassembly, fault generation},
abstract = {This paper presents the didactical use of a Virtual Reality based simulation of a Flexible Manufacturing System and the benefits of the application of Virtual Reality in Flexible Manufacturing and in teaching process. The basic concepts of Virtual Reality and Flexible Manufacturing Systems are provided. The Flexible Manufacturing System FMS-200 is presented and simulations of the incorporate stations are achieved. Finally, the "Virtual Repair" is discussed.}
}
@article{LIGEOUR2005523,
title = {AUGMENTED REALITY INTERFACE FOR FREE TELEOPERATION},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {523-528},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.01357},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016373694},
author = {Vincent LE LIGEOUR and Samir OTMANE and Malik MALLEM},
keywords = {Telerobotics, teleoperation, tracking systems, distributed system, stereo vision, virtual reality},
abstract = {Augmented Reality (AR) can provide to a Human Operator (HO) a real help to achieve complex tasks such as remote control of robots and cooperative teleassistance. Using appropriate augmentations, the HO can interact faster, safer and easier with the remote real world. This paper presents a new Man Machine Interface (MMI) allowing high-level telewok, using augmented reality technologies. An example of telework is presented in this paper as a teleoperation of the remote 4 Degree of Freedom (DoF) robot. An important feature of this interface is based on the use of two new modes devoted for HO perception and interaction. In a new perception mode, a wide screen with stereo glasses are used to allow stereoscopic perception of the remote real environment. The new interaction mode is given thanks to ART* optical tracking system that allows HO to interact freely with the remote 4 DoF robot. An other important work presented in this paper is a distributed software architecture developed with three client/server modules (robot control, stereoscopic video feedback and tracking) and using appropriate methods (communication protocols, image compressions etc.). This new interface is tested and compared with ARITI interface (acronym of Augmented Reality Interface for Teleoperation via Internet) and some preliminary results are presented and discussed.}
}
@article{AFYOUNI202448,
title = {Usability Evaluation of Handheld and Wearable AR devices: Exploring Collaboration and the Role of Physical Props},
journal = {Procedia Computer Science},
volume = {231},
pages = {48-55},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923021683},
author = {Imad Afyouni and Mohammed Lataifeh and Zulaiha Afrah and Naveed Ahmed},
keywords = {Augmented Reality, Collaboration, Physical Props, User Interaction},
abstract = {This work presents a study to evaluate the usability of handheld and wearable Augmented Reality (AR) devices for object manipulation in an individual and collaborative AR environment. We propose a scenario that incorporates physical props to foster an immersive AR experience, with tasks involving moving a virtual light source, changing the color of a virtual object anchored to the prop, and placing the object on the light. A controlled evaluation was performed that includes both quantitative and qualitative analyses. Two test sessions were conducted - an individual and a collaborative session. In the individual session, the participants completed the experiment using a smartphone and Magic Leap One AR headset, with tasks carried out via a touch screen on the smartphone and controller / hand gestures on the AR headset. Twelve participants were paired up to perform the same tasks in a shared AR space for the collaborative sessions. We performed statistical analysis on the collected data to provide insights into statistically significant results. The results demonstrated that though AR headsets are better suited for interactions that require spatial movements in the three-dimensional space, the difference in time taken to execute these interactions compared to a smartphone was not statistically significant. On the other hand, executing gestures on a smartphone was significantly faster compared to hand-based gestures using the AR headset.}
}
@article{MOURTZIS202197,
title = {Collaborative manufacturing design: a mixed reality and cloud-based framework for part design},
journal = {Procedia CIRP},
volume = {100},
pages = {97-102},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004741},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {CAD design, Augmented Reality, Collaborative Design, Artificial Intelligence},
abstract = {Computer Aided Design (CAD) and Computer Aided Manufacturing (CAM) can be considered as the cornerstones of the lifecycle of a manufacturing asset. Since the above-mentioned processes often involve multiple engineers, from different departments even from different companies, it is crucial to ensure the flawless communication between the different individuals as well as to make the design process more intuitive. In the era of digitalization and Internet of Things, digital technologies such as Mixed Reality (MR) are utilized from engineers in order to leverage the capabilities of existing computer aided tools (CAx). Therefore, in this research work, the design and development of a Cloud-based and Mixed Reality-based framework is presented. The purpose of the framework is to facilitate engineers during the design and manufacturing phase of new components by providing a mobile application and common design tools via MR graphical user interfaces (GUI). Further to that, taking into consideration the advanced capabilities of MR devices in terms of user interaction, special emphasis is given on the interaction of engineers with the holograms of the components via simple hand gestures. The resulting 3D geometries can be imported in CAD suites either for further development or for manufacturing the designed component. The applicability of the proposed framework is tested and validated in a laboratory-based machine shop.}
}
@article{TIEMANN2021472,
title = {A concept for secure interaction with large scale haptic devices in virtual reality environments},
journal = {Procedia CIRP},
volume = {99},
pages = {472-477},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.067},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003498},
author = {Maik Franz Tiemann and Simon Kind and Rainer Stark},
keywords = {Hmd, Virtual reality, Haptic interaction, Virtual assembly, Prototyping, Smart hybrid prototyping},
abstract = {Because of an increased level of product complexity, new technologies and methods for early virtual validation are researched. In the context of early product validation and user experience studies, HMDs are getting more applicable in industrial use cases. The next step is to enhance the virtual validation environments with haptic feedback, so that the user can not only see, but also feel and experience specific characteristics of their product. In this paper, the connection between hybrid prototyping approaches and a large scale haptic device is described. A concept for secure interaction with these devices coupled with HMDs is given.}
}
@article{DIPAOLO2023113670,
title = {Early design validation on the Vacuum Vessel ports sealing interface installation and removal with Virtual Reality in ITER TBM port cells},
journal = {Fusion Engineering and Design},
volume = {193},
pages = {113670},
year = {2023},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2023.113670},
url = {https://www.sciencedirect.com/science/article/pii/S0920379623002533},
author = {Chiara {Di Paolo} and Stéphane Gazzotti and Sarah Griffiths and Thibault {Plantin de Hugues} and Jean-Pierre Martins and Yannick {Le Tonqueze} and Benoit Manfreo and Jean-Pierre Friconneau and Luciano Giancarli and Fabien Josseaume and Eamonn Quinn and Margherita Peruzzini and Cyril Kharoua},
keywords = {eXtended reality, Virtual reality, Test blanket modules, ALARA, Integration, Early design validation},
abstract = {Port Cell (PC) rooms in the ITER tokamak building host the devices required for the Port Plugs operation and provide connection to services. During Plasma Operation State, the Vacuum Vessel ports sealing interface provides vacuum tightness and first confinement barrier as essential functions. When a Port Plug needs to be replaced or maintained, this significant sealing component must be removed and reinstalled. The general strategy for this operation has been reviewed resulting in modification proposals to address the requirements for static and dynamic confinement and management of radiation considering the ALARA (As Low As Reasonably Achievable) approach. As Dose Reduction Measures (DRM), additional provisions have been proposed to ensure a robust confinement control during the machine shutdown operation. They mostly consist in an airlock that allows the extension of the first confinement barrier equipped with specific handling equipment to perform human-assisted operations on the sealing flanges. In particular, this paper presents the study carried out in TBM dedicated PC during the conceptual design phase of the maintenance cabin airlock and the handling tools by means of Virtual Reality (VR). VR techniques allow working with virtual prototypes at full scale from the early design stage to ease the design process and the concepts validation with important results concerning the assessments of accessibility issues. The study describes also the outcomes of the VR studies and explains how VR can be an effective instrument to conduct integration studies with complex kinematics and human access operations in the execution of the engineering validation process.}
}
@article{SHEIKHBAHAEI2021102250,
title = {A case study for risk assessment in AR-equipped socio-technical systems},
journal = {Journal of Systems Architecture},
volume = {119},
pages = {102250},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102250},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001715},
author = {Soheila {Sheikh Bahaei} and Barbara Gallina and Marko Vidović},
keywords = {Socio-technical systems, Augmented reality, Risk assessment, ISO 26262, ISO/PAS 21448-SOTIF},
abstract = {Augmented Reality (AR) technologies are used as human–machine interface within various types of safety-critical systems. Several studies have shown that AR improves human performance. However, the introduction of AR might introduce risks due to new types of dependability threats. In order to avoid unreasonable risk, it is required to detect new types of dependability threats (faults, errors, failures). In our previous work, we have designed extensions for the SafeConcert metamodel (a metamodel for modeling socio-technical systems) to capture AR-related dependability threats (focusing on faults and failures). Despite the availability of various modeling techniques, there has been no detailed investigation of providing an integrated framework for risk assessment in AR-equipped socio-technical systems. Hence, in this paper, we provide an integrated framework based on our previously proposed extensions. In addition, in cooperation with our industrial partners, active in the automotive domain, we design and execute a case study. We aim at verifying the modeling and analysis capabilities of our framework and finding out if the proposed extensions are helpful in capturing system risks caused by new AR-related dependability threats. Our conducted qualitative analysis is based on the Concerto-FLA analysis technique, which is included in the CHESS toolset and targets socio-technical systems.}
}
@article{ROBBINS20091628,
title = {The use of virtual reality and intelligent database systems for procedure planning, visualisation, and real-time component tracking in remote handling operations},
journal = {Fusion Engineering and Design},
volume = {84},
number = {7},
pages = {1628-1632},
year = {2009},
note = {Proceeding of the 25th Symposium on Fusion Technology},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2009.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S0920379609000817},
author = {Edward Robbins and Stephen Sanders and Adrian Williams and Peter Allan},
keywords = {Virtual reality, Databases, Remote handling operations},
abstract = {The organisation of remote handling (RH) operations in fusion environments is increasingly critical as the number of tasks, components and tooling that RH operations teams must deal with inexorably rises. During the recent JET EP1 RH shutdown the existing virtual reality (VR) and procedural database systems proved essential for visualisation and tracking of operations, particularly due to the increasing complexity of remote tasks. A new task planning system for RH operations is in development, and is expected to be ready for use during the next major shutdown, planned for 2009. The system will make use of information available from the remote operations procedures, the RH equipment human–machine interfaces, the on-line RH equipment control systems and also the virtual reality (VR) system to establish a complete database for the location of plant items and RH equipment as RH operations progress. It is intended that the system be used during both preparation and implementation of shutdowns. In the preparations phase the system can be used to validate procedures and overall logistics by allowing an operator to increment through each operation step and to use the VR system to visualise the location and status of all components, manipulators and RH tools. During task development the RH operations engineers can plan and visualise movement of components and tooling to examine handling concepts and establish storage requirements. In the implementation of operations the daily work schedules information will be integrated with the RH operations procedures tracking records to enable the VR system to provide a visual representation of the status of remote operations in real time. Monitoring of the usage history of items will allow estimates of radiation dosage and contaminant exposure to be made. This paper describes the overall aims, structure and use of the system, discusses its application to JET and also considers potential future developments.}
}
@article{KAVUR2021101950,
title = {CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation},
journal = {Medical Image Analysis},
volume = {69},
pages = {101950},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101950},
url = {https://www.sciencedirect.com/science/article/pii/S1361841520303145},
author = {A. Emre Kavur and N. Sinem Gezer and Mustafa Barış and Sinem Aslan and Pierre-Henri Conze and Vladimir Groza and Duc Duy Pham and Soumick Chatterjee and Philipp Ernst and Savaş Özkan and Bora Baydar and Dmitry Lachinov and Shuo Han and Josef Pauli and Fabian Isensee and Matthias Perkonigg and Rachana Sathish and Ronnie Rajan and Debdoot Sheet and Gurbandurdy Dovletov and Oliver Speck and Andreas Nürnberger and Klaus H. Maier-Hein and Gözde {Bozdağı Akar} and Gözde Ünal and Oğuz Dicle and M. Alper Selver},
keywords = {Segmentation, Challenge, Abdomen, Cross-modality},
abstract = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.}
}
@article{SUN2024109199,
title = {Generating trajectory data without behavior modeling: An online virtual reality method in wayfinding performance evaluation for buildings},
journal = {Journal of Building Engineering},
volume = {88},
pages = {109199},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109199},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224007678},
author = {Chengyu Sun and Mingyan Zou and Shuyang Li and Dongdong Zhang},
keywords = {Online virtual environment, Wayfinding simulation, Post-occupancy evaluation, Large public buildings, Activity-based model},
abstract = {Wayfinding performance is one of the key issues in pre- or post-occupancy evaluation for large-scale public buildings and such evaluation typically revolves around participants’ wayfinding trajectories. Traditional evaluation methods, including laboratory virtual reality (VR) experiments and computer simulations, have their respective advantages in accuracy or cost-efficiency. However, they either suffer from high costs and low efficiency or sacrifice the diversity of behavioral data for higher efficiency and lower costs. Therefore, this paper proposes a novel method named “sampling as simulation” (SaS) via an online VR wayfinding experiment. This method allows for the rapid collection of behavioral data at a lower cost to generate highly accurate wayfinding trajectory data for evaluation purposes. It utilizes a model-image integrated approach to create realistic online virtual environments and employs a spatiotemporal replay algorithm to generate a crowd of true-trajectory-driven avatars, significantly enhancing the immersion and realism of the online virtual scenes. This method was demonstrated and validated with a wayfinding performance post-occupancy evaluation (POE) project at the International Departure of Satellite Terminal One (S1) in Shanghai Pudong International Airport (PVG). In this example, a total of 13,712 original trajectories were collected within a 72-h time window, serving as the original dataset for evaluation. Based on this dataset, the study effectively simulated wayfinding behavior in three different scenarios and evaluated wayfinding performance. Compared to existing methods, the SaS method not only improves data collection efficiency and reduces costs but also maintains the diversity of behavior by generating transformed trajectories directly from human data without behavior modeling.}
}
@article{SCHEFFER2023782,
title = {Supporting maintenance operators using augmented reality decision-making: visualize, guide, decide & track},
journal = {Procedia CIRP},
volume = {119},
pages = {782-787},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123005899},
author = {S.E. (Sara) Scheffer and A. (Alberto) Martinetti and R.G.J. (Roy) Damgrave and L.A.M. (Leo) {van Dongen}},
keywords = {Augmented Reality, Decision-support system, Maintenance},
abstract = {In railway maintenance activities, sophisticated socio-technical interactions are required to achieve efficient and reliable operations. Maintenance technicians carry out their daily tasks based on expertise and knowledge gained from both training and personal experience. In train design, information technology systems and operational technology systems converge, resulting in complex train failures, maintenance procedures, and activities. Troubleshooting train failures becomes extremely difficult and time-consuming as more data and information are available and filtering and selecting them becomes cumbersome. New technology developments and interactive interfaces and environments that speed up the process of understanding troubleshooting decision-making and facilitate design collaboration are required. Augmented reality (AR) is a technology that provides real-time, on-site, and structured information that offers great potential for visualizing, structuring and contextualizing data to facilitate well considered choices for decision-making. Therefore, an AR decision-making tool is developed based on structuring, visualizing and contextualizing data in an AR solution space. The tool captures real-life system conditions, comprehends troubleshooting activities, facilitates problem-solving decisions, and tracks maintenance procedures. A case study validates the tool by implementing: (1) object recognition for visualization, (2) a what-if analysis for troubleshooting directions, and (3) capturing maintenance timing and procedures. Laboratory testing is used as input for future design building blocks.}
}
@article{BREDIKHIN2022254,
title = {Diagnostics of motion sickness (kinetosis) and training of resistance to it in VR simulators},
journal = {Procedia Computer Science},
volume = {212},
pages = {254-263},
year = {2022},
note = {11th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017008},
author = {Artem Bredikhin and Maxim Liulukin and Ekaterina Nikitina and Dmitry Nikushchenko and Anton Stiopin and Yulia Mikholazhina},
keywords = {virtual reality, marine simulator, motion sickness, kinetosis, symptomatology, feedback},
abstract = {Approximately 5-15% of adults suffer from motion sickness (ICD-10: T75.3, ICD-9: 994.6), also known as kinetosis, in one way or another. Being prone to it may negatively affect not only the daily activities of an average person, but also maritime specialists, whose professional activity is directly related to sailing in sea. The study of motion sickness, as well as the development of simulators that would allow training a resistance and adaptation to it and its symptomatology is an urgent task for the maritime industry. Various approaches for the physiological motion sickness resistance and adaptation training exist, among which the use of virtual reality (hereinafter referred to as VR) technologies is of high importance. Key advantage of using VR is the ability to simulate any visual environment, limited only by the task, hardware and software capabilities of the researcher. Currently, VR is already being used in the study of motion sickness [1]. We investigated not only the possibility of motion sickness, but also methods of reducing manifestations and symptoms by dynamically changing the viewing angles, as well as assessing the influence of peripheral and stereoscopic vision, including their various variations. The primary goal of the study is to investigate the direct effect of a visual channel on the symptoms of motion sickness, including changes of channel parameters (e.g., changes in viewing angles), and to assess the possibility of training a control group of users with vivid symptoms to reduce them. This paper considers 50 participants to be involved in an experiment, which is based on the developed research methodology for physiological motion sickness. Series of tests is carried out to identify symptoms and conduct a training, after which the research methodology has been evaluated.}
}
@article{SIESS201920,
title = {User color temperature preferences in immersive virtual realities},
journal = {Computers & Graphics},
volume = {81},
pages = {20-31},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S009784931930041X},
author = {Andreas Siess and Matthias Wölfel},
keywords = {Virtual reality, Color temperature, Perception, Cognition,},
abstract = {Virtual reality is currently experiencing a renaissance, with more and more fields of application emerging and more and more users spending an increasing amount of time using head mounted displays (HMD). While technical parameters such as field of view or pixel density are heavily researched, some fundamental aspects of perception and in particular the effects on human cognition and physical constitution are less investigated. One of these aspects to be addressed in this study is the perceived color temperature—a stimulus to which every color-seeing person reacts both consciously and subconsciously. A total of 86 test persons were asked to adjust the color temperature according to their personal taste in five photorealistic scenarios; once using a PC screen and once using a HMD. Between these two devices significant differences in the personal preferences of the test persons were found. This study will open a broad discussion about the effects of color temperature as an omnipresent stimulus and how external factors like daytime or season may effect these preferences in immersive environments.}
}
@article{MARQUES201896,
title = {Deep spherical harmonics light probe estimator for mixed reality games},
journal = {Computers & Graphics},
volume = {76},
pages = {96-106},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301377},
author = {Bruno Augusto Dorta Marques and Esteban Walter Gonzalez Clua and Cristina Nader Vasconcelos},
keywords = {Mixed reality, Augmented reality, Virtual reality, Games, Lighting estimation, Deep learning},
abstract = {The recent developments in virtual and mixed reality by the video game and entertainment industries are responsible for increasing user’s visual immersion and provide a better user experience in games and other interactive simulations. However, the interaction between the user and simulated environment still relies on game controllers or other unnatural handheld devices. In the mixed reality context, the usage of more natural and immersive alternative to the game controllers, such as the user’s hands, may drastically increase the game interface experience, allowing a personalized visual feedback of the user’s interactions in the real-time simulation. There are basically two approaches for including the user’s hand: a 3D reconstruction based method, typically based on depth cameras, or an image-based approach, composing the virtual scene with the real images of the user’s hands. In the composition of the user’s hands and virtual elements, perceptual discrepancies in the illumination of objects may occur, generating an inconsistency in the illumination of the mixed reality environment. A consistent illumination of the environment greatly improves the user’s immersion in the mixed reality application. One way to ensure consistent illumination is by estimating the real-world illumination and use this information to adapt the virtual world lighting setting. We present the Spherical Harmonics Light Probe Estimator, a deep learning based technique that estimates the lighting setting of the real-world environment. The method uses a single RGB image and does not requires prior knowledge of the scene. The estimator outputs a light probe of the real-world lighting, represented by 9 spherical harmonics coefficients. The estimated light probe is used to create a composite image containing both real and virtual elements in an environment with a consistent illumination. We validate the technique through synthetic tests achieving an RMS error of 0.0573. We show the usage of the method in an augmented virtuality application.}
}
@article{CAVALCANTI2019100,
title = {Usability and effects of text, image and audio feedback on exercise correction during augmented reality based motor rehabilitation},
journal = {Computers & Graphics},
volume = {85},
pages = {100-110},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S009784931930158X},
author = {Virgínia Carrazzone Cavalcanti and Maria Iziane de Santana Ferreira and Veronica Teichrieb and Ricardo Rossiter Barioni and Walter Franklin Marques Correia and Alana Elza Fontes {Da Gama}},
keywords = {Augmented reality, Motor rehabilitation, Error correction feedback, Usability, Serious games, Kinect},
abstract = {This paper aims to evaluate text, image and audio feedback regarding usability, exercise correction and user preferences when using Augmented Reality (AR) to guide movements execution in rehabilitation applications. For that, this study used an AR rehabilitation tool, named ARkanoidAR, which attempts to correct the wrong movement performed by a user through text, image and audio feedback, while he/she is playing a game controlled based on the therapy protocol defined by a physiotherapist for that particular user. The ARkanoidAR uses a Kinect sensor to track the human body and its movements, and it is completely configurable according to patient therapy needs. To evaluate feedback effects individually, this study collected quantitative data to measure success rate, which represents the amount of time that the user executed the movements correctly when using the application with each feedback individually activated. Experiments were composed of five phases: control, text, image, audio and control. The feedback sequence was randomized before the tests in order to avoid bias. Quantitative results show that users understood the instructions for performing the movements correctly since the success rate along time increased. Tests also included observation and qualitative analysis through questionnaires and a semi-structured interview to evaluate the usability perception of users. The qualitative data helped to detect different types of problems for each feedback based on users’ answers. This study was carried out with healthy people, and did not predict the particularities and limitations of physiotherapy patients.}
}
@article{ROMPAPAS201924,
title = {Towards large scale high fidelity collaborative augmented reality},
journal = {Computers & Graphics},
volume = {84},
pages = {24-41},
year = {2019},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849319301359},
author = {Damien Constantine Rompapas and Christian Sandor and Alexander Plopski and Daniel Saakes and Joongi Shin and Takafumi Taketomi and Hirokazu Kato},
keywords = {Augmented reality, High fidelity, Game design, Large scale interaction},
abstract = {In recent years, there has been an increasing amount of Collaborative Augmented Reality (CAR) experiences, classifiable by the deployed scale and the fidelity of the experience. In this paper, we create HoloRoyale, the first large scale high fidelity (LSHF) CAR experience. We do this by first exploring the LSHF CAR design space, drawing on technical implementations and design aspects from AR and video games. We then create and implement a software architecture that improves the accuracy of synchronized poses between multiple users. Finally, we apply our target experience and technical implementation to the explored design space. A core design component of HoloRoyale is the use of visual repellers as crowd control elements to guide players away from undesired areas. To evaluate the effectiveness of the employed visual repellers in a LSHF CAR context we conducted a user study, deploying HoloRoyale in a 12.500 m2 area. The results from the user study suggest that visual repellers are effective crowd control elements that do not significantly impact the user’s overall immersion. Overall our main contribution is the exploration of a design space, discussing several means to address the challenges of LSHF CAR, the creation of a system capable of LSHF CAR interactions along with an experience that has been fitted to the design space, and an indepth study that verifies a key design aspect for LSHF CAR. As such, our work is the first to explore the domain of LSHF CAR and provides insight into designing experiences in other AR domains.}
}
@article{PINAL2024376,
title = {Mixed reality and digital twins for astronaut training},
journal = {Acta Astronautica},
volume = {219},
pages = {376-391},
year = {2024},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2024.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S0094576524000420},
author = {Octavio Piñal and Amadeo Arguelles},
keywords = {Digital twins, Mixed reality, Virtual reality, Augmented reality, Astronauts training, Data analysis},
abstract = {Astronaut training is complex due to its specific infrastructure and equipment requirements, which are often costly. Immersive technologies can simulate entire environments, allowing for interactive user experiences. Previous research has confirmed the effectiveness of immersive technologies in training and learning. However, simulations alone are insufficient; participants must also engage with vehicles, machinery, and spacecraft systems to understand their impact on the environment and adjacent objects. This study develops four scenarios within three modules, inspired by NASA and ESA astronaut training programs. The first module presents a theoretical scenario using mixed reality (MR), incorporating topics from ESA's training modules. The second module offers two practical scenarios: an ISS emergency simulation that includes Hohmann transfer and circular motion concepts, and a spacewalk for repair tasks. In these scenarios, digital twins of the ISS propulsion and navigation systems, as well as a spacesuit, were created. The third module simulates a spacecraft launch, utilizing 3D models from SpaceX's Falcon Heavy and digital twins of its propulsion and navigation systems. Participants interacted with the digital twins and scenarios, generating data that was stored and analyzed against the ISS dataset from the Jet Propulsion Laboratory (JPL) and telemetry from the SpaceX Falcon Heavy launch. In this work's initial phase, the main objective was to assess whether digital twins could be integrated with immersive technologies for effective training. A mean squared error metric was employed to compare the digital twins with the physical object data, confirming the alignment of the developed digital twins with the actual systems, thereby validating their utility for astronaut training.}
}
@article{ANTONELLI2015556,
title = {Enhancing the Quality of Manual Spot Welding through Augmented Reality Assisted Guidance},
journal = {Procedia CIRP},
volume = {33},
pages = {556-561},
year = {2015},
note = {9th CIRP Conference on Intelligent Computation in Manufacturing Engineering - CIRP ICME '14},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.06.076},
url = {https://www.sciencedirect.com/science/article/pii/S2212827115007192},
author = {Dario Antonelli and Sergey Astanin},
keywords = {Augmented reality, Welding, Quality Monitoring},
abstract = {Manual spot welding loses the comparison with automated spot welding, not because of a higher execution time, but due to an inferior quality of welded points, mostly a low repeatability. It is not a human fault. Human welder is compelled to operate without having at disposal the knowledge of significant process features that are known by the robot: exact position of the welding spot, electric parameters to be adopted for every specific point, quality of the welded spot and, based on it, possible need for repetition of a defective weld. The research shows that, using an augmented reality device, it is possible to display this very data to the human operator, in order to enhance the manual process execution. The adopted device is a tablet mounted on the welding gun. It displays the working area seen by the built-in camera. The image is augmented by the superposition of computer generated images of the welding spots and their properties. The state of the spot (welded or not) and its execution quality (good or defective weld) is transmitted by some graphic features, like point color, size and way of blinking. The paper describes the algorithms used in the development of the program for this application, focusing on the problem of real time localization of the welding gun position. The augmented reality application was actually installed on a experimental station by a welding gun manufacturer and the results of the tests are presented and discussed.}
}
@article{PARK2020106585,
title = {Deep learning-based mobile augmented reality for task assistance using 3D spatial mapping and snapshot-based RGB-D data},
journal = {Computers & Industrial Engineering},
volume = {146},
pages = {106585},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106585},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220303193},
author = {Kyeong-Beom Park and Sung Ho Choi and Minseok Kim and Jae Yeol Lee},
keywords = {Mobile augmented reality (AR), Spatial mapping, Deep learning-based AR, Task assistance},
abstract = {This paper proposes a new deep learning-based mobile AR for intelligent task assistance by conducting 3D spatial mapping without pre-registration using AR markers, which can match virtual AR objects to their corresponding physical objects automatically and accurately using single snapshot-based RGB-D data. Firstly, the proposed approach applies a deep learning-based instance segmentation method to the snapshot-based RGB-D data to detect real object instances and to segment their surrounding regions in 3D point cloud data. Then, an iterative closest point (ICP) algorithm is used to perform a 3D spatial mapping between the segmented point cloud of the real object and its corresponding virtual model. Therefore, the virtual information can be seamlessly and automatically synchronized with its corresponding real object. To prove the effectiveness of the proposed method, we performed comparative experiments quantitatively and qualitatively, which evaluated the accuracy, basic task performance, and usability. Experimental results verify that the proposed deep learning-based 3D spatial mapping approach is more accurate and more suitable for mobile AR-based visualization and interaction than previous studies. We have also implemented several applications in actual working situations, which verifies the applicability and extensibility of the proposed approach.}
}
@article{BARNHILL2017133,
title = {Nonlinear multiscale regularisation in MR elastography: Towards fine feature mapping},
journal = {Medical Image Analysis},
volume = {35},
pages = {133-145},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2016.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S136184151630041X},
author = {Eric Barnhill and Lyam Hollis and Ingolf Sack and Jürgen Braun and Peter R. Hoskins and Pankaj Pankaj and Colin Brown and Edwin J.R. {van Beek} and Neil Roberts},
keywords = {Elastography, Magnetic resonance elastography, Wave inversion, Complex dualtree wavelet, Denoising},
abstract = {Fine-featured elastograms may provide additional information of radiological interest in the context of in vivo elastography. Here a new image processing pipeline called ESP (Elastography Software Pipeline) is developed to create Magnetic Resonance Elastography (MRE) maps of viscoelastic parameters (complex modulus magnitude |G*| and loss angle ϕ) that preserve fine-scale information through nonlinear, multi-scale extensions of typical MRE post-processing techniques. Methods: A new MRE image processing pipeline was developed that incorporates wavelet-domain denoising, image-driven noise estimation, and feature detection. ESP was first validated using simulated data, including viscoelastic Finite Element Method (FEM) simulations, at multiple noise levels. ESP images were compared with MDEV pipeline images, both in the FEM models and in three ten-subject cohorts of brain, thigh, and liver acquisitions. ESP and MDEV mean values were compared to 2D local frequency estimation (LFE) mean values for the same cohorts as a benchmark. Finally, the proportion of spectral energy at fine frequencies was quantified using the Reduced Energy Ratio (RER) for both ESP and MDEV. Results: Blind estimates of added noise (σ) were within 5.3% ± 2.6% of prescribed, and the same technique estimated σ in the in vivo cohorts at 1.7 ± 0.8%. A 5 × 5 × 5 truncated Gabor filter bank effectively detects local spatial frequencies at wavelengths λ ≤ 10px. For FEM inversions, mean |G*| of hard target, soft target, and background remained within 8% of prescribed up to σ=20%, and mean ϕ results were within 10%, excepting hard target ϕ, which required redrawing around a ring artefact to achieve similar accuracy. Inspection of FEM |G*| images showed some spatial distortion around hard target boundaries and inspection of ϕ images showed ring artefacts around the same target. For the in vivo cohorts, ESP results showed mean correlation of R=0.83 with MDEV and liver stiffness estimates within 7% of 2D-LFE results. Finally, ESP showed statistically significant increase in fine feature spectral energy as measured with RER for both |G*| (p<1×10−9) and ϕ (p<1×10−3). Conclusion: Information at finer frequencies can be recovered in ESP elastograms in typical experimental conditions, however scatter- and boundary-related artefacts may cause the fine features to have inaccurate values. In in vivo cohorts, ESP delivers an increase in fine feature spectral energy, and better performance with longer wavelengths, than MDEV while showing similar stability and robustness.}
}
@article{YUN2022234,
title = {Immersive and interactive cyber-physical system (I2CPS) and virtual reality interface for human involved robotic manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {234-248},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S027861252100248X},
author = {Huitaek Yun and Martin B.G. Jun},
keywords = {Smart manufacturing, Cyber-physical system (CPS), Virtual reality, Robotic manufacturing, Middleware},
abstract = {Smart manufacturing promotes the demand of new interfaces for communication with autonomies such as big data analysis, digital twin, and self-decisive control. Collaboration between human and the autonomy becomes imperative factor for improving productivity. However, current human-machine interfaces (HMI) such as 2D screens or panels require human knowledge of process and long-term experience to operate, which is not intuitive for beginning workers or is designed to work with the autonomy. This study proposes a human interface framework of cyber-physical system (CPS) based on virtual reality, named as immersive and interactive CPS (I2CPS), to create an interface for human-machine-autonomy collaboration. By combination of data-to-information protocol and middleware, MTConnect and Robot Operating System (ROS), heterogeneous physical systems were integrated with virtual assets such as digital models, digital shadows, and virtual traces of human works in the virtual reality (VR) based interface. All the physical and virtual assets were integrated in the interface that human, autonomy, and physical system can collaborate. Applying constraints in the VR interface and deploying virtual human works to industrial robots were demonstrated to verify the effectiveness of the I2CPS framework, showing collaboration between human and autonomy: augmentation of human skills by autonomy and virtual robot teaching to generate automatic robot programs.}
}
@article{DORTAMARQUES2022257,
title = {Spatially and color consistent environment lighting estimation using deep neural networks for mixed reality},
journal = {Computers & Graphics},
volume = {102},
pages = {257-268},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001710},
author = {Bruno Augusto {Dorta Marques} and Esteban Walter {Gonzalez Clua} and Anselmo Antunes Montenegro and Cristina {Nader Vasconcelos}},
keywords = {Lighting estimation, Spherical harmonics, Deep learning, Environment map, Mixed reality, Augmented Reality},
abstract = {The representation of consistent mixed reality (XR) environments requires adequate real and virtual illumination composition in real-time. Estimating the lighting of a real scenario is still a challenge. Due to the ill-posed nature of the problem, classical inverse-rendering techniques tackle the problem for simple lighting setups. However, those assumptions do not satisfy the current state-of-art in computer graphics and XR applications. While many recent works solve the problem using machine learning techniques to estimate the environment light and scene’s materials, most of them are limited to geometry or previous knowledge. This paper presents a CNN-based model to estimate complex lighting for mixed reality environments with no previous information about the scene. We model the environment illumination using a set of spherical harmonics (SH) environment lighting, capable of efficiently represent area lighting. We propose a new CNN architecture that inputs an RGB image and recognizes, in real-time, the environment lighting. Unlike previous CNN-based lighting estimation methods, we propose using a highly optimized deep neural network architecture, with a reduced number of parameters, that can learn high complex lighting scenarios from real-world high-dynamic-range (HDR) environment images. We show in the experiments that the CNN architecture can predict the environment lighting with an average mean squared error (MSE) of 7.85× 10−4 when comparing SH lighting coefficients. We validate our model in a variety of mixed reality scenarios. Furthermore, we present qualitative results comparing relights of real-world scenes.}
}
@article{THAVATHURAIRAJA2021120887,
title = {Method to estimate design resilient modulus (Mr) of unbound materials for rehabilitation in M−E design},
journal = {Construction and Building Materials},
volume = {267},
pages = {120887},
year = {2021},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2020.120887},
url = {https://www.sciencedirect.com/science/article/pii/S0950061820328920},
author = {(Jey) Jeyakaran Thavathurairaja and Adam J. Hand and Murugaiyah Piratheepan and Peter E. Sebaaly and Elie Y. Hajj},
keywords = {Resilient modulus, Unbound materials, Pavement rehabilitation, M−E analysis},
abstract = {The resilient moduli (Mr) of the unbound layers and subgrade (SG) soils play a major role in estimating the pavement responses and analyses for mechanistic-empirical (M−E) pavement design. Most of the analysis procedures require a single representative Mr value for each unbound layer. However, estimating one representative values for each unbound layer is difficult as it depends on the state of the stresses within the layer. In this study, the stepwise mechanistic analysis approach is used for determining representative Mr values for the unbound materials typically used in Nevada. The Nevada Department of Transportation (NDOT) uses mainly three types of unbound material in the pavement structure; crushed aggregate base (CAB), borrow material, and SG soils. The unbound materials were sampled from multiple locations throughout Nevada District 1 and evaluated through several laboratory tests, which include sieve analysis, moisture-density relationships, and resilient modulus. The resilient modulus values for unbound material at multiple states of stresses were measured in accordance with AASHTO T307. The measured resilient modulus values were fitted to multiple constitutive models to assess the suitability of the models. The CAB and borrow materials fit the theta model very well while the SG soils fits the Uzman model and universal model. The ILLI-PAVE 2005 finite element (FE) program was employed for computing stresses as well as deflection basins in typical conventional flexible pavements under a representative tire loading. The calculated deflection basin was used for estimating the Mr values of the unbound layers using MODULUS 6.1 software.}
}
@article{CASCINI2020103308,
title = {Exploring the use of AR technology for co-creative product and packaging design},
journal = {Computers in Industry},
volume = {123},
pages = {103308},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103308},
url = {https://www.sciencedirect.com/science/article/pii/S016636152030542X},
author = {Gaetano Cascini and Jamie O'Hare and Elies Dekoninck and Niccolo Becattini and Jean-François Boujut and Fatma {Ben Guefrache} and Iacopo Carli and Giandomenico Caruso and Lorenzo Giunta and Federico Morosi},
keywords = {Spatial augmented reality, Co-creation, Design representation, Co-design, Prototype},
abstract = {Extended Reality technologies, including Virtual Reality (VR) and Augmented Reality (AR), are being applied in a wide variety of industrial applications, but their use within design practice remains very limited, despite some promising research activities in this area over the last 20 years. At the same time, design practice has been evolving to place greater emphasis on the role of the client or end-user in the design process through ‘co-creative design’ activities. Whilst offering many benefits, co-creative design activities also present challenges, notably in the communication between designers and non-designers, which can hinder innovation. In this paper, we investigate the potential of a novel, projection-based AR system for the creation of design representations to support co-creative design sessions. The technology is tested through benchmarking experiments and in-situ trials conducted with two industrial partners. Performance metrics and qualitative feedback are used to evaluate the effectiveness of the new technology in supporting co-creative design sessions. Overall, AR technology allows quick, real-time modifications to the surfaces of a physical prototype to try out new ideas. Consequently, designers perceive the possibility to enhance the collaboration with the end-users participating in the session. Moreover, the quality and novelty of ideas generated whilst using projection-based AR outperform conventional sessions or handheld display AR sessions. Whilst the results of these early trials are not conclusive, the results suggest that projection-based AR design representations provide a promising approach to supporting co-creative design sessions.}
}
@article{RAHMAN2023753,
title = {The Effectiveness of Augmented Reality Using Flash Card in Education to Learn Simple English Words as a Secondary Language},
journal = {Procedia Computer Science},
volume = {227},
pages = {753-761},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.580},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017477},
author = {Muhammad Akbar Rahman and Rosyidan Rouf Faisal and Cuk Tho},
keywords = {Augmented Reality, Education, Flash Card, English, Effective, Technology},
abstract = {Considering the potential and benefits of augmented reality and the development of education efficiency is essential, it is necessary to utilize the benefits and potential suitable to be implemented in the education field. One of the benefits is the fun learning experience compared to the traditional method. Furthermore, this study aims to analyze the effectiveness of using augmented reality in learning simple English words. The technology method is marker-based tracking, where the marker is flashcards that will be uploaded into Vuforia SDK Kit. Also, this research method uses ADDIE because it is an instructional design that makes the researcher easily to keep track of the progress and result. Then, the method is tested and analyzed using paired t-tests to measure its effectiveness and the User Experience Questionnaire to measure the user experience. Based on the analysis, the result shows that the result is effective, and the questionnaire has a good overall score. In conclusion, the result corresponds to the contribution of this research that enables the education learning experience to be effective, fun, and interactive.}
}
@article{ZHANG20101167,
title = {A multi-regional computation scheme in an AR-assisted in situ CNC simulation environment},
journal = {Computer-Aided Design},
volume = {42},
number = {12},
pages = {1167-1177},
year = {2010},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2010.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448510001442},
author = {J. Zhang and S.K. Ong and A.Y.C. Nee},
keywords = {Augmented reality, CNC simulation, Tracking, Registration, Template matching},
abstract = {Computer numerical control (CNC) simulation systems based on 3D graphics have been well researched and developed for NC tool path verification and optimization. Although widely used in the manufacturing industries, these CNC simulation systems are usually software-centric rather than machine tool-centric. The user has to adjust himself from the 3D graphic environment to the real machining environment. Augmented reality (AR) is a technology that supplements a real world with virtual information, where virtual information is augmented on to real objects. This paper builds on previous works of integrating the AR technology with a CNC machining environment using tracking and registration methodologies, with an emphasis on in situ simulation. Specifically configured for a 3-axis CNC machine, a multi-regional computation scheme is proposed to render a cutting simulation between a real cutter and a virtual workpiece, which can be conducted in situ to provide the machinist with a familiar and comprehensive environment. A hybrid tracking method and an NC code-adaptive cutter registration method are proposed and validated with experimental results. The experiments conducted show that this in situ simulation system can enhance the operator’s understanding and inspection of the machining process as the simulations are performed on real machines. The potential application of the proposed system is in training and machining simulation before performing actual machining operations.}
}
@article{WANG2023106661,
title = {Multimodal registration of ultrasound and MR images using weighted self-similarity structure vector},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106661},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106661},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523001269},
author = {Yifan Wang and Tianyu Fu and Chan Wu and Jian Xiao and Jingfan Fan and Hong Song and Ping Liang and Jian Yang},
keywords = {Self-similarity structure, Texture weight, Multimodal image registration, Ultrasound, Liver},
abstract = {Propose
Multimodal registration of 2D Ultrasound (US) and 3D Magnetic Resonance (MR) for fusion navigation can improve the intraoperative detection accuracy of lesion. However, multimodal registration remains a challenge because of the poor US image quality. In the study, a weighted self-similarity structure vector (WSSV) is proposed to registrate multimodal images.
Method
The self-similarity structure vector utilizes the normalized distance of symmetrically located patches in the neighborhood to describe the local structure information. The texture weights are extracted using the local standard deviation to reduce the speckle interference in the US images. The multimodal similarity metric is constructed by combining a self-similarity structure vector with a texture weight map.
Results
Experiments were performed on US and MR images of the liver from 88 groups of data including 8 patients and 80 simulated samples. The average target registration error was reduced from 14.91 ± 3.86 mm to 4.95 ± 2.23 mm using the WSSV-based method.
Conclusions
The experimental results show that the WSSV-based registration method could robustly align the US and MR images of the liver. With further acceleration, the registration framework can be potentially applied in time-sensitive clinical settings, such as US-MR image registration in image-guided surgery.}
}
@article{DU201851,
title = {Zero latency: Real-time synchronization of BIM data in virtual reality for collaborative decision-making},
journal = {Automation in Construction},
volume = {85},
pages = {51-64},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309172},
author = {Jing Du and Zhengbo Zou and Yangming Shi and Dong Zhao},
keywords = {BIM, Virtual reality, Information latency, Metadata, Cloud computing},
abstract = {Virtual Reality (VR) has attracted increasing attention of the Architecture, Engineering, Construction and Facility Management (AEC/FM) industry in recent years, as it shows a great potential to improve workflow efficiency through enhanced common understanding. A problem with current VR applications in AEC/FM is that the manual conversation from official design data (e.g., a BIM model) to VR displays is difficult and time consuming. There is a lack of automated and efficient data transfer approach between BIM and VR. In this paper, we will introduce a BIMVR real-time synchronization system called BVRS, which is based on an innovative Cloud-based BIM metadata interpretation and communication method. BVRS allows users to update BIM model changes in VR headsets (such as Oculus Rift DK2) automatically and simultaneously. We tested BVRS in a variety of design change scenarios including changing object dimensions, changing object locations and changing object types. Results confirmed the usability and efficiency of BVRS.}
}
@article{IFTENE2018166,
title = {Enhancing the Attractiveness of Learning through Augmented Reality},
journal = {Procedia Computer Science},
volume = {126},
pages = {166-175},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.220},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311943},
author = {Adrian Iftene and Diana Trandabăț},
keywords = {Augmented Reality, eLearning, Autism, Usability Testing},
abstract = {Over the last years, augmented reality was used in various domains, from medical, industrial design, modeling and production, robot teleoperation, military, entertainment, leisure activities to translation, facial recognition, assistance while driving, interior and exterior design, virtual friends, internet of things and eLearning. In eLearning, the combination between classical and augmented content (the later coming with 3D models, images, sounds, animations, Internet browsing, etc.) can help the teacher to better explain the content of the courses. In this paper, we present four augmented reality applications, created with the aim to improve communication and collaboration skills (two of them) and to ease the learning of biology and geography (the other two). The motivation behind these applications is to enhance the attractiveness of the classes, allow students to retrain new information more easily and reduce the stress behind tests when presented as games.}
}
@article{MOURTZIS2020546,
title = {An Augmented Reality Collaborative Product Design Cloud-Based Platform in the Context of Learning Factory},
journal = {Procedia Manufacturing},
volume = {45},
pages = {546-551},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.076},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920311185},
author = {Dimitris Mourtzis and Vasilis Siatras and John Angelopoulos and Nikos Panopoulos},
keywords = {Collaborative Design, Augmented Reality, Cloud Technologies, Learning Factory},
abstract = {In the modern market scheme, Market 4.0, there is an evident need for improved quality, increased quantities and lower delivery times of products. However, customers seek for highly personalized products. Consequently, production engineers must include customers in the product design phase so as to meet their unique requirements. On the contrary, during the fourth industrial revolution, cutting-edge digital technologies, such as Augmented Reality (AR) and Cloud technologies have reached a high level of maturity. AR is a digital technology, capable of super-imposing digital information to the user’s physical environment. Furthermore, with the adoption of the Learning Factory concept, academia contributes to the development of new skills and competencies, for the new generation of engineers, that align with the ever-increasing global market demands. As such, in this paper, a framework enabling collaborative product design based on AR is presented, which acts as the digital thread for connecting engineers to customers. The participants will design and develop a Cloud platform for supporting file exchange and storage. Moreover, the Cloud platform can be realized as the communication layer between the stakeholders. In addition to that, an end-user application for the product design, customization and visualization with the use of AR will also be developed. Finally, the proposed framework will be tested in vitro, in a machine shop, in order to gather useful insight into the skills and competencies acquired by the participants during the development of the project.}
}
@article{THEES2020106316,
title = {Effects of augmented reality on learning and cognitive load in university physics laboratory courses},
journal = {Computers in Human Behavior},
volume = {108},
pages = {106316},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106316},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300704},
author = {Michael Thees and Sebastian Kapp and Martin P. Strzys and Fabian Beil and Paul Lukowicz and Jochen Kuhn},
keywords = {Augmented reality, Smartglasses, Cognitive load, Split-attention effect, STEM laboratory courses, Physics laboratory courses},
abstract = {Recent studies emphasize a positive impact of learning with augmented reality (AR) systems in various instructional scenarios. Especially combining real and virtual learning components according to spatial and temporal contiguity principles is claimed to foster learning and to reduce extraneous cognitive processing. We applied these principles to a physics laboratory experiment examining heat conduction where students measure the temperature along heated metal rods via a thermal imaging camera. However, the traditional setup leads to a time delay between measuring and receiving data, and spatially separates relevant visualizations causing resource-consuming search processes. Using see-through smartglasses, traditional displays were transformed into virtual representations which were anchored to corresponding objects of the experimental setup, resulting in an integrated AR view of real-time data. Both traditional and AR-assisted workflows of data collection were investigated in a field study with undergraduate students (N=74) during a graded laboratory course. Performance and cognitive load were assessed as dependent variables. Although the AR condition did not show a learning gain in a conceptual knowledge test, they nonetheless reported a significant lower extraneous cognitive load than the traditional condition. These results contrast with recent findings on AR and integrated formats but reveal a significant impact on cognitive load research.}
}
@article{LOPEZIBANEZ2021100404,
title = {Using gestural emotions recognised through a neural network as input for an adaptive music system in virtual reality},
journal = {Entertainment Computing},
volume = {38},
pages = {100404},
year = {2021},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100404},
url = {https://www.sciencedirect.com/science/article/pii/S187595212100001X},
author = {Manuel {López Ibáñez} and Maximiliano Miranda and Nahum Alvarez and Federico Peinado},
keywords = {Video games, Gesture recognition, Adaptive audio, Human-computer interaction, Machine learning},
abstract = {In this article, a head gesture recognition system is developed in order to identify emotional inputs and provide them to an adaptive music system (LitSens) in virtual reality applications, improving virtual presence in the process. Two iterations of this system, both founded on neural networks, are presented: the first one is based on a multi-layer perceptron, whereas the second one consists of a hybrid one-dimensional convolutional neural network. In both cases, the system is able to recognise fear by analysing head gestures. Whereas the first implementation is quicker when recognising this emotion, the second one is slower, but much more accurate, which makes it a better option overall for soundtrack adaptation. An experiment is then detailed, aimed towards validating the behaviour of a gestural recogniser when detecting fear in players. The results achieved through this validation are generally positive, but evince the need for an improvement in terms of system responsiveness.}
}
@article{LI2023104833,
title = {Development of a mixed reality method for underground pipelines in digital mechanics experiments},
journal = {Tunnelling and Underground Space Technology},
volume = {132},
pages = {104833},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104833},
url = {https://www.sciencedirect.com/science/article/pii/S0886779822004734},
author = {Wei Li and Yajian Wang and Hailu Yang and Zhoujing Ye and Pengpeng Li and Yang {Aron Liu} and Linbing Wang},
keywords = {Underground Pipeline Network, Mechanical Experiment, Mixed Reality, Sensors Communication, Game Engine},
abstract = {The underground pipeline network (UPN) is an essential underground structure and infrastructure. Its full-cycle digital twin system is an important part of the smart city. However, traditional information transmission between humans and computers lacks understanding and interaction in the digital twin experiments. From the perspective of perception, a space interaction system based on mixed reality (MR) technology is used to solve the problem of interaction for traditional mechanical experiments. Firstly, during the design phase, Building Information Modeling (BIM) is used to integrate models and information storage, including physical geometry data and material information. Secondly, the analytical algorithm is constructed for sensors by data filtering and material mechanics theory; based on sensors communication, C#/C++, and Socket to develop human–computer interactive methods through Game Engines and Mixed Reality Toolkits (MRTK). Finally, based on comparing the mechanical parameters, the testing of digital pipeline experiments by spatial anchors. Twenty-four experimenters are selected to test this method. The advantages and prospects of spatial interaction with MR are summarized through qualitative and quantitative analysis. This study realizes the data perception of the UPN and maps it to the real environment, which can provide a technical reference for engineering digital experiments.}
}
@article{ALEJANDROHUERTATORRUCO2022108079,
title = {Effectiveness of virtual reality in discrete event simulation models for manufacturing systems},
journal = {Computers & Industrial Engineering},
volume = {168},
pages = {108079},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108079},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222001498},
author = {Víctor {Alejandro Huerta-Torruco} and Óscar Hernández-Uribe and Leonor {Adriana Cárdenas-Robledo} and Noé {Amir Rodríguez-Olivares}},
keywords = {Virtual reality, Discrete event simulation, Industry 4.0},
abstract = {Virtual reality (VR) and discrete event simulation (DES) have become fundamental technologies of industry 4.0. There is a growing interest in such technologies in both academic and industry sectors. Nevertheless, few studies have reported the use of them simultaneously. This study develops a DES model and proposes an integrated approach combining self-reported and objective measurements to determine the VR effectiveness, based on data collection and analysis of 72 volunteers randomly assigned either a VR system or a laptop computer. The model represents a real manufacturing system where the user can move around, interact with objects, modify variables, run different scenarios, and identify the best one based on the system’s key performance indicators. Results showed that experimentation time is significantly less when users interact with the VR interface, and there is no significant difference between the two interfaces when comparing the decision-making time. However, the VR users obtained a higher percentage of correct answers. Finally, self-reported feedback indicated users preferred the VR system, in addition to the fact that discomfort and presence questionnaires exhibit typical values of the VR experience, and the usability questionnaire yields higher values than the laptop computer.}
}
@article{FUCHS2022103590,
title = {A collaborative knowledge-based method for the interactive development of cabin systems in virtual reality},
journal = {Computers in Industry},
volume = {136},
pages = {103590},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103590},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001974},
author = {Mara Fuchs and Florian Beckert and Jörn Biedermann and Björn Nagel},
keywords = {Aerospace, Aircraft cabin, Virtual reality, Knowledge-based modeling, Digital design},
abstract = {Progressive digitization in the development phase of systems is leading to shorter development times and lower costs. At the same time, the interactions in more complex systems are increasing and become more nested, which affects the understanding of system dependencies for humans as well as modeling these. This results in the challenge of digitizing the knowledge (rules, regulations, requirements, etc.) required to describe the system and its interrelationships. An example of such a system is the aircraft. In practice, usually, the technical design of the cabin and its systems is done separately from the preliminary aircraft design and the cabin results will be integrated late in the aircraft development process. In this paper, a proposal is given for a conceptual design method that enables a cabin systems layout based on preliminary aircraft design data (parameter set). Therefore, a central data model is developed that links cabin components to several disciplines to enable an automated layout. Here, knowledge is stored in an ontology. Linking the ontology with design rules and importing external parameters, missing information needed for preliminary design of cabin systems can be generated. The design rules are based on requirements, safety regulations as well as expert knowledge for design interpretation that has been collected and formalized. Using the ontology, an XML data structure can be instantiated which contains all information about properties, system relationships and requirements. So, the metadata and results of heterogenous domain-specific models and software tools are accessible for all experts of the layout process in a holistic manner and ensure data consistency. Using this XML data structure, a 3D virtual cabin mockup is created in which users have the possibility to interact with cabin modules and system components via controllers. This virtual development platform enables an interaction with complex product data sets like the XML file by visualizing metadata and analysis results along with the cabin geometry, making it even better comprehensible and processable for humans. So, various new cabin system designs can be iterated, evaluated, and optimized at low cost before the concepts are validated in a real prototype. For this, the virtual environment provides a platform that integrates all related disciplines, experts, research partners or the entire supply chain to improve communication among all stakeholders by directly participating and intervening in the evaluation and optimization process. Moreover, the use of VR is being investigated as a new technology in pre-design phase to exploit the potential of knowledge acquisition in immersive environments early in the development stage.}
}
@article{SILVA2023100022,
title = {Effect of an augmented reality app on academic achievement, motivation, and technology acceptance of university students of a chemistry course},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100022},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000168},
author = {Mónica Silva and Karina Bermúdez and Karina Caro},
keywords = {Academic level, Motivation, Technology acceptance, Chemistry, Augmented reality},
abstract = {The purpose of the current research was to explore the effect of an augmented reality app on the academic level, motivation, and technology acceptance of students of a university-level chemistry course. The study followed a pre/post-test design with a control group. At the end of a lecture on carbon bonds, we requested 95 university students to develop three models using modeling clay. The experimental group used the augmented reality app, while the control group used 2D pictures. The academic achievement increased for the students who used the augmented reality app. Motivation scores were not different between the control and experimental group. Our results indicate that augmented reality technology could be helpful in an academic setting.}
}
@article{MECHETER2022232,
title = {Brain MR images segmentation using 3D CNN with features recalibration mechanism for segmented CT generation},
journal = {Neurocomputing},
volume = {491},
pages = {232-243},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222003332},
author = {Imene Mecheter and Maysam Abbod and Habib Zaidi and Abbes Amira},
keywords = {Pseudo CT, MR images, Segmentation, CNN, Features recalibration},
abstract = {The segmentation of MR (magnetic resonance) images is a simple approach to create Pseudo CT images which are useful for many medical imaging analysis applications. One of the main challenges of this process is the bone segmentation of brain MR images. Deep convolutional neural networks (CNNs) have been widely and efficiently applied to perform MR images segmentation. The aim of this work is to propose a novel excitation-based CNN by recalibrating the network features adaptively to enhance the bone segmentation by segmenting the brain MR images into three tissue classes: bone, soft tissue, and air. The proposed method combines two types of features excitation mechanisms namely: (1) spatial squeeze and channel excitation block (cSE) and (2) channel squeeze and spatial excitation block (sSE). The two blocks are combined sequentially and integrated seamlessly into a 3D convolutional encoder decoder network. The novelty of this work emerges in the combination of the two excitation blocks sequentially to improve the segmentation performance and reduce the model complexity. The proposed approach is evaluated through a comparison with computed tomography (CT) images as ground truth and validated with other methods in the literature that applied deep CNN approaches to perform MR image segmentation for PET attenuation correction. Brain MR and CT datasets which consist of 50 patients are used to evaluate the proposed method. The segmentation performance of the three brain classes is evaluated using precision, recall, dice similarity coefficient (DSC), and Jaccard index. The presented method improves the bone tissue segmentation compared to the baseline model and other methods in the literature where the DSC is improved from 0.6278 ± 0.0006 to 0.6437  ±  0.0006 with an improvement percentage of 2.53% for bone class. The proposed excitation-based segmentation network architecture demonstrates promising and competitive results compared with other methods in the literature and reduces the model complexity thanks to the sequential combination of the two excitation blocks.}
}
@article{ZHANG2023152,
title = {Statistical analysis of the non-stationary binomial AR(1) model with change point},
journal = {Applied Mathematical Modelling},
volume = {118},
pages = {152-165},
year = {2023},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23000331},
author = {Rui Zhang},
keywords = {Binomial AR(1) model, Change point, CUSUM test, Forecasting, Non-stationary},
abstract = {In this paper, we conduct the statistical inference for a new class of non-stationary binomial AR(1) models with change point. The parameter estimation problem is considered, as well as the testing problem for the possible change point. For this task, a residual-based cumulative sum (CUSUM) test is employed, and we show that the limiting null distributions take the form of the functions of Brownian bridge. In the simulation study, we evaluate the performance of the estimation methods and the respective CUSUM test. We furthermore propose a method for the forecasting of the model. Finally, two real data applications were provided in the field of epidemiology.}
}
@article{ZHAO2023107779,
title = {Psychodynamic-based virtual reality cognitive training system with personalized emotional arousal elements for mild cognitive impairment patients},
journal = {Computer Methods and Programs in Biomedicine},
volume = {241},
pages = {107779},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107779},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723004455},
author = {Yanfeng Zhao and Liang Li and Xu He and Shuluo Yin and Yuxuan Zhou and Cesar Marquez-Chin and Wenjie Yang and Jiang Rao and Wentao Xiang and Bin Liu and Jianqing Li},
keywords = {Mild cognitive impairment, Psychodynamic-based cognitive training, Virtual reality, Personalized emotional arousal elements, Assessment},
abstract = {Background and objective
Mild cognitive impairment (MCI) is a serious threat to the physical health and quality of life of the elderly, as well as a heavy burden on families and society. The current computer-based rehabilitation training ignores the role of emotions in cognitive impairment rehabilitation, making it difficult to improve patient engagement and efficiency. To address this, a psychodynamics-based cognitive rehabilitation training method with personalized emotional arousal elements was proposed using virtual reality technology.
Methods
Our proposed method contains four training tasks, which cover (audiovisual memory, attention & processing, working memory, abstract & Logic, spatial pathfinding) and six positive emotional arousal elements (sensory feedback, achievement system, multiplayer interaction, score comparison, relaxation scenarios, and peaceful videos) to motivate participants to persist during cognitive training continuously and maintain a positive mental attitude toward training. The six emotional arousal elements were divided into two personalized combinations—full combination and half combination—based on the results of the pre-assessment and were dynamically distributed throughout both the training tasks and post-training.
Results
Fifteen participants with MCI were recruited to complete the proposed experiment and validate the effectiveness of the system. They were first asked to complete two assessments (e.g., the big five scale and the positive and negative affect scale) to investigate their personalities. Based on the results of the assessments, they were provided with a full or half combination of arousal elements in the training tasks and post-training. Finally, the acceptability of the system and task experience were assessed using questionnaires. Notably, there was a significant increase in training scores for participants who completed a six-week training period (66.7%, 33.4%, and 25.0% for attention and processing, working memory, and abstraction and logic, respectively). The results show that positive emotional arousal had a positive effect on the MCI participants. The training tasks and arousal elements can improve cognitive function and enhance the confidence and engagement of participants. There were no significant differences in cognitive domain training scores between the two groups.
Conclusions
This personalized cognitive training system has the potential to serve as a convenient solution for complementary treatment of MCI.}
}
@article{BALL2021101728,
title = {Virtual reality adoption during the COVID-19 pandemic: A uses and gratifications perspective},
journal = {Telematics and Informatics},
volume = {65},
pages = {101728},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101728},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321001672},
author = {Christopher Ball and Kuo-Ting Huang and Jess Francis},
keywords = {Virtual reality, VR, Uses and gratifications theory, COVID-19, Pandemic},
abstract = {The coronavirus disease 2019 (COVID-19) pandemic has impacted all aspects of people's lives, including how we work, play, learn, exercise, and socialize. Virtual reality (VR) technology has the potential to mitigate many of the challenges brought about by the pandemic, which has spurred increased adoption. However, relatively low adoption overall and limited software still restrict the power of VR to address COVID-19 difficulties effectively. This study examines how the perceived impacts of COVID-19 might lead to different VR uses and gratifications and device ownership / variability. Furthermore, we investigate the importance of social interactivity within VR for increasing adoption intentions. We surveyed 298 Amazon Mechanical Turk users during the Fall of 2020. Results indicate that the pandemic's perceived impacts influenced the likelihood of acquiring VR for education, tourism, and work. For VR ownership and variability, those who purchased VR during the pandemic were more likely to report buying it for work. Those with access to high-end VR hardware were more likely to report a broader range of uses, including socializing, health, and telemedicine. Validating the importance of various applications during the pandemic, we found that the desire for social interactivity mediates the impacts of COVID-19 on future adoption intentions. Theoretically, we propose several gratifications sought via the use of VR during the pandemic. Practically, we discuss recommendations for future VR research, marketing, and software design.}
}
@article{YANG201348,
title = {Image-based 3D scene reconstruction and exploration in augmented reality},
journal = {Automation in Construction},
volume = {33},
pages = {48-60},
year = {2013},
note = {Augmented Reality in Architecture, Engineering, and Construction},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2012.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0926580512001690},
author = {Ming-Der Yang and Chih-Fan Chao and Kai-Siang Huang and Liang-You Lu and Yi-Ping Chen},
keywords = {Augmented reality, 3D reconstruction, Feature extraction, Corresponding feature, Structure from motion},
abstract = {Augmented Reality (AR) is an integrated technique of image processing and display system of complex information, which involves real-time computing, motion tracking, pattern recognition, image projection, database linking, feature extraction, and coordinate transformation. In this study, such techniques through Structure From Motion (SFM), Clustering Views for Multi-View Stereo (CMVS), Patch-based Multi-View Stereo (PMVS), and Poisson surface reconstruction were smoothly integrated into a 3D reconstruction system with comparative efficiency in computation. The system can be applied to regular images taken by amateur cameras, smart phones, tablet PCs, and other mobile devices, without need of a priori internal and external camera parameters. To verify the ability of the established reconstruction system, indoor and outdoor objects at various scales, such as a helmet (a small object), a corridor (an indoor medium object), an arbor (outdoor medium object), and a building (outdoor large object) were tested. Through tracking and registration, the reconstructed 3D models were loaded in an AR environment to facilitate displaying, interacting, and rendering that provides AR applications in construction design and management for better and qualitative communication in economical and handy ways.}
}
@article{LI2024108142,
title = {Design and development of a personalized virtual reality-based training system for vascular intervention surgery},
journal = {Computer Methods and Programs in Biomedicine},
volume = {249},
pages = {108142},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108142},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400138X},
author = {Pan Li and Boxuan Xu and Xinxin Zhang and Delei Fang and Junxia Zhang},
keywords = {Surgical training, Vascular intervention surgery, Virtual reality, Deformation simulation, Collision detection, Unity3D},
abstract = {Background and objectives
Virtual training has emerged as an exceptionally effective approach for training healthcare practitioners in the field of vascular intervention surgery. By providing a simulated environment and blood vessel model that enables repeated practice, virtual training facilitates the acquisition of surgical skills in a safe and efficient manner for trainees. However, the current state of research in this area is characterized by limitations in the fidelity of blood vessel and guidewire models, which restricts the effectiveness of training. Additionally, existing approaches lack the necessary real-time responsiveness and precision, while the blood vessel models suffer from incompleteness and a lack of scientific rigor.
Methods
To address these challenges, this paper integrates position-based dynamics (PBD) and its extensions, shape matching, and Cosserat elastic rods. By combining these approaches within a unified particle framework, accurate and realistic deformation simulation of personalized blood vessel and guidewire models is achieved, thereby enhancing the training experience. Furthermore, a multi-level progressive continuous collision detection method, leveraging spatial hashing, is proposed to improve the accuracy and efficiency of collision detection.
Results
Our proposed blood vessel model demonstrated acceptable performance with the reduced deformation simulation response times of 7 ms, improving the real-time capability at least of 43.75 %. Experimental validation confirmed that the guidewire model proposed in this paper can dynamically adjust the density of its elastic rods to alter the degree of bending and torsion. It also exhibited a deformation process comparable to that of real guidewires, with an average response time of 6 ms. In the interaction of blood vessel and guidewire models, the simulator blood vessel model used for coronary vascular intervention training exhibited an average response time of 15.42 ms, with a frame rate of approximately 64 FPS.
Conclusions
The method presented in this paper achieves deformation simulation of both vascular and guidewire models, demonstrating sufficient real-time performance and accuracy. The interaction efficiency between vascular and guidewire models is enhanced through the unified simulation framework and collision detection. Furthermore, it can be integrated with virtual training scenarios within the system, making it suitable for developing more advanced vascular interventional surgery training systems.}
}
@article{KWIATEK2019102935,
title = {Impact of augmented reality and spatial cognition on assembly in construction},
journal = {Automation in Construction},
volume = {108},
pages = {102935},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102935},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518312482},
author = {C Kwiatek and M Sharif and S Li and C Haas and S Walbridge},
abstract = {Applications of augmented reality have the potential to help radically improve construction productivity, reduce rework and improve communication of design intent. A hand-held augmented reality application that has been developed to assist in assembly and inspection of pipe spools using 3D scanning, scan-vs-BIM, flexible workflow, and a touch sensitive user interface is described here. Representing a class of augmented reality applications, it is used to examine the impact of augmented reality and its relation to human spatial cognitive abilities. Experiments are conducted with 21 professional pipe fitters and 40 engineering students, whose spatial cognitive abilities are measured, and who are then asked to assemble a complex pipe spool using conventional or augmented reality assisted means. Statistical analysis of the experimental results for these groups of subjects validates several observations. Augmented reality can help save substantial time in pipe spool assembly over the conventional methods for both untrained engineers and for well trained professional pipe fitters. Those whose cognitive abilities are measured as low experienced more benefit than the other subjects from the augmented reality application. More applications of augmented reality need to be studied to understand its potential impact on construction work.}
}
@article{LATINI2021111508,
title = {Exploring the use of immersive virtual reality to assess occupants’ productivity and comfort in workplaces: An experimental study on the role of walls colour},
journal = {Energy and Buildings},
volume = {253},
pages = {111508},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111508},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821007921},
author = {Arianna Latini and Elisa {Di Giuseppe} and Marco D'Orazio and Costanzo {Di Perna}},
keywords = {Indoor environmental quality, Immersive virtual environment, Virtual reality, Wall colour, Workers’ productivity, Indoor comfort, Human-centric design},
abstract = {Virtual Reality application in holistic investigations for energy and cost-saving building design, aimed at humans’ well-being and performance, is still emerging and needs validation. In this study, tests in real and virtual scenarios of an office room were performed, investigating the impact of three walls colours (red, white, blue) and two indoor air temperatures (17–22 °C) on 23 participants’ work productivity (through a proofreading task) and thermal and visual sensations (through surveys). The first goal was the comparison of the results obtained in the real and virtual settings; the second one the assessment of the effect of walls’ colour and temperature levels on the mentioned variables in each environment. Statistical analyses were then performed “between groups” (Wilcoxon and t-tests, among datasets of the two environments) and “within groups” (ANOVA and Scheirer test, within each environment). The study revealed no statistically significant variations in productivity and sensation votes, thus supporting the suitability of VR as a proper research technology in this domain. The study also demonstrated no statistically significant effects of colour and temperature on productivity and comfort results within the tested settings. Future investigations should involve a wider range of temperatures and colours and address a wider subjects’ sample.}
}
@article{TRAPPEY2022100331,
title = {VR-enabled engineering consultation chatbot for integrated and intelligent manufacturing services},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100331},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200005X},
author = {Amy J.C. Trappey and Charles V. Trappey and Min-Hua Chao and Chun-Ting Wu},
keywords = {Chatbot, Virtual reality, Power transformer, Natural language processing, Manufacturing service, Product-service system},
abstract = {Industrial equipment manufacturing, such as electric power transformers, require highly customized design, assembly, installation and maintenance services for customers. Engineering consultation is an essential manufacturing service which requires deep domain knowledge. This research develops and implements the framework and the prototype of a Virtual Reality (VR) enabled intelligent engineering consultation chatbot system with natural language communication capabilities. The consultation chatbot case focuses on the power transformer knowledge domain. The system is constructed using a comprehensive knowledge base of Frequently Asked Questions (FAQs) and answers from a power transformer manufacturer. A total of 490,000 technical articles (with more than 1.1 billion words) from Wikipedia and a comprehensive domain-specific document set are used to train a word embedding model for retrieving questions and answers (QA) from the FAQ knowledge base. Immersive VR technology provides a highly interactive QA chatbot with realistic graphical views for informative engineering counselling. The case example demonstrates the accuracy of the proposed VR-enabled chatbot for transformer QAs exceeds 91%.}
}
@article{DIDONATO201570,
title = {Text legibility for projected Augmented Reality on industrial workbenches},
journal = {Computers in Industry},
volume = {70},
pages = {70-78},
year = {2015},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2015.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166361515000421},
author = {Michele {Di Donato} and Michele Fiorentino and Antonio E. Uva and Michele Gattullo and Giuseppe Monno},
keywords = {Spatial Augmented Reality, Industrial applications, Text legibility, Visualization},
abstract = {Augmented Reality is a promising technology for the product lifecycle development, but it is still not established in industrial facilities. The most relevant issues to be addressed relate to the ergonomics: avoid the discomfort of Head-Worn Displays, allow the operators to have free hands and improve data visualization. In this work we study the possibility to use projection-based Augmented Reality (projected AR), as optimal solution for technical visualization on industrial workbenches. In particular, text legibility in projected AR is difficult to optimize since it is affected by many parameters: environment conditions, text style, material and shape of the target surface. This problem is poorly addressed in literature and in the specific industrial field. We analyze the legibility of a set of colors prescribed by international standards for the industrial environments, on six widely used industrial workbenches surfaces. We compared the performance of 14 subjects using projected AR, with that using a traditional LCD monitor. We collected about 2500 measurements (times and errors) through the use of a test application, followed by qualitative interviews. The results showed that, as regards legibility, projected AR can be used in place of traditional monitors in most of the cases. Another not trivial finding is that the influence on legibility of surface irregularities (e.g., grooves, prominences) is more important than that of surface texturization. A possible limitation for the use of projected AR is given by the blue color, whose performance turned out to be lower than that of other colors with every workbench surface.}
}
@article{SANAGUANOMORENO2024200306,
title = {Real-time impulse response: a methodology based on Machine Learning approaches for a rapid impulse response generation for real-time Acoustic Virtual Reality systems},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200306},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200306},
url = {https://www.sciencedirect.com/science/article/pii/S266730532300131X},
author = {D.A. Sanaguano-Moreno and J.F. Lucio-Naranjo and R.A. Tenenbaum and G.B. Sampaio-Regattieri},
keywords = {Auralization, Acoustic Virtual Reality, Variational Auto-encoders, Binaural Room Impulse Responses, Generative models, Long Short-Term Memory},
abstract = {Simulation of high-definition binaural room impulse responses using conventional approaches involves a significant amount of computational resources, resulting in high computational time, making these approaches incapable of performing real-time high quality acoustic virtual reality. This research implemented a methodology for the rapid impulse response generation using the position of a moving listener inside a fixed sound field. The rapid generation of the impulse response is performed using its representative compressed dimension, with a smaller dimension than the original impulse response, learned by variational autoencoders and long short-term memory neural networks. First, the methodology selects a representative number of impulse responses covering the area of interest using a reliable room acoustic simulator. Second, it generates a dataset with sufficient impulse responses uniformly distributed through a data augmentation approach using a modified bilinear interpolation from the impulse responses previously simulated. Third, it applies an unsupervised model to positionally cluster the impulse responses to reduce the variability of the impulse responses in the given environment. Fourth, it splits the impulse response into time segments and generates a dataset per segment and cluster. Fifth, it trains a variational autocoder with a long short-term memory neural network model for each time segment cluster of impulse responses to infer the correspondent compressed impulse response part. In summary, the impulse response is generated by assigning the current listener position to the corresponding cluster and executing the decoders of the variational autoencoders with long short-term memory, trained previously. The findings are encouraging; the normalized mean absolute error of the impulse responses gathered by the interpolator and the impulse responses generated by the proposed model is less than 15% in the 88% of impulse responses reserved for testing. Moreover, the average of the absolute error of the interaural cross-correlation coefficient between the impulse responses obtained from the simulator and the proposed model is around 16%, implying that most acoustic characteristics of the real impulse are preserved. In addition, the computational time for generating the impulse response segment of 300 ms is approximately 65 ms, which is almost haft than the total system latency for a realistic auralization, 112 ms.}
}
@article{GAZZOTTI2021112780,
title = {Virtual and Augmented Reality Use Cases for Fusion Design Engineering},
journal = {Fusion Engineering and Design},
volume = {172},
pages = {112780},
year = {2021},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2021.112780},
url = {https://www.sciencedirect.com/science/article/pii/S0920379621005561},
author = {S. Gazzotti and F. Ferlay and L. Meunier and P. Viudes and K. Huc and A. Derkazarian and J-P. Friconneau and B. Peluso and J-P. Martins},
keywords = {Virtual Reality, Augmented Reality, WEST tokamak, ITER, Test Blanket System, Health & Safety, MIFI},
abstract = {As augmented and virtual reality technology is revolutionising the way products are designed and tested, many companies are expanding the range of use cases across their organisations. After 10 years using Virtual Reality (VR) & Augmented Reality (AR) technology during the life cycle of fusion components, the CEA IRFM has improved its skills developing immersive visualisation scenes for the WEST tokamak [6, 5] project and several ITER Site Support Agreement such as Magnet Infrastructure Facilities for ITER (MIFI) [7] and Test Blanket Module System (TBM) [8]. Data preparation process has been optimized, reducing time cost to convert CAD data to an interactive and immersive model. For the WEST Project, VR engineers developed immersive scenes to simulate assembly sequences, to check component introduction using physics and to show 2D temperature data from diagnostics in a 3D environment. For the TBM case, health and safety oriented model environment is developed. Features for team simulated human access and hands on operations for As Low As Reasonably Achievable (ALARA) optimisation process with motion-tracking system and virtual avatars with ventilated protective pressurised suit simulating worker protection in nuclear environment. For MIFI application, a specific assembly tool has been developed for TF coils interface located on the top and at the bottom junction. During a first phase, the development was driven by iterative design loop (CAD modelling analysis and cinematic by using VR simulations). By means of interactive simulations and immersive conditions, the interface connection task was validated against feasibility including accessibility. AR has also been introduced in the process since a physical mock-up has been augmented to consider the constraint environment of TF Coils. For TBM application, on site AR simulation of the port Cell equipment will provide enhanced environment description. Collaborative immersive context, digital twins and combination between AR/VR & physical mock-ups for validation of maintenance since component design phase will constitute challenge for the near future.}
}
@article{MOSCOSO2022108962,
title = {Stereoscopic Images and Virtual Reality techniques in daylighting research: A method-comparison study},
journal = {Building and Environment},
volume = {214},
pages = {108962},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322002049},
author = {Claudia Moscoso and Marzieh Nazari and Barbara Szybinska Matusiak},
keywords = {Stereoscopic images, Virtual reality, Method comparison, Visualization techniques, Architectural space, Daylight},
abstract = {Visualization and presentation techniques are experiencing a rapid development and application in the architectural profession and beyond. As such, Stereoscopic Images (SI) and Virtual Reality (VR), both advanced visualization techniques, have found their way in daylighting research. This paper explores the correspondence between these two techniques using both quantitative and qualitative methods. The study focused on two small rooms with the same dimensions, with alternately white and black surfaces, and three different windows sizes. Seven architectural qualities were studied: Pleasantness, Calmness, Interest, Excitement, Complexity, Spaciousness, and Amount of View. The attributes were evaluated by 20 participants using a Likert-type scale. The collected data was analyzed using Wilcoxon signed-rank tests and the Bland-Altman method. The results revealed that the attributes Pleasantness, Calmness, Interest, and Complexity in rooms with both white and black surfaces, and Excitement and Spaciousness in a room with white surfaces are evaluated similarly in both VR and SI. The attributes Excitement and Spaciousness in a room with black surfaces and the Amount of View in rooms with both white and black surfaces were not evaluated similarly with both methods. In addition, qualitative results indicated that the visualization technique affects how a space is perceived. Indeed, the Virtual Reality's nature as an immersive environment provides the feeling of ‘presence’ which does not apply to SI. The results of this paper can help researchers working with daylighting in buildings in selecting the appropriate visualization technique to reduce experimental and logistical constraints caused by varying daylight conditions.}
}
@article{ZHOU202364,
title = {Design and validation of a navigation system of multimodal medical images for neurosurgery based on mixed reality},
journal = {Visual Informatics},
volume = {7},
number = {2},
pages = {64-71},
year = {2023},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X23000177},
author = {Zeyang Zhou and Zhiyong Yang and Shan Jiang and Tao Zhu and Shixing Ma and Yuhua Li and Jie Zhuo},
keywords = {Augmented reality, Neurosurgery, Image-guided intervention, Multimodal images},
abstract = {Purpose:
This paper aims to develop a navigation system based on mixed reality, which can display multimodal medical images in an immersive environment and help surgeons locate the target area and surrounding important tissues precisely.
Methods:
To be displayed properly in mixed reality, medical images are processed in this system. High-quality cerebral vessels and nerve fibers with proper colors are reconstructed and exported to mixed reality environment. Multimodal images and models are registered and fused, extracting their key information. The multiple processed images are fused with the real patient in the same coordinate system to guide the surgery.
Results:
The multimodal image system is designed and validated properly. In phantom experiments, the average error of preoperative registration is 1.003 mm and the standard deviation is 0.096 mm. The average proportion of well-registered areas is 94.9%. In patient experiments, the surgeons who participated in the experiments generally indicated that the system had excellent performance and great application prospect for neurosurgery.
Conclusion:
This article proposes a navigation system of multimodal images for neurosurgery based on mixed reality. Compared with other navigation methods, this system can help surgeons locate the target area and surrounding important tissues more precisely and rapidly.}
}
@article{YAO2002963,
title = {MR damper and its application for semi-active control of vehicle suspension system},
journal = {Mechatronics},
volume = {12},
number = {7},
pages = {963-973},
year = {2002},
issn = {0957-4158},
doi = {https://doi.org/10.1016/S0957-4158(01)00032-0},
url = {https://www.sciencedirect.com/science/article/pii/S0957415801000320},
author = {G.Z. Yao and F.F. Yap and G. Chen and W.H. Li and S.H. Yeo},
abstract = {In this paper, a semi-active control of vehicle suspension system with magnetorheological (MR) damper is presented. At first a MR damper working in flow mode is designed. Performance testing is done for this damper with INSTRON machine. Then a mathematical model, Bouc–Wen model, is adopted to characterize the performance of the MR damper. With optimization method in MATLAB® and experimental results of MR damper, the coefficients of the model are determined. Finally, a scaled quarter car model is set up including the model of the MR damper and a semi-active control strategy is adopted to control the vibration of suspension system. Simulation results show that with the semi-active control the vibration of suspension system is well controlled.}
}
@article{MUKHOPADHYAY202255,
title = {Virtual-reality-based digital twin of office spaces with social distance measurement feature},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {1},
pages = {55-75},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000043},
author = {Abhishek Mukhopadhyay and GS Rajshekar Reddy and KamalPreet Singh Saluja and Subhankar Ghosh and Anasol Peña-Rios and Gokul Gopal and Pradipta Biswas},
keywords = {Virtual environment, Digital twin, 3D visualization, Convolutional neural network, Object detection, Social distancing},
abstract = {Background
Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
Methods
In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
Results
We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
Conclusions
A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.}
}
@article{FLASH2024105458,
title = {Assessing the usability of an immersive virtual reality grocery store in healthy controls},
journal = {International Journal of Medical Informatics},
volume = {187},
pages = {105458},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105458},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001217},
author = {Sara Flash and Denise M. Goldsmith and Tanna L. Nelson and William Thompson and Patricia {Flatley Brennan}},
keywords = {Immersive Virtual Reality, User Experience Testing, Usability Testing, Virtual Grocery Store},
abstract = {Background
Immersive virtual reality (IVR) as a research platform to study human behaviors is an emerging field and may be useful for studying self-care management, especially in the gap between formal healthcare recommendations and day-to-day living. Self-care activities, such as grocery shopping, can be challenging for people with chronic illness. We developed an IVR environment that simulates a real-life grocery store and conducted a usability study to demonstrate the safety and acceptability of IVR as an experimental environment.
Methods
This study was a three-arm randomized control trial involving 24 participants, conducted as a usability study to evaluate aspects of the experimental condition including the effectiveness of a training exposure, the occurrence of undesirable effects associated with IVR, and participants’ experiences of realism, immersion, and spatial presence. The experiment, using a head mounted device and handheld controllers, included a 10-minute training exposure, followed by one of three unique 30-minute experimental conditions which exposed participants to different combinations of tasks and stimuli, and a post-experience interview. We measured controller errors, undesirable symptoms associated with IVR, and the perception of realism, immersion, and spatial presence.
Results
Participants used controllers effectively to interact within the IVR environment. Hand controller use errors were fewer during the experimental conditions compared to the training exposure. Minimal undesirable IVR symptoms were reported. Presence was rated in the middle range with no significant differences based on experimental condition. Overall, user experience feedback was positive.
Conclusions
We demonstrated that participants could engage in our IVR environment without excessive error or experiencing undesirable effects and confirmed that the virtual experience attained a level of presence necessary to effectively engage in the study. These findings give us confidence that this IVR intervention designed to explore instrumental activities of daily living is safe, effective and provides a credible, controlled simulated community-like setting.}
}
@article{RIEMANN20201,
title = {Agile Implementation of Virtual Reality in Learning Factories},
journal = {Procedia Manufacturing},
volume = {45},
pages = {1-6},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920310672},
author = {Thomas Riemann and Antonio Kreß and Lisa Roth and Sven Klipfel and Joachim Metternich and Petra Grell},
keywords = {Virtual Reality, Learning Factory, Agile Project Management, Learning Scenario, Kano Model},
abstract = {The concept of learning factories fulfills current learning theoretical requirements in terms of the situation, process orientation, as well as authenticity. Nevertheless, due to the high complexity of the industrial production environment, it is challenging to transfer learned skills into the operational application situation. With Virtual Reality, training participants have the ability to learn with transfer-oriented action tasks in virtual space directly after the training in physical learning environments. The learning process can be personalized and adapted in the virtual learning environment. Each participant in the training can individually determine elements of the learning situation. For example, the entire learning environment can be adapted to the individual real production environment of the training participant. Through Virtual Reality, new forms of reflection are possible, e.g. recording the learning process. Technical, didactic and organizational requirements were identified by a systematic literature analysis. The research project is based on training courses in the process learning factory “Center for industrial Productivity” (CiP) located at TU Darmstadt. In order to assess and prioritize the requirements, expert surveys were conducted. The surveys are based on the Kano model in order to classify requirements. Must-be quality requirements are implemented in a minimum viable product (MVP). The MVP allows fast learning by testing and experimenting. Based on the agile manifesto, further requirements can be implemented agilely in the virtual environment.}
}
@article{IQBAL2021103841,
title = {Augmented reality in robotic assisted orthopaedic surgery: A pilot study},
journal = {Journal of Biomedical Informatics},
volume = {120},
pages = {103841},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103841},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421001702},
author = {Hisham Iqbal and Fabio Tatti and Ferdinando {Rodriguez y Baena}},
keywords = {Augmented Reality, Surgical Workflows, Human-Machine Interfacing},
abstract = {Background
The research and development of augmented-reality (AR) technologies in surgical applications has seen an evolution of the traditional user-interfaces (UI) utilised by clinicians when conducting robot-assisted orthopaedic surgeries. The typical UI for such systems relies on surgeons managing 3D medical imaging data in the 2D space of a touchscreen monitor, located away from the operating site. Conversely, AR can provide a composite view overlaying the real surgical scene with co-located virtual holographic representations of medical data, leading to a more immersive and intuitive operator experience.
Materials and Methods
This work explores the integration of AR within an orthopaedic setting by capturing and replicating the UI of an existing surgical robot within an AR head-mounted display worn by the clinician. The resulting mixed-reality workflow enabled users to simultaneously view the operating-site and real-time holographic operating informatics when carrying out a robot-assisted patellofemoral-arthroplasty (PFA). Ten surgeons were recruited to test the impact of the AR system on procedure completion time and operating surface roughness.
Results and Discussion
The integration of AR did not appear to require subjects to significantly alter their surgical techniques, which was demonstrated by non-significant changes to the study’s clinical metrics, with a statistically insignificant mean increase in operating time (+0.778 s, p = 0.488) and a statistically insignificant change in mean surface roughness (p = 0.274). Additionally, a post-operative survey indicated a positive consensus on the usability of the AR system without incurring noticeable physical distress such as eyestrain or fatigue.
Conclusions
Overall, these study results demonstrated a successful integration of AR technologies within the framework of an existing robot-assisted surgical platform with no significant negative effects in two quantitative metrics of surgical performance, and a positive outcome relating to user-centric and ergonomic evaluation criteria.}
}
@article{WANG2022100351,
title = {BIM Information Integration Based VR Modeling in Digital Twins in Industry 5.0},
journal = {Journal of Industrial Information Integration},
volume = {28},
pages = {100351},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100351},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200022X},
author = {Weixi Wang and Han Guo and Xiaoming Li and Shengjun Tang and You Li and Linfu Xie and Zhihan Lv},
keywords = {Digital Twins, Building Information Modeling (BIM), VR Modeling, Information Integration, Industry 5.0},
abstract = {This study aims to improve the efficiency of construction efficiency to ensure the infrastructure needs of urban development. Targeting at realizing the intelligent manufacturing of buildings, the digital twins are applied to all stages of building construction, and the three-dimensional (3D) modeling is implemented. A digital twins framework in the construction field is constructed based on the building information modeling (BIM). From physical construction site entity modeling (PCSE modeling) and digital twins virtual body modeling (DTVB modeling) to virtual and real interaction modeling (VRI modeling), the digital twins process is fully applied to the various stages of building construction. The performance of the method proposed is further verified through the analysis of specific data. It is found that the model proposed shows better performance on the same platform and node compared with other models, and it shows the best scalability in the iterative calculation test on the Flink platform through efficient memory management and incremental iteration mechanism. In addition, the model has a delay of less than 100 milliseconds: in the Sleep scenario, the system delay of the latest model is about 5 times that of on the Flink, and the system interruption probability will firstly decrease and then increase when the time division factor (TDF) is increased. This study provides important reference value for the intelligent development of the construction industry and the high-quality development of buildings.}
}
@article{MURA2016340,
title = {An Integrated Environment Based on Augmented Reality and Sensing Device for Manual Assembly Workstations},
journal = {Procedia CIRP},
volume = {41},
pages = {340-345},
year = {2016},
note = {Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2015.12.128},
url = {https://www.sciencedirect.com/science/article/pii/S221282711501207X},
author = {Michela Dalle Mura and Gino Dini and Franco Failli},
keywords = {Augmented Reality, Assembly, Error Detection},
abstract = {One of the main problems encountered in manual assembly workstations is human error in performing the operations. Several approaches are currently used to face this problem, such as intensive training of personnel, poka-yoke devices or invasive sensing systems (e.g. sensing gloves) used for monitoring the process and detect wrong procedures or errors in joining the parts. This paper proposes an innovative system based on the interaction between a force sensor and an augmented reality (AR) equipment used to give to the worker the necessary information about the correct assembly sequence and to alert him in case of errors. The force sensor is placed under the workbench and it is used to monitor the assembly process by collecting force and torque data with respect to an XYZ reference system; a pattern recognition technique allows the error identification and the selection of the appropriate recovery procedure. Two AR devices have been tested in this application: a video-mixing spatial display and an optical see-through apparatus, comparing the pro and cons of these two solutions. The first device includes a CCD camera positioned over the workstation and an LCD display used by the worker as a support for the correct execution of assembly operations and receiving instructions about recovery procedures. The latter consists of a head mounted display (HMD) having the capability of reflecting projected images in front of the worker's eyes, allowing a real-world view with the superimposition of virtual objects. The CCD camera is also used for identifying errors that are not detectable by the force sensor. At the end, a case study concerning a typical assembly procedure is presented and discussed.}
}
@article{GUHL2018167,
title = {Enabling Human-Robot-Interaction via Virtual and Augmented Reality in Distributed Control Systems},
journal = {Procedia CIRP},
volume = {76},
pages = {167-170},
year = {2018},
note = {7th CIRP Conference on Assembly Technologies and Systems (CATS 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S221282711830043X},
author = {Jan Guhl and Johannes Hügle and Jörg Krüger},
keywords = {Augmented reality, Robot, Human-machine interaction},
abstract = {Production and assembly lines are nowadays transforming into flexible and interconnected cells due to rapidly changing production demands. Changes are, for example, varying locations and poses for the processed work pieces and tools as well as the involved machinery like industrial robots. Even a variation in the combination or sequence of different production steps is possible. In case of older involved machines the task of reconfiguration and calibration can be time consuming. This may lead, in addition with the expected upcoming shortage of highly skilled workers, to future challenges, especially for small and medium sized enterprises. One possibility to address these challenges is to use distributed or cloud-based control for the participating machines. These approaches allow the use of more sophisticated and therefore in most cases computationally heavier algorithms than offered by classic monolithic controls. Those algorithms range from simple visual servoing applications to more complex scenarios, like sampling-based path planning in a previously 3D-reconstructed robot cell. Moving the computation of the machine’s reactions physically and logically away from the machine control complicates the supervision and verification of the computed robot paths and trajectories. This poses a potential threat to the machine’s operator since he/she is hardly capable of predicting or controlling the robot’s movements. To overcome this drawback, this paper presents a system which allows the user to interact with industrial robot and other cyber physical systems via augmented and virtual reality. Captured topics in this paper are the architecture and concept for the distributed system, first implementation details and promising results obtained by using a Microsoft HoloLens and other visualization devices.}
}
@article{VANDAALEN2022119,
title = {Determining fresh tomato weight using depth images from an AR headset},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {32},
pages = {119-123},
year = {2022},
note = {7th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.125},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322027586},
author = {Tim {van Daalen} and Joseph Peller and Jos Balendonck},
keywords = {HoloLens, Depth imaging, Volume estimation, Handsfree measurement},
abstract = {There is a need for handsfree devices in greenhouses for training and harvesting assistance. Augmented Reality could offer this in the near future. We showed that it is possible to detect ripeness characteristics of tomatoes using the 3D scanning capabilities of the HoloLens. We verified this in an experimental setup with multiple tomato varieties. Our results show the possibilities and problems of this technique for future development.}
}
@article{BRUNO2018140,
title = {Augmented reality visualization of scene depth for aiding ROV pilots in underwater manipulation},
journal = {Ocean Engineering},
volume = {168},
pages = {140-154},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818317189},
author = {Fabio Bruno and Antonio Lagudi and Loris Barbieri and Domenico Rizzo and Maurizio Muzzupappa and Luigi {De Napoli}},
keywords = {Underwater manipulation, Forward kinematics, Optical-stereo camera, Augmented reality},
abstract = {Underwater manipulation is a key technology for marine industries and exploration that can be efficiently adopted in other application fields, such as underwater archaeology, biological manipulation, scientific expedition, as well as offshore construction in the Oil and Gas industry. It is performed remotely by expert pilots thanks to the visual feedbacks provided by one or more cameras but without any information about the distance between the end-effector and the target. To this end, the paper presents a novel system based on a sensorized robotic arm, stereoscopic 3D perception and augmented reality visualization to support ROV's pilots in underwater manipulation tasks. The system, thanks to the adoption of an optical-stereo camera, provides a visual feedback of the underwater scene on which a depth map of the underwater workspace is augmented on. In particular, combining the kinematics of the robotic arm and the standard photogrammetric model of the stereo camera, it is possible to generate a depth map that shows to the pilots the distances of the surface of the scene objects from the end-effector's pose. Experimental tests carried out in the context of the CoMAS (In-situ conservation planning of Underwater Archaeological Artefacts) project have demonstrated the effectiveness of the proposed system.}
}
@article{MUNSINGER2023103368,
title = {Virtual reality for improving cyber situational awareness in security operations centers},
journal = {Computers & Security},
volume = {132},
pages = {103368},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103368},
url = {https://www.sciencedirect.com/science/article/pii/S016740482300278X},
author = {Brita Munsinger and Nicole Beebe and Turquoise Richardson},
keywords = {Cyber situational awareness, Virtual reality, Security operations centers, Network monitoring},
abstract = {Security operations centers (SOCs) are the 911 centers of many organizational networks, except they not only respond, but also monitor. SOC operators are charged with detection, response, and mitigation. This is a tall task when one considers the volume, velocity, and variety of both internal and external organizational network and system data. SOC operations are truly a big data problem. Security orchestration, incident event management, data fusion, and anomaly detection systems help, but more is needed. This study examines the impact virtual reality (VR) can have on SOC operator performance and perceived task load. We developed a VR based network monitoring tool and assigned human subjects to one of three conditions – VR only, traditional tool only, or both. Our results, though small in scale, provide very promising indication that VR based technology may be beneficial for improving cyber situational awareness (SA), particularly with overall data perception involving novice SOC operators. The results are promising, but the sample size is small, so future research should validate this pilot study. Given the workforce challenges in the cybersecurity space, and the need to perceive large quantities of data, VR may be a very good addition to SOCs.}
}
@article{BINGCHAN2018221,
title = {Maintenance and Management of Marine Communication and Navigation Equipment Based on Virtual Reality},
journal = {Procedia Computer Science},
volume = {139},
pages = {221-226},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.254},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319239},
author = {Li Bingchan and Bo Mao and Jie Cao},
keywords = {Marine Communication, Navigation, Maintenance, Management, Virtual Reality},
abstract = {According to the present situation and existing problems of marine communication and navigation equipment maintenance and management, this paper expounds the virtual reality technology in equipment maintenance and management. We design and implement a VR based maintenance and management system for marine communication and navigation equipment. The system consists of four parts: VR editing module, VR training module, user behavior analysis module, VR intelligent deduction module and user management module function module. The design and hardware structure of the virtual system is given, and the management task modules on this platform are designed, finally realized the function simulation. Functional verification proved the system can effectively improve the equipment management level of electro-technical officer (ETO).}
}
@article{LI20221516,
title = {Non-photorealistic Visualization of 3D City Models using Visual Variables in Virtual Reality Environments},
journal = {Procedia Computer Science},
volume = {214},
pages = {1516-1521},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.338},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020506},
author = {Bingchan Li and Zhangsong Luo and Bo Mao},
keywords = {visual variable, 3D city model, non-photorealistic visualization, virtual reality},
abstract = {Visual variables are important factors for visualization in 2D maps, and the study of non-photorealistic visualization of 3D city models in virtual environments using visual variables is also necessary to represent the attributes of city such as energy consumption. This study proposed a set of visual variable mapping model based on 3D geographical environment and conducted user cognitive test. The goal of this research is to verify the applicability of visual variables in 3D space and explore the impact of disturbing visual variables on user cognition. The results show that visualization of compound visual variables in 3D space is more effective than single variable mapping, and it makes up for the shortcomings of single mapping. It is effective to study the visualization effect in three-dimensional geographical environment using virtual reality technology. The results from this case study provide guidance for 3D user-centric visualization technology and make contribute to the development of smart city.}
}
@article{YEOM2024114165,
title = {Managing energy consumption and indoor environment quality using augmented reality based on the occupants’ satisfaction and characteristics},
journal = {Energy and Buildings},
volume = {311},
pages = {114165},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114165},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824002810},
author = {Seungkeun Yeom and Jongbaek An and Taehoon Hong and Choongwan Koo and Kwangbok Jeong and Jaewook Lee},
keywords = {Augmented reality, Energy consumption, Indoor environmental quality, Physiological response, Occupants’ characteristics},
abstract = {This study investigated the effects of visualizing energy consumption data of indoor environmental quality (IEQ) control devices (air cooler, humidifier, and light) through an augmented reality (AR) system. The research assessed changes in energy usage, IEQ, and subjective satisfaction. Physiological responses, such as heart rate variability (HRV), electroencephalogram (EEG), electrodermal activity (EDA), and skin temperature (SKT), were measured to quantitatively gauge subjective satisfaction. The study also explored variations in energy consumption, IEQ, satisfaction, and physiological responses based on user characteristics like age, gender, and body mass index (BMI). This study found that visualizing the energy consumption of IEQ control devices through AR significantly reduced their energy usage. The air cooler's energy consumption decreased by 19.36%, the humidifier's by 42.86%, and the light's by 27.57%. This led to changes in indoor temperature and illuminance, without affecting subjects' stress levels. The analysis based on subjects' characteristics revealed correlations, such as skin temperature impacting gender, humidifier energy affecting BMI, and visual sensation being linked to age. Consequently, the study suggests a hyper-personalized AR system, considering occupants' traits, to enhance thermal and visual comfort. This system is anticipated to play a crucial role in the future development of smart home energy management systems.}
}
@article{OKHOVVAT2024109508,
title = {Generating common spaces through virtual reality telepresence and shared scene synthesis},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109508},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109508},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010763},
author = {Maryam Okhovvat and Elham Andaroodi and Morteza Okhovvat},
keywords = {Generative model, Scene synthesis, Spatial computing, Telepresence, Virtual reality},
abstract = {This study aims to create a shared virtual space for remote telepresence, expanding the range of activities beyond traditional 2D screen-based communication. The challenge lies in seamlessly connecting the virtual and physical environments, enabling free movement and interaction. To address this, in the proposed approach advanced techniques in scene understanding, spatial mapping, and virtual environment generation are employed. Primary data is collected from each participant's space, analyzing crucial information like object placement, room layouts, and interactive elements. This forms the basis for generating a virtual scene that aligns with these features, creating a unified environment. The proposed approach ensures compatibility and meets participants' needs by utilizing a shared function optimization module. Additionally, it enhances the scene further through the use of deep-learning and conditional techniques. The created scene optimally supports shared functionalities for walking, sitting, and working, all of which mirror the physical objects present in the users' actual surroundings. Experiments using the MatterPort3D dataset evaluate the proposed approach, alongside a comparative user study. Results demonstrate the potential of the proposed approach in overcoming challenges of remote telepresence, enabling immersive and interactive experiences in the virtual space. In conclusion, this research tackles the problem of creating a shared virtual space for remote telepresence. Analyzing input data and employing advanced techniques generates a coherent virtual scene.}
}
@article{ARIANSYAH202419,
title = {Augmented reality training for improved learnability},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {48},
pages = {19-27},
year = {2024},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1755581723001554},
author = {Dedy Ariansyah and Bens Pardamean and Eddine Barbaro and John Ahmet Erkoyuncu},
keywords = {Augmented Reality, Learnability, Training, Industry 4.0, Industry 5.0},
abstract = {In the current era of Industry 4.0, many new technologies offer manufacturing industries to achieve high productivity. Augmented Reality (AR) is one of the emerging technologies that has been adopted in industries to aid users in acquiring complex skills and carrying out many complicated tasks such product assembly and maintenance. Nevertheless, most AR applications have been developed without clear understanding of how such technology can facilitate improved learnability in terms of knowledge reusability. This paper proposed an enhanced AR-based training system that provides multimodal information with a contextualized information to improve task comprehension and knowledge reusability compared with traditional AR that presents unimodal and decontextualized information. An empirical test was carried out to assess the task performance and the task learnability aspects of this enhanced AR compared to the traditional AR and the paper-based document. The experiment consisted of a training phase where participants carried out an electrical connection task of a sensor followed by a knowledge reuse phase where participants had to wire a second sensor using their previous training. A pre-test quiz was given before the experiment followed by the post-tests phase after the training. Post-tests consist of one post-test given directly after the experiment (short-term retention test) and a second post-test quiz given one week later (long-term retention test) to measure information retention. The results indicated that AR-based approaches could enhance knowledge acquisition by around 18 % for traditional AR and almost 25 % for enhanced AR as compared to paper-based approach. While all training systems achieved relatively equivalent well for short-term retention test, trainees who used the enhanced AR training systems statistically outperformed those in the paper-based group for long term retention test. Furthermore, there was a positive correlation between the score of short-term retention test and the score in the knowledge reusability which was also shown by the higher scores in knowledge reusability for the enhanced AR training system compared to the other two approaches. These findings are discussed in relation to the Industry 5.0′s human centric core value.}
}
@article{PARK2020101887,
title = {Deep learning-based smart task assistance in wearable augmented reality},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {63},
pages = {101887},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101887},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518306240},
author = {Kyeong-Beom Park and Minseok Kim and Sung Ho Choi and Jae Yeol Lee},
keywords = {Smart task assistance, Wearable augmented reality, Deep learning-based task assistance, AR smart glasses},
abstract = {Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.}
}
@article{LI2022112780,
title = {Virtual-reality-based online simulator design with a virtual simulation system for the docking of unmanned underwater vehicle},
journal = {Ocean Engineering},
volume = {266},
pages = {112780},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112780},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822020637},
author = {Hongjuan Li and Fanghao Huang and Zheng Chen},
keywords = {Virtual reality, Underwater docking navigation, Online simulator, Fuzzy PID controller, Unmanned underwater vehicle},
abstract = {The simulation of the docking process is an important solution for the control and navigation design of Unmanned Underwater Vehicle (UUV), which can be used to evaluate the reliability of the algorithm in an economic way. However, most of the existing simulations are offline, which cannot interact with the real sensors mounted on the UUV, and cannot visually simulate and display the immersive motion of the UUV in the underwater environment. Therefore, to address the aforementioned problem, an online simulator is designed in this paper, which provides a virtual simulation system for the docking task simulation of the UUV. Two modes are designed with one to provide the virtual simulation with simulated UUV model and sensor, while another to provide the real platform simulation with intuitive visualization of the real tasks achieved by the recorded data. The host and lower computers are designed to communicate and exchange the data. Namely, the lower computer can calculate the mathematical model, and simulate or interact with the real sensor during the docking process; The host computer can obtain the simulated or recorded data from the lower computer, and implement the navigation and control of the UUV, which can be finally displayed in the accurately created virtual docking scene. The fuzzy PID controller and four different navigation modes are designed to implement the experiment with the specific motion verification and underwater docking for the UUV, and the results verify the effectiveness of the online simulator for the underwater tasks simulation with lower cost.}
}
@article{KUTEJ201913,
title = {Fostering Additive Manufacturing of Special Parts with Augmented-Reality On-Site Visualization},
journal = {Procedia Manufacturing},
volume = {39},
pages = {13-21},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.223},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920302675},
author = {Dominik Kutej and Wolfgang Vorraber},
keywords = {Information Systems, Augmented Reality, Additive Manufacturing, Service Design},
abstract = {This paper presents the engineering and evaluation of a novel use case in the area of production that seizes the potentials of two disruptive technologies – additive manufacturing (AM) and augmented reality (AR). A smartphone-based AR application was developed to support the workflow of producing special components based on AM. The developed app fosters the evaluation of individually engineered parts based on virtual three-dimensional (3D)-computer-aided design (CAD) models projected to the intended installation-site. This supports the detection of problems caused by inaccessible raw measurements or difficult to anticipate movements of the produced parts. The designed app was tested in a real-world industrial setting. In total, six production engineers conducted 12 test runs. The tests were video recorded and complemented by questionnaires. Both data sources served as a basis for the conducted analysis aiming to evaluate this case. This paper shares key insights gained by the evaluation of the implemented application in a real-world industrial setting and provides evidence of how disruptive new technologies may be used to support shop floor production processes.}
}
@article{ALARCON2020191,
title = {Augmented Reality for the enhancement of space product assurance and safety},
journal = {Acta Astronautica},
volume = {168},
pages = {191-199},
year = {2020},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2019.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0094576519313268},
author = {Raul Alarcon and Fridolin Wild and Christine Perey and Marc Marin Genescà and Josefa Gavaldà Martínez and Josep Xavier {Ruiz Martí} and Maria José {Simon Olmos} and Diana Dubert},
keywords = {Augmented reality, Product assurance, Industrial revolution},
abstract = {A growing number of companies in the aerospace industry are already leading projects to deploy Augmented Reality (AR) to improve their workplace performance, knowledge transfer, as well as workforce productivity. In parallel, national and international agencies in aerospace, such as the European Space Agency (ESA), are running studies to evaluate the application of AR to enhance the quality and cost effectiveness of space missions. We hereby present the results of a study performed by ESA to assess the maturity and potential business value of AR for application to space product assurance and safety activities. For this purpose, we conduct an on-line survey and interviews with product assurance and safety professionals. Moreover, we provide a detailed review of space industry use case requirements and readiness levels of potentially involved technology components. Findings indicate that maturity of many components enabling AR may not fully satisfy the space industry's requirements, while, at the same time, there is great potential for impact and long-term benefits from AR introduction.}
}
@article{AWADALLAH2023100024,
title = {Automated multiclass structural damage detection and quantification using augmented reality},
journal = {Journal of Infrastructure Intelligence and Resilience},
volume = {2},
number = {1},
pages = {100024},
year = {2023},
issn = {2772-9915},
doi = {https://doi.org/10.1016/j.iintel.2022.100024},
url = {https://www.sciencedirect.com/science/article/pii/S277299152200024X},
author = {Omar Awadallah and Ayan Sadhu},
keywords = {Augmented reality, Damage classification, Damage quantification, Deep learning, Multiclass identification, Structural health monitoring},
abstract = {Civil infrastructure worldwide is ageing and enduring increasingly adverse weather conditions. Traditional structural health monitoring (SHM) involves the expensive and time-consuming installation of contact sensors. For example, inspectors use costly large-scale equipment to reach a certain area of the structure and at different heights to inspect it, which can pose a risk to the inspector's safety. Moreover, the inspectors rely only on the batch data acquired during the inspection period, which are analyzed by engineers at a later time due to the limited availability of a real-time visualization approach for structural inspection within the traditional mode of SHM. To address these timely challenges, an Augmented Reality (AR)-based automated multiclass damage identification and quantification methodology is proposed in this paper. The interactive visualization framework of AR is integrated with the autonomous decision-making of Artificial Intelligence (AI) in a unified fashion to incorporate human-sensor interaction. The proposed system uses an AI model that is trained and optimized using the YOLOv5 architecture to detect and classify four different types of anomalies/damages (i.e., cracks, spalls, pittings, and joints). The AI model is then updated to quantify the length, area, and perimeter of any damage using segmentation to further assess its severity. Once the model is developed, the model is embedded with the AR device and tested through its interactive environment for SHM of various structures. The paper concludes that the proposed approach successfully classifies four types of damage with an accuracy of more than 90% for up to 2 ​m, and it also quantifies the length, area, and perimeter with less than 2% of error.}
}
@article{ERGAN2022101476,
title = {Developing an integrated platform to enable hardware-in-the-loop for synchronous VR, traffic simulation and sensor interactions},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101476},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101476},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002263},
author = {Semiha Ergan and Zhengbo Zou and Suzana Duran Bernardes and Fan Zuo and Kaan Ozbay},
keywords = {Hardware in the loop, Traffic simulation, Work Zone safety, Virtual reality, Sensing},
abstract = {Hardware-in-the-loop (HIL) is a testing paradigm where physical sensors (e.g., monitoring sensors such as cameras and proximity sensors, and alarm sensors such as screaming traffic cones) are connected to a virtual test system that simulates reality (e.g., virtual work zone with simulated dangerous situations). This paradigm is well-suited for conducting user studies for work zone safety because virtual test systems can be implemented using virtual reality (VR), which allows for safe and realistic testing of a sensing system without putting workers in danger along with avoiding high upfront costs needed to generate physical research testbeds. However, when recreating physical work zones in VR, researchers face various challenges while representing traffic patterns in VR, such as the lack of bi-directional communication between traffic simulation platforms and user behaviors in VR, hardware compatibility and integration issues, and customization inflexibility during implementation. Researchers, who need to develop such platforms for research studies that involve high-risk exposure to participants, are in need of evaluating the options available for bringing together the components of such platforms. This study provides an overview of alternative ways to integrate components of platforms that enable hardware-in-the-loop for synchronous VR, traffic simulation, and sensor interactions to position researchers to make decisions based on the pros and cons of each alternative. This paper also presents the implementation of such an integrated platform that allows a two-way interface between traffic simulation and VR environments for work zone safety analysis. Outcomes of this work will lay out the steps in implementing the integrated and immersive platform to be used in work zone safety studies based on the guidance presented.}
}
@article{DARWISH2023102104,
title = {Extended reality for enhancing spatial ability in architecture design education},
journal = {Ain Shams Engineering Journal},
volume = {14},
number = {6},
pages = {102104},
year = {2023},
note = {Smart cites: Challenges, Opportunities, and Potential},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.102104},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922004154},
author = {MOHAMED DARWISH and SHAIMAA KAMEL and AYMAN ASSEM},
keywords = {Design Education, Architecture Design, Extended Reality, Spatial Ability, Virtual Reality, Augmented Reality},
abstract = {The use of Extended Reality (XR) has evolved to engage in architecture design and education. XR allows the students to experience their design project in real scale. Educators confirmed that spatial ability is central to design studio tasks. They also confirmed that spatial ability can be developed using XR. The aim of the study is to present the results of an experiment for implementing XR technology in the early design studio in architectural education and its impact on students’ general spatial ability levels. The method used was a baseline and post-project general spatial ability test to examine the correlation between the use of XR and the scores of the participants. The study found that the mean scores for the control group did not change significantly before and after the experiment. Whereas using XR technology significantly increased the scores of the students compared to the control group.}
}
@article{ARPAIA2022111188,
title = {Performance enhancement of wearable instrumentation for AR-based SSVEP BCI},
journal = {Measurement},
volume = {196},
pages = {111188},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111188},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122004420},
author = {Pasquale Arpaia and Egidio {De Benedetto} and Lucio {De Paolis} and Giovanni D’Errico and Nicola Donato and Luigi Duraccio},
keywords = {Augmented reality, Brain–computer interface, BCI, Electroencephalography, EEG, Industry 4.0, SSVEP, Real-time systems, Wearable systems, Instruments},
abstract = {This work addresses an innovative processing strategy to improve the classification of Steady-State Visually Evoked Potentials (SSVEPs). This strategy resorts to the combined use of fast Fourier transform and Canonical Correlation Analysis in time domain, and manages to outperform by over 5% previous results obtained for highly wearable, single-channel Brain–Computer Interfaces. In fact, a classification accuracy of 90% is reached with only 2-s time response. Then, the proposed algorithm is employed for an experimental characterization of three different Augmented Reality (AR) devices (namely, Microsoft Hololens I, Epson Moverio BT-350, and Oculus Rift S). These devices are used to generate the flickering stimuli necessary to the SSVEP induction. Also, in the three pieces of instrumentation under test, the number of simultaneous visual stimuli was increased with respect to the state-of-art solutions. The aim of the experimental characterization was to evaluate the influence of different AR technologies on the elicitation of user’s SSVEPs. Classification accuracy, time response, and information transfer rate were used as figures of merit on nine volunteers for each piece of instrumentation. Experimental results show that choosing an adequate AR headset is crucial for obtaining satisfying performance: in fact, it can be observed that the classification accuracy obtained with Microsoft Hololens is about 20% greater than Epson Moverio one.}
}
@article{TADEJA2023134,
title = {Exploring the repair process of a 3D printer using augmented reality-based guidance},
journal = {Computers & Graphics},
volume = {117},
pages = {134-144},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002546},
author = {Sławomir K. Tadeja and Luca O. {Solari Bozzi} and Kerr D.G. Samson and Sebastian W. Pattinson and Thomas Bohné},
keywords = {Augmented reality, AR, AR-guided repair, Immersive interface, 3D printing, 3D printer repair, Right to repair},
abstract = {In recent years, additive manufacturing (AM) techniques have transcended their typical rapid prototyping role and become viable methods to directly manufacture end products in a highly versatile manner. Due to its low cost and relative ease of use, fused deposition modeling (FDM) has become the most universally applied AM technology. Nonetheless, skilled operators are often still required to perform maintenance, diagnostic, and repair tasks. Such operators need to be adequately trained. Here, Augmented reality (AR) technology could be used to automate this training and help to promptly provide new operators with the necessary skills to perform specific tasks as required. However, the most effective approach to designing such AR-based assistance systems has not yet been fully explored. Consequently, we address this need by reporting on how to design such guiding systems using well-known design engineering methodologies. We then further assess the applicability of our approach through a user study with domain experts. In addition, we complete our assessment with heuristical verification of system expressiveness to reason about the influence of cognitively important components of the AR interface on the operators.}
}
@incollection{SCHUBERT2009333,
title = {Development and Experimental Verification of Model-Based Process Control using Mixed-Reality Environments},
editor = {Jacek Jeżowski and Jan Thullie},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {26},
pages = {333-337},
year = {2009},
booktitle = {19th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70056-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609700562},
author = {Udo Schubert and Harvey Arellano-Garcia and Günter Wozny},
keywords = {advanced process control, model predictive control, estimation, uncertainty},
abstract = {In this paper, an extension of the PARSEX reactor introduced by Kershenbaum & Kittisupakorn, 1994 is presented. The proposed setup improves the capabilities of the original idea and constitutes a mixed-reality environment by replacing the reactor with a virtual counterpart. Furthermore, scenarios for the application of nonlinear model predictive control under the explicitly consideration of uncertainty on the mixed-reality plant are given. Simulation studies and experiments are used to validate the characteristic dynamic behavior in order to provide challenging aspects for control and estimation algorithms.}
}
@incollection{PREIM2014625,
title = {Chapter 18 - Image-Guided Surgery and Augmented Reality},
editor = {Bernhard Preim and Charl Botha},
booktitle = {Visual Computing for Medicine (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {625-663},
year = {2014},
isbn = {978-0-12-415873-3},
doi = {https://doi.org/10.1016/B978-0-12-415873-3.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158733000183},
author = {Bernhard Preim and Charl Botha},
keywords = {Intraoperative registration, Calibration, Tracking, Intraoperative visualization, Head-mounted display, Augmented microscope, Augmented endoscope, Projection-based augmented reality, Workflow analysis, Validation},
abstract = {Surgeons and interventional radiologists benefit from intraoperative guidance to deliver the treatment precisely, as it was planned. Printouts from a planning system are often of limited value, since they are hard to translate to the intraoperative situation. This chapter focuses on techniques that are used during an intervention. Hence, they are called intraoperative techniques. In intraoperative visualization, a lot of research is motivated by the challenges of minimally-invasive surgery and interventions, such as needle insertion, laparoscopic and endoscopic interventions. In particular, we introduce augmented reality (AR) techniques where relevant information from preoperative images is superimposed with live images from surgery. Metaphorically, many people refer to this technology as X-ray vision, i.e., the surgeon is able to see through the skin or an organ and observes the operative site before he or she actually arrives there. Such technologies were introduced in the 1990s in industrial applications, such as the assembly and maintenance of aircrafts and cars. Workers were supported by placing blueprints onto surfaces. In a similar way, planned resection lines, surgical targets or risk structures may be displayed along with organ surfaces, the skull or other anatomical structures. Thus, the target regions, e.g., a deep-seated tumor, may be reached faster with reduced risks of harming critical structures. Thus, implants in total knee replacement, hip or shoulder replacement may be fitted more accurately and, hopefully, the rate of revision surgery is reduced. In very difficult cases, e.g., a large deep-seated tumor close to vital structures, navigation and intraoperative visualization may even shift the border of operability. Major application areas are ENT and neurosurgery as well as orthopedics. Intraoperative visualization requires an appropriate dataset of the patient, preprocessing, e.g., segmentations, a sufficiently accurate registration procedure that maps the patient’s dataset to the patient himself. Once this registration is obtained and carefully validated, visual output from the dataset and surgical instruments can be mapped to the patient. These instruments need to be tracked by a navigation system. Thus, surgeons can locate instruments on the patient (with their eyes) and in the dataset (through the registration mapping). This setup with a direct correlation between the patient’s dataset and the patient is referred to as image-guided surgery (IGS). We also want to make the reader aware of potential and real drawbacks which are due to a complex processing chain that may involve significant errors. Moreover, the use of the underlying technology is challenging and the necessary setup times are often significant.}
}
@article{BEUTHEL2001185,
title = {Maintenance Support Through Wearable Computer and Augmented Reality Technology},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {9},
pages = {185-190},
year = {2001},
note = {IFAC Conference on Telematics Applications in Automation and Robotics, Weingarten, Germany, 24-26 July 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41703-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017417034},
author = {Carsten Beuthel and Pia Jonsson},
keywords = {Hand held mobile computers, augmented reality, service and maintenance},
abstract = {Computers nowadays influence the daily work in nearly all businesses. The service and maintenance of machines and facilities is still an area where only in very special cases computers are used but this is about to change dramatically. This paper will describe the current status of wearable computers usable for this kind of applications and will present two interesting approaches tested at out lab including interaction methods like speech input and output. One application is dealing with the inspection of machinery the other one demonstrates how augmented reality could be used for maintenance by the inspection of a drive. The discussion will present an outlook on what can be expected in the future to improve service and maintenance.}
}